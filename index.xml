<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>TzJ&#39;s Net</title>
    <link>https://tzj2006.github.io/</link>
    <description>Recent content on TzJ&#39;s Net</description>
    <generator>Hugo -- 0.142.0</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Feb 2026 00:00:00 -0500</lastBuildDate>
    <atom:link href="https://tzj2006.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Command Dictionary</title>
      <link>https://tzj2006.github.io/bugjournal/commanddictionary/</link>
      <pubDate>Fri, 11 Nov 2011 11:11:11 +1111</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/commanddictionary/</guid>
      <description>All the usefull commands</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-17</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-17/</link>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-17/</guid>
      <description>在天河服务器上全面推进Error Recovery Benchmark项目（M5非冲量场景生成达454个、M6多策略评估调试、VLA服务器修复）、为Motion-based Self-Reflection Framework编写综合文档，并修复ccusage claude-opus-4-6零成本计费bug及对全部历史日期重新导出。</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-16</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-16/</link>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-16/</guid>
      <description>在MIHD空间组学项目中完善了VLA研究报告、实现4种新融合策略并完成基准测试；在机器人错误恢复基准项目中完成了策略错误检测分类系统v4.3、VLA Policy Server集成修复、50次自然错误捕获rollout（场景总量达271个），同时扩展了日报工具的会话摘要功能</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-15</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-15/</link>
      <pubDate>Sun, 15 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-15/</guid>
      <description>在 MacBook/TzJsDesktop 完成历史日报发布到 Hugo/GitHub Pages 及 daily_summary.py 多项核心增强（原子写入、LLM 结构化输出、多设备 rclone 同步、export 幂等跳过），在 DCC 集群推进 MIHD benchmark（Q-Former/scGPT 评估），在天河集群完成 Error Recovery Benchmark 的中性动作修复、批量可视化、策略驱动场景生成框架及多类型错误检测器扩展，并研究了 VLA 技术在空间组学中的应用方向</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-14</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-14/</link>
      <pubDate>Sat, 14 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-14/</guid>
      <description>在DCC高性能集群上推进MIHD多模态空间转录组学benchmark（完成两阶段Pipeline架构实现）、修复ErrorRecoveryBenchmark中力注入机制和pre_grasp检测器的多个根本性bug、为CalendarPro Discord Bot新增批量删除功能并修复嵌套Claude环境问题，以及为gadget/summarize工具完整实现配置文件、rclone云盘同步和Claude CLI后端支持。</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-13</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-13/</link>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-13/</guid>
      <description>今日工作横跨四个项目：MIHD多模态基准测试框架完成7个Phase增强与286个实验自动化调度；CalendarPro完成代码整理、P0稳定性修复、P1智能功能（学习采集、能量个性化、随机想法系统）、循环日程与定时自检；gadget日报工具重构为多设备两阶段架构并推送GitHub；error_recovery_benchmark修复demo回放环境配置不匹配根本bug、cvel角速度读反误触发、移植3个新注入器并成功生成30个有效错误场景。</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-12</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-12/</link>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-12/</guid>
      <description>在DCC集群上为MIHD空间转录组多模态融合框架制定并开始实施大规模增强计划，涵盖嵌入归一化、Q-Former/LLaVA MLP融合、QueST风格的niche查询与批次效应校正、以及全局超参数配置化。</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-09</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-09/</link>
      <pubDate>Mon, 09 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-09/</guid>
      <description>在MIHD项目中为聚类可视化添加ARI/NMI指标叠加，并运行所有DLPFC切片的RM-Ideal基准测试，完成多种embedding方法的系统性评估。</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-08</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-08/</link>
      <pubDate>Sun, 08 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-08/</guid>
      <description>在HPC服务器上为QueST和MIHD项目实现了RM-Ideal（Region Matching Ideal）评分功能，从零开始移植WWL图核算法，并集成到两个独立的代码库中</description>
    </item>
    <item>
      <title>Bug Journal 2026-02-07</title>
      <link>https://tzj2006.github.io/bugjournal/2026-02-07/</link>
      <pubDate>Sat, 07 Feb 2026 00:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-02-07/</guid>
      <description>在DCC服务器上同时推进两个生信项目：为QueST项目创建CLAUDE.md并调研RM-Ideal评分指标，以及修复MIHD基准框架中STAIG严格对齐模式的四个关键差异，最终将151673切片ARI从~0.09提升至0.54。</description>
    </item>
    <item>
      <title>Bug Journal 2026-01-09</title>
      <link>https://tzj2006.github.io/bugjournal/2026-01-09/</link>
      <pubDate>Fri, 09 Jan 2026 15:00:00 -0400</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2026-01-09/</guid>
      <description>&lt;p&gt;今天开始测试 recovery benchmark&lt;/p&gt;
&lt;p&gt;和Gemini讨论了一下，Gemini建议我从issac环境开始，先做一个demo出来&lt;/p&gt;
&lt;p&gt;所以就开始配置issac环境&lt;/p&gt;
&lt;p&gt;第一个碰倒到的bug居然是网络问题&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1767989278653&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2026-01-09/1767989278653.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;问题如下，这个问题的报错是 &lt;code&gt;No route to host&lt;/code&gt;, 这意味着无法连接到代理服务器&lt;/p&gt;
&lt;p&gt;用人话说就是计算节点是没法联网的&lt;/p&gt;
&lt;p&gt;要回到登录节点才行&lt;/p&gt;
&lt;p&gt;然后回退就work了（（&lt;/p&gt;
&lt;p&gt;今天做的第二件事就是配置Claude Code + GLM的配置&lt;/p&gt;
&lt;p&gt;为什么要用GLM呢？因为GLM一个月￥20, 但是用量可以到达100M tokens&lt;/p&gt;
&lt;p&gt;据说比Github Copilot 更加聪明&lt;/p&gt;
&lt;p&gt;所以我想试试&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.bigmodel.cn/cn/coding-plan/tool/claude&#34;&gt;具体的链接在这里&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;但是呢，在加入VS code的时候遇到了方法一失败的问题，这时候可以使用方法二，即可解决。&lt;/p&gt;
&lt;p&gt;这个有点NB，这样在所有的服务器上都可以用claude code帮我解决问题了&lt;/p&gt;
&lt;p&gt;这几天试试看看好不好用&lt;/p&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-12-05</title>
      <link>https://tzj2006.github.io/leetcode/2025-12-05/</link>
      <pubDate>Sat, 06 Dec 2025 17:15:36 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-12-05/</guid>
      <description>Solution to 3432. Count Partitions with Even Sum Difference</description>
    </item>
    <item>
      <title>LeetCode Question P.312 Burst Balloons</title>
      <link>https://tzj2006.github.io/leetcode/p312_burst_balloons/</link>
      <pubDate>Sat, 06 Sep 2025 15:59:08 -0400</pubDate>
      <guid>https://tzj2006.github.io/leetcode/p312_burst_balloons/</guid>
      <description>Solution to P.312</description>
    </item>
    <item>
      <title>Bug Journal 2025-08-29</title>
      <link>https://tzj2006.github.io/bugjournal/2025-08-29/</link>
      <pubDate>Fri, 29 Aug 2025 21:09:52 -0400</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-08-29/</guid>
      <description>&lt;h3 id=&#34;fast-efficient-action-tokenization-for-vision-language-action-models&#34;&gt;FAST: Efficient Action Tokenization for Vision-Language-Action Models&lt;/h3&gt;
&lt;p&gt;RSS 2025 By Physical Intelligence&lt;/p&gt;
&lt;p&gt;当前方法局限：都是离散数据，没有办法获取高频机器人控制数据的时间相关性&lt;/p&gt;
&lt;p&gt;因此使用一种名字叫做DCT的压缩算法，可以把连续信号转移到频率域(一种离散域)&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1756516611535&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-08-29/1756516611535.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;(话虽如此，但是真的需要用到这么高的频率吗？)&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1756516932637&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-08-29/1756516932637.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt; ：传统的朴素分箱方案在处理高频连续动作时，由于动作变化小，会导致许多连续时间步的动作被离散化为相同或高度相似的标记。这意味着每个新标记带来的“边际信息”（即在已知之前标记的情况下，新标记所包含的额外信息）非常低。&lt;/li&gt;
&lt;li&gt;朴素分箱方案独立地处理每个时间步和每个维度，完全忽略了连续动作序列中固有的时间相关性（例如，机器手臂的移动通常是平滑且连续的）。这使得模型难以学习到动作的动态模式。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unified-world-models-coupling-video-and-action-diffusion-for-pretraining-on-large-robotic-datasets&#34;&gt;Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets&lt;/h3&gt;
&lt;p&gt;UW + Toyota&lt;/p&gt;
&lt;p&gt;读前问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这个模型有什么优势？&lt;/li&gt;
&lt;li&gt;这个模型是如何实现“视频无标签学习&amp;quot;的？&lt;/li&gt;
&lt;li&gt;什么叫&amp;quot;naturally facilitate video learning&amp;quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;觉得有意思的点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Imitation learning do not explicitly capture temporal dynamics that are naturally present in demonstration trajectories or videos (Page 1, Introduction)&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;一个 Diffusion Model：&lt;/strong&gt; UWM 使用一个单一的 Diffusion Transformer 模型来处理所有的输入和输出。它不是为每种任务（策略、正向动力学、逆向动力学、视频预测）训练一个单独的模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokenization 和输入：&lt;/strong&gt; 就像您说的，无论是当前的观测（图片）、动作、还是未来的观测，它们都会被“tokennize”成模型能够理解的表示形式（例如，图片通过编码器转换成图像特征，动作也被编码）。这些 token 以及动作和未来观测的独立扩散时间步长 &lt;code&gt;t_a&lt;/code&gt; 和 &lt;code&gt;t_o&#39;&lt;/code&gt; 会一起输入到 Diffusion Transformer 中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;灵活的输出：&lt;/strong&gt; 通过巧妙地控制这些扩散时间步长 &lt;code&gt;t_a&lt;/code&gt; 和 &lt;code&gt;t_o&#39;&lt;/code&gt;，UWM 可以在推理时灵活地得到您想要的结果。例如：
&lt;ul&gt;
&lt;li&gt;如果想得到 &lt;strong&gt;策略（Policy）&lt;/strong&gt; ，就将未来观测的时间步长 &lt;code&gt;t_o&#39;&lt;/code&gt; 设置为最大值 T（表示未来观测完全被噪声掩盖，模型只关注动作），然后模型会输出动作。&lt;/li&gt;
&lt;li&gt;如果想得到 &lt;strong&gt;正向动力学（Forward Dynamics）&lt;/strong&gt; ，就将动作时间步长 &lt;code&gt;t_a&lt;/code&gt; 设置为 0（表示给定明确的动作输入），然后模型会输出未来观测。&lt;/li&gt;
&lt;li&gt;如果想得到 &lt;strong&gt;逆向动力学（Inverse Dynamics）&lt;/strong&gt; ，就将未来观测时间步长 &lt;code&gt;t_o&#39;&lt;/code&gt; 设置为 0（表示给定明确的未来观测输入），然后模型会输出动作。&lt;/li&gt;
&lt;li&gt;如果想得到 &lt;strong&gt;视频预测（Video Prediction）&lt;/strong&gt; ，就将动作时间步长 &lt;code&gt;t_a&lt;/code&gt; 设置为最大值 T（表示动作完全被噪声掩盖，模型只关注视频），然后模型会输出未来观测。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;模态独立的扩散时间步长 (Modality-Specific Diffusion Timesteps)：&lt;/strong&gt; 这是 UWM 最核心的设计之一。传统的多模态扩散模型可能使用一个共享的时间步长来对所有模态进行加噪和去噪。然而，UWM 引入了两个独立的扩散时间步长：&lt;code&gt;t_a&lt;/code&gt; 用于动作 (actions)，&lt;code&gt;t_o&#39;&lt;/code&gt; 用于未来观测 (future observations)。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;原因：&lt;/strong&gt; 这种分离使得模型能够独立控制每种模态的噪声水平。通过在推理时灵活地设置这些时间步长（例如，将某个时间步长设为 T 表示完全加噪以“掩蔽”该模态，设为 0 表示完全去噪以“条件化”该模态），UWM 可以实现前文提到的策略、正向动力学、逆向动力学和视频预测等多种推理模式。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果：&lt;/strong&gt; 它使得一个单一的模型能够充当多种角色，极大地提高了模型的通用性和灵活性，同时增强了模型对模态间因果关系的理解。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;统一的 Transformer 架构 (Unified Transformer Architecture)：&lt;/strong&gt; UWM 采用单一的 Transformer 骨干网络来处理所有模态的输入（当前观测、动作、未来观测）并预测其噪声。Transformer 以编码后的图像特征、动作 token 以及独立的扩散时间步长作为输入。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;原因：&lt;/strong&gt; 这种统一的架构促进了不同模态之间的特征共享。在 Transformer 内部，动作和图像信息可以相互交流和融合，从而使模型能够学习到更丰富、更全面的表征，并捕获它们之间的潜在因果关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果：&lt;/strong&gt; 避免了为每个任务设计独立模型的复杂性，简化了模型结构，并可能提高学习效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;寄存器令牌 (Register Tokens)：&lt;/strong&gt; UWM 在 Transformer 的输入序列中加入了随机初始化的“寄存器令牌”。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;原因：&lt;/strong&gt; 经验性研究发现，这些冗余令牌有助于提高模型性能，尤其是在多模态数据上。研究人员推测，动作和潜在图像块是不同的模态，寄存器可以作为一个中间媒介，存储来自任一模态的信息，并在后续 Transformer 层中被检索，从而促进模态间的信息交换和更好的多模态特征共享。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果：&lt;/strong&gt; 尽管这些令牌最终会被丢弃，但它们在中间层起到了重要的信息桥梁作用，提升了模型的表现力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adaptive Layer Normalization (AdaLN) 条件化：&lt;/strong&gt; UWM 使用 AdaLN [33] 来将扩散时间步长（以及图像特征）条件化到 Transformer 的每个块中。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;原因：&lt;/strong&gt; AdaLN 是一种有效的条件化机制，它通过调整层归一化 (Layer Normalization) 的参数（缩放和偏移）来注入条件信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果：&lt;/strong&gt; 这种方式允许模型以一种非侵入性且高效的方式将时间步长信息融入到 Transformer 的处理流程中，有助于模型更好地理解噪声水平和生成过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;潜在扩散范式 (Latent Diffusion Paradigm)：&lt;/strong&gt; 对于图像扩散，UWM 采用了潜在扩散范式。它使用一个冻结的预训练 VAE (Variational AutoEncoder)（例如来自 Stable Diffusion XL [34]）将原始高分辨率图像压缩到低维的潜在空间中进行加噪和去噪，最后再解码回全尺寸图像。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;原因：&lt;/strong&gt; 直接在高分辨率像素空间进行扩散通常计算成本高昂且效率低下。潜在扩散可以在更小的潜在空间中操作，显著减少计算量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果：&lt;/strong&gt; 提高了图像处理的效率，同时保持了高质量的图像生成能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些特殊设计共同使得 UWM 能够有效地应对多模态机器人学习的挑战，实现了一个能够从大规模异构数据中学习并灵活执行多种推理任务的强大模型。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-08-25</title>
      <link>https://tzj2006.github.io/bugjournal/2025-08-25/</link>
      <pubDate>Mon, 25 Aug 2025 22:37:11 -0400</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-08-25/</guid>
      <description>SOTA VLA</description>
    </item>
    <item>
      <title>Bug Journal 2025-08-08</title>
      <link>https://tzj2006.github.io/bugjournal/2025-08-08/</link>
      <pubDate>Fri, 08 Aug 2025 19:00:30 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-08-08/</guid>
      <description>&lt;p&gt;Hi, 你好，我的读者。&lt;/p&gt;
&lt;p&gt;好久不见了，有没有想我呀(&lt;em&gt;╹▽╹&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;上次见面的时候还是7月16号，已经隔了快一个月了呢&lt;/p&gt;
&lt;p&gt;这个月其实也发生了许多事情，但最后我发现，原来，有时候就是遵从自己的内心也是煎熬的。&lt;/p&gt;
&lt;p&gt;我现在开始相信本我、自我和超我了，本我就是最原本的“内心&amp;quot;，而超我则是最原本的”幻想&amp;quot;&lt;/p&gt;
&lt;p&gt;他们两个就像是两个截然不同的 loss function 一样，不断对抗着，希望我朝着某一个方向前进&lt;/p&gt;
&lt;p&gt;而这个 loss function 的正则项，&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-07-16</title>
      <link>https://tzj2006.github.io/bugjournal/2025-07-16/</link>
      <pubDate>Wed, 16 Jul 2025 11:11:34 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-07-16/</guid>
      <description>&lt;p&gt;现在遇到了如图所示的这个问题：&lt;/p&gt;
&lt;p&gt;“现阶段的VLA本质上就是数据量不足，就像你说VLA让他去开车，根本不可能，神经网络没有见过这样的数据就是不理解，现阶段还是要搞数据工程，逼近scaling law”&lt;/p&gt;
&lt;p&gt;“确实，只要数据够多就没有out of distribution了！”&lt;/p&gt;
&lt;p&gt;而对于 lifelong task 来说，这种情况更是一个问题：&lt;/p&gt;
&lt;p&gt;“为什么不直接加数据训练？而是要通过 lifelong 持续的学习”&lt;/p&gt;
&lt;p&gt;记录一下我现在的想法&lt;/p&gt;
&lt;p&gt;首先是持续学习&lt;/p&gt;
&lt;p&gt;这个有什么好处呢，好处就是，这样的模型可以不用在最开始的时候就学会所有东西&lt;/p&gt;
&lt;p&gt;而是可以等到之前的东西学完之后然后再继续学下一个 task&lt;/p&gt;
&lt;p&gt;什么意思呢，就是说，我之前学习到了一个 distribution 下的所有的内容&lt;/p&gt;
&lt;p&gt;我让机械臂学会了这些知识&lt;/p&gt;
&lt;p&gt;但是呢，我现在遇到了一些 OOD (Out of distribution) 的 task&lt;/p&gt;
&lt;p&gt;这种情况怎么办呢？ 现在的 model 就直接束手无策，束手就擒了&lt;/p&gt;
&lt;p&gt;但是 lifelong learning 的 task 就可以在这种情况下，在不遗忘之前学习的 task 的情况下学习到 OOD 的 task&lt;/p&gt;
&lt;p&gt;这样这种模型的拓展能力就会更好&lt;/p&gt;
&lt;p&gt;可是。。。&lt;/p&gt;
&lt;p&gt;如前文所说，只要数据够多就没有out of distribution了&lt;/p&gt;
&lt;p&gt;而在 lifelong setting 的情境下，本来就要有数据训练，那为什么不直接 finetune 原本的模型呢&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-07-11</title>
      <link>https://tzj2006.github.io/bugjournal/2025-07-11/</link>
      <pubDate>Fri, 11 Jul 2025 11:33:41 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-07-11/</guid>
      <description>LoRA tutorial</description>
    </item>
    <item>
      <title>Bug Journal 2025-07-09</title>
      <link>https://tzj2006.github.io/bugjournal/2025-07-09/</link>
      <pubDate>Thu, 10 Jul 2025 10:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-07-09/</guid>
      <description>&lt;h2 id=&#34;huggingface-dataset-使用说明&#34;&gt;Huggingface Dataset 使用说明&lt;/h2&gt;
&lt;h3 id=&#34;load_dataset-函数&#34;&gt;load_dataset 函数&lt;/h3&gt;
&lt;p&gt;详情请见：&lt;a href=&#34;https://huggingface.co/docs/datasets/v4.0.0/loading#slice-splits&#34;&gt;Hugging Face 页面&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;split参数&#34;&gt;Split参数&lt;/h4&gt;
&lt;p&gt;Hugging Face 的 &lt;code&gt;load_dataset(..., split=...)&lt;/code&gt; 参数非常强大，下面是详细说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;split=&amp;quot;train&amp;quot;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;加载整个训练集。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;split=[&amp;quot;train&amp;quot;,&amp;quot;test&amp;quot;]&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;返回一个元组&lt;/strong&gt; &lt;code&gt;(train_ds, test_ds)&lt;/code&gt;，分别为训练集和测试集。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;split=&amp;quot;train+test&amp;quot;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将训练集和测试集合并为一个数据集。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;切片（slice）支持&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;你可以对拆分集按行号或百分比进行切片：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;行号切片&lt;/strong&gt; ：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;split=&amp;quot;train[10:20]&amp;quot;&lt;/code&gt; → 加载训练集第 10 至 19 条记录（10 包含，20 不包含）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;百分比切片&lt;/strong&gt; ：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;split=&amp;quot;train[:10%]&amp;quot;&lt;/code&gt; → 前 10% 数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;split=&amp;quot;train[-20%:]&amp;quot;&lt;/code&gt; → 最后 20% 数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;split=&amp;quot;train[10%:20%]&amp;quot;&lt;/code&gt; → 从第 10% 开始到第 20% 结束&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;组合切片&lt;/strong&gt; ：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;split=&amp;quot;train[:10%]+train[-80%:]&amp;quot;&lt;/code&gt; → 取前 10% 和最后 80% 数据拼接&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;高级用法：交叉验证切片&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;splits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;train[{k}%:{k+10}%]&amp;#34;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;datasets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bookcorpus&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;splits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;生成 10 个 10% 验证切片，方便交叉验证&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-07-10</title>
      <link>https://tzj2006.github.io/bugjournal/2025-07-10/</link>
      <pubDate>Thu, 10 Jul 2025 10:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-07-10/</guid>
      <description>&lt;h2 id=&#34;huggingface-transformers-使用说明&#34;&gt;Huggingface Transformers 使用说明&lt;/h2&gt;
&lt;p&gt;详情请见：&lt;a href=&#34;https://huggingface.co/docs/transformers&#34;&gt;Hugging Face 页面&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-07-08</title>
      <link>https://tzj2006.github.io/bugjournal/2025-07-08/</link>
      <pubDate>Tue, 08 Jul 2025 10:47:40 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-07-08/</guid>
      <description>&lt;h2 id=&#34;torchvision-使用说明&#34;&gt;Torchvision 使用说明&lt;/h2&gt;
&lt;p&gt;详情请见：&lt;a href=&#34;https://docs.pytorch.org/vision/main/auto_examples/index.html&#34;&gt;Torchvision 页面&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;torchvisiontransforms&#34;&gt;torchvision.transforms&lt;/h3&gt;
&lt;h4 id=&#34;compose&#34;&gt;Compose&lt;/h4&gt;
&lt;p&gt;这个函数的作用就是把几个 transforms 连到一起，有点像 nn.Sequential 那样&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-07-07</title>
      <link>https://tzj2006.github.io/bugjournal/2025-07-07/</link>
      <pubDate>Mon, 07 Jul 2025 10:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-07-07/</guid>
      <description>&lt;p&gt;我现在的思路是这样的：
之前我们发现了 Libero 作为 benchmark 的一些可能的不合理的点&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据太少&lt;/li&gt;
&lt;li&gt;任务简单&lt;/li&gt;
&lt;li&gt;没有长程任务&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以及 DMPEL 作为 lifelong task 的一些可能的不合理的点&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;lifelone task 的定义可能太过狭隘 (只在训练过的 task 上能够表现好)&lt;/li&gt;
&lt;li&gt;方法可能聚焦于如何把 LIBERO 数据集的榜刷高，但是场景变化一点点就无法解决问题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以我现在想做的是：
用一些 数字/分数 来证明我的猜想是成立的
比如：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在场景变换的时候 DMPEL 成功率的下降&lt;/li&gt;
&lt;li&gt;当任务变复杂的时候 DMPEL 成功率的下降&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;现在我已经做的是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于开柜子的任务，无论是柜子移动，旋转，还是更换语义相同但是词语不同的句子，DMPEL 的成功率都会骤降 (100% -&amp;gt; 5%)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但是，开柜子的 task 可能过难了，
所以我现在准备测试 pick and place 的 task
当物品移动，旋转，以及更换语义相同但是词语不同的句子的时候 DMPEL 的成功率会下降多少&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-30</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-30/</link>
      <pubDate>Mon, 30 Jun 2025 17:47:09 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-30/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;1751276858157&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-30/1751276858157.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;这是现在的成果，总之是让环境里的物品移动了位置了&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-26</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-26/</link>
      <pubDate>Thu, 26 Jun 2025 16:51:16 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-26/</guid>
      <description>DMPEL &amp;amp; Libero</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-25</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-25/</link>
      <pubDate>Wed, 25 Jun 2025 13:27:12 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-25/</guid>
      <description>&lt;h3 id=&#34;libero-dataset-的可视化&#34;&gt;Libero dataset 的可视化&lt;/h3&gt;
&lt;h4 id=&#34;发现了一个讲-libero-讲得很详细的-blog&#34;&gt;发现了一个讲 Libero 讲得很详细的 blog&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_53610475/article/details/136421802&#34;&gt;强烈推荐阅读这篇 blog！&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一共有 9 讲，9 讲的链接全部在这一讲中写了，直接点感兴趣的讲看就 ok 了&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;但是，纸上得来终觉浅，绝知此事要躬行
有一些部分可能因为环境的不同不能适应所有情况，比如 headless server (没有显示输出的 server)
所以下面我会根据我的测试写针对 headless server 的 Libero dataset 可视化&lt;/p&gt;
&lt;h4 id=&#34;我的-libero-测试报告&#34;&gt;我的 Libero 测试报告&lt;/h4&gt;
&lt;p&gt;首先，第一个要注意的点就是：.libero 文件夹在 &lt;strong&gt;主目录下&lt;/strong&gt; (我认为此处说 &lt;strong&gt;根目录&lt;/strong&gt; 下更准确)
也就是在 &lt;code&gt;~/&lt;/code&gt; 下, 而不是 clone libero 下来的主目录。&lt;/p&gt;
&lt;p&gt;剩下的就很丝滑，所有的测试都平平无奇的通过了&lt;/p&gt;
&lt;p&gt;直到&amp;hellip;&lt;/p&gt;
&lt;p&gt;真正开始运行 Robosuit Demo 的时候，我遇到了困难：&lt;/p&gt;
&lt;p&gt;在运行完这段代码之后 python 就会直接崩溃：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;env&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;suite&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;env_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Lift&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# try with other tasks like &amp;#34;Stack&amp;#34; and &amp;#34;Door&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;robots&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Panda&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# try with other robots like &amp;#34;Sawyer&amp;#34; and &amp;#34;Jaco&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;has_renderer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;has_offscreen_renderer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;use_camera_obs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这是因为我正在用的服务器上并没有可以 render 的设备，
没法打开一个新窗口，所以就会直接崩溃&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-24</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-24/</link>
      <pubDate>Tue, 24 Jun 2025 12:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-24/</guid>
      <description>&lt;h3 id=&#34;这两天做了啥&#34;&gt;这两天做了啥：&lt;/h3&gt;
&lt;p&gt;Basically, 做了这些事情：&lt;/p&gt;
&lt;h4 id=&#34;发了一个issue&#34;&gt;发了一个issue&lt;/h4&gt;
&lt;p&gt;Hi DMPEL team, thanks for releasing the code! I’ve managed to run the full lifelong-learning pipeline on the &lt;code&gt;libero_goal&lt;/code&gt; benchmark, but the success rates I obtain are lower than those reported in the paper. I’d like to confirm whether I’m missing any important tricks or recommended hyper-parameter settings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reproduction details&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Command:
bash exp_scripts/lifelong_scripts/dmpel.sh:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;torchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        policy=bc_foundation_dmpel_policy lifelong=dmpel \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;GPU: 1 x A100
CUDA / Driver: CUDA12.0, Driver Version 525.147.05
python: 3.8.13
package: same as requirements.txt
seed: 100&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-20</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-20/</link>
      <pubDate>Fri, 20 Jun 2025 16:52:08 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-20/</guid>
      <description>Jornal</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-19</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-19/</link>
      <pubDate>Thu, 19 Jun 2025 11:14:06 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-19/</guid>
      <description>Journal</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-18</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-18/</link>
      <pubDate>Wed, 18 Jun 2025 12:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-18/</guid>
      <description>Data Download</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-17</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-17/</link>
      <pubDate>Tue, 17 Jun 2025 08:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-17/</guid>
      <description>VLA 中的持续学习</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-16</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-16/</link>
      <pubDate>Mon, 16 Jun 2025 14:14:45 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-16/</guid>
      <description>Avoid Catastrophy forget</description>
    </item>
    <item>
      <title>Bug Journal CVPR2025-Summary</title>
      <link>https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/</link>
      <pubDate>Fri, 13 Jun 2025 13:50:37 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/</guid>
      <description>CVPR 2025 Robotics Paper Summary</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-14</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-14/</link>
      <pubDate>Fri, 13 Jun 2025 10:28:11 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-14/</guid>
      <description>Prompt for fast paper read</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-13</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-13/</link>
      <pubDate>Fri, 13 Jun 2025 10:24:57 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-13/</guid>
      <description>Paper review of CVPR 2025</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-12</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-12/</link>
      <pubDate>Thu, 12 Jun 2025 14:49:08 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-12/</guid>
      <description>CVPR 2025 Robotics summary</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-11</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-11/</link>
      <pubDate>Wed, 11 Jun 2025 14:20:50 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-11/</guid>
      <description>Find Robotics in CVPR 2025</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-10</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-10/</link>
      <pubDate>Tue, 10 Jun 2025 14:39:21 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-10/</guid>
      <description>paper review</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-09</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-09/</link>
      <pubDate>Mon, 09 Jun 2025 11:36:45 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-09/</guid>
      <description>&lt;h2 id=&#34;robo-dm-data-management-for-large-robot-datasets&#34;&gt;Robo-DM: Data Management For Large Robot Datasets&lt;/h2&gt;
&lt;p&gt;ICRA 2025 BestPaper on Robot Learning&lt;/p&gt;
&lt;p&gt;from UCB &amp;amp; Google Deepmind&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;做数据库的&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;以前的数据都没压缩过，太大了; 存储，传输成本也高&lt;/li&gt;
&lt;li&gt;这样的话加载也会很慢&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt=&#34;1749461291726&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-09/1749461291726.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小&lt;/p&gt;
&lt;p&gt;他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1749461487133&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-09/1749461487133.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;最后把信息都通过 EBML file 存储&lt;/p&gt;
&lt;p&gt;为什么选择 EBML 呢?&lt;/p&gt;
&lt;p&gt;因为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;支持嵌套结构&lt;/li&gt;
&lt;li&gt;是自包含的，更方便复用&lt;/li&gt;
&lt;li&gt;支持流处理，不需要一次性全部导入到内存中&lt;/li&gt;
&lt;li&gt;支持自动时间同步&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于视频，主要选择了.H264 来压缩，显著降低了文件大小&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1749461874888&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-09/1749461874888.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1749461827040&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-09/1749461827040.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;最后这个数据集又小又快&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1749461984813&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-09/1749461984813.png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;achieving-human-level-competitive-robot-table-tennis-a-comprehensive-overview&#34;&gt;Achieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview&lt;/h2&gt;
&lt;p&gt;ICRA 2025 Best Paper on Robot Learning Finalist&lt;/p&gt;
&lt;p&gt;from Google Deepmind&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;机器人打乒乓球&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;打乒乓球要又快又准。所以是理想的机器人测试器&lt;/p&gt;
&lt;p&gt;对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-06</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-06/</link>
      <pubDate>Fri, 06 Jun 2025 10:08:22 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-06/</guid>
      <description>&lt;h2 id=&#34;leveraging-locality-to-boost-sample-efficiency-in-robotic-manipulation&#34;&gt;Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.10615&#34;&gt;发表时间：15 Jun 2024&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;动机&#34;&gt;动机&lt;/h4&gt;
&lt;p&gt;当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \pi(o_t)，即从全图像或全状态观察中直接输出动作。但：&lt;/p&gt;
&lt;p&gt;问题 1：学习难度高（需要从高维图像中学全局策略）；
问题 2：泛化差（模型可能过拟合于训练视角或场景）；
问题 3：sample efficiency 差（训练数据需求量大）&lt;/p&gt;
&lt;p&gt;作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的，
也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。&lt;/p&gt;
&lt;h4 id=&#34;主要论点&#34;&gt;主要论点&lt;/h4&gt;
&lt;p&gt;提出 Local Policy Networks（LPN）：
将策略函数设计为一组 局部策略（local policy heads）；
每个 head 只负责“在自己 patch 上预测动作”；
用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）；
最终策略通过对多个 local head 输出聚合（weighted sum）得到。&lt;/p&gt;
&lt;h4 id=&#34;模型流程图&#34;&gt;模型流程图&lt;/h4&gt;
&lt;p&gt;&lt;img alt=&#34;1749017331301&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-04/1749017331301.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作&lt;/p&gt;
&lt;h4 id=&#34;实验-setting&#34;&gt;实验 setting:&lt;/h4&gt;
&lt;p&gt;使用数据集：RT-1（Robotics Transformer 1）:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Google Everyday Robot（移动底座 + 7-DoF机械臂 + gripper）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;数据来源
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;数据规模
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;~130k 条实际机器人操作轨迹，覆盖 700+ 种任务
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;采样频率
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;每条轨迹包含约 50–100 帧关键帧（图像 + 动作）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;场景
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;家庭式办公环境（桌面、水槽、地面）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;物体
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;语言指令
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型）
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;对于每一条指令：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-05</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-05/</link>
      <pubDate>Thu, 05 Jun 2025 10:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-05/</guid>
      <description>&lt;h2 id=&#34;demogen-synthetic-demonstration-generation-for-data-efficient-visuomotor-policy-learning&#34;&gt;DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2502.16932&#34;&gt;发表时间：24 Feb 2025&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;动机&#34;&gt;动机&lt;/h4&gt;
&lt;p&gt;Robust，还得是Robust。&lt;/p&gt;
&lt;p&gt;为什么不加数据(ο´･д･)??&lt;/p&gt;
&lt;p&gt;过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。&lt;/p&gt;
&lt;p&gt;那怎么办呢(ο´･д･)??&lt;/p&gt;
&lt;p&gt;很简单，纯计算模拟不就是了。&lt;/p&gt;
&lt;h4 id=&#34;主要论点&#34;&gt;主要论点&lt;/h4&gt;
&lt;p&gt;用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人&lt;/p&gt;
&lt;p&gt;这样就不需要实际操作，但是可以直接获得大量数据。&lt;/p&gt;
&lt;h4 id=&#34;模型流程图&#34;&gt;模型流程图&lt;/h4&gt;
&lt;p&gt;&lt;img alt=&#34;1749026226827&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-04/1749026226827.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.&lt;/p&gt;
&lt;p&gt;思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以&lt;/p&gt;
&lt;p&gt;机器人一共有两段动作&lt;/p&gt;
&lt;p&gt;一段是碰到物体前的动作&lt;/p&gt;
&lt;p&gt;一段是碰到物体后的动作&lt;/p&gt;
&lt;p&gt;对于第一段，直接用一个变换矩阵变换&lt;/p&gt;
&lt;p&gt;对于第二段，直接规划一个新路径 (use RRT-Connect)&lt;/p&gt;
&lt;p&gt;现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)&lt;/p&gt;
&lt;p&gt;如果可以用的话&lt;/p&gt;
&lt;p&gt;然后通过模拟环境生成这个路径的图像&lt;/p&gt;
&lt;h4 id=&#34;实验设定&#34;&gt;实验设定&lt;/h4&gt;
&lt;p&gt;虚拟环境：1 条 GroundTruth&lt;/p&gt;
&lt;p&gt;真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）&lt;/p&gt;
&lt;p&gt;一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)&lt;/p&gt;
&lt;h4 id=&#34;结果&#34;&gt;结果&lt;/h4&gt;
&lt;p&gt;&lt;img alt=&#34;1749093303146&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-05/1749093303146.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1749093327428&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-05/1749093327428.png&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;vs-roboground&#34;&gt;V.S. RoboGround&lt;/h4&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;strong&gt;方面&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;DemoGen&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;RoboGround&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;目标问题&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;数据高效、空间泛化性差的视觉模仿学习&lt;/td&gt;
          &lt;td&gt;多任务泛化能力差、语义-空间信息连接弱&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;核心思想&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;从少量人类演示中&lt;strong&gt;合成大量视觉演示数据&lt;/strong&gt;用于模仿学习&lt;/td&gt;
          &lt;td&gt;grounding mask（掩码）作为embedding增强泛化&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;数据生成方式&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像&lt;/td&gt;
          &lt;td&gt;构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;人类演示&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;一条&lt;/td&gt;
          &lt;td&gt;在仿真中自动生成，无需真实 rollouts&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;任务表征形式&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;(图像帧, 末端动作)&lt;strong&gt;对&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;图像 + mask + 指令 + robot state&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;中间表示&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;None（直接预测动作）&lt;/td&gt;
          &lt;td&gt;掩码（mask）作为空间引导&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;依赖模型&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Immitation Learning&lt;/td&gt;
          &lt;td&gt;利用 VLM + Grounded Perceiver 构建 mask-guided policy&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;泛化方式&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;利用空间重定向与图像合成覆盖更多初始状态&lt;/td&gt;
          &lt;td&gt;通过 grounding masks 和多样 instruction 提升语义-空间泛化&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h1 id=&#34;doglove-dexterous-manipulation-with-a-low-cost-open-source-haptic-force-feedback-glove&#34;&gt;DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove&lt;/h1&gt;
&lt;p&gt;&lt;img alt=&#34;1749095773362&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-05/1749095773362.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-04</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-04/</link>
      <pubDate>Wed, 04 Jun 2025 11:16:31 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-04/</guid>
      <description>&lt;h2 id=&#34;leveraging-locality-to-boost-sample-efficiency-in-robotic-manipulation&#34;&gt;Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.10615&#34;&gt;发表时间：15 Jun 2024&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;动机&#34;&gt;动机&lt;/h4&gt;
&lt;p&gt;当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \pi(o_t)，即从全图像或全状态观察中直接输出动作。但：&lt;/p&gt;
&lt;p&gt;问题 1：学习难度高（需要从高维图像中学全局策略）；
问题 2：泛化差（模型可能过拟合于训练视角或场景）；
问题 3：sample efficiency 差（训练数据需求量大）&lt;/p&gt;
&lt;p&gt;作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的，
也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。&lt;/p&gt;
&lt;h4 id=&#34;主要论点&#34;&gt;主要论点&lt;/h4&gt;
&lt;p&gt;提出 Local Policy Networks（LPN）：
将策略函数设计为一组 局部策略（local policy heads）；
每个 head 只负责“在自己 patch 上预测动作”；
用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）；
最终策略通过对多个 local head 输出聚合（weighted sum）得到。&lt;/p&gt;
&lt;h4 id=&#34;模型流程图&#34;&gt;模型流程图&lt;/h4&gt;
&lt;p&gt;&lt;img alt=&#34;1749017331301&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-04/1749017331301.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作&lt;/p&gt;
&lt;h4 id=&#34;实验-setting&#34;&gt;实验 setting:&lt;/h4&gt;
&lt;p&gt;使用数据集：RT-1（Robotics Transformer 1）:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Google Everyday Robot（移动底座 + 7-DoF机械臂 + gripper）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;数据来源
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;数据规模
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;~130k 条实际机器人操作轨迹，覆盖 700+ 种任务
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;采样频率
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;每条轨迹包含约 50–100 帧关键帧（图像 + 动作）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;场景
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;家庭式办公环境（桌面、水槽、地面）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;物体
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;语言指令
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型）
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;对于每一条指令：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-03</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-03/</link>
      <pubDate>Tue, 03 Jun 2025 11:50:32 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-03/</guid>
      <description>&lt;h2 id=&#34;catch-it-learning-to-catch-in-flight-with-mobile-dexterous-hands&#34;&gt;Catch It! Learning to Catch in Flight with Mobile Dexterous Hands&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.10319&#34;&gt;发表时间: 16 Sep 2024&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;动机&#34;&gt;动机&lt;/h4&gt;
&lt;p&gt;Basically, 这是之前那篇 &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.08809&#34;&gt;DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands&lt;/a&gt;&lt;/em&gt; 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。&lt;/p&gt;
&lt;h4 id=&#34;模型流程图&#34;&gt;模型流程图&lt;/h4&gt;
&lt;p&gt;&lt;img alt=&#34;1748922875088&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-03/1748922875088.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;这是真机器人的样子：
一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;1748922862160&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-06-03/1748922862160.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;训练的过程分为两步：&lt;/p&gt;
&lt;p&gt;第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。
第二步是微调灵巧手让手抓住这个物体。&lt;/p&gt;
&lt;p&gt;最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。&lt;/p&gt;
&lt;h4 id=&#34;解决的难点&#34;&gt;解决的难点&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;部署到真机器人上&lt;/li&gt;
&lt;li&gt;一步到位 end-to-end 效果没那么好&lt;/li&gt;
&lt;li&gt;抓不住从未见过的物体&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;还需要解决的难点&#34;&gt;还需要解决的难点&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;从未见过的物体还是不好抓&lt;/li&gt;
&lt;li&gt;仍然没有考虑材质之类的问题&lt;/li&gt;
&lt;li&gt;还是无法在当物体在空中时就判断物体的形状&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;reinforcement-learning-with-foundation-priors-let-the-embodied-agent-efficiently-learn-on-its-own&#34;&gt;Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.02635&#34;&gt;发布时间: 4 Oct. 2023&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-02</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-02/</link>
      <pubDate>Mon, 02 Jun 2025 11:24:12 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-02/</guid>
      <description>Review of Three base VLA models and three basic CNNs.</description>
    </item>
    <item>
      <title>Bug Journal 2025-06-01</title>
      <link>https://tzj2006.github.io/bugjournal/2025-06-01/</link>
      <pubDate>Sun, 01 Jun 2025 12:00:14 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-06-01/</guid>
      <description>&lt;h1 id=&#34;主流视觉-文本多模态模型技术分析&#34;&gt;主流视觉-文本多模态模型技术分析&lt;/h1&gt;
&lt;p&gt;近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。&lt;/p&gt;
&lt;h2 id=&#34;clip-openai-2021&#34;&gt;CLIP (OpenAI, 2021)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;整体架构设计：&lt;/strong&gt; CLIP 采用 &lt;strong&gt;双编码器架构&lt;/strong&gt; ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到&lt;strong&gt;相同维度&lt;/strong&gt;的向量空间&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder&#34;&gt;lightly.ai&lt;/a&gt;。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder&#34;&gt;lightly.ai&lt;/a&gt;。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到&lt;strong&gt;共享的多模态嵌入空间&lt;/strong&gt;&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder&#34;&gt;lightly.ai&lt;/a&gt;。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模态对齐方式：&lt;/strong&gt; CLIP通过&lt;strong&gt;对比学习&lt;/strong&gt;实现视觉-语言对齐。训练时，模型给定一批图文对，学习&lt;strong&gt;预测哪张图像与哪段文本匹配&lt;/strong&gt;&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations&#34;&gt;lightly.ai&lt;/a&gt;。具体而言，CLIP使用 &lt;strong&gt;对称的跨模态对比损失&lt;/strong&gt; （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations&#34;&gt;lightly.ai&lt;/a&gt;。这种训练使图像和文本编码器产生的特征在共享空间中&lt;strong&gt;成对靠近&lt;/strong&gt;&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations&#34;&gt;lightly.ai&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入 token 表达统一：&lt;/strong&gt; CLIP并未显式统一图像和文本的输入表示格式。 &lt;strong&gt;图像和文本各有独立的token化和编码流程&lt;/strong&gt; ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder&#34;&gt;lightly.ai&lt;/a&gt;。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过&lt;strong&gt;独立编码+对齐空间&lt;/strong&gt;的方式，规避了直接将图像作为序列token处理的不统一问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失函数与训练策略：&lt;/strong&gt; 损失采用 &lt;strong&gt;对比学习的InfoNCE损失&lt;/strong&gt; （实现为带温度系数的归一化softmax交叉熵）&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and&#34;&gt;research.google&lt;/a&gt;。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了&lt;strong&gt;大批量&lt;/strong&gt;训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,2023%2C%20in%20their&#34;&gt;lightly.ai&lt;/a&gt;。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=insensitive%20to%20the%20capacity%20of,the%20text%20encoder&#34;&gt;lightly.ai&lt;/a&gt;。CLIP从&lt;strong&gt;随机初始化&lt;/strong&gt;开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder&#34;&gt;lightly.ai&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集及伪标签：&lt;/strong&gt; CLIP在一个超大规模的图文配对数据集上预训练，包含约&lt;strong&gt;4亿对图像-文本&lt;/strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model&#34;&gt;arxiv.org&lt;/a&gt;（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖&lt;strong&gt;自然语言的弱监督&lt;/strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model&#34;&gt;arxiv.org&lt;/a&gt;。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;六大难点应对：&lt;/strong&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;模态对齐困难：&lt;/em&gt;  CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations&#34;&gt;lightly.ai&lt;/a&gt;。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;token格式不统一：&lt;/em&gt;  采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder&#34;&gt;lightly.ai&lt;/a&gt;。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;语义粒度不匹配：&lt;/em&gt; CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如&lt;strong&gt;Fine-Grained CLIP&lt;/strong&gt;等正是受限于CLIP在局部语义对齐上的不足&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder&#34;&gt;lightly.ai&lt;/a&gt;&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=SigLIP%3A%20Optimising%20the%20loss%20function,for%20better%20scaling&#34;&gt;lightly.ai&lt;/a&gt;。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;多模态上下文保持：&lt;/em&gt; CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 &lt;strong&gt;多模态上下文&lt;/strong&gt; （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;训练数据稀缺：&lt;/em&gt; CLIP通过&lt;strong&gt;大规模弱标注数据&lt;/strong&gt;从根本上缓解了数据稀缺的问题&lt;a href=&#34;https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model&#34;&gt;arxiv.org&lt;/a&gt;。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;计算开销高：&lt;/em&gt; 训练CLIP确实需要巨大的算力和显存，但相对来说，其&lt;strong&gt;双塔架构&lt;/strong&gt;使训练可并行展开，推理时也可分别预编码图文后做相似度计算，&lt;strong&gt;具有一定的效率优势&lt;/strong&gt;&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations&#34;&gt;lightly.ai&lt;/a&gt;。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,Training%E2%80%9D%2C%20propose%20to&#34;&gt;lightly.ai&lt;/a&gt;。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=implementation%20is%20numerically%20unstable%2C%20and,additional%20bias%20terms%2C%20and%20calculations&#34;&gt;lightly.ai&lt;/a&gt;。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;参考：&lt;/strong&gt; CLIP 的论文&lt;a href=&#34;https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model&#34;&gt;arxiv.org&lt;/a&gt;详细描述了其对比预训练方法，OpenAI 的博客也提供了概述&lt;a href=&#34;https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations&#34;&gt;lightly.ai&lt;/a&gt;。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库&lt;a href=&#34;https://huggingface.co/docs/transformers/en/model_doc/clip#:~:text=CLIP%20uses%20an%20image%20encoder,the%20same%20number%20of&#34;&gt;huggingface.co&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;align-google-2021&#34;&gt;ALIGN (Google, 2021)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;整体架构设计：&lt;/strong&gt; ALIGN（ &lt;strong&gt;A Large-scale ImaGe and Noisy-Text embedding&lt;/strong&gt; ）延续了与CLIP相同的&lt;strong&gt;双编码器对比学习架构&lt;/strong&gt;&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained&#34;&gt;research.google&lt;/a&gt;。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：&lt;strong&gt;EfficientNet-L2卷积网络&lt;/strong&gt;作为图像编码器，&lt;strong&gt;BERT-Large&lt;/strong&gt;作为文本编码器，并均从随机初始化开始训练&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and&#34;&gt;research.google&lt;/a&gt;。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模态对齐方式：&lt;/strong&gt; ALIGN采用&lt;strong&gt;对比损失（normalized softmax）&lt;strong&gt;来训练，使匹配的图文对嵌入向量接近，不匹配的远离&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and&#34;&gt;research.google&lt;/a&gt;。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以&lt;/strong&gt;批为单位的跨模态对比&lt;/strong&gt;训练，使模型学到强大的图文对齐表示。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入 token 表达统一：&lt;/strong&gt; ALIGN同样没有将图像直接离散为token序列，而是通过&lt;strong&gt;双通道&lt;/strong&gt;处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出&lt;strong&gt;embedding空间&lt;/strong&gt;实现统一表示&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and&#34;&gt;research.google&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失函数与训练策略：&lt;/strong&gt; 使用 &lt;strong&gt;对比学习损失&lt;/strong&gt; （InfoNCE变体），在&lt;strong&gt;大批量&lt;/strong&gt;上训练模型&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained&#34;&gt;research.google&lt;/a&gt;。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 &lt;strong&gt;最小程度的过滤&lt;/strong&gt; ，通过&lt;strong&gt;数据规模&lt;/strong&gt;来弥补噪声&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集及伪标签：&lt;/strong&gt; ALIGN的亮点在于使用了&lt;strong&gt;超过10亿对图像-Alt文本&lt;/strong&gt;的超大规模数据集&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 &lt;strong&gt;无需人工标注&lt;/strong&gt; 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN &lt;strong&gt;放宽过滤标准&lt;/strong&gt; ，只做了基于频率的简单过滤，最终得到约&lt;strong&gt;18亿对&lt;/strong&gt;图文数据&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20this%20work%2C%20we%20follow,text%20pairs&#34;&gt;research.google&lt;/a&gt;。这些文本描述可能包含噪声甚至与图像无关，但研究表明 &lt;strong&gt;规模弥补质量&lt;/strong&gt; ：如此海量的数据使模型学到泛化的视觉语言表示&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;六大难点应对：&lt;/strong&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;模态对齐困难：&lt;/em&gt; ALIGN证明了&lt;strong&gt;数据规模&lt;/strong&gt;在对齐中的重要作用。通过&lt;strong&gt;十倍于CLIP的数据规模&lt;/strong&gt;和强大的对比损失，模型学到了稳健的跨模态对齐能力&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,L2&#34;&gt;research.google&lt;/a&gt;。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;token格式不统一：&lt;/em&gt; 与CLIP类似，ALIGN通过&lt;strong&gt;双编码器架构&lt;/strong&gt;回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained&#34;&gt;research.google&lt;/a&gt;。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;语义粒度不匹配：&lt;/em&gt; ALIGN的训练目标依旧作用在 &lt;strong&gt;全局图像-句子层面&lt;/strong&gt; ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对&lt;strong&gt;细粒度语义&lt;/strong&gt;的不匹配没有特殊解决方案。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;多模态上下文保持：&lt;/em&gt; ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;，未涉及多模态对话等情境。因此，ALIGN在&lt;strong&gt;多轮交互&lt;/strong&gt;或&lt;strong&gt;长上下文&lt;/strong&gt;问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;训练数据稀缺：&lt;/em&gt; ALIGN的策略是&lt;strong&gt;极端扩增数据规模&lt;/strong&gt;以消除数据稀缺瓶颈&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 &lt;strong&gt;弱标注的大数据&lt;/strong&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;计算开销高：&lt;/em&gt; 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and&#34;&gt;research.google&lt;/a&gt;。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and&#34;&gt;research.google&lt;/a&gt;。尽管计算开销高昂，ALIGN通过 &lt;strong&gt;冻结架构复杂性&lt;/strong&gt; （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，&lt;strong&gt;优先扩大数据规模&lt;/strong&gt;比增加模型复杂度更有效&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained&#34;&gt;research.google&lt;/a&gt;。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;参考：&lt;/strong&gt; ALIGN 的研究细节发表于 ICML 2021&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20,We&#34;&gt;research.google&lt;/a&gt;。Google Research 官方博客提供了对ALIGN的通俗描述&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable&#34;&gt;research.google&lt;/a&gt;&lt;a href=&#34;https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained&#34;&gt;research.google&lt;/a&gt;。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-05-31</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-31/</link>
      <pubDate>Fri, 30 May 2025 17:06:08 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-31/</guid>
      <description>&lt;h4 id=&#34;主要动机&#34;&gt;主要动机&lt;/h4&gt;
&lt;p&gt;目前有三大挑战：数据稀缺、泛化能力不足，以及在复杂物理环境中效果不好
作者希望通过引入大规模语言及VLA模型在自然语言处理和计算机视觉中的成功经验解决这个问题.&lt;/p&gt;
&lt;h4 id=&#34;主要论点&#34;&gt;主要论点&lt;/h4&gt;
&lt;p&gt;在预训练的视-语言模型（VLM，本文采用 PaliGemma）基础上，增加一个“动作专家”（action expert），通过条件流匹配（diffusion policy）生成高频、连续的动作序列（每秒可达50 Hz）&lt;/p&gt;
&lt;h4 id=&#34;模型流程图&#34;&gt;模型流程图&lt;/h4&gt;
&lt;p&gt;&lt;img alt=&#34;1748596153733&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-05-31/1748596153733.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;收集数据 -&amp;gt; 训练$\phi_0$ -&amp;gt; Zero-Shot/微调/Fine-tune&lt;/p&gt;
&lt;p&gt;数据来自7种不同的机器人，68个不同的任务，总计 10k小时。&lt;/p&gt;
&lt;p&gt;每一个单独机械臂的自由度在 6-7 (有些机器人有多机械臂)&lt;/p&gt;
&lt;p&gt;输入有 3 块，分别是：Image, Language, and State&lt;/p&gt;
&lt;p&gt;Image 用 400M Pretrained VIT SigLip 得到 embedding, 一共会有up to 3 个 Image， 所以最会有 3 个 embedding (没有就填 0)&lt;/p&gt;
&lt;p&gt;Language 用 2.6B Pretrained LLM Gemma, 得到 embedding&lt;/p&gt;
&lt;p&gt;最后是关节信息，最多会有 18 个(没有就填 0)。&lt;/p&gt;
&lt;p&gt;之后运用 Diffusion Policy 来生成每一步的动作&lt;/p&gt;
&lt;p&gt;生成的是 $p(A_t|O_t)$, 即，在给定条件(环境下)，每一个动作的概率。&lt;/p&gt;
&lt;p&gt;这时给一个 chunk 内的真实动作加噪音，然后让模型学习如何去噪。&lt;/p&gt;
&lt;p&gt;方式如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;随机采一个噪声级别 $\tau$，采一个高斯噪声 $\varepsilon$；&lt;/li&gt;
&lt;li&gt;构造带噪动作块 $A_t^\tau = \tau A_t + (1 - \tau)\varepsilon$；&lt;/li&gt;
&lt;li&gt;用观测 $o_t$ 与 $A_t^\tau$ 输入网络，预测去噪速度场 $v_\theta(A_t^\tau, o_t) &amp;lt;-&amp;gt; \varepsilon - A_t$；&lt;/li&gt;
&lt;li&gt;以 $\left| v_\theta - (\varepsilon - A_t) \right|^2$ 作为监督信号；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;推理的时候就会把这个过程分成 10-20步，每一步去噪一点点，最终得到一个动作
&lt;em&gt;注：&amp;ldquo;这个过程&amp;quot;指的是随机生成一个噪声，然后从这个噪声去噪的过程&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-05-30</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-30/</link>
      <pubDate>Fri, 30 May 2025 11:04:48 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-30/</guid>
      <description>Docker 的安装和调试</description>
    </item>
    <item>
      <title>Bug Journal 2025-05-29</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-29/</link>
      <pubDate>Thu, 29 May 2025 09:30:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-29/</guid>
      <description>Paper review 2025-05-29</description>
    </item>
    <item>
      <title>Bug Journal 2025-05-28</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-28/</link>
      <pubDate>Wed, 28 May 2025 14:41:55 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-28/</guid>
      <description>2025-05-28 论文阅读笔记</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-27</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-27/</link>
      <pubDate>Tue, 27 May 2025 21:19:57 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-27/</guid>
      <description>Solution to 2894. Divisible and Non-divisible Sums Difference</description>
    </item>
    <item>
      <title>Bug Journal 2025-05-27</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-27/</link>
      <pubDate>Tue, 27 May 2025 16:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-27/</guid>
      <description>A template for reading AI papers</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-25</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-25/</link>
      <pubDate>Sun, 25 May 2025 12:17:29 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-25/</guid>
      <description>Solution to 2131. Longest Palindrome by Concatenating Two Letter Words</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-24</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-24/</link>
      <pubDate>Sat, 24 May 2025 11:19:20 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-24/</guid>
      <description>Solution to 2942. Find Words Containing Character</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-23</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-23/</link>
      <pubDate>Fri, 23 May 2025 21:16:05 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-23/</guid>
      <description>Solution to 3068. Find the Maximum Sum of Node Values</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-22</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-22/</link>
      <pubDate>Thu, 22 May 2025 22:33:45 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-22/</guid>
      <description>Solution to 3362. Zero Array Transformation III</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-18</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-18/</link>
      <pubDate>Wed, 21 May 2025 15:40:56 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-18/</guid>
      <description>Solution to 2375. Construct Smallest Number From DI String</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-21</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-21/</link>
      <pubDate>Wed, 21 May 2025 14:53:20 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-21/</guid>
      <description>Solution to 73. Set Matrix Zeroes</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-20</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-20/</link>
      <pubDate>Tue, 20 May 2025 11:34:09 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-20/</guid>
      <description>Solution to 3355. Zero Array Transformation I</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-19</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-19/</link>
      <pubDate>Mon, 19 May 2025 09:52:16 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-19/</guid>
      <description>Solution to 3024. Type of Triangle</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-18</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-18/</link>
      <pubDate>Sun, 18 May 2025 10:34:30 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-18/</guid>
      <description>Solution to 1931. Painting a Grid With Three Different Colors</description>
    </item>
    <item>
      <title>Bug Journal 2025-05-17</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-17/</link>
      <pubDate>Sat, 17 May 2025 22:45:33 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-17/</guid>
      <description>&lt;p&gt;Mac 监控：
磁盘信息：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;brew&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;smartmontools&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;smartctl&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;disk0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;效果：
&lt;img alt=&#34;1&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-05-17/1.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;CPU GPU占用信息：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;brew install macmon
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;macmon
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;效果展示：
&lt;img alt=&#34;2&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-05-17/2.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-17</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-17/</link>
      <pubDate>Sat, 17 May 2025 15:18:23 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-17/</guid>
      <description>Solution to 1079. Letter Tile Possibilities</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-16</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-16/</link>
      <pubDate>Sat, 17 May 2025 14:50:49 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-16/</guid>
      <description>Solution to</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-05-17</title>
      <link>https://tzj2006.github.io/leetcode/2025-05-17/</link>
      <pubDate>Sat, 17 May 2025 13:37:19 +0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-05-17/</guid>
      <description>Solution to 75. Sort Colors</description>
    </item>
    <item>
      <title>Bug Journal 2025-05-25</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-14/</link>
      <pubDate>Thu, 15 May 2025 00:00:00 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-14/</guid>
      <description>Summary of Robotics papers</description>
    </item>
    <item>
      <title>Bug Journal 2025-05-09</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-09/</link>
      <pubDate>Fri, 09 May 2025 18:52:47 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-09/</guid>
      <description>遇到的问题</description>
    </item>
    <item>
      <title>Bug Journal 20250508</title>
      <link>https://tzj2006.github.io/bugjournal/2025-05-08/</link>
      <pubDate>Thu, 08 May 2025 19:30:40 +0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-05-08/</guid>
      <description>初识具身智能</description>
    </item>
    <item>
      <title>Bug Journal 2025-03-28</title>
      <link>https://tzj2006.github.io/bugjournal/2025-03-28/</link>
      <pubDate>Fri, 28 Mar 2025 08:36:01 -0600</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-03-28/</guid>
      <description>&lt;h1 id=&#34;talk-6-scdesign3-semi-synthetic-negative--positive-control&#34;&gt;Talk 6: scDesign3: Semi-synthetic Negative &amp;amp; Positive Control&lt;/h1&gt;
&lt;p&gt;数据不够的时候需要一些 simulated data&lt;/p&gt;
&lt;p&gt;那这时候 simlator 就需要 interpretable &amp;amp; realistic (real data characteristic &amp;amp; contains ground truth data)&lt;/p&gt;
&lt;p&gt;e.g. single cell RNA simulator -&amp;gt; 考虑 gene gene correlation -&amp;gt; 考虑别的 cell types and omics -&amp;gt; RNA count to RNA read&lt;/p&gt;
&lt;p&gt;这个可以用来给数据预处理+降噪&lt;/p&gt;
&lt;h1 id=&#34;talk-7-spacial-omics-data&#34;&gt;Talk 7: spacial omics data&lt;/h1&gt;
&lt;p&gt;理解 low-rank property of Hi-C chromatin contact maps.&lt;/p&gt;
&lt;p&gt;数据上有 3 维（question what type of data / or do you have ground truth of the location of the spatial data? are the model predicting the location or the gene expression or both of them? ）&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bug Journal 2025-03-27</title>
      <link>https://tzj2006.github.io/bugjournal/2025-03-27/</link>
      <pubDate>Thu, 27 Mar 2025 13:24:41 -0600</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-03-27/</guid>
      <description>&lt;p&gt;MCBIOS Conference Day 1:&lt;/p&gt;
&lt;h1 id=&#34;talk-1-sharing-data&#34;&gt;Talk 1: sharing data:&lt;/h1&gt;
&lt;h2 id=&#34;httpsdatacommonscancergov&#34;&gt;&lt;a href=&#34;https://datacommons.cancer.gov/&#34;&gt;https://datacommons.cancer.gov/&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;介绍了很多数据：特点是比较多，比较新，并且user-friendly. 还有 NIH founding 可以 use start-up server.&lt;/p&gt;
&lt;h2 id=&#34;httpscomputationalcancergov&#34;&gt;&lt;a href=&#34;https://computational.cancer.gov/&#34;&gt;https://computational.cancer.gov/&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;介绍了很多模型：都是用来预处理数据的&lt;/p&gt;
&lt;p&gt;比如有一个 AI-based toolbox (类似scanpy) 可以预处理所有数据&lt;/p&gt;
&lt;p&gt;还有 Automated Data Collection&lt;/p&gt;
&lt;p&gt;link TBD&lt;/p&gt;
&lt;p&gt;LLM 翻译诊断结果&lt;/p&gt;
&lt;h1 id=&#34;talk-2-write-code-with-github-copilot&#34;&gt;Talk 2: Write code with Github Copilot.&lt;/h1&gt;
&lt;h2 id=&#34;你能用这个干嘛&#34;&gt;你能用这个干嘛&lt;/h2&gt;
&lt;p&gt;当然是写代码啦，还能干嘛（&lt;/p&gt;
&lt;p&gt;好处是可以直接在你的 IDE 里面生成(虽然现在 ChatGPT.app 也可以了，(反正都是一家的[doge])&lt;/p&gt;
&lt;p&gt;比如说你可以先写一段注释来让 Copilot 生成你想要的代码&lt;/p&gt;
&lt;p&gt;然后选中这段代码并且点击旁边的小星星来让 Copilot 更改这段代码&lt;/p&gt;
&lt;p&gt;用于重复的项目效果更佳。比如分离数据集&lt;/p&gt;
&lt;p&gt;修bug还挺好用的(虽然有时候越修越多[doge])&lt;/p&gt;
&lt;p&gt;修改代码的语言：比如把 R code 换成 python code.&lt;/p&gt;
&lt;p&gt;还可以 explain what the code is doing (by using /explain).&lt;/p&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-03-25</title>
      <link>https://tzj2006.github.io/leetcode/2025-03-25/</link>
      <pubDate>Tue, 25 Mar 2025 01:14:18 -0400</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-03-25/</guid>
      <description>Solution to 3394. Check if Grid can be Cut into Sections</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-03-19</title>
      <link>https://tzj2006.github.io/leetcode/2025-03-19/</link>
      <pubDate>Tue, 18 Mar 2025 22:07:40 -0400</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-03-19/</guid>
      <description>Solution to 3191. Minimum Operations to Make Binary Array Elements Equal to One I</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-15</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-15/</link>
      <pubDate>Sat, 15 Feb 2025 00:13:13 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-15/</guid>
      <description>Solution to 2698. Find the Punishment Number of an Integer</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-14</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-14/</link>
      <pubDate>Fri, 14 Feb 2025 15:29:59 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-14/</guid>
      <description>Solution to 1352. Product of the Last K Numbers</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-13</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-13/</link>
      <pubDate>Thu, 13 Feb 2025 00:06:39 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-13/</guid>
      <description>Solution to 3066. Minimum Operations to Exceed Threshold Value II</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-12</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-12/</link>
      <pubDate>Tue, 11 Feb 2025 21:06:38 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-12/</guid>
      <description>Solution to 2342. Max Sum of a Pair With Equal Sum of Digits</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-11</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-11/</link>
      <pubDate>Mon, 10 Feb 2025 20:21:28 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-11/</guid>
      <description>Solution to 1910. Remove All Occurrences of a Substring</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-10</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-10/</link>
      <pubDate>Sun, 09 Feb 2025 19:07:12 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-10/</guid>
      <description>Solution to 3174. Clear Digits</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-09</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-09/</link>
      <pubDate>Sun, 09 Feb 2025 14:50:35 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-09/</guid>
      <description>Solution to 2364. Count Number of Bad Pairs</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-08</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-08/</link>
      <pubDate>Sat, 08 Feb 2025 14:35:49 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-08/</guid>
      <description>Solution to 2349. Design a Number Container System</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-07</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-07/</link>
      <pubDate>Thu, 06 Feb 2025 23:22:59 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-07/</guid>
      <description>Solution to 3160. Find the Number of Distinct Colors Among the Balls</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-06</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-06/</link>
      <pubDate>Wed, 05 Feb 2025 21:36:51 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-06/</guid>
      <description>Solution to 1726. Tuple with Same Product</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-05</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-05/</link>
      <pubDate>Wed, 05 Feb 2025 21:36:48 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-05/</guid>
      <description>Solution to 1790. Check if One String Swap Can Make Strings Equal</description>
    </item>
    <item>
      <title>Bug Journal 2025-02-04</title>
      <link>https://tzj2006.github.io/bugjournal/2025-02-04/</link>
      <pubDate>Tue, 04 Feb 2025 13:26:59 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-02-04/</guid>
      <description>&lt;p&gt;. &amp;ndash;&amp;gt; find all characters except &amp;lsquo;\n&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;^ &amp;ndash;&amp;gt; find all the start of the string&lt;/p&gt;
&lt;p&gt;$ &amp;ndash;&amp;gt; find all the end of the string.&lt;/p&gt;
&lt;p&gt;[&amp;hellip;] find a character inside the [].&lt;/p&gt;
&lt;p&gt;[^] find characters not inside the [].&lt;/p&gt;
&lt;p&gt;\. find the special characters.&lt;/p&gt;
&lt;p&gt;\d all numbers &amp;lt;=&amp;gt; [0-9]&lt;/p&gt;
&lt;p&gt;\D all not numbers &amp;lt;=&amp;gt; [^0-9]&lt;/p&gt;
&lt;p&gt;\s all space characters&lt;/p&gt;
&lt;p&gt;\S all not space characters&lt;/p&gt;
&lt;p&gt;\w all letter, num, and &lt;em&gt;. [a-z-A-Z0-9&lt;/em&gt;].&lt;/p&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-04</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-04/</link>
      <pubDate>Tue, 04 Feb 2025 13:19:11 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-04/</guid>
      <description>Solution to 1800. Maximum Ascending Subarray Sum</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-03</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-03/</link>
      <pubDate>Tue, 04 Feb 2025 13:19:09 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-03/</guid>
      <description>Solution to 3105. Longest Strictly Increasing or Strictly Decreasing Subarray</description>
    </item>
    <item>
      <title>Bug Journal 2025-02-02</title>
      <link>https://tzj2006.github.io/bugjournal/2025-02-02/</link>
      <pubDate>Sun, 02 Feb 2025 21:31:49 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-02-02/</guid>
      <description>Speed of M4 Pro</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-02</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-02/</link>
      <pubDate>Sun, 02 Feb 2025 12:21:21 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-02/</guid>
      <description>Solution to 1752. Check if Array Is Sorted and Rotated</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-30</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-30/</link>
      <pubDate>Sat, 01 Feb 2025 13:07:45 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-30/</guid>
      <description>Solution to 2493. Divide Nodes Into the Maximum Number of Groups</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-02-01</title>
      <link>https://tzj2006.github.io/leetcode/2025-02-01/</link>
      <pubDate>Sat, 01 Feb 2025 12:29:35 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-02-01/</guid>
      <description>Solution to 3151. Special Array I</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-31</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-31/</link>
      <pubDate>Fri, 31 Jan 2025 12:55:41 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-31/</guid>
      <description>Solution to 827. Making A Large Island</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-29</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-29/</link>
      <pubDate>Wed, 29 Jan 2025 10:58:22 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-29/</guid>
      <description>Solution to 684. Redundant Connection</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-28</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-28/</link>
      <pubDate>Tue, 28 Jan 2025 01:17:20 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-28/</guid>
      <description>Solution to 2658. Maximum Number of Fish in a Grid</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-27</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-27/</link>
      <pubDate>Sun, 26 Jan 2025 19:58:52 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-27/</guid>
      <description>Solution to 1462. Course Schedule IV</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-26</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-26/</link>
      <pubDate>Sat, 25 Jan 2025 21:16:36 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-26/</guid>
      <description>Solution to 2127. Maximum Employees to Be Invited to a Meeting</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-25</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-25/</link>
      <pubDate>Sat, 25 Jan 2025 21:16:34 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-25/</guid>
      <description>Solution to 2948. Make Lexicographically Smallest Array by Swapping Elements</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-24</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-24/</link>
      <pubDate>Fri, 24 Jan 2025 11:49:30 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-24/</guid>
      <description>ipykernel, PID</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-24</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-24/</link>
      <pubDate>Thu, 23 Jan 2025 22:31:15 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-24/</guid>
      <description>Solution to 802. Find Eventual Safe States</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-23</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-23/</link>
      <pubDate>Wed, 22 Jan 2025 21:23:43 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-23/</guid>
      <description>Solution to 1267. Count Servers that Communicate</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-22</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-22/</link>
      <pubDate>Wed, 22 Jan 2025 12:57:27 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-22/</guid>
      <description>Solution to 1765. Map of Highest Peak</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-21</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-21/</link>
      <pubDate>Mon, 20 Jan 2025 20:39:17 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-21/</guid>
      <description>Solution to 2017. Grid Game</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-20</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-20/</link>
      <pubDate>Mon, 20 Jan 2025 20:39:14 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-20/</guid>
      <description>Solution to 2661. First Completely Painted Row or Column</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-19</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-19/</link>
      <pubDate>Sun, 19 Jan 2025 14:51:51 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-19/</guid>
      <description>Solution to 407. Trapping Rain Water II</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-18</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-18/</link>
      <pubDate>Sat, 18 Jan 2025 17:47:12 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-18/</guid>
      <description>Solution to 1368. Minimum Cost to Make at Least One Valid Path in a Grid</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-17</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-17/</link>
      <pubDate>Fri, 17 Jan 2025 14:14:30 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-17/</guid>
      <description>Ollama</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-17</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-17/</link>
      <pubDate>Fri, 17 Jan 2025 12:06:57 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-17/</guid>
      <description>Solution to 2683. Neighboring Bitwise XOR</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-16</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-16/</link>
      <pubDate>Thu, 16 Jan 2025 15:35:24 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-16/</guid>
      <description>Solution to 2425. Bitwise XOR of All Pairings</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-15</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-15/</link>
      <pubDate>Wed, 15 Jan 2025 10:28:02 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-15/</guid>
      <description>Solution to 2429. Minimize XOR</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-14</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-14/</link>
      <pubDate>Tue, 14 Jan 2025 14:53:30 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-14/</guid>
      <description>&lt;h2 id=&#34;part-module-load&#34;&gt;Part module load&lt;/h2&gt;
&lt;p&gt;When you want to use something on the server, please first check whether it is on the server or not.&lt;/p&gt;
&lt;p&gt;Use this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;module avail
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then load using this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;module load &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;your model name&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can always email &lt;a href=&#34;rescomputing@duke.edu&#34;&gt;rescomputing@duke.edu&lt;/a&gt; to get support.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-13</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-14/</link>
      <pubDate>Mon, 13 Jan 2025 21:19:48 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-14/</guid>
      <description>Solution to 2657. Find the Prefix Common Array of Two Arrays</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-12</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-12/</link>
      <pubDate>Sun, 12 Jan 2025 22:45:56 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-12/</guid>
      <description>&lt;p&gt;When facing this problem, you can stop using GPU and change to CPU and check the error.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250112224658415&#34; loading=&#34;lazy&#34; src=&#34;https://tzj2006.github.io/images/2025-01-12GPUError.jpg&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-13</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-13/</link>
      <pubDate>Sun, 12 Jan 2025 22:44:37 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-13/</guid>
      <description>&lt;ol start=&#34;3223&#34;&gt;
&lt;li&gt;Minimum Length of String After Operations&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-12</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-12/</link>
      <pubDate>Sat, 11 Jan 2025 23:33:48 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-12/</guid>
      <description>&lt;ol start=&#34;2116&#34;&gt;
&lt;li&gt;Check if a Parentheses String Can Be Valid&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-11</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-11/</link>
      <pubDate>Fri, 10 Jan 2025 23:40:52 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-11/</guid>
      <description>&lt;ol start=&#34;1400&#34;&gt;
&lt;li&gt;Construct K Palindrome Strings&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-10</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-10/</link>
      <pubDate>Thu, 09 Jan 2025 23:42:20 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-10/</guid>
      <description>&lt;ol start=&#34;916&#34;&gt;
&lt;li&gt;Word Subsets&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-09</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-09/</link>
      <pubDate>Thu, 09 Jan 2025 13:42:16 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-09/</guid>
      <description>&lt;h2 id=&#34;part-matplotlib&#34;&gt;Part Matplotlib&lt;/h2&gt;
&lt;p&gt;If you want to make the word in matplotlib the a word that could be edited in Adobe illustration, please use the code below at the top of your code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mpl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rcParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;pdf.fonttype&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mpl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rcParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ps.fonttype&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-09</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-09/</link>
      <pubDate>Wed, 08 Jan 2025 23:33:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-09/</guid>
      <description>&lt;ol start=&#34;2185&#34;&gt;
&lt;li&gt;Counting Words With a Given Prefix&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-08</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-08/</link>
      <pubDate>Wed, 08 Jan 2025 13:42:16 -0500</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-08/</guid>
      <description>&lt;h2 id=&#34;part-ssh&#34;&gt;Part SSH&lt;/h2&gt;
&lt;p&gt;To use ssh for UF server, eduVPN app must be downloaded.&lt;/p&gt;
&lt;p&gt;The tutorial is in &lt;a href=&#34;https://docs.rc.ufl.edu/access/federated_login/?h=eduvpn#eduvpn-connection&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-08</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-08/</link>
      <pubDate>Wed, 08 Jan 2025 10:00:00 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-08/</guid>
      <description>&lt;ol start=&#34;3042&#34;&gt;
&lt;li&gt;Count Prefix and Suffix Pairs I&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-07</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-07/</link>
      <pubDate>Tue, 07 Jan 2025 00:38:22 -0500</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-07/</guid>
      <description>&lt;ol start=&#34;1408&#34;&gt;
&lt;li&gt;String Matching in an Array&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-06</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-06/</link>
      <pubDate>Mon, 06 Jan 2025 16:00:00 -0600</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-06/</guid>
      <description>&lt;ol start=&#34;1769&#34;&gt;
&lt;li&gt;Minimum Number of Operations to Move All Balls to Each Box&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-05</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-05/</link>
      <pubDate>Sun, 05 Jan 2025 10:22:44 -0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-05/</guid>
      <description>&lt;ol start=&#34;2381&#34;&gt;
&lt;li&gt;Shifting Letters II&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-04</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-04/</link>
      <pubDate>Sat, 04 Jan 2025 12:25:34 -0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-04/</guid>
      <description>&lt;ol start=&#34;1930&#34;&gt;
&lt;li&gt;Unique Length-3 Palindromic Subsequences&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-03</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-03/</link>
      <pubDate>Fri, 03 Jan 2025 16:55:18 -0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-03/</guid>
      <description>run ollama on the server &amp;amp; model tuning</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-03</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-03/</link>
      <pubDate>Thu, 02 Jan 2025 23:11:27 -0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-03/</guid>
      <description>2270. Number of Ways to Split Array</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-02</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-02/</link>
      <pubDate>Thu, 02 Jan 2025 20:00:00 -0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-02/</guid>
      <description>update hugo to fix bugs</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-02</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-02/</link>
      <pubDate>Thu, 02 Jan 2025 13:12:57 -0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-02/</guid>
      <description>2559. Count Vowel Strings in Ranges</description>
    </item>
    <item>
      <title>Bug Journal 2025-01-01</title>
      <link>https://tzj2006.github.io/bugjournal/2025-01-01/</link>
      <pubDate>Wed, 01 Jan 2025 21:27:04 -0800</pubDate>
      <guid>https://tzj2006.github.io/bugjournal/2025-01-01/</guid>
      <description>create hugo website &amp;amp; Install condo</description>
    </item>
    <item>
      <title>LLM Study</title>
      <link>https://tzj2006.github.io/posts/llm-study/</link>
      <pubDate>Wed, 01 Jan 2025 16:01:34 -0800</pubDate>
      <guid>https://tzj2006.github.io/posts/llm-study/</guid>
      <description>&lt;h2 id=&#34;part-chatgpt-4o&#34;&gt;Part ChatGPT 4o&lt;/h2&gt;
&lt;h3 id=&#34;chatgpt-4o-api-demo&#34;&gt;ChatGPT 4o API demo&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;openai&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;model_use&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt-4o-2024-08-06&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;api_key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Your-API-key&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;completion&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;beta&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;completions&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Extract the event information.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Alice and Bob are going to a science fair on Friday.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;event&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completion&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;message&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parsed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note: I tried to use model &amp;ldquo;gpt-4o&amp;rdquo; but failed.&lt;/p&gt;
&lt;h3 id=&#34;how-to-create-chatgpt-api-key&#34;&gt;How to create ChatGPT API Key&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Log in to &lt;a href=&#34;https://platform.openai.com/docs/overview&#34;&gt;openai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use the search bar to search &amp;ldquo;API keys&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Create a new secret key (Shown only once, invisible after closing the tab)&lt;/li&gt;
&lt;li&gt;Go to billing to add some credit to the account&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;part-unitox-chatgpt&#34;&gt;Part UniTox ChatGPT&lt;/h2&gt;
&lt;h3 id=&#34;read-from-fdagov&#34;&gt;Read from fda.gov&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Read the label of the drug we are interested in from a .csv file.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LeetCode Daily Question 2025-01-01</title>
      <link>https://tzj2006.github.io/leetcode/2025-01-01/</link>
      <pubDate>Wed, 01 Jan 2025 13:09:56 -0800</pubDate>
      <guid>https://tzj2006.github.io/leetcode/2025-01-01/</guid>
      <description>1422. Maximum Score After Splitting a String</description>
    </item>
    <item>
      <title>Random Ideas</title>
      <link>https://tzj2006.github.io/random/</link>
      <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://tzj2006.github.io/random/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;笔记本的 RAM 在关闭屏幕后还耗电吗&lt;/li&gt;
&lt;li&gt;markdown 插入图片无法在网站上自动显示&lt;/li&gt;
&lt;li&gt;Random 中加一个 checkbox&lt;/li&gt;
&lt;li&gt;desktop video 英文版 √&lt;/li&gt;
&lt;li&gt;孤波算法是什么&lt;/li&gt;
&lt;li&gt;desktop video 多语言切换 √&lt;/li&gt;
&lt;li&gt;是什么成就了一个奢侈品？&lt;/li&gt;
&lt;li&gt;Desktop Video 锁屏界面播放&lt;/li&gt;
&lt;li&gt;M4 pro V.S. M3 Max&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
