[{"content":"Linux 服务器下的指令 Docker 的安装和调试 Docker相当于一台虚拟机。安装之后就可以在这台虚拟机上跑代码了。\nDocker 的安装 Docker 的安装方式： 首先上 Dockerhub 挑选一个心仪的 docker, 下面以 nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 为例：\n然后运行以下代码：\ndocker run -it -rm\\ --name \u0026lt;your-instance-name\u0026gt; \\ --network host \\ nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 注：这里的 -it 指的是打开一个可交互界面，-rm 指的是用后即焚\n这时候就会自动下载 docker 并打开一个 bash 来用。\n现在你会发现这个虚拟机里面什么都没有，所以就需要 apt-get install\n另外，如果你的宿主机器的根目录比较小，想要挂载一个硬盘的话，就在 docker run 中间加上：\n-v /path/to/large/storage:/somepath \\ 这样就可以在 somepath 下挂载这个硬盘了。\n注：不能挂载在根目录下，必须挂载在一个文件夹下\n这里在测试的时候建议加上 -rm,这样不会产生很多个休眠中的 docker 但是在要频繁使用的时候不建议使用 -rm, 而是就让 docker休眠就好。\n不知道我以前在这里写的啥 应该是说，反正休眠的 docker 可以通过 docker start 唤醒，如果不是必要就别删了呗\ndocker 的调试 常用的 docker 指令： # 启动 docker docker start \u0026lt;容器ID或名字\u0026gt; # 关闭 docker docker stop \u0026lt;容器ID或名字\u0026gt; # 重启 docker docker restart \u0026lt;容器ID或名字\u0026gt; # 删除容器 docker rm \u0026lt;容器ID或名字\u0026gt; # 进入容器 docker exec -it \u0026lt;容器ID或名字\u0026gt; bash # 查看正在运行的容器 docker ps # 查看所有容器（包括停止的） docker ps -a # 列出本地镜像 docker images # 删除镜像 docker rmi \u0026lt;镜像ID或名字\u0026gt; # 挂载目录 docker run -v /host/path:/container/path # 增加环境变量 docker run -e HTTP_PROXY=http://localhost:10086 代理的使用 这里使用的是 xray。\nxray 是这样运行的：\n./xray run -c config.json 运行之后你就可以看到哪个端口放开了，就可以在哪个端口上使用代理,比如 port: 10086。\n这时候如果你想使用代理就需要：\n输入几行代码\n这样你的下载就会走代理辣。\n非常重要 (大坑)\napt-get install 对代理的要求较高，没那么稳定的代理会很稳定的挂，报 Error 503 Service Unavailable. 这时候就直接换清华源就行了，别使用代理了，等之后下别的再用。\ndocker初始化 #!/bin/bash set -e echo \u0026#34;🔧 更新 apt 并安装基础工具...\u0026#34; apt update \u0026amp;\u0026amp; apt install -y \\ vim \\ git \\ curl \\ wget \\ ca-certificates \\ software-properties-common \\ build-essential \\ htop \\ unzip \\ tmux \\ sudo \\ lshw \\ libgl1-mesa-glx \\ libegl1-mesa-dev \\ libosmesa6-dev \\ patchelf \\ cmake \\ build-essential echo \u0026#34;安装 conda...\u0026#34; cd /work curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # 下载过了之后就可以删除或者注释掉 /work/miniconda3/bin/conda init source ~/.bashrc echo \u0026#34;修改 huggingface 下载路径\u0026#34; cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt;\u0026gt; ~/.bashrc export CACHE_ROOT=\u0026#34;/work/models\u0026#34; export HF_HOME=\u0026#34;$CACHE_ROOT\u0026#34; export HF_HUB_CACHE=\u0026#34;$CACHE_ROOT/hub\u0026#34; export HF_DATASETS_CACHE=\u0026#34;$CACHE_ROOT/datasets\u0026#34; export HF_ASSETS_CACHE=\u0026#34;$CACHE_ROOT/assets\u0026#34; export TRANSFORMERS_CACHE=\u0026#34;$CACHE_ROOT/transformers\u0026#34; export CUDA_VISIBLE_DEVICES=0,1,2,3 EOF 注意：不要在~/.bashrc 中添加 source ~/.bashrc，而是就运行一遍 source ~/.bashrc 就可以了 否则蹦服警告⚠️\ndocker 中关闭某个程序 这里以 python 为例\n这里的假设是：现在关闭了 SSH, 找不到原本正在运行的 terminal 了\n那我们就可以用这个方法寻找：\nps aux | grep python or\nps -eo pid,cmd | grep python 这样我们就可以定位到我们要找的进程 PID 了\n之后 kill \u0026lt;PID\u0026gt; 即可\n不行就 kill -9 \u0026lt;PID\u0026gt;\nPython Python 的妙妙小方法 noqa 一个注释，用于忽略一些警告或者报错\n比如 # noqa: F401 就可以跳过 unimport error\ncprint 可以输出带有颜色的字符，记得pip install cprint\nConda \u0026amp; pip 安装Conda环境 conda --version conda create -n \u0026lt;yourname\u0026gt; python=\u0026lt;yourversion\u0026gt; -y conda activate \u0026lt;yourname\u0026gt; 自动激活与取消激活 base 环境 #修改默认配置 conda config --set auto_activate_base false\t# 默认不进入base环境 conda config --set auto_activate_base true\t# 默认进入base环境 conda 目录设置 conda config --remove envs_dirs /home/yourcondaenv # 移除不想要的路径 conda config --add envs_dirs /home/yourcondaenv # 添加新的 envs 目录 或者可以在 ~/.condarc 中修改这个参数\nenvs_dirs: - /path/to/your/env package 配置 scipy package scipy.misc.derivative 弃用 在 1.16.0 + 版本中，scipy 提供了\nscipy.differentiate.derivative 作为替代\n使用方法：\nfrom scipy.differentiate import derivative scipy._lib._util._lazywhere 弃用 请更换成 np.where\nrpy2 package rpy2 package 找不到 R library 可能问题 1： 环境中根本就没有 R 解决方案： 安装 R 或者 module load R\n可能问题 2 (当使用 jupyternotebook + Module load R 的时候出现)： 由于一些原因，jupyter notebook 先于 Module load R 调用 因此jupyter notebook 中的 R 的指针指向了一个不能被 user 访问的 R 解决方案： 换 Code Server 即可解决\nVisual Studio Code VS code 连接不上但是 SSH 能连接上： 原因： 原因分析\n结论：\nVS code 远端服务器版本和本地版本不一致。 网络环境导致无法同步。 断点续传机制导致无法通过重启解决问题。 解决方案：\n在 VS Code 中打开命令面板 快捷键：(Ctrl/Cmd + Shift + P) , 输入 Remote-SSH: Kill VS Code Server on Host… ，选择对应主机，强制终止并清除旧实例 ssh 登录远程服务器，输入 rm -rf ~/.vscode-server 清除远程服务器缓存 Jupyter Notebook 取消 Warning import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) 如果想要取消 python 的 warning, 可以在运行的时候输入：\npython -W ignore Hugging face Hugging face 数据下载 方法1：Git LFS 但是GIT LFS有时候会丢失一部分数据，不知道为什么\n比如我现在正在下载libero dataset, 但是他的 Git LFS clone 就只会下载 Libero 10, Libero Spatial, Libero Object, 和 Libero Goal, 并不会下载 Libero 100\n我认为是仓库的设置有点问题，因为 Git LFS clone 下来的数据格式和hugging face上下下来的格式有点不同。\n但是 Git LFS真的是最简单轻松的下载方式了\n只需要点击这里然后跟着操作做就完事了。\n下载的速度也比较快，基本跑满了这个服务器的代理网络。\n听说Github不能断点续传，但是实测下来几乎没有断点，真的是最稳定的那个服务了。\n方法2： hugging_face CLI huggingface-cli 是 Hugging Face 官方提供的命令行工具，自带完善的下载功能。\n但是实际用起来体验很差，经常莫名其妙就 Internet Error\n无论是使用代理还是镜像体验都不是很好。\n使用方法：\n代理：\npip install -U huggingface_hub pip install -U hf_transfer # 先下载 huggingface-cli 本体和 hf_transfer 加速插件 # hf_transfer插件真的很快，特别是在境外的服务器速度真的很快 export HF_HUB_ENABLE_HF_TRANSFER=1 # 打开 hf_transfer huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 镜像：\npip install -U huggingface_hub # 还是先安装这个 huggingface-cli export HF_ENDPOINT=https://hf-mirror.com # 这里以 hf-mirror.com 为例 # 剩下的都一样的 huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 方法3：snapshot_download 同样是 hugging face 出品，同样的容易崩溃\n区别是这个可以在 python 中使用\n使用也很简单：\nfrom huggingface_hub import snapshot_download snapshot_download( # repo_type=\u0026#39;dataset\u0026#39;, # 这一条就看你是不是下数据的时候选择加还是不加了 repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False, # 加上这一条可以所见即所得 # 不会出现最后是个指针文件的情况 ) 方法4: hf-mirror 镜像站下载 hf-mirror镜像站 推出了 hfd, 一个 huggingface 专用下载工具，基于成熟工具 aria2，可以做到稳定高速下载不断线。\n这是 hf-mirror 网站给出的 tutorial, 方法清晰简单: 1. 下载hfd\nwget https://hf-mirror.com/hfd/hfd.sh chmod a+x hfd.sh 2. 设置环境变量\n# Linux export HF_ENDPOINT=https://hf-mirror.com or\n# Windows Powershell $env:HF_ENDPOINT = \u0026#34;https://hf-mirror.com\u0026#34; 3.1 下载模型\n./hfd.sh gpt2 3.2 下载数据集\n./hfd.sh wikitext --dataset 根据实测，速度也不赖\n就是他的这个 Copy 有点问题，要一行一行的 Copy 才可以正常运行\n或者直接从我这里copy也行\n方法5: 手动下载 打开 files \u0026amp; Versions, 点击下载即可\n环境变量配置 代理开启和关闭命令： 开启：\nexport http_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export https_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTP_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTPS_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; 关闭：\nunset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY 模型下载地址设置 命令行版本 export CACHE_ROOT=\u0026#34;/work/models\u0026#34; export HF_HOME=\u0026#34;$CACHE_ROOT\u0026#34; export HF_HUB_CACHE=\u0026#34;$CACHE_ROOT/hub\u0026#34; export HF_DATASETS_CACHE=\u0026#34;$CACHE_ROOT/datasets\u0026#34; export HF_ASSETS_CACHE=\u0026#34;$CACHE_ROOT/assets\u0026#34; export TRANSFORMERS_CACHE=\u0026#34;$CACHE_ROOT/transformers\u0026#34; python版本 import os os.environ[\u0026#34;CACHE_ROOT\u0026#34;] = \u0026#34;/work/models\u0026#34; os.environ[\u0026#34;HF_HOME\u0026#34;] = os.environ[\u0026#34;CACHE_ROOT\u0026#34;] os.environ[\u0026#34;HF_HUB_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;hub\u0026#34;) os.environ[\u0026#34;HF_DATASETS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;datasets\u0026#34;) os.environ[\u0026#34;HF_ASSETS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;assets\u0026#34;) os.environ[\u0026#34;TRANSFORMERS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;transformers\u0026#34;) GPU 使用设定 export CUDA_VISIBLE_DEVICES=0,1,2,3 Ollama 安装 在 Linux 上，安装非常简单，只需要一行指令：\ncurl -fsSL https://ollama.com/install.sh | sh 环境配置 OLLAMA 核心配置 export OLLAMA_GPU_LAYER=cuda export OLLAMA_HOST=0.0.0.0 export OLLAMA_KEEP_ALIVE=-1 export OLLAMA_MAX_LOADED_MODELS=3 export OLLAMA_MODELS=/work/models export OLLAMA_NUM_GPU=8 export OLLAMA_NUM_PARALLEL=4 export OLLAMA_SCHED_SPREAD=1 Hugo Hugo如何置顶一篇文章 添加 weight 参数 e.g.:\n--- title: \u0026#34;My Post\u0026#34; date: 2020-09-15T11:30:03+00:00 weight: 1 --- 这里 weight 越小置顶优先级越高\n除了文章的 weight, 还可以设置 tag weight 在同一个 tag 下排序： tags_weight: 10\nHugo 如何打开侧边目录 在页面 yaml 处添加：\n--- ShowToc: true TocOpen: true --- ShowToc 为 True 表示展示目录 TocOpen 为 True 代表展开所有\nGadgets 妙妙小道具 PDF压缩 注：需要先安装这个 package: apt-get install ghostscript\ngs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/default -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf input.pdf GPU占用可视化 nvitop 一个好用的 GPU 占用显示插件 (based on nvidia-smi, 需要 Nvidia 驱动)：\npip install nvitop nvitop ssh ssh 连接 没有 ssh 怎么办，先看(这篇文章)[https://blog.csdn.net/GitHub_miao/article/details/135050696]\n想要免密登录怎么办，再看(这篇文章)[https://zhuanlan.zhihu.com/p/350160634]\n一言以蔽之，在有 ssh 的情况下：\n# 示例代码：生成SSH密钥对, 名字可以改的 ssh-keygen -t rsa -b 4096 -f ~/.ssh/my_key # 上传公钥 注意是公钥 scp-copy-id user@host # 这一步也可以通过 # 复制 .pub 文件到 ./ssh 文件下 # 然后把这个文件用 cat xx.pub \u0026gt;\u0026gt; authorized_keys # 这样就可以啦 最后，记得在本地 VSCode 里修改 ssh config\n# 示例代码：为远程主机配置别名 Host my_server HostName remote_server User user IdentityFile ~/.ssh/my_key AI 技巧 GPT Prompts 文章阅读 Prompt 请仔细阅读这篇文章，并告诉我： 1. 这篇文章的动机是什么，要解决什么问题 2. 这篇文章大概讲了什么 3. 这篇文章的创新点是什么 4. 这篇文章解决了什么问题，之前的人为什么不能解决 5. 这篇文章还有什么问题没解决 6. 这篇文章有什么需要我注意的点 7. 这篇文章是如何做实验的，setting 是什么 8. 这篇文章的算力要求是多少，多少卡运行了多久，用了什么数据集，是不是可以公开获取的，模型代码呢，能不能公开获取 如果这篇文章提出了一个模型，那请告诉我 (如果没有提出模型请告诉我为什么这篇文章不需要提出新模型)： 1. 这个模型的输入是什么 2. 输出是什么 3. 输入和输出数据经过了什么处理 4. 这个模型是如何处理输入和输出数据的 Tensorboard 清除 tensorboard 端口占用： 这条适用于 tensorboard 端口占用无法打开新的 tensorboard 的时候\nkill $(ps -e | grep \u0026#39;tensorboard\u0026#39; | awk \u0026#39;{print $1}\u0026#39;) Mojoco Mujoco GPU 使用设置 MUJOCO_GL=egl # GPU render MUJOCO_GL=OSMesa # CPU render MUJOCO_EGL_DEVICE_ID=0 # Mujoco Device set Mujoco 多 GPU 配置 Mujoco 默认只会调用主 GPU, 也就是 GPU 0\n那如何让 Mujoco 调用其他的 GPU 呢\n很简单，只需重装一下驱动即可\n首先先 nvidia-smi一下看看 Nvidia-driver 是什么版本的， 这里以 CUDA 12.0; Nvidia-driver Version 525 为例\n直接\napt-get install nvidia-driver-525 如果遇到 Driver 配置问题，请见此处的解决方案\n软链接 有时候，我们会因为一些原因把一份文件复制得到处都是\n但是大部分时候，这种文件都非常的大\n那复制到很多个地方就会占用掉很多的磁盘空间\n在这种情况下呢，软连接就起作用了\n软连接可以理解为桌面快捷方式，其实就是一个指向真正文件的指针\n那要怎么做呢？\n很简单，只要：\nln -s 即可。\n具体用法：\nln -s /original_file /target_file 这样的话就可以把original_file 链接到 target_file那里去\n这样就可以在target_file 中访问 original_file 中的内容辣。\nBug Fix 环境配置 bug 如果是 apt-get 导致的冲突，请先试试 apt --fix-broken install\nPackage 过期 bug scipy package scipy.misc.derivative 弃用 在 1.16.0 + 版本中，scipy 提供了\nscipy.differentiate.derivative 作为替代\n使用方法：\nfrom scipy.differentiate import derivative GPU 环境配置 bug nvidia-driver版本冲突 通常发生在重装 nvidia-driver 之后\n无法通过 apt --fix-broken install 解决\n这时候可以：\n方法 1：\nsudo dpkg --purge --force-all \\ nvidia-driver-525 \\ nvidia-dkms-525 \\ xserver-xorg-video-nvidia-525 \\ libnvidia-decode-525 \\ libnvidia-encode-525 sudo apt-get clean sudo apt-get update sudo dpkg --configure -a sudo apt-get -f install sudo apt-get autoremove --purge -y sudo apt-get install aptitude sudo aptitude purge \u0026#39;~i nvidia-*\u0026#39; sudo aptitude safe-upgrade 方法 2：\nsudo apt-mark unhold libnvidia-* nvidia-* # 先解除 hold sudo apt-get remove --purge \\ libnvidia-* nvidia-* \\ --allow-change-held-packages # 然后完成删除整条依赖链 sudo dpkg --configure -a sudo apt --fix-broken install sudo apt autoremove -y sudo apt clean # 最后让 apt 来收尾 方法3 (强行复写 apt package, 不建议在物理机上使用):\n# 先让 dpkg 走“强覆写”把损坏状态收尾 sudo apt-get -o DPkg::Options::=\u0026#34;--force-overwrite\u0026#34; \\ --fix-broken install # 如果还有半配置包，再跑一次 sudo dpkg --configure -a SSH bug 远程 X server 有时候可以用 X server 来把服务器上的窗口回传到本地：\n使用方法如下：\nssh -X name@ip 或者在 VS Code 中修改 config:\n在 config 中增加如下信息：\n# 相当于 ssh -X ForwardX11 yes # 使用受信任的 X11 转发（相当于 ssh -Y） ForwardX11Trusted yes # 如果 xauth 不在默认路径，可指定其位置（macOS 安装 XQuartz 后） # XAuthLocation /opt/X11/bin/xauth Hugging face bug 遇到 API 报错时的解决方案 详细说明：\n“requests.exceptions.MissingSchema: Invalid URL \u0026#39;/api/resolve-cache/models/openai/clip-vit-base-patch16/57c216476eefef5ab752ec549e440a49ae4ae5f3/config.json?%2Fopenai%2Fclip-vit-base-patch16%2Fresolve%2Fmain%2Fconfig.json=\u0026amp;etag=%229f7102db4ae77c02982bfec1c16a63039fbc78db%22\u0026#39;: No scheme supplied. Perhaps you meant https:///api/resolve-cache/models/openai/clip-vit-base-patch16/57c216476eefef5ab752ec549e440a49ae4ae5f3/config.json?%2Fopenai%2Fclip-vit-base-patch16%2Fresolve%2Fmain%2Fconfig.json=\u0026amp;etag=%229f7102db4ae77c02982bfec1c16a63039fbc78db%22?” 首先先看看 hugging face hub 有没有更新\npip install -U huggingface_hub transformers MacOS 下的指令 Gadgets 小组件 硬盘 硬盘信息 brew install smartmontools # 安装 smartctl -a disk0 # 使用 效果： 软件占用信息 brew install macmon # 安装 macmon # 使用 效果： 彩蛋 这个 blog 的时间是 2011 年 11 月 11 日 11 点 11 分 11 秒，时区是 UTC + 11:11\n这个时间是故意设定的最近最长连续数字时间，一共刚好 16 个 1\n","permalink":"https://tzj2006.github.io/bugjournal/commanddictionary/","summary":"All the usefull commands","title":"Command Dictionary"},{"content":"我现在的思路是这样的： 之前我们发现了 Libero 作为 benchmark 的一些可能的不合理的点\n数据太少 任务简单 没有长程任务 以及 DMPEL 作为 lifelong task 的一些可能的不合理的点\nlifelone task 的定义可能太过狭隘 (只在训练过的 task 上能够表现好) 方法可能聚焦于如何把 LIBERO 数据集的榜刷高，但是场景变化一点点就无法解决问题 所以我现在想做的是： 用一些 数字/分数 来证明我的猜想是成立的 比如：\n在场景变换的时候 DMPEL 成功率的下降 当任务变复杂的时候 DMPEL 成功率的下降 现在我已经做的是：\n对于开柜子的任务，无论是柜子移动，旋转，还是更换语义相同但是词语不同的句子，DMPEL 的成功率都会骤降 (100% -\u0026gt; 5%) 但是，开柜子的 task 可能过难了， 所以我现在准备测试 pick and place 的 task 当物品移动，旋转，以及更换语义相同但是词语不同的句子的时候 DMPEL 的成功率会下降多少\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-08/","summary":"\u003cp\u003e我现在的思路是这样的：\n之前我们发现了 Libero 作为 benchmark 的一些可能的不合理的点\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e数据太少\u003c/li\u003e\n\u003cli\u003e任务简单\u003c/li\u003e\n\u003cli\u003e没有长程任务\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e以及 DMPEL 作为 lifelong task 的一些可能的不合理的点\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003elifelone task 的定义可能太过狭隘 (只在训练过的 task 上能够表现好)\u003c/li\u003e\n\u003cli\u003e方法可能聚焦于如何把 LIBERO 数据集的榜刷高，但是场景变化一点点就无法解决问题\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e所以我现在想做的是：\n用一些 数字/分数 来证明我的猜想是成立的\n比如：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e在场景变换的时候 DMPEL 成功率的下降\u003c/li\u003e\n\u003cli\u003e当任务变复杂的时候 DMPEL 成功率的下降\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e现在我已经做的是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e对于开柜子的任务，无论是柜子移动，旋转，还是更换语义相同但是词语不同的句子，DMPEL 的成功率都会骤降 (100% -\u0026gt; 5%)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e但是，开柜子的 task 可能过难了，\n所以我现在准备测试 pick and place 的 task\n当物品移动，旋转，以及更换语义相同但是词语不同的句子的时候 DMPEL 的成功率会下降多少\u003c/p\u003e","title":"Bug Journal 2025-07-08"},{"content":"\n这是现在的成果，总之是让环境里的物品移动了位置了\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-30/","summary":"\u003cp\u003e\u003cimg alt=\"1751276858157\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-30/1751276858157.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是现在的成果，总之是让环境里的物品移动了位置了\u003c/p\u003e","title":"Bug Journal 2025-06-30"},{"content":"DMPEL \u0026amp; Libero 环境的总结和疑惑 在 DMPEL 模型 和 Libero 模拟环境的设定下，他们是这样设定他们的训练和验证集的：\n训练 train：\n首先，他们会在一个 90 条数据的大数据集上先做预训练，得到他们的 backbone 模型 然后对于4个 各 10 条数据的小数据集，他们会在这 4 个数据集上分别做训练，作为 lifelong learning training\n验证 validation：\n之后他们会对他们的训练做验证，验证方式如下： 对于第 x 个 lifelong learning task, 他们会计算训练这个 task 的时候的 10 个 epoch 的平均成功率，作为他们的成功指标 1 （希望模型学得越快越好） 他们还会计算前 x 个 task 的平均成功率，作为他们的成功指标 2 （防止模型遗忘）\n但是在这里我就产生了一个疑问🤔： 我认为这种设置并不非常合理\n他们没有对预训练的那 90 个 task 去做遗忘测试 他们的 task 的定义似乎有点太窄了，比如：都是对于“打开盒子的第一个盖子”， 盒子放在环境的左上角和左下角就算是两个 task,感觉泛化能力也不太行 在预训练的时候那 90 个 task 在设定的时候会比 lifelong task 复杂 (训练的时候一般有 3 个步骤， 但是 lifelong task 只有 1 个，比如“把碗放进抽屉并合上” VS \u0026ldquo;打开抽屉\u0026rdquo;)， 但是他们的模型在 Zero-shot lifelong task 的时候表现也很差，很少有成功的例子 在可视化结果的时候，我发现其实模型并没有理解要做什么，而是顺着之前的 task 做。 比如，上一个 task 做的是“抓起一个碗”，下一个是“打开柜子”，在一开始，机械臂会一直想去抓碗而不是开柜子 感觉 Libero 这个数据集的数据量也不是很大，难度也不是很高，(1-2 epoch 就可以让成功率 \u0026gt; 90%) 导致了像上面那样的 overfit 的情况 测试 于是我做了更多的测试：\n现在这是一个 Libero_long 的测试，任务难度会比之前更高\n目前这个任务是：“打开炉子并且把烧水壶放在炉子上”\n您的浏览器不支持 video 标签。 但是我们看到了：\n您的浏览器不支持 video 标签。 直接拿锅的机械臂\n您的浏览器不支持 video 标签。 没开炉子的机械臂\n您的浏览器不支持 video 标签。 摔倒了之后不知所措的机械臂\n之后我们再来看看所谓成功的 task\n您的浏览器不支持 video 标签。 放上去就是成功（\n最后，是真正成功的 task\n您的浏览器不支持 video 标签。 接下来该做的方向：\n首先，我认为 Libero 的设定不太合理\n所以我们不去刷 Libero 的榜\n我猜测已有模型的泛化能力不行，benchmark设置的不合理\n所以我要通过实验依据作证清楚，然后提出更合理的设置和新的评估benchmark\n最后在此基础上研究我的新方法\n那现在有这些方向可以走：\n如果我把任务分解和lifelong结合起来，分解primitive，直接跳出现在这些paper， 那就是很好的paper\n总结：\n这是我现在要做的事情：\n首先先研究 benchmark 设置，看看怎么样的设置能够把上面说的方向结合进来\n然后再看看现有的 method 在这些 setting 下表现如何\n最后看看我的方法比起这些方案有多少进步\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-26/","summary":"DMPEL \u0026amp; Libero","title":"Bug Journal 2025-06-26"},{"content":"Libero dataset 的可视化 发现了一个讲 Libero 讲得很详细的 blog 强烈推荐阅读这篇 blog！\n一共有 9 讲，9 讲的链接全部在这一讲中写了，直接点感兴趣的讲看就 ok 了\n但是，纸上得来终觉浅，绝知此事要躬行 有一些部分可能因为环境的不同不能适应所有情况，比如 headless server (没有显示输出的 server) 所以下面我会根据我的测试写针对 headless server 的 Libero dataset 可视化\n我的 Libero 测试报告 首先，第一个要注意的点就是：.libero 文件夹在 主目录下 (我认为此处说 根目录 下更准确) 也就是在 ~/ 下, 而不是 clone libero 下来的主目录。\n剩下的就很丝滑，所有的测试都平平无奇的通过了\n直到\u0026hellip;\n真正开始运行 Robosuit Demo 的时候，我遇到了困难：\n在运行完这段代码之后 python 就会直接崩溃：\nenv = suite.make( env_name=\u0026#34;Lift\u0026#34;, # try with other tasks like \u0026#34;Stack\u0026#34; and \u0026#34;Door\u0026#34; robots=\u0026#34;Panda\u0026#34;, # try with other robots like \u0026#34;Sawyer\u0026#34; and \u0026#34;Jaco\u0026#34; has_renderer=True, has_offscreen_renderer=False, use_camera_obs=False, ) 这是因为我正在用的服务器上并没有可以 render 的设备， 没法打开一个新窗口，所以就会直接崩溃\n因此这里要改成这样：\n# %% import numpy as np import robosuite as suite # MuJoCo gets imported *inside* robosuite # Camera frames are returned in the observation dict because we pass use_camera_obs=True env = suite.make( env_name=\u0026#34;Lift\u0026#34;, robots=\u0026#34;Panda\u0026#34;, has_renderer=False, # no on-screen window has_offscreen_renderer=True, # build a single off-screen context use_camera_obs=True, # include images in obs dict camera_names=(\u0026#34;frontview\u0026#34;,), # at least one camera is required when use_camera_obs=True camera_heights=480, camera_widths=640, ) obs = env.reset() # robosuite allocates context here # The observation dict now has an RGB image rgb = obs[\u0026#34;frontview_image\u0026#34;] # shape (480, 640, 3), dtype uint8 print(\u0026#34;Initial RGB image shape:\u0026#34;, rgb.shape) 下一步就是模拟机械臂操作，一共随机 1000 步：\n# %% frames = [] for _ in range(1000): a = 0.1 * np.random.randn(env.robots[0].dof) # small random torques obs, reward, done, info = env.step(a) frames.append(obs[\u0026#34;frontview_image\u0026#34;]) frames = [np.rot90(frame, k=2) for frame in frames] print(f\u0026#34;Collected {len(frames)} frames\u0026#34;) 但是从这里采集出来的图片是上下颠倒的\n这是因为 Mujoco 和 numpy 坐标系的原点位置不一样\n所以我们还得对这个 frame 做一个 180° 的颠倒\n现在，我们就可以输出我们的结果了：\n比如这是第一帧：\nimport matplotlib.pyplot as plt plt.imshow(frames[0]) plt.axis(\u0026#34;off\u0026#34;) 那至于我们要如何把这些帧变成一个视频呢？\n我们可以通过 imageio 这个 package 来实现\nimport numpy as np import imageio # \u0026#39;frames\u0026#39; is your list of (480, 640, 3) uint8 RGB arrays collected earlier # e.g., frames = [obs[\u0026#34;frontview_image\u0026#34;] for _ in range(1000)] # Create an MP4 writer at 120 fps writer = imageio.get_writer(\u0026#34;simulation.mp4\u0026#34;, fps=120) # wraps ffmpeg :contentReference[oaicite:6]{index=6} # Append each frame for frame in frames: writer.append_data(frame) # send numpy array to ffmpeg :contentReference[oaicite:7]{index=7} # Finalize and close the file writer.close() # flushes buffers and writes trailer :contentReference[oaicite:8]{index=8} 您的浏览器不支持 video 标签。 最终，我们通过 moviepy 来实现 jupyter notebook 中播放视频的效果：\nfrom moviepy.editor import VideoFileClip # 载入前面生成的 simulation.mp4 clip = VideoFileClip(\u0026#34;simulation.mp4\u0026#34;) print(\u0026#34;原始视频时长：\u0026#34;, clip.duration, \u0026#34;秒；分辨率：\u0026#34;, clip.size) clip.ipython_display(width=640) 在 DMPEL 中的可视化 那么至此我们可以发现：其实想做一个可视化非常的简单\n只需要找到创建 env 的地方，每一次出现 env.step 的时候我们就记录一下就可以了\n比如上面是\nobs, reward, done, info = env.step(a) frames.append(obs[\u0026#34;frontview_image\u0026#34;]) 那我们就也学着这里这样做就行了\n不过呢，有一个小区别：一个是这里是一个并行的环境\n更具体地说：为了保证 CPU 和 GPU 的利用率，DMPEL 设置了 20 个环境并行运行\n那么我们就要根据环境的不同设置不同的输出路径\n周游这个区别而已。\n另外呢，因为我想看一下这个 模型到底是在什么时候成功，又是在什么时候失败\n所以对于所有成功和失败的 task,我会分开两个文件夹存储\n好的，现在我们就完成了 DMPEL 的可视化\n现在，我们获得了一些结果：\n您的浏览器不支持 video 标签。 ❌ 完全打不开柜子的1号机械臂\n您的浏览器不支持 video 标签。 ✅ 犹犹豫豫打开正确柜子的2号机械臂\n您的浏览器不支持 video 标签。 ❌ 打开错误柜子的3号机械臂\n您的浏览器不支持 video 标签。 ✅ 干脆利落打开正确柜子的4号机械臂\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-25/","summary":"\u003ch3 id=\"libero-dataset-的可视化\"\u003eLibero dataset 的可视化\u003c/h3\u003e\n\u003ch4 id=\"发现了一个讲-libero-讲得很详细的-blog\"\u003e发现了一个讲 Libero 讲得很详细的 blog\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://blog.csdn.net/weixin_53610475/article/details/136421802\"\u003e强烈推荐阅读这篇 blog！\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e一共有 9 讲，9 讲的链接全部在这一讲中写了，直接点感兴趣的讲看就 ok 了\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e但是，纸上得来终觉浅，绝知此事要躬行\n有一些部分可能因为环境的不同不能适应所有情况，比如 headless server (没有显示输出的 server)\n所以下面我会根据我的测试写针对 headless server 的 Libero dataset 可视化\u003c/p\u003e\n\u003ch4 id=\"我的-libero-测试报告\"\u003e我的 Libero 测试报告\u003c/h4\u003e\n\u003cp\u003e首先，第一个要注意的点就是：.libero 文件夹在 \u003cstrong\u003e主目录下\u003c/strong\u003e (我认为此处说 \u003cstrong\u003e根目录\u003c/strong\u003e 下更准确)\n也就是在 \u003ccode\u003e~/\u003c/code\u003e 下, 而不是 clone libero 下来的主目录。\u003c/p\u003e\n\u003cp\u003e剩下的就很丝滑，所有的测试都平平无奇的通过了\u003c/p\u003e\n\u003cp\u003e直到\u0026hellip;\u003c/p\u003e\n\u003cp\u003e真正开始运行 Robosuit Demo 的时候，我遇到了困难：\u003c/p\u003e\n\u003cp\u003e在运行完这段代码之后 python 就会直接崩溃：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eenv\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esuite\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emake\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eenv_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Lift\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e# try with other tasks like \u0026#34;Stack\u0026#34; and \u0026#34;Door\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003erobots\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Panda\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# try with other robots like \u0026#34;Sawyer\u0026#34; and \u0026#34;Jaco\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ehas_renderer\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ehas_offscreen_renderer\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003euse_camera_obs\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这是因为我正在用的服务器上并没有可以 render 的设备，\n没法打开一个新窗口，所以就会直接崩溃\u003c/p\u003e","title":"Bug Journal 2025-06-25"},{"content":"这两天做了啥： Basically, 做了这些事情：\n发了一个issue Hi DMPEL team, thanks for releasing the code! I’ve managed to run the full lifelong-learning pipeline on the libero_goal benchmark, but the success rates I obtain are lower than those reported in the paper. I’d like to confirm whether I’m missing any important tricks or recommended hyper-parameter settings.\nReproduction details\nCommand: bash exp_scripts/lifelong_scripts/dmpel.sh:\ntorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth GPU: 1 x A100 CUDA / Driver: CUDA12.0, Driver Version 525.147.05 python: 3.8.13 package: same as requirements.txt seed: 100\nMy Results\nMethod Paper’s success rate (LIBERO-Goal, 10 tasks) Re-run success rate DMPEL ≈ 28 % (Figure 5, “Lifelong” column) \u0026lt; 5 % ISCIL ≈ 23 % (same figure) \u0026lt; 5 % Additional findings\nPre-trained checkpoint sanity-check The provided multitask checkpoint matches the paper on its pre-training tasks (good success rate, most of them over 85%). Questions\nIs there a specific environment configuration (e.g., in MuJoCo or Robosuite) that significantly impacts performance? Do specific seeds, task-order curriculum strategies, or “hidden” hyperparameters (beyond what’s documented) affect performance in a meaningful way? Could you advise on recommended approaches or tools for visualizing results (e.g., exporting the robot move to .mp4 files) to diagnose where the pipeline may be underperforming? 测试了一下 DMPEL 的结果 首先，要运行他们的pipeline, 有几个地方的 code 是一定要改的：\n一个是 libero/libero/main.py 69 行 for i inrange(n_manip_tasks): 这个 for 循环里面\n要把 try 中的 get_dataset 函数的 demos parameter 删除或者注释掉\n之后在下面 90 行左右有个 manip_datasets.append(task_i_dataset) 也要移动到 try 里面去\n不然的话这个 try except 根本就没有作用，该报错还是报错\n同样，在 libero/libero/dataset.py 这个文件中的 get_dataset 函数的 demos 参数也要删除/注释掉\n因为 robomimic.utils.dataset.SequenceDataset 里面根本就没有 demos 参数。\n这样的话，这份代码就可以正常跑起来了\nP.S. 运行用的代码是这一条：\nbash exp_scripts/lifelong_scripts/dmpel.sh: i.e. torchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth 我发现，诶，怎么结果这么差？\n然后我思考了一下🤔\n认为可能是 pretrain checkpoint 有问题\n所以我就去试了一下 pretrain checkpoint\n验证方式也非常简单，就是直接拿 pretrain checkpoint 去做 Libero_90, 也就是 pretrain checkpoint 训练的时候用的数据集\n这要怎么做呢？\n很简单，只要稍微改变一下运行的命令就可以了：\ntorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_90 \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/90 \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth 没毛病吧，一下就从 Libero_goal 变回 Libero 90 了\n结果非常的 Amazing 啊，发现这个算出来还挺正常的，绝大部分的成功率都在 80% 以上\n注：其实很多点都重合在一起了，能看到的都是 outliers 🤦‍♂️\n那就说明问题一定是出在 lifelong learning 这一步了\n所以我就测试了一下 其他的 lifelong learning methods.\n结果发现，除了 ER 有一点起色之外，其他 lifelong learning methods 都是 0% 成功率\n这就表明一定是某处的设定有问题了，但是我找了一天也没发现到底是哪里有问题\n所以我就写了上面那个 issue 给 DMPEL 的团队\n最后发现竟然是 train.epoch 被设置成了 0\n所以最后的结果就是 Pretrain model 在做 zero-shot!\n在此之前，我其实做了一个实验：如果我执意让 Pretrain model 做 Zero-shot, 确实得到了一个非常相近的结果\n但是我没有怀疑到 epoch = 0 上面来，因为在测试的时候有一个点干扰了我的判断：\n在 Pretrain model Zero shot test 中, GPU 是基本没有占用的\n但是在之前的 test 中, 哪怕 train.epoch = 0 也仍然是有一个训练的 epoch 的\n只不过那个训练出来的结果并没有被拿出来 validate\n或者说就算拿出来 validate 了效果也不算好\n因此这个点干扰了我的判断。\n最终，我得到了一个和论文中相近的结果：\n在这个lifelong learning训练过程中, 显存占用不超过 20GB, 所以只要是 3090 + 级别的显卡都能运行\n最终，这两天我终于成功跑通了 DMPEL 的代码\n下一步我准备可视化他们的代码，看看哪里做得好，哪里做得不好，所以我得去研究一下 Libero 数据集了。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-24/","summary":"\u003ch3 id=\"这两天做了啥\"\u003e这两天做了啥：\u003c/h3\u003e\n\u003cp\u003eBasically, 做了这些事情：\u003c/p\u003e\n\u003ch4 id=\"发了一个issue\"\u003e发了一个issue\u003c/h4\u003e\n\u003cp\u003eHi DMPEL team, thanks for releasing the code! I’ve managed to run the full lifelong-learning pipeline on the \u003ccode\u003elibero_goal\u003c/code\u003e benchmark, but the success rates I obtain are lower than those reported in the paper. I’d like to confirm whether I’m missing any important tricks or recommended hyper-parameter settings.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eReproduction details\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCommand:\nbash exp_scripts/lifelong_scripts/dmpel.sh:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        policy=bc_foundation_dmpel_policy lifelong=dmpel \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eGPU: 1 x A100\nCUDA / Driver: CUDA12.0, Driver Version 525.147.05\npython: 3.8.13\npackage: same as requirements.txt\nseed: 100\u003c/p\u003e","title":"Bug Journal 2025-06-24"},{"content":"WoW, 6 月 20 了呢，过得好快(✧∀✧)\n上次看到的时候还是 5 月初、5 月底，现在一下就 6 月 20 号了。\n算起来今天是来实验室的一个月了，该总结一下了。\n这个月看了很多和机器人、具身智能有关的文章，这是我目前对这个领域的总结：\n目标： 目标和做视频生成的其实差不多，都是根据一个条件(环境)生成下一部分东西(机器人 move) 这里说和视频生成差不多其实是因为视频生成的维度比文字生成要大得多，更加接近机器人 move 的维度 维度这个问题还是很恐怖的，因为机器人是在一个连续的三维空间中的，所以理论上来说有无限的自由度。 虽然说目前 FP8 的精度其实已经非常非常够用了，但你想，机器人的自由度非常高，至少有 6 - 7 DoF. 那如果每一个 DoF 都是 1000 个度量，那么一共就至少会有 $10^{18}$ 种不同的情况。 这就会让传统的 SoftMax 很难招架住了，因为这种情况得要一个 $10^{18}$ head 的 MLP 才行 所以这时候预测的值就不再是一个 softmax了，而是像 VAE 那样的一个 Distribution,只有这种 Distribution 才不会出问题 所以从目前我的角度来看，其实做机器人和做 LLM 完全就是一回事，唯二的区别是: LLM 很容易获得大量知识，许多 LLM 都能获得互联网级别的数据，而且现在的多模态技术使得几乎互联网上的所有数据都能被喂到 LLM 中。反观机器人，目前机器人在显示场景中的数据仍然非常依赖人工采集，因此数据不可能有非常多。当然，对于做学术的我来说这个不是问题，因为在学术界想要获得互联网级别的数据，和能够运行这么多数据的计算资源也是极为困难的。因此对于我们来说，其实做小数据大成果也许是一个更好的选择。 第二个区别其实就是把最后一层的 softmax 给他去掉，然后让两个 Distribution 做 loss. 学到的知识： 这是一个很大的一个板块了，但是我尽量不重不漏地把我学的全部写在这里。\n首先，目前有两个主流的机器人训练方法\n行为模仿 Behavior Cloning (下面简称 BC) 强化学习 Reinforcement Learning (下面简称 RL) 当然，也出现了这两个的有机结合，比如：先 BC 再 RL, 先 RL 再 BC, 动态分配 RL 和 BC 的比例。。。\n总之，一般来说，RL 更加难训练，BC 更加容易训练。RL 的效果上限更高，但是也更容易过拟合。\n另外，目前也有两个主流的机器人训练模型\n自回归 (Auto-regression) Diffusion policy (and/or Flow matching) 自回归理解起来比较容易，其实只要理解一个式子就行：\n$$ X_{t_i} = F(X_{t_{i-1}}, X_{t_{i-2}}, \\dots) = \\alpha_{t_{i-1}}X_{t_{i-1}} + \\alpha_{t_{i-2}}X_{t_{i-2}} + \\dots $$\n对吧，这就是自回归的意思：对于自己做回归，让所有前面的状态影响到后面的状态\n突然想起以前人们对 人生 和 DP 的总结：\n人生就像 DP, 不一定要走局部最优解，但要走全局最优解 但是我认为人生更像自回归，因为你并不能从其他状态转移过来，你只能从你之前的时刻回归到你现在的时刻。\n至于 Diffusion Policy, 他的思路也很简单：\n图片 from weijian.sun\n去马赛克。\n意思是：既然我们可以通过给一段数据不断加噪音得到一个完全噪音，那我们也可以通过一个噪音反过来还原一段数据。\n这个理念其实在很早就已经出现过了，不管是 GAN 还是 VAE 都用到了这个 Idea.\nThat is. 如果我们有一个函数从一个随机数映射到一个域中，这个域中的每一个数都是我想要的\n那每一次我就只需要随机生成一个数，经过这个函数之后就能得到我想要的东西了，有点点石成金的意思了。\n那 Diffusion 的关键是什么。其实还是数据。\n用 Diffusion 我很轻松就可以用一段数据弄出 1000 个数据，并且这 1000 个数据都是比较优质的数据：\n假设这段数据和噪声之间的差是$\\Delta$, 那每一次我只需要在这段数据上加上$\\frac{\\Delta}{1000}$不就创造了一个新的数据出来吗？\n那我现在要学的东西是：我让一个模型去预测这个$\\frac{\\Delta}{1000}$, 不就完事了吗。\n那为什么这里要设定 1000 步这么多呢，这是因为：模型在去噪的能力上其实并不是无敌的，而是有一个极限的\n这个极限在这里是马尔科夫的小方差近似。\n什么意思呢\n首先我们先回顾一下 Diffusion 在做什么\nDiffusion 的本质是在求解一个函数到另一个函数的映射\n在这里，所有的数值都是抽象的，是连续的，所以必须由一个函数来表示。\n但是计算机中的模型是离散的，所以我们要通过随机采样来近似。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-20/","summary":"Jornal","title":"Bug Journal 2025-06-20"},{"content":"今天真的是 journal 了\n因为其实没有看文章，只是在解决一些工程上的问题。\n首先是数据下载，这个下了好久才下下来\n而且下的时候还出了很多 bug, 但总之已经搞定了\n下一步就是要去把代码跑通，不过这个估计得明天才能弄了，毕竟这个数据还有 3 个小时才下完\n这个代码我看了一下他给的 pipeline,我发现：\n首先要跑一个 pretrain model 这个 model 不是 baseline,就是要 pretrain 之后让模型有个初步认知\n之后才能加 MOE 进去跑 continue learning\n我看到 hugging face 上面有他的跑好的 check point,到时候下下来看一下\n到时候有机会的话自己也跑一跑他的 pretrain 的那部分，积累一点经验。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-19/","summary":"Journal","title":"Bug Journal 2025-06-19"},{"content":"今天折腾了一天的数据下载，踩了一些坑，这里记录一下。\n这是一篇不错的笔记\n首先是从Hugging Face下载数据，目前看起来最稳定的最简单方式就是Git LFS clone.\n所以我们先来介绍一下Git LFS\n方法1：Git LFS 但是GIT LFS有时候会丢失一部分数据，不知道为什么\n比如我现在正在下载libero dataset, 但是他的 Git LFS clone 就只会下载 Libero 10, Libero Spatial, Libero Object, 和 Libero Goal, 并不会下载 Libero 100\n我认为是仓库的设置有点问题，因为 Git LFS clone 下来的数据格式和hugging face上下下来的格式有点不同。\n但是 Git LFS真的是最简单轻松的下载方式了\n只需要点击这里然后跟着操作做就完事了。\n下载的速度也比较快，基本跑满了这个服务器的代理网络。\n听说Github不能断点续传，但是实测下来几乎没有断点，真的是最稳定的那个服务了。\n方法2： hugging_face CLI huggingface-cli 是 Hugging Face 官方提供的命令行工具，自带完善的下载功能。\n但是实际用起来体验很差，经常莫名其妙就 Internet Error\n无论是使用代理还是镜像体验都不是很好。\n使用方法：\n代理：\npip install -U huggingface_hub pip install -U hf_transfer # 先下载 huggingface-cli 本体和 hf_transfer 加速插件 # hf_transfer插件真的很快，特别是在境外的服务器速度真的很快 export HF_HUB_ENABLE_HF_TRANSFER=1 # 打开 hf_transfer huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 镜像：\npip install -U huggingface_hub # 还是先安装这个 huggingface-cli export HF_ENDPOINT=https://hf-mirror.com # 这里以 hf-mirror.com 为例 # 剩下的都一样的 huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 方法3：snapshot_download 同样是 hugging face 出品，同样的容易崩溃\n区别是这个可以在 python 中使用\n使用也很简单：\nfrom huggingface_hub import snapshot_download snapshot_download( # repo_type=\u0026#39;dataset\u0026#39;, # 这一条就看你是不是下数据的时候选择加还是不加了 repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False, # 加上这一条可以所见即所得 # 不会出现最后是个指针文件的情况 ) 方法4: hf-mirror 镜像站下载 hf-mirror镜像站 推出了 hfd, 一个 huggingface 专用下载工具，基于成熟工具 aria2，可以做到稳定高速下载不断线。\n这是 hf-mirror 网站给出的 tutorial, 方法清晰简单: 1. 下载hfd\nwget https://hf-mirror.com/hfd/hfd.sh chmod a+x hfd.sh 2. 设置环境变量\n# Linux export HF_ENDPOINT=https://hf-mirror.com or\n# Windows Powershell $env:HF_ENDPOINT = \u0026#34;https://hf-mirror.com\u0026#34; 3.1 下载模型\n./hfd.sh gpt2 3.2 下载数据集\n./hfd.sh wikitext --dataset 根据实测，速度也不赖\n就是他的这个 Copy 有点问题，要一行一行的 Copy 才可以正常运行\n或者直接从我这里copy也行\n方法5: 手动下载 当然，最后的最后还可以再网站上手动下载。\n但是对于这个分了1629个断点的数据集来说，手动下载太痛苦了。\n所以此处暂且按下不表。\nAppendix 代理开启和关闭命令：\n开启：\nexport http_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export https_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTP_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTPS_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; 关闭：\nunset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY ","permalink":"https://tzj2006.github.io/bugjournal/2025-06-18/","summary":"Data Download","title":"Bug Journal 2025-06-18"},{"content":"对于这些任务，以下是所有任务都必须面对的问题：“灾难性遗忘” 简单来说，就是如何让模型学得又快又好？\n在这里，快是指：如何只用很少的样例 / 数据就可以让模型学到某一个任务\n好是指：如何让模型不会在学习新任务的时候忘掉旧任务\n那为什么会忘掉旧任务呢？\n其实是因为 SGD 太强了，毕竟在初始化的时候模型是一个随机化的值 而经过训练，模型就能“学会”这些动作。\n所以在学习新的任务的时候不刻意保留原本的值， 就会如同从随机模型学到一个新的动作一样，直接“遗忘掉”过去学到的知识， 变成只会新动作的机器人了\n而下面的大部分办法都是为了解决这个问题而存在的。\nLOTUs: Continual lmitation Learning for Robot Manipulation Through Unsupervised Skil Discovery 目标： 灾难性遗忘：神经网络在新的数据分布上训练时，往往会覆盖先前学习的知识，导致早期任务的性能下降。\n样本效率：真实世界的机器人学习受到数据收集高成本的限制，这使得需要大量数据的方法变得不切实际。\n任务复杂性：基于视觉的操作任务涉及复杂的感知和控制挑战，使得单片学习方法尤其困难。\n知识转移：在任务之间高效地转移知识（无论是向前还是向后）对于有效的持续学习至关重要，但难以实现。\n方法： 构建一个技能库，这个技能库是这个模型能够学习的新技能的上限 然后每次在获得一个新的数据的时候，模型会自动判断应该是更新旧的技能还是学习新的技能 最后通过模仿学习更新这个技能 那如何识别这个技能是旧的技能还是新的技能呢？\n答案是判断这个任务和之前的某个任务是否有 Sematic 上的相似\n这里是设定了一个阈值 $\\tau$, 如果这个新的数据的任务在经过一个 DinoV2 之后 和某一个技能的聚类的相似度超过 $\\tau$, 那就更新这个技能\n问题：如果多个任务相似性都超过了这个阈值，那么就更新一个相似度最高的任务\n结果 M2distill: Multi-modal distillation for lifelong imitation learning 目标 想要解决这些问题：\n潜在表征漂移：随着模型按顺序学习新任务，它们内部对先前学习任务的特征表征会逐渐变形。这意味着机器人对熟悉物体、空间关系和任务上下文的理解会随时间扭曲。\n动作分布不一致性：随着策略参数的更新以适应新技能，机器人对先前学习任务产生适当动作的能力会下降。\n方法 蒸馏\n啥是蒸馏?_?\n其实就是最小化 embedding 的差\n和谁蒸馏呢？\n其实就是和上一步做蒸馏\n啥意思呢。\n就是说：\n我保存了上一个任务训出来的参数，然后也保存了这个任务的参数 我希望在学习到这个新任务的基础上，这个参数的变化越小越好。\n结果 比上一篇Lutos好点\nFew-Shot Vision-Language Action-Incremental Policy Learning 目标 在少数据的情况下不遗忘之前学过的旧数据\n方法 引入了两个特殊的机制来解决这个问题：\n任务特定提示（TSP）：可学习的提示向量，与多模态输入数据交互，以从有限的演示中提取任务特定信息 持续演化策略（CES）：一种构建和利用任务关系图以在任务之间传递知识并减轻灾难性遗忘的机制 什么是任务特定提示呢？\n任务特定提示就是： $$ Z_p = \\text{MVTransformer}([Z_v, Z_l, P]) $$\n其中，$Z_v$表示视觉 tokens，$Z_l$表示语言 tokens，$P$表示任务 prompt。\n就是增加训练了这么一个 encoder 就可以提高 15%-17% 的成功率\n那什么是持续演化策略呢？\n持续演化策略就是： 首先，训练一个 base network, 这个 base network 在训练的时候会有 multi-task 这样的话，这个模型就可以有一个还不错的通用性能 然后，建立一个方法库，对于每一个新任务放入一个新的方法库中 对于每次训练的时候可以把之前学过的旧任务的权重加权平均，融入新策略中 这个权重是根据旧任务和新任务之间的相似性来的。 注：这里的任务库中的任务非常大 包含了一个完整的 action head.\n结果 Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning 问题背景与动机 当前的终身学习方法主要分为三类，每类都存在显著局限性：\n重放方法存储并重训练旧数据，表现良好但需要过多的内存和计算资源。对于涉及高维视觉数据的机器人操作任务，这种方法成本高得令人望而却步。\n正则化方法，如弹性权重整合（EWC），通过限制参数更新来保留旧知识，但在长任务序列中难以平衡可塑性-稳定性。\n架构方法，包括参数高效微调（PEFT）技术，创建任务特定模块，但面临两个关键问题：\n在测试时依赖预言机任务标识符，这在实际部署中不切实际 任务之间的知识隔离，阻碍了有效的前向迁移 DMPEL通过将PEFT的效率与动态专家选择和知识共享机制相结合，解决了这些局限性。\n方法 动态的专家模型: 这个保证了在只需要微调的基础上新学习到知识，同时不会忘记旧知识(因为之前学习到的参数没有变) 一个 novel 经验回放的方法: 文中的经验回放不是把之前的场景回放一遍，只回放了\u0026quot;如何选取专家\u0026quot;的权重。这样既做到了\u0026quot;回放\u0026quot;的效果，算力开销也不算大 上下文感知专家路由器：一个轻量级神经网络，它以多模态上下文（视觉、语言、本体感受）作为输入，并生成专家组合的系数，从而无需预言机任务标识符。 其他细节：\nEncoder 用的是 CLIP ViT-B/16 amazing\n对于每个新任务，DMPEL：\n使用正交初始化方式初始化一个新的低秩专家，以最大程度地减少干扰 在当前任务上训练该专家，同时冻结基础策略 任务完成后，将训练好的专家添加到专家库中 更新专家路由器，将新专家纳入动态组合中\n结果 在 FWT 上稍逊于 M2 Distill, 但是 AUC 远高于 M2 Distill, 差距 \u0026gt; 10%\nSPECl: Skill Prompts based Hierarchical Continual lmitation Learning for Robot Manipulation 目标 希望机器人能够持续学习并且适应新任务\n方法 SPECI 框架对机器人学习领域做出了几项重要贡献：\n新型分层 CIL 框架：SPECI 将多模态感知和融合、上下文感知技能推断和低级动作执行统一到一个有凝聚力的流程中。\n动态技能获取：可扩展的技能代码本支持隐式持续技能获取，无需手动技能抽象，从而促进有效的技能级别知识转移。\n增强的知识转移：模式近似通过使用特定任务和任务共享知识丰富策略来增强任务级别的知识转移。\n同样的可扩展的技能代码本：对于每个新任务，都会分配一个新的技能向量子集，而现有的技能向量则被冻结。这种方法允许持续扩展技能库。\n同样的注意力驱动的技能选择：一种机制根据技能向量与当前状态的相似性，选择前 C 个相关的技能向量，并通过加权求和将它们组合起来，从而形成合成的潜在技能。\n同样的时间 Transformer 架构：此组件旨在推断随时间的逻辑技能，从而捕获技能执行过程中的时间依赖性。\n区别是，这里不再用 LoRA,而是使用 $W = W_0 + ΔU * V^T$ 去微调\n结果 效果都比 DMPEL 差，特别是 LIBERO-GOAL 和 LIBERO-LONG\n图中其他模型效果和 DMPEL 中有些偏差\n推测是 DMPEL中的 ER 效果更好\n可惜 DMPEL 并没有测试没有 ER 的效果。\nIncremental Learning of Retrievable Skills for Efficient Continual Task Adaptation 目标 and 动机 IsCiL 的动机源于认识到终身代理，特别是家用机器人，必须在以下环境中运行：\n完整的专家演示昂贵且难以获取 任务频繁变化，没有明确的边界 隐私问题需要能够“遗忘”特定行为 不同任务之间的通用技能应被利用以提高效率 解决了三个主要挑战：\n收集全面专家演示要很高成本 在非平稳环境中进行鲁棒适应的需求 隐私问题 方法 当在新任务演示中遇到一个新技能时，做以下事情：\n原型初始化：系统在新技能数据处理时识别出最常检索到的现有技能原型。 适配器初始化：新技能的适配器参数通过复制最相似的现有技能适配器进行初始化。 适配器更新：使用模仿损失在新技能的演示上对初始化的适配器进行微调。 系统更新：新的技能原型及其适配器映射被添加到系统中。 结果 You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations 目标 双臂协作机器人训练\n该框架解决了当前双臂机器人学习方法中的根本局限。传统方法要么依赖预定义的动作分类法，限制了通用性；要么需要大量耗时且易受噪声影响的远程操作数据收集。YOTO通过从人类演示中提取丰富的时空信息，并将这个单一的教学实例快速增殖为适合现代模仿学习算法的多样化训练数据，从而弥补了这一差距。\n方法 手部运动提取与注入 提取模块通过几个复杂的处理步骤，将原始人类视频转换为机器人可执行的动作。\n3D手部轨迹处理：YOTO并未依赖可能不稳定的直接3D手部网格重建，而是采用了两阶段方法。系统首先使用WiLoR检测手部边界框并估计3D手部形状，然后将这些3D点投影到2D图像平面上，再通过立体匹配将其提升回3D。这种投影-提升策略显著提高了轨迹的稳定性和一致性。\n基于关键帧的动作表示：系统并未建模可能包含噪声和冗余的连续轨迹，而是提取对应于重要运动事件的稀疏关键帧——例如抓取器状态变化、速度极值或轨迹拐点。这使得动作空间维度从可能数百个连续帧减少到一组可管理的离散关键姿态。\n运动掩码生成：一个关键创新是运动掩码的推导，它编码了协调策略。这些二值掩码指示在每个关键帧时，每只手臂是应该移动还是保持，有效地捕获了双手任务的时间协调模式。这解决了双手操作中的一个核心挑战——学习手臂何时应该协作以及何时应该独立操作。\n演示增殖策略 增殖模块通过两种互补的方法解决了单次学习中的基本数据稀缺问题。\n自动回放验证：提取的关键帧具有高度可解释性和可编辑性，允许系统地修改物体姿态和属性。通过调整关键帧参数（例如物体的6自由度姿态或用相似类别的物体替换）并在真实机器人上执行这些修改，系统能够快速生成多样化、经过验证的演示。这种方法比传统的远程操作显著更快，同时保持了高数据质量。\n几何变换增强：系统对被操作物体的3D点云执行受控的几何变换——在机器人可达工作空间内的旋转和平移。这些变换自动生成关键帧动作的相应更新，创建了理论上无限的合成变体，而无需额外的机器人执行。\n这些策略的结合使得从单个真人演示中生成数百个多样化的训练示例成为可能，有效地弥补了单次示教与现代深度学习方法所需数据之间的差距。\n双手扩散策略 BiDP（双手扩散策略）代表了扩散模型在双手操作任务中的专门适应。\n以物体为中心的观察：BiDP并非处理完整的场景点云或RGB图像，而是专门关注任务相关物体的3D点云。这种设计选择减少了视觉噪声，加速了训练收敛，并通过移除不相关的环境变化提高了泛化能力。\n关键姿态预测：该策略预测离散的关键姿态而非连续动作序列，显著降低了扩散空间的维度并简化了学习问题。这种方法与提取阶段基于关键帧的动作表示自然契合。\n运动遮罩整合：运动遮罩在统一动作空间表示方面起着至关重要的作用。对于异步任务，遮罩能够将双手动作重组为时间有序的单臂动作序列，有效消除冗余的“保持”状态。对于同步任务，两只手臂始终保持活动。这种统一的表示允许单一的策略架构处理两种协调模式。\n底层扩散模型使用基于PointNet的修改编码器，具有SIM(3)等变性，用于处理点云观测，使其对尺度和位置变化具有鲁棒性。FiLM层提供条件机制，而卷积U-Net处理动作预测。\n结果 比较性能：BiDP在所有任务中取得了76.8%的平均成功率，显著优于包括ACT (5.7%)、标准扩散策略 (15.8%)、3D扩散策略 (19.4%) 和 EquiBot (23.4%) 在内的强大基线。这种性能差距证明了双手操作专门设计选择的有效性。\n消融研究：系统的消融实验验证了每个组件的贡献。从完整场景到仅对象观测的转变将成功率从24.1%提高到48.1%。稀疏关键帧表示进一步提高到51.9%。运动遮罩机制带来了额外的增益，而几何数据增殖显示出最显著的影响，将性能从61.1%提升至77.8%。\n泛化能力：对新颖、未见过对象的域外测试显示了BiDP卓越的泛化能力。尽管基线方法在分布外设置中表现出显著的性能下降，但BiDP保持了相对强大的性能（平均成功率为35.0%，而次优基线为8.8%），展示了强大的迁移能力。\nIn Praise of Stubbornness: The Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLM 目标 LLM是怎么出现“灾难性遗忘”的\n方法 和 结果 首先，检测大模型中是否出现了“不一致行为”\n什么是不一致行为呢\n就是说：大模型中有些行为是“新”的，有些是“收悉”的，而有些是“不一致”的。\n什么意思呢？就是说，在训练的时候我们可以从大模型的中间状态来判断：\n这个完全没学过，这个曾经学过，还是这个新学的和之前学的不一样。\n分析表明，结合激活值和梯度特征始终优于单独使用任何一种特征集。基于梯度的特征在微调模型中表现出更强的判别性，这可能是因为过拟合为熟悉信息与不熟悉信息创建了清晰的梯度信号。对于预训练模型，激活值和梯度都做出了显著贡献，这表明内部表示和学习动态对于区分信息类型都很重要。\n这篇文章指出：只要没有矛盾，LLM 就是稳定的\n否则，LLM 就会很不稳定\nJoint Flashback Adaptation for Forgetting-Resistant Instruction Tuning 目标 如何使用指令微调让LLM避免“灾难性遗忘”\n方法 核心创新在于在训练新任务期间引入有限数量的“闪回”——来自先前学习任务的提示。该框架通过几个关键机制运行：\n新任务学习：模型在新的任务数据上进行标准的监督微调，使用交叉熵损失（$L_{SFT}$），保持任务无关操作，无需显式任务识别。\n闪回整合：一小部分来自旧任务的提示作为“闪回”，无需其对应的标签即可使用。这些提示可以从验证集中采样、手动制作或由LLM合成。\n散度损失预防：一个关键组件通过散度损失（$L_{DIV}$）来防止遗忘，该损失衡量当前模型输出与参考模型在闪回提示上的输出之间的偏差。该方法采用双向KL散度来惩罚生成令牌分布的差异：\n$L_{DIV} = \\sum [KL(P_{ref} || P_{current}) + KL(P_{current} || P_{ref})]$\n组合优化：最终目标结合了两种损失：$L = L_{SFT} + \\alpha * L_{DIV}$，其中 $\\alpha$ 平衡了新任务学习与旧任务保留。PCGrad（投影冲突梯度）在优化过程中解决了竞争目标之间的冲突。\n为了解决闪回数据稀疏性并促进知识共享，JFA通过潜在任务表示整合了联合任务学习：\n潜在任务表示：系统维护潜在任务作为元组 ($e_j$, $\\Delta \\theta_j$)，其中 $e_j$ 表示任务编码，$\\Delta \\theta_j$ 包含通过LoRA实现的相应权重增量，以提高参数效率。\n动态知识检索：对于每个输入，系统使用预训练编码器（例如 RoBERTa）对其进行编码，并根据与冻结的正交潜在任务键的余弦相似度检索 k 最近邻潜在任务。\n参数重参数化：模型不是直接更新参数，而是通过添加检索到的潜在任务增量的加权平均值来动态重参数化有效参数：$\\theta\u0026rsquo;$ = $\\theta_{base}$ + $\\Delta \\theta_i$。\n联合优化：系统同时优化基础模型参数和潜在任务权重增量，从而实现跨不同输入和任务的共享知识的持续改进。\n结果 新任务性能：在SNI上，JFA在两种模型的所有指标上都取得了最高分。对于Llama3.1-8B，JFA得分43.72 (BLEU) 和51.45 (ROUGE-1)，显著优于SFT (41.50, 50.18) 和其他基线。\n旧任务保留：JFA在旧任务上始终提供顶级的性能。对于Llama3.1-8B，它在GSM-8K (83.09%) 和SVAMP (84.40%) 上取得了最高准确率，甚至超过了那些能够访问大量旧任务数据的方法。\n灾难性遗忘示例：简单的SFT在旧任务上表现出显著的性能下降（例如，Llama3.1-8B在GSM-8K上的准确率从82.79%下降到8.95%），清楚地表明了灾难性遗忘的严重性，并凸显了JFA的有效性。\nLibero: Benchmarking knowledge transfer for lifelong robot learning Please visit this file\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-17/","summary":"VLA 中的持续学习","title":"Bug Journal 2025-06-17"},{"content":"持续学习中避免灾难性遗忘：具身智能领域的研究进展综述 引言与问题背景 持续学习（Continual Learning，也称终身学习）指模型在数据分布和学习目标不断变化的情境下，能够连续学习新任务且不忘记已有知识的能力arxiv.orgar5iv.org。传统深度学习假设训练数据 i.i.d.且一次性可用，这在现实具身智能（如机器人、自主系统）中往往不成立ar5iv.orgar5iv.org。其中最大挑战之一是灾难性遗忘（catastrophic forgetting） ，即模型在顺序学习多个任务时，新知识的获取会导致旧知识被快速、大幅遗忘en.wikipedia.org。这一现象最早由 McCloskey 和 Cohen 于1989年在联结主义神经网络中发现arxiv.org，体现了所谓 稳定-可塑性权衡 ：模型既要对新信息足够可塑（plasticity），又要对既有知识保持稳定（stability）en.wikipedia.org。与生物神经系统相比，人工神经网络在顺序学习时更容易“灾难性”遗忘过去经验，而人类和动物通常表现出渐进、有选择的遗忘cell.comcell.com。如何在保持模型泛化能力的同时避免旧知识被破坏，成为持续学习研究的核心问题cell.com。为此，大量研究围绕不同技术路线展开，包括利用附加数据回放、正则化约束、动态模块化架构和外部记忆等方法来缓解遗忘arxiv.org。本文将按时间脉络梳理持续学习避免遗忘的关键进展，重点聚焦具身智能场景的应用，并比较不同方法在这些场景中的优劣差异。\n早期探索：灾难性遗忘的提出与初步对策 灾难性遗忘现象的提出（1980s–1990s）: 神经网络中的灾难性遗忘问题由 McCloskey 和 Cohen (1989)en.wikipedia.org以及 Ratcliff (1990) 首次严谨描述en.wikipedia.org。他们发现序列训练两个任务时，后学任务会显著干扰先前任务的记忆en.wikipedia.orgen.wikipedia.org。这一问题可视为Grossberg提出的稳定-可塑性两难的极端表现en.wikipedia.org。此后，研究者逐步意识到，若要让人工智能具备人类般持续学习的能力，必须解决神经网络的遗忘灾难。1990年代中期，一些学者开始探索初步对策。例如，Robins (1995) 提出了重演/伪重演 (rehearsal/pseudorehearsal) 方法，即在学习新任务时将旧任务样本或由模型生成的“伪样本”一并训练，以巩固旧知识ar5iv.org。这种方法模拟了生物大脑在睡眠中重放记忆的过程，缓解了遗忘问题，被视为后续生成式回放方法的雏形ar5iv.orgar5iv.org。与此同时，Grossberg 等人在稳定-塑性理论指导下发展**自适应共振理论 (ART)**网络，通过网络结构与记忆单元的设计减少旧记忆被覆盖的风险。这一时期的工作揭示了灾难性遗忘的严重性，并奠定了若干基本思想：通过保留过去经验（真实或模拟）或限制参数剧烈更新来保护已有知识cell.com。然而，由于当时神经网络规模和应用场景有限，这些早期方法未形成统一框架，但为后续深度学习时代的持续学习研究提供了宝贵思路。\n深度学习时代兴起前的持续学习理念: 在2000年前后，机器学习领域也出现了一些“终身学习”思想，例如 Thrun 和 Mitchell 等人在机器人领域讨论让机器人不断积累知识、自主适应新环境的算法。但受限于模型能力，这些工作多偏向理论构想或特定场景下的增量学习算法。值得一提的是，Silver et al. (2013) 等人在认知科学领域提出**“永不停歇学习”(Never-Ending Learning)的愿景，旨在构建能无限获取新知识的AI系统。这些理念上的探索进一步强调了持续学习的重要性，但真正有效的算法突破还要等待深度学习的成熟。进入2010年代，深度神经网络在图像、语音等任务上取得突破，但其遗忘现象依然明显**。Goodfellow et al. (2014) 的实证研究表明，即使现代深度网络在顺序学习多任务（如不同MNIST变换）时，仍然发生严重的性能遗忘，他们尝试用Dropout等正则策略略微缓解遗忘cs.uic.edu。这一阶段的研究重新量化了深度模型遗忘的程度，引发了学界对持续学习的关注，也为随后的关键方法发明做好了铺垫。\n深度学习时代的方法演进（2016–2020） 进入深度学习时代后，大量持续学习算法被提出。总体而言，这些方法可分为以下几类： 参数正则化 、 经验回放 （或生成回放）、 参数隔离/模块化 、以及外部记忆等arxiv.org。各类方法都有经典代表工作，我们按时间演进介绍主要方法及其贡献。\n参数正则化方法 Learning without Forgetting (LwF, 2016): Li 和 Hoiem 提出“学习不遗忘”算法arxiv.org。他们假设只能获取新任务数据，无法重温旧任务数据的现实情况，通过让模型在学习新任务时蒸馏(distillation)旧任务模型的输出分布来正则化模型参数变化arxiv.org。具体而言，在训练新任务时对旧任务模型的预测进行保持，使新模型尽可能产生与旧模型相似的输出，从而保护原有能力。LwF是知识蒸馏用于持续学习的开创性工作，实现了只用新任务数据也能较好保留旧任务性能arxiv.org。该方法在2016年ECCV发表，此后蒸馏正则化成为持续学习的重要手段之一。\nElastic Weight Consolidation (EWC, 2017): Kirkpatrick 等人（DeepMind）在PNAS 2017发表了著名的EWC算法arxiv.org。EWC通过近似计算每个参数对旧任务的重要程度（利用费舍尔信息矩阵对参数敏感度进行估计），在学习新任务的损失中添加项，惩罚对重要参数的大幅更新arxiv.org。直观来说，模型会“放慢”那些对旧任务重要参数的学习速率，以免遗忘旧知识arxiv.org。EWC在经典分类任务（如顺序MNIST）和强化学习任务（顺序Atari游戏）上验证了有效性arxiv.org。作为持续学习领域里程碑，EWC证明通过软约束参数更新，可以在一定程度上同时保持先前任务性能和新任务学习能力arxiv.org。其思路后来衍生出许多变体，例如利用更精细近似二阶信息的方法等ar5iv.org。\nSynaptic Intelligence (SI, 2017): Zenke 等人在ICML 2017提出了另一本质类似EWC的正则化方法SIarxiv.org。不同于EWC预先计算参数重要度，SI在训练过程中实时累积每个参数对损失的贡献度，结束当前任务时将其视为该参数的重要性arxiv.org。这种“智能突触”机制借鉴了生物突触强度调节的复杂性，每个突触（参数）在多个任务中累积“任务相关信息”，新任务来时利用这些信息调整学习率arxiv.org。SI在多个连续分类任务上显著降低了遗忘，同时保持了计算高效arxiv.org。与EWC相比，SI无需存储旧任务样本，同样不增加模型容量，对于资源受限的设备具有吸引力arxiv.org。但正如多数正则化方法的局限，当任务数量增多或差异较大时，单纯靠增加惩罚会导致模型学习新任务受限，需要在稳定与塑性间权衡ar5iv.orgar5iv.org。\n其他正则化进展: 除上述，2017年前后还出现了多种参数正则化方案。例如 Li et al. (2017) 的增量时刻匹配（IMM）方法通过匹配参数分布的方式融合旧新任务模型；Aljundi et al. (2018) 提出的MAS（Memory Aware Synapses）利用输出敏感度评估参数重要性。这些方法本质均为在损失函数中添加某种形式的正则项，使模型避免过度调整关键参数以保留旧知识ar5iv.orgar5iv.org。正则化方法的优点是 无需存储旧样本，内存占用低 ，易于在嵌入式/机器人等设备上实现ar5iv.org。它们的不足在于当任务间差异巨大、参数冲突严重时效果受限，而且累积过多任务后模型可能进入“过度稳定”状态难以学习新任务ar5iv.org。在具身智能场景中，由于设备算力和存储有限，正则化方法仍是常用选择之一。例如 EWC 被用于机器人连续控制任务以保护低层政策参数不被遗忘arxiv.org。但如果机器人遇到全新领域任务，正则化可能限制其适应新技能的能力，这是后续方法力图解决的问题。\n经验重放与生成式回放方法 显式经验重放 (Experience Replay): 针对持续学习，最直接的思路是 在学习新任务时重温部分旧任务的数据 ，仿佛让模型“复习”以前的知识ar5iv.org。Rebuffi 等人在CVPR 2017提出的 iCaRL 方法将这一思想与深度学习结合arxiv.org。iCaRL在每学新类别时， 保存每个旧类别少量代表样本（记忆库） ，训练时将这些旧样本与新数据一起用于更新模型，并对模型输出进行知识蒸馏以防决策边界偏移arxiv.org。通过同时学习分类新类别和回顾旧类别，iCaRL实现了深度网络在长时间增量学习许多类时，比仅新数据训练的策略遗忘显著减少arxiv.org。iCaRL开创了样本记忆回放+蒸馏结合的范式。之后许多增量学习方法沿用了“小样本记忆”思路，如 Hou et al. (2019) 的UCIR、Wu et al. (2019) 的BiC等，都在如何精选和高效利用少量旧样本上下功夫。经验重放策略也直接应用于强化学习领域——在非平稳环境中，智能体可将过去经历的轨迹保存一部分，在策略更新时混入重放，以避免策略完全偏离先前成功经验。这与深度Q网络（DQN）的经验回放缓冲理念一脉相承，只是这里目的是防遗忘而不仅是破除相关性。在机器人学习中，经验重放意味着机器人在学习新技能时定期练习已掌握的技能（通过模拟旧技能的传感器输入等），这在实践中提高了多技能机器人系统的鲁棒性。\nGradient Episodic Memory (GEM, 2017): Lopez-Paz \u0026amp; Ranzato 在 NeurIPS 2017 提出的 GEM 则进一步创新了重放的使用方式arxiv.org。与直接将旧样本混入训练不同，GEM把少量旧任务样本存入 episodic memory ，在每次参数更新时，通过约束新梯度与旧样本梯度的内积为非负，确保新任务训练不会增加旧任务损失arxiv.org。这种基于优化约束的方法保证了模型对记忆样本性能不下降，实现了一定的“向后迁移”能力：在学习新任务的同时还有可能改进旧任务表现arxiv.org。GEM开创了利用回放样本的梯度信息指导优化的思路，其后续简化版A-GEM (Chaudhry et al. 2018)降低了计算成本。对于具身智能而言，GEM这类方法的优势在于即使少量记忆也能通过优化约束起效，而且不需要明确任务边界（可以对任意过去经验施加约束）。不过其劣势是需要实时计算并存储梯度，复杂度较高，且仍需维护一个小型记忆库。\n生成式回放 (Generative Replay): 当直接存储原始旧样本受限时，另一策略是训练生成模型来产生日前学过的数据，从而实现回放。Shin et al. 在 NeurIPS 2017 提出的 Deep Generative Replay (DGR) 是该思路的里程碑arxiv.org。DGR构建了一个生成模型（如GAN或VAE）作为“记忆仿真器”，在学习新任务时利用生成模型产生日前各任务的合成样本，并与新数据混合训练Solver模型arxiv.org。在每完成一个任务后，Solver的参数固定，然后训练生成模型去拟合更新后的Solver分布，以便下次产生更新的数据分布ar5iv.org。这种双模型协同框架受到大脑“海马-新皮层”互作机制的启发，即利用快速变化的“海马体”生成回忆来训练慢更新的“皮层”网络arxiv.org。DGR实验证明，即使不保存任何真实旧样本，模型仍能通过生成模拟数据达到与有存储时相近的效果ar5iv.orgarxiv.org。生成回放的优势是 不直接占用存储真实数据 ，在隐私敏感或内存极小的设备上尤为有用arxiv.org。其缺点在于生成模型本身也面临持续学习问题（如何不忘记早期的数据分布），且训练开销较大ar5iv.org。后续不少工作改进了生成回放，如 Wu et al. (2018) 将GAN与变分特征结合（MeRGAN），Rios et al. (2018) 用生成对抗网络生成特征而非像素，提高效率等ar5iv.orgar5iv.org。生成回放还被应用到强化学习的状态生成中，例如 Caselles-Dupré et al. (2019) 提出的自触发生成回放（S-TRIGGER）用于连续学习环境状态表示ar5iv.org。总的来看，回放类方法（包括经验重放和生成回放）在各种基准上往往表现突出，被认为是目前抗遗忘最有效的范式之一。然而它们对存储或生成能力有要求，在具身智能中需权衡内存/算力和性能：对于机器人等设备，存储少量关键经验（如图像片段、关键帧）进行回放在实践中较常用，而实时训练复杂生成模型则相对少见。\n模块化架构与参数隔离方法 Progressive Neural Networks (PNN, 2016): Rusu 等人提出的渐进神经网络是持续学习的架构派代表arxiv.org。PNN在每遇到新任务时 冻结已有网络 ，并侧旁新增一组“列”网络用于学习新任务arxiv.org。同时，通过旁路连接让新任务列能够利用之前各列学到的特征（实现知识迁移）arxiv.org。这种架构确保旧任务的参数永不修改，从而彻底避免遗忘arxiv.org；而新增模块可以专门学习新任务，有充足的模型容量。PNN在Atari游戏和3D迷宫导航等一系列强化学习任务上取得优于微调的成绩，并显示出显著的前向迁移能力arxiv.org。其缺点也很明显：每增加一个任务网络规模就线性增长，在任务数很多时不切实际arxiv.org。尽管如此，PNN证明了模块隔离在避免遗忘上的有效性，许多后续方法受此启发引入可扩展或可选择激活的架构。\nPathNet (2017) 与 PackNet (2018): 为了缓解PNN网络爆炸的问题，Fernando 等人 (2017) 提出的 PathNet 利用进化算法在固定网络中为每个任务选择一条互不干扰的子网络路径，相当于在共享参数的前提下实现参数隔离。Mallya 和 Lazebnik (2018) 则提出 PackNet ，通过反复剪枝和重训练来为新任务腾出参数空间arxiv.org。具体来说，PackNet先训练初始任务模型，然后剪除一定比例不重要的参数（权重置零但保留位置），学习第二个任务时仅利用空闲参数；如此迭代，将多个任务“打包”进单个网络中arxiv.org。实验表明，在ImageNet等大型数据上，PackNet可在一个VGG模型中连续容纳多个细粒度分类任务，性能接近于单独训练arxiv.org。PackNet无需存储旧数据，也不引入新参数，因此相比PNN更高效arxiv.org。但PackNet需要预先设定剪枝比例，且剪枝过多可能损害旧任务性能，过少则限制新任务空间。后来一些变体如 “Piggyback” (Mallya, 2018) 则改为学习任务特定的掩码，更灵活地实现参数复用。总体而言，参数隔离类方法（含动态扩张和网络剪枝）通过结构上的硬约束避免了遗忘，其优势是旧知识完全保留、无干扰arxiv.org。在机器人等具身智能中，如果任务集是离散且有限的，这类方法可考虑使用。例如在多任务机器人控制中，可为每个任务分配专属网络模块或参数子集，新任务加入时扩展网络并冻结旧模块，从而保持以往技能arxiv.org。然而，在开放环境下任务可能连续涌现且无法预知数量，单纯无限扩展网络不切实际。因此近期一些工作尝试结合元学习或 条件网络 ，自动决定何时复用旧参数、何时增加新参数，以兼顾模型规模和遗忘防护。比如 Serra et al. (2018) 提出的 HAT 方法对每层参数学习可训练门控，通过门控向量的稀疏化实现在相同网络中隔离不同任务的激活区域，从而在不显著增加参数的情况下减少干扰。\n脑启发的双记忆体系: 值得注意的是，一些方法从神经科学的双重内存理论汲取灵感，将快速学习模块和稳定长时模块结合起来应对遗忘。例如 Kemker 和 Kanan (2018) 提出的 FearNet 模型采用“大脑 海马-新皮层 ”的架构arxiv.org：用一个类似海马体的小网络专门快速学习当前任务，并在适当时机（模拟睡眠）将新知识整合（consolidate）到另一个类似皮层的大网络中做长期存储arxiv.org。同时还有一个类似杏仁核的模块，根据输入判断应该用哪套记忆系统回答arxiv.org。FearNet不需存储旧样本，依赖生成式机制回忆旧类数据，达到与iCaRL相当的性能arxiv.org。这类方法实质上属于架构+回放的混合策略（因为短期网本身可看作一种内生记忆生成器）。双记忆策略对具身智能有自然的意义：机器人或代理可以配置一个“小而快”的在线学习器来及时适应新变化，同时定期将知识固化到“大而稳”的长期模型中，从而两全其美。不过如何确定巩固频率以及双网络的容量匹配仍在探索中。\n方法对比与小结 不同持续学习方法各有优劣，在具身智能场景下需要平衡选择ar5iv.org。正则化方法不需保存样本、开销低，适合嵌入式设备在线更新，但在任务变化剧烈时可能束缚新知识获取ar5iv.org。回放方法往往效果最佳，即便少量样本重放也能显著降低遗忘arxiv.org；对于机器人这种可反复与环境交互的场景，还可通过自主采样过去环境状态进行重演。然而存储真实数据可能受限于隐私或容量，而训练生成模型又对计算资源有较高要求arxiv.org。模块化/参数隔离方法彻底杜绝了遗忘，在多任务机器人系统（任务有限且可拆分）中很有价值，但在开放任务中扩展性受限arxiv.org。外部记忆和双重内存策略提供了一种折中：通过引入专门的记忆模块，模型可以在不反复调整主要网络权重的情况下查询和更新知识。例如在多人对话交互机器人中，引入一个可读写的记忆单元存储历史对话要点，有助于长期一致的对话理解。但引入记忆也增大了系统复杂度，需要设计高效的检索和写入机制。\n此外，许多先进方法不再局限于单一策略，而是混合多种机制以取长补短ar5iv.org。例如 Schwarz 等人 (2018) 提出的 Progress \u0026amp; Compress 框架将动态架构与蒸馏结合：使用Progressive Network扩展新任务列，然后通过蒸馏将新列知识压缩回主干网络，从而既避免遗忘又控制模型规模arxiv.org。再如 von Oswald et al. (2019) 的 MER 方法将元学习思想融入记忆重放，通过元训练提高模型表示对新旧任务的解耦，从而辅助减少干扰。这些综合方法在近年不断涌现，说明持续学习领域正朝着多策略融合与自动适应方向发展。\n具身智能场景中的持续学习应用 具身智能领域（如机器人、自主车辆、智能代理）为持续学习提供了最实际也最具挑战的用武之地arxiv.org。与静态数据集不同，具身智能体在物理世界中连续感知和行动，环境非平稳且任务边界往往不明确ar5iv.orgar5iv.org。以下我们重点考察持续学习方法在几个具身场景的应用进展：\n机器人视觉与物体识别： 服务机器人需要在不断变化的环境中识别新对象、适应新场景，这正是开放域的持续学习问题。为评测算法，2020年提出了OpenLORIS-Object机器人视觉数据集，包含随时间推移环境光照、视角、物距等变化的数据流sciencedirect.com。在该数据集上，Lomonaco 等人组织了持续学习挑战，促进了算法在真实机器人感知条件下的比较。一系列方法被测试：如 iCaRL 的小样本存储结合知识蒸馏策略在这种增量物体识别中取得稳健表现；又如 IROS 2020 的 Latent Replay 方法，Pellegrini et al. 提出只在特征空间保存和重放旧数据link.springer.com。具体而言，机器人摄像头图像经卷积网络得到中间表示，将这些低维激活缓存代替原始高维图像，可大幅减少存储并实现实时回放ieeexplore.ieee.org。实验表明，在OpenLORIS这种持续视觉任务中，Latent Replay比直接存图像几乎不降性能，却更高效满足机器人实时性需求ieeexplore.ieee.org。另一最新进展是 Hajizada et al. (2024) 提出的 Continually Learning Prototypes (CLP) 算法arxiv.orgarxiv.org。CLP针对机器人少样本在线学习和开放世界场景设计：它采用原型向量表征每类知识，并通过元可塑性机制动态调整每个原型的学习速率来平衡新旧知识稳定性arxiv.orgarxiv.org。同时CLP具备新类别自我检测与无监督学习能力（即机器人遇到未知物体时可判断新类别并自主创建原型学习）arxiv.orgarxiv.org。重要的是，CLP不使用任何显式回放数据且兼容神经形态芯片，实现了超低能耗下的持续学习arxiv.orgarxiv.org。这对于内存和电池有限的移动机器人具有现实意义。总的来看，在机器人视觉领域，混合使用 小样本记忆 、 特征回放 、适应性学习率等技术已取得显著效果，使机器人能逐步扩展认知能力且遗忘受控。\n人机交互与多模态学习： 具身智能体常涉及多模态感知（视觉、听觉、语言）和人机交互，这带来了持续学习的新课题。例如社交机器人需要持续学习新的对话内容、新的手势动作等。NLP领域已有针对增量学习的综述（如 Biesialska et al. , 2020link.springer.com），其中提到自然语言处理任务在持续学习中面临词汇和语义随时间演变的问题。一些方法通过动态扩充词典或嵌入空间缓解了“遗忘”早期语义的现象。对于多模态交互，Kulkarni et al. (2019) 提出在对话系统中使用弹性权重约束来保留模型早期对话技能，同时新增新领域对话意图。交互学习中一个重要方面是 用户在环（human-in-the-loop） ：机器人可通过用户反馈实时修正知识。近期有工作探索 交互式持续学习 ，如 Hazifa et al. (2022) 结合神经形态计算，利用片上在线学习快速吸收用户教授的新知识，同时通过正则保护已有知识dl.acm.org。虽然具体算法仍在早期，但这些尝试指出了方向——未来的具身智能体应能通过持续人机交互 自我进化 ，并且做到“学而不忘”。\n连续控制与强化学习： 在自主驾驶、机器人控制等连续决策场景，持续学习同样关键。例如自动驾驶车辆遇到新道路场景，需要学习新策略而不忘记基本驾驶技能。Shaheen et al. (2022) 的综述link.springer.comlink.springer.com总结了三类自主系统（无人车、无人机和移动机器人）中的持续学习挑战：模型需在在线方式从大量顺序数据中学习，且资源受限、须保障安全稳定link.springer.comlink.springer.com。一些研究采用策略蒸馏或迁移学习避免遗忘旧任务策略。如 Rusu et al. 在DeepMind的机器人实验中，用渐进网络将仿真训练的技能迁移到现实机器人上，同时保持仿真技能不丢失arxiv.org。又如 Traoré et al. (2019) 提出的 DiscoRL 框架，将旧策略压缩为策略库，再用Policy Distillation（策略蒸馏）技术在新环境中融合旧策略以加速学习，同时旧策略作为教师防止遗忘ar5iv.org。在连续控制中，策略往往以神经网络表示，类似分类任务的遗忘也会发生：新环境下调整策略网络，会导致旧环境下性能下降。为此 Rolnick et al. (2019) 提出的 CLEAR 方法，将off-policy经验重放引入强化学习的策略梯度训练，既提高新任务样本效率又维持旧任务价值函数不变。该方法在Atari游戏顺序学习中取得好结果，被视为强化学习领域对抗遗忘的有效方案之一。需要强调的是，强化学习场景中任务界限往往模糊，甚至代理可能在一个不断演变的环境中持续学习（如运营多年的家庭服务机器人，会不断遇到新任务）。这接近无任务标签 (task-agnostic)的持续学习。Aljundi et al. (2019) 针对此提出了在线持续学习方案：通过检测网络对新数据的干扰程度动态触发记忆重放（MIR）link.springer.com，以及使用梯度稀疏化挑选对旧任务干扰最大的记忆样本来更新，从而在无明确任务边界下也能抑制遗忘。此类方法在机器人持续感知与导航中具有潜力，因为现实中机器人很难知道自己何时“切换了任务”，只能根据环境变化连续调整。\n开放世界和自主适应: 具身智能体经常处于开放世界，可能遇到训练时未见过的全新情况。持续学习的终极目标是在这种开放环境中实现持续适应而不崩溃。Open-world持续学习需要综合上述技术，还涉及新知识的自主发现和 主动学习 。比如前述CLP方法引入了新类检测机制，让机器人在开放世界下识别何时需要学习新对象arxiv.orgarxiv.org。又如 Mundt et al. (2020) 探讨了结合异常检测和持续学习，使模型在检测到输入分布偏移时能触发新任务学习流程。对自主车而言，面对从未见过的道路情况（极端天气、新施工区域），如果能自动检测出“新情境”并调用持续学习模块更新模型，将大幅提高安全性。当然，这也带来安全约束下的学习稳定性问题，需要确保新学习不会在尚未充分验证时投入决策。近期一些研究主张引入 不确定性估计 （如Bayesian NN）判断模型何时需要学习新任务，以及学习后的性能变化link.springer.com。这些探索尚属前沿，但对于真正长期自主运作的智能体至关重要。\n近年新进展（2020–2025）与展望 过去五年中，持续学习领域涌现了一系列新趋势和方法，进一步提高了模型在复杂环境中的持续适应能力：\n任务无关持续学习: 越来越多工作关注在无明确任务边界、数据连续流动场景下的学习link.springer.comlink.springer.com。这更贴近现实中的机器人/代理感知流。为此，方法上强调在线更新、有限内存和即时评估。例如 2020 年的 GDumb 方法提出一种极端简单但强大的baseline：始终只训练当前模型在收到的全部数据上（存储一定量最近数据），每次新数据到来直接从头训练。这种方法虽谈不上高明，却在一些线上学习赛道表现接近更复杂的方法，提示我们需要重新审视评价指标。在可预见的将来， 线上持续学习 （Single-Pass Continual Learning）将成为研究热点，它要求算法一次遍历数据且不泄露未来信息，在边学习边推理的同时抗遗忘arxiv.orgarxiv.org。具身智能如实时视频流分析、持续语音识别都属于这种场景。 持续学习评测基准丰富化: 近年构建了许多新数据集和基准来评测持续学习算法在更复杂任务上的性能。如 CORe50、OpenLORIS 等视觉序列数据集用于评测在线物体识别link.springer.com；ACL 2021 Lifelong NLP挑战提供持续自然语言理解任务；还有不同领域的持续强化学习基准、连续无人驾驶仿真环境等。这些基准推动算法从依赖任务ID的小规模实验，走向更贴近真实的情境。评测指标也越发丰富，除了遗忘率、累计精度外，开始考虑模型的计算效率、内存开销以及在长期学习中的稳定性link.springer.comlink.springer.com。这些综合指标对于具身智能系统尤为重要，因为实际应用中资源受限且需要持续运行。 联邦持续学习与分布式学习: 在物联网和边缘计算兴起的背景下，联邦持续学习成为新方向arxiv.org。即多个分散设备（如一群机器人或智能传感器）在各自持续学习的同时，定期交流模型更新，从而在保证隐私下实现知识共享和共同进化。诸如 FedWeIT、FCL 等算法探索了如何在联邦场景下减少遗忘并高效通信arxiv.org。对具身智能来说，这意味着例如一队协作机器人的经验可以融合，使整体学习速度加快且每个体遗忘降低。该领域仍在起步，面临异步学习、设备差异等挑战，但前景值得期待。 理论分析与可解释性: 持续学习理论方面，近年有人尝试从信息论和最优化角度给出遗忘的分析框架，如用Fisher信息界定参数迁移平衡ar5iv.org。另外，对持续学习过程中的可解释性要求也在提高——在机器人应用中，理解模型为何遗忘某能力、何时需要触发新学习，对于建立用户信任很重要。一些研究利用可视化技术观察随着任务增添，网络内部表示如何演化，以寻找缓解遗忘的线索。还有工作将神经符号方法引入持续学习，以借助符号逻辑的约束保持旧知识。这些方向虽属于前沿探索，但表明社区已不仅满足于经验提升性能，也在寻求持续学习更深层的理论和可解释支撑。 综上，持续学习作为迈向真正智能系统的关键一步，近年来在算法和应用上都取得了显著进展。从最初发现问题、提出启发式对策，到如今各种融合策略在复杂环境中落地，我们离“像人一样终身学习”的AI越来越近。在具身智能领域，实现持续学习将赋予机器人和自主代理长久的自主适应能力，使其能够随着环境和任务变化不断成长，而无需频繁人工干预。展望未来，持续学习研究需要进一步结合元学习、强化学习、因果推断等范式，研发更加通用高效的算法。同时，在真实世界大规模部署持续学习系统时，还需重视 安全机制 （防止在学习过程中性能突然退化）、 伦理与隐私 （学习过程中对用户数据的处理）等问题。可以预见，随着研究的深化，持续学习将在机器人自主导航、智能助理、自动驾驶乃至通用人工智能等领域扮演日益重要的角色，推动人工智能从“静态聪明”走向“动态成长”。\n参考文献：\nMcCloskey, M. \u0026amp; Cohen, N. J. (1989). Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem . In Psychology of Learning and Motivation , vol. 24, pp. 109–165en.wikipedia.org. ( 首次揭示神经网络顺序学习遗忘问题 ) French, R. M. (1999). Catastrophic forgetting in connectionist networks . Trends in Cognitive Sciences, 3 (4):128–135cell.com. ( 灾难性遗忘综述，分析原因并讨论可能解决方案 ) Robins, A. (1995). Catastrophic forgetting, rehearsal and pseudorehearsal . Connection Science, 7 (2):123–146ar5iv.org. ( 提出伪重演方法，用随机伪样本重放旧知识 ) Goodfellow, I. et al. (2014). An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks . In ICLR 2014 . ( 实证分析深度网络遗忘现象，评估基本缓解策略 ) Li, Z. \u0026amp; Hoiem, D. (2016). Learning without Forgetting . In ECCV 2016arxiv.org. ( 知识蒸馏用于持续学习，只用新任务数据保持旧任务性能 ) Kirkpatrick, J. et al. (2017). Overcoming catastrophic forgetting in neural networks . PNAS, 114 (13):3521–3526arxiv.org. ( 提出EWC，通过弹性权重凝固保护重要参数arxiv.org ) Zenke, F. et al. (2017). Continual Learning Through Synaptic Intelligence . In ICML 2017arxiv.org. ( 提出SI算法，智能累积参数重要性减少遗忘arxiv.org ) Rebuffi, S.-A. et al. (2017). iCaRL: Incremental Classifier and Representation Learning . In CVPR 2017arxiv.org. ( 提出增量分类策略，结合样本保存和蒸馏避免遗忘arxiv.org ) Lopez-Paz, D. \u0026amp; Ranzato, M. (2017). Gradient Episodic Memory for Continual Learning . In NeurIPS 2017arxiv.org. ( 提出GEM算法，用梯度约束保证新任务不增大旧任务损失arxiv.org ) Shin, H. et al. (2017). Continual Learning with Deep Generative Replay . In NeurIPS 2017arxiv.org. ( 提出深度生成回放DGR，通过生成模型重现旧样本融合训练arxiv.org ) Rusu, A. A. et al. (2016). Progressive Neural Networks . arXiv:1606.04671arxiv.org. ( 提出渐进网络架构，扩展新列避免遗忘并实现知识迁移arxiv.org ) Mallya, A. \u0026amp; Lazebnik, S. (2018). PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning . In CVPR 2018arxiv.org. ( 通过迭代剪枝为新任务腾出容量，实现单网络多任务无遗忘arxiv.org ) Schwarz, J. et al. (2018). Progress \u0026amp; Compress: A scalable framework for continual learning . In ICML 2018arxiv.org. ( 提出进展-压缩框架，结合渐进扩展和蒸馏压缩，实现无增长持续学习arxiv.org ) Aljundi, R. et al. (2019). Online Continual Learning with Maximal Interfered Retrieval . In NeurIPS 2019 . ( 提出在线持续学习算法MIR，选择干扰最大的记忆样本回放，任务无关场景有效 ) Pellegrini, L. et al. (2020). Latent Replay for Real-Time Continual Learning . In IROS 2020link.springer.com. ( 提出在特征空间进行重放，支持机器人实时持续学习，降低存储与计算需求 ) Kemker, R. \u0026amp; Kanan, C. (2018). FearNet: Brain-Inspired Model for Incremental Learning . In ICLR 2018arxiv.org. ( 提出双内存脑启发模型，不存原始数据通过生物式记忆系统整合知识arxiv.org ) Hajizada, E. et al. (2024). Continually Learning Prototypes . arXiv:2404.00418arxiv.orgarxiv.org. ( 提出原型持续学习方法，少样本在线学习并支持开放世界新类发现，无需存储回放 ) Shaheen, K. et al. (2022). Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks . Journal of Intelligent \u0026amp; Robotic Systems, 105 (9)link.springer.comlink.springer.com. ( 面向自主系统的持续学习综述，分析算法在无人车、无人机等中的性能和挑战 ) Lesort, T. et al. (2020). Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges . Information Fusion, 58 :52–68arxiv.orgar5iv.org. ( 持续学习在机器人领域的综述，提出评测框架和跨领域方法借鉴思路 ) 引用\n[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[2312.10549] Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomyhttps://arxiv.org/abs/2312.10549Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[PDF] Continual Learning and Catastrophic Forgettinghttps://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1706.08840] Gradient Episodic Memory for Continual Learninghttps://arxiv.org/abs/1706.08840[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1805.06370] Progress \u0026amp; Compress: A scalable framework for continual learninghttps://arxiv.org/abs/1805.06370[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Towards lifelong object recognition: A dataset and benchmarkhttps://www.sciencedirect.com/science/article/abs/pii/S0031320322003004Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccLatent Replay for Real-Time Continual Learning - IEEE Xplorehttps://ieeexplore.ieee.org/document/9341460/Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccInteractive continual learning for robots: a neuromorphic approachhttps://dl.acm.org/doi/10.1145/3546790.3546791Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfcc[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfcchttps://arxiv.org/pdf/2302.00487https://arxiv.org/pdf/2302.00487Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccFederated Continual Learning for Edge-AI: A Comprehensive Surveyhttps://arxiv.org/html/2411.13740v1\n2024–2025 年具身智能持续学习新方法综述：缓解灾难性遗忘的创新策略 持续学习（Continual Learning）旨在让智能体能顺序学习多个任务而不遗忘已学知识。然而传统方法（如 EWC、iCaRL、GEM、PackNet、DGR 等）主要在静态数据上验证，在具身智能场景中效果有限。具身智能中的持续学习面临真实环境的挑战：任务顺序模糊、样本稀少、实时交互、高维感知等。这要求新机制在稳定-可塑性间取得更佳平衡，避免旧知识被覆盖（灾难性遗忘）同时高效习得新技能。以下我们综述 2024–2025 年出现的多篇新论文，每篇提供方法简介、核心创新、与传统方法的区别及其在具身场景中的优势分析。 Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation (Yuanqi Yao 等, 2025 年, arXiv) 方法简介：这项工作提出了PSPL（Primitive-level Skill Prompt Learning）方法，用于机器人操作的无重温持续学习。PSPL 将技能解构为可复用的“原语”（primitives），通过前缀提示（prefix prompts）来表示，并在两阶段训练方案中不断扩展技能库。首先进行多技能联合预训练，学习一组运动感知的技能提示（motion-aware skill prompts），提取不同技能间的共性语义与运动原语。随后在增量学习新技能时，为每个新技能添加新的前缀提示，通过与旧技能提示的跨注意力机制共享知识，从而加速新技能学习。这一提示学习策略使机器人能够在无需重放旧任务数据的情况下连续获取新技能。 **核心创新：PSPL 将提示学习（prompt learning）引入机器人持续学习领域，创新性地使用“技能前缀提示”作为可扩展的模块化单元来表示和存储技能知识。相比以往为每个新任务增添适配器的方式，PSPL 的提示可以在技能之间共享和重用，显式捕捉技能间的共性原语（包括语义和运动层面的共性）。此外提出的文本+光流联合提示查询（text-flow query）机制，将语言指令与视觉运动信息融合，用于检索适当的原语提示，从而关联语义截然不同但运动模式相似的技能。这一机制解决了仅靠文本嵌入查询时，不同技能间缺乏关联的问题。 与传统方法区别：相较于 EWC 等对网络权重施加正则的方式，PSPL 并不限制参数更新，而是通过前缀提示的模块化扩展来避免遗忘：旧技能的提示向量固定，新技能通过新增提示获取模型容量，从而防止旧知识被覆盖。这类似 PackNet 的“逐任务分配参数”思想，但PSPL的提示可以通过跨注意力与旧提示交互，实现旧新知识的共享迁移，弥补 PackNet 类方法模块隔离导致的无法迁移问题。与 iCaRL、GEM 等需要存储旧样本回放不同，PSPL 不需要任何经验回放——模型通过提示机制保留旧技能，因此在训练新任务时无需重温旧数据便可避免遗忘。相比于 LwF 这类基于知识蒸馏的方法，PSPL直接在模型内部保留了原技能的“软提示”，避免了仅凭日志概率约束可能出现的信息不足。 具身场景下的优势：首先，无重放需求降低了实际机器人系统的存储和隐私负担（无需记录旧任务视频或演示数据），非常适合那些无法无限存储过去经验的场景。其次，PSPL 的原语提示捕捉了跨任务的共性动作模式，使机器人能在少样本情况下将已学技能迁移到新任务上，例如仅观察少量演示也能通过提示召回相关原语来加速学习。再次，该方法对每个技能只新增少量提示向量，属于参数高效的扩展方式，不像全面微调那样消耗大量新参数，适合机器人有限算力的约束。最后，在 LIBERO 基准的模拟和真实操作实验中，PSPL 在前向迁移（FWT）和背向保留（BWT）指标上均达到当前最好水平，验证了其在长期任务序列中的遗忘防御和跨技能知识共享能力。 Preserving and Combining Knowledge in Robotic Lifelong Reinforcement Learning (Yuan Meng 等, 2025 年, Nature Machine Intelligence) 方法简介：该研究提出了名为LEGION的机器人终身强化学习框架 nature.com 。LEGION 构建了一个非参数贝叶斯知识空间（knowledge space），用以逐步积累和保存机器人的技能知识。具体而言，作者使用Dirichlet 过程混合模型（DPMM）来根据任务生成的表征不断拓展知识空间的簇，以容纳新任务知识，同时保留旧任务的簇不被覆盖。每当机器人从连续到来的一次性任务流中学习完一个任务，DPMM 就在知识空间中产生或调整相应的知识组分，实现对该任务知识的分离存储与渐进扩充。此外，LEGION 将语言嵌入引入任务表征，使智能体理解任务语义，从而在学习和推理中利用语言指导任务推断。在执行阶段，机器人能够将知识空间中多个任务的技能组合重新应用于长序列复合任务（例如依次执行多步操作完成“清理桌子”等目标）。 核心创新：LEGION 的突出创新在于引入贝叶斯非参模型管理知识：不同于传统固定架构，DPMM 知识空间可以动态增加新的知识组分，无需预先限定任务数量。这解决了过去结构模块化方法在任务数未知时难以扩展的问题 nature.com 。知识空间中的每个簇本质上代表一类技能或策略的“知识单元”，新任务到来时，如果其知识无法用现有簇解释，就会自动生成新簇存储，避免了旧知识的遗忘与冲突。同时，每个任务的知识以概率混合模型形式保存在连续的先验空间中，便于在需要时被识别和重新调用。另一个创新是融合语言描述提升任务区分和推理能力：语言嵌入提供了任务的高层语义提示，帮助知识空间进行任务归属推断和知识检索，使得在执行长视距任务时，智能体可以更准确地推理需要调用哪些已有技能。 与传统方法区别：LEGION 综合了多种持续学习理念但又有所突破。相较于 EWC 等正则化方法，LEGION 不直接作用于网络参数，而是在参数输出的高层空间保存知识，因而不存在权重被覆盖的问题。相比 PackNet 这类结构隔离方法，LEGION 的知识空间无需预设模块数且能持续增长，解决了传统结构方法难以应对未知数量任务的局限 nature.com 。同时，有别于纯经验回放策略（如 GEM）必须存储大量过往数据，LEGION 通过贝叶斯记忆高效概括了过去任务的精华，不需要完整保留旧样本即可实现类似回放的效果。虽然训练中仍使用有限缓冲作为强化学习的 replay（SAC 算法自身需要）, 但由于知识已存于非参空间，LEGION 即使在不增加缓冲容量的情况下依然能逐任务提高成功率。另外，LEGION 与以往假定任务明确分布范围的多任务学习不同，它能够适应非参数任务变化，处理现实中出现的新规则或新交互形式——这些情景下老方法（例如基于高斯混合的元学习）因需预设任务数量而难以胜任。 具身场景下的优势：LEGION 在真实机器人持续学习中展示出多项优势。首先，其知识保存与扩展机制使机器人几乎无遗忘地累积技能——实验表明，即使经过长时间训练再次回访早期任务，成功率仍接近初始掌握水平，证明旧技能通过知识空间得到有效保留。其次，LEGION 可以将已学知识用于新任务的快速推理和组合，体现了少样本快速迁移能力：作者展示了机器人在学习一系列基本任务后，能够在一次提示下将相关技能串联完成复杂长任务，例如“制作咖啡”，这源于知识空间支持对已学技能的自由组合。再次，LEGION 引入语言降低了感知与任务推断难度，使机器人在多模态指令下表现出色——通过语言描述任务，机器人无需大量探索即可定位对应知识簇，从而更快适应新任务语境 nature.com 。最后，在对比实验中，LEGION 在顺序任务中稳定提升成功率，显著优于“完美记忆”（无限重放）、“reservoir采样回放”和 A-GEM 等方法。这表明在数据受限、任务未知的真实环境中，LEGION 的知识空间机制比传统持续学习策略更适合维持和重用机器人知识。 LEAGUE++: Empowering Continual Robot Learning through Guided Skill Acquisition with Language Models (Zhaoyi Li 等, 2024 年, VLMNM 2024 研讨会) 方法简介：LEAGUE++ 提出了一种利用大语言模型（LLM）引导机器人持续学习技能的框架。该方法将任务与运动规划（TAMP）和深度强化学习（DRL）相结合：先由预训练的大语言模型将复杂长任务自动分解为一系列子任务操作序列（即生成操作符及步骤规划），并产生日志级别的稠密奖励信号指导每个子任务的强化学习训练。机器人通过 DRL 学习各子步骤的低层技能，同时维护一个符号化技能库记录已学的技能。当新任务到来时，LEAGUE++ 使用已有技能库中的技能作为Warm-Start（预热），加速学习新技能。整个系统在四个模拟任务域上进行了验证，包括家务操作等复杂任务场景。结果表明，LEAGUE++ 相比基线有更快的学习速度和更高的任务成功率，并能通过技能重用在新域中显著缩短收敛时间。 核心创新：LEAGUE++ 最大的创新在于将大模型的规划能力和传统强化学习有机结合，实现“边规划、边学习”。具体来说，它利用 LLM 强大的语义理解和分解能力，自动将高层指令分解为低层动作序列（解决了RL直接面对长远稀疏奖励任务时的困难），从而大幅降低了长视距任务的学习难度。其次，它提出了符号技能库概念：已学技能以可调用的符号模块形式存储，使机器人在新任务时可以检索并复用相关技能，而非从零学习。这种技能库机制相当于一种渐进专家网络：随着任务增多，库中专家技能增多，策略可以根据需要调用对应子政策，类似人类在新任务中调用过往经验。最后，LEAGUE++ 将现有预训练模型用于策略初始化（warm-start），例如视觉模型用于感知、行为克隆模型用于初始策略等。这减少了新任务的探索开销，使持续学习更高效稳定。 与传统方法区别：传统持续学习多半依赖网络自身调节参数来避免遗忘，而 LEAGUE++ 则更像一种“学习+规划”双轨方案。它没有采用 EWC/GEM 等直接在RL算法中增加遗忘惩罚，而是通过任务分解将复杂问题拆解，降低每阶段学习干扰。这种方式避免了以往RL持续学习中，不同任务策略存于同一网络导致的梯度干扰，从根本上缓解了遗忘。此外，与 PackNet 等为每任务固定网络模块不同，LEAGUE++ 通过符号调用机制在同一策略网络中执行不同技能，相当于软模块化——技能库中的技能相当于可切换的策略段，无需冻结参数就能隔离任务干扰。相比纯回放或迁移学习，LEAGUE++ 依赖 LLM 提供额外知识（如何解任务、设计奖励），扩充了信息来源：这类似人类导师提供提示，加速了学习而非仅靠算法克服遗忘。可以说，LEAGUE++ 将知识提炼（通过LLM）与技能留存（通过库）结合，超出了传统持续学习框架。 具身场景下的优势：LEAGUE++ 非常契合具身智能体执行复杂任务的需求。首先，LLM 提供的分层规划使机器人哪怕面对长且模糊的指令（例如“准备早餐”）也能一步步拆解执行，避免在长时间任务中遗忘目标或迷失——这在人类复杂指令场景尤为关键。其次，技能库的重用大幅提升了数据效率：当机器人进入新环境或任务，只要能从库中找到类似子任务，就无需从头练习，少样本即可适应。这特别适合真实机器人训练昂贵、无法大量试错的情况。第三，由于采用符号层指导和技能初始化，LEAGUE++ 在每个任务的学习过程都更快、更稳定，降低了资源消耗和试错成本。作者的实验表明，在多个复杂模拟环境（如家庭清洁、装配任务）中，LEAGUE++ 全面优于纯RL的持续学习基线。例如在新域任务上，因复用了旧域技能，学习速度显著快于从零开始的对照。这证明在真实机器人需要反复学习不同任务的长期部署中，引入LLM指导和技能库的框架能够实现效率和稳健性的兼顾，这是传统方法难以同时达到的。 Scalable and Efficient Continual Learning from Demonstration via a Hypernetwork-generated Stable Dynamics Model (Sayantan Auddy 等, 2024 年, arXiv) 方法简介：该工作针对机器人示教学习（LfD）场景提出了一个持续学习方法，利用超网络（Hypernetwork）和稳定动力学模型来保持多技能学习过程中的稳定性和不遗忘。核心思想是训练一个超网络，生成两个子网络：一个用于学习每项示范任务的轨迹动力学模型，另一个生成与之配套的李雅普诺夫稳定性函数。通过在每次引入新示范任务时，让超网络输出相应的轨迹模型并辅以稳定性约束，该方法确保机器人复现的轨迹是收敛稳定的，不会在插值或续航过程中发散。更重要的是，这种引入稳定性的方式还显著提升了持续学习性能：作者发现，相较不考虑稳定性的基线，引入李雅普诺夫函数后，机器人在序列学习多个技能时的遗忘显著减少。此外，为了提高效率，作者提出了分块超网络（chunked hypernetwork）和随机正则化策略，将训练多个任务的总时间从原先的 $O(N^2)$ 降低到 $O(N)$。整个方法在实时的机器人绘画轨迹学习任务（包括 LASA 数据集扩展到高维、和作者自建的 RoboTasks9 实验）上验证，结果显示无论在轨迹精度还是持续学习指标上都优于现有方法。 核心创新：本方法的创新点在于将鲁棒控制中的稳定性保障引入持续示教学习。通过让超网络同时学轨迹模型和Lyapunov 函数，每新增一项技能，系统不仅学会了模仿轨迹本身，还获得了对该技能的稳定约束，使其在整个状态空间具有吸引域。这解决了过去很多 LfD 方法关注模仿误差、却忽视了轨迹泛化稳定性的问题——即使没有回放旧示范，稳定性保障使旧技能的轨迹在训练新技能时不易被扰乱。其次，使用超网络作为核心架构，使得多任务知识统一存储在超网络的权重中，通过不同输入（如任务ID或上下文）生成相应任务的模型。相比传统逐任务微调网络，超网络天然具备参数共享和隔离的双重特性：共享是指不同任务的共性部分自动在超网络中得到整合，隔离是指超网络输出不同任务模型，相当于为每个任务保留了一套隐含的参数。这种方式巧妙地避免了不同任务参数直接冲突覆盖。再次，提出的随机正则项训练方法，通过每次训练仅对一个随机选取的旧任务施加约束，代替以往对所有旧任务都正则化的做法，大幅降低了训练复杂度。这一技巧保持性能不变却将训练开销线性化，提升了持续学习的可扩展性。 与传统方法区别：相比 EWC 等对参数层面的稳定约束，本方法将稳定性提升到行为层面：EWC关心参数不剧烈变化，而该方法直接确保每个技能的输出轨迹在引入新技能后依然收敛不发散，因而对遗忘的防御更直接有效。与 iCaRL/GEM 等需要存储和重放示范不同，该方法不需要对旧示范数据回放：旧技能知识蕴含在超网络权重中，加之稳定性函数的限制，新任务训练不会破坏旧技能轨迹，无需样本重温也能保护旧技能。同时，相较 PackNet 逐任务固定部分网络的硬隔离，超网络属于软隔离：所有任务共享同一个生成网络，但通过任务索引输出不同技能模型，相当于用函数生成参数，既避免干扰又保持容量节省。与之前一项采用超网络+神经ODE方法做 continual LfD 的工作相比（作者提及的最近方法），本方法增加了严格的稳定性保障，因此旧技能保真度和新技能学习效果都有明显提升。总的来说，新方法在理念上融合了控制理论和持续学习，提供了一种不同于以往任何单一范式的框架。 具身场景下的优势：在实际机器人示教应用中，该方法具有显著优势。首先，稳定性约束确保机器人在执行已学轨迹时不会因为学习了新技能而出现意外失稳或偏差，这对物理系统尤为重要——避免了因遗忘造成的运行危险，提高了安全性和可靠性。其次，由于无需反复重训旧示范，机器人能更高效地学习新技能：作者在真实机器人轨迹跟踪任务中展示，新技能加入后旧技能仍能零-shot 执行，表现几乎不下降，这意味着机器人随时可用, 不需要频繁校准旧技能。第三，该方法在高维技能扩展上表现良好，证明其可应用于涉及多自由度的复杂操作（例如机械臂同时控制位置和姿态的轨迹）。传统方法在高维连续控制上的持续学习往往不稳定，而本方法借助Lyapunov函数确保了每个技能在高维空间都是吸引子的，因此具备更强的鲁棒性。最后，在资源受限的机器人上，训练效率的线性提升非常有价值：每新增技能的训练开销不会随着技能数量指数增长，使得机器人可以规模化地持续学习几十上百个技能而不会因为训练时间过长变得不可行。综上，该方法为现实中机器人持续学习大量示教任务提供了安全、高效、稳健的解决方案。 Online Continual Learning for Interactive Instruction Following Agents (Byeonghwi Kim 等, 2024 年, ICLR 2024) 方法简介：这项工作定义了更贴近现实的交互式指令跟随持续学习场景，并提出了信心感知滑动平均（CAMA）算法来缓解遗忘。作者指出，过去大多假设智能体一开始就能获取所有训练任务的数据，而不符合机器人应持续探索、持续学习的现实。为此论文提出两个设置：(1) 行为增量学习（Behavior-IL）：机器人不断学习新指令行为；(2) 环境增量学习（Environment-IL）：在新环境中执行之前学过的指令。困难在于任务边界可能不明确，传统“数据先验”方法（如基于旧任务输出日志概率的蒸馏）需要明确任务边界和缓存旧任务信息。CAMA 则不需要任务边界：智能体在训练过程中对每个观察到的指令计算模型输出的置信度，并以滑动平均的方式更新其对旧任务的输出分布估计。当模型学习新行为时，如果对某类以前学过的指令置信度下降，CAMA 会根据之前维护的滑动平均分布对模型进行轻微调整，以拉回对旧知识的信心，从而达到持续无边界学习的目的。实验证明，在作者设计的新基准上（包含连续新增指令和环境的条件），CAMA 相较现有方法取得了显著更好的表现。 核心创新：CAMA 的创新在于引入任务无关的置信度追踪机制：传统方法如 LwF 或 iCaRL 需要存储每个旧任务的模型输出分布或实例样本，当新任务到来时通过知识蒸馏或重放维持旧任务性能。这实际隐含了已知的任务边界和任务身份。而 CAMA 不存储具体样本或明确任务ID，而是对模型输出信心做持续监控。它维护一个滑动平均估计，可视作模型对过去经验的“模糊记忆”，随着时间衰减地记录旧任务的输出倾向。当模型学新任务时，如果这种倾向发生显著漂移，表明遗忘在发生，那么CAMA依据滑动平均的差异，对模型参数进行微调修正（如调整输出层偏置等），无需知道具体哪个任务受影响，只基于置信度变化即可纠正。这种方法跳出了任务级别的框架，实现了任务无关（task-free）**的持续自稳训练。另外，CAMA 所需存储的信息量极小，仅为每个输出类别一个滑动均值，计算与存储开销极低，非常简洁却有效。 与传统方法区别：与 EWC 等正则方法需要在参数变化上加全局约束不同，CAMA 工作在输出空间：它不直接限制权重，而是看模型对以前输出的信心水平是否下降。这有点类似知识蒸馏（LwF）的思想，但不需要旧模型保存——蒸馏通常要保存旧模型输出作为固定目标，而CAMA持续更新的滑动平均本质上充当了“旧模型记忆”，避免了存储多个旧模型或大量旧样本。相比 iCaRL 维护每类样本代表来近似旧任务分布，CAMA 维护的是压缩的统计量（置信度均值），因而不需要样本库也没有额外模型，只需少量内存。与 GEM 这类基于梯度投影的方法相比，CAMA 算法更简单，不需要复杂的二次规划求解，只以移动平均做调整，在线即可执行。另外，CAMA 属于任务无边界的方法，解决了传统方法大多假定任务切换已知、或者需要在检测到切换后才能应用策略的问题。在现实机器人持续学习中，任务变化往往连续发生且未必有明显分界，CAMA 正是面向这种情况设计的。 具身场景下的优势：CAMA 所针对的交互式指令跟随场景极具代表性：机器人在不断遇到新指令、新环境的过程中持续学习，这对服务机器人等非常实际。CAMA 能在无监督任务切换的前提下，让机器人保持之前学会的指令技能。例如，一个家庭助理机器人在学会一系列语音指令后，被主人带到新房间教它新任务，CAMA 确保机器人在学新任务时不会忘记旧房间的指令执行方法。由于不需要存储大量过往数据，CAMA 也适合长期部署：机器人可以不断学习数十数百种指令而不必担心存储开销或隐私问题（无需录音或录像存档）。同时，CAMA 的实时性很高，它在模型训练时即可动态调整参数，无需离线阶段，适合机器人在线学习的需求。作者报告该方法在提出的 Behavior-IL 和 Environment-IL 基准上显著优于已有的持续学习算法，这表明它成功地解决了任务模糊情况下的遗忘问题，让机器人在变化的环境和任务中保持稳健性能。总之，CAMA 为具身智能体提供了一种轻量级但有效的持续学习机制，使其能像人一样一边执行指令、一边随着环境改变不断积累本领。 ICAL: In-Context Abstraction Learning for Multimodal Agents (Gabriel Sarch 等, 2024 年, arXiv) 方法简介：ICAL 提出了一种利用大模型自身的生成与内省能力，持续提升多模态智能体决策的方案。该方法关注这样的问题：大型语言或视觉-语言模型（LLM/VLM）虽具有强大的Few-shot能力，但需要高质量范例作提示。人为提供高质量示例既昂贵又不具通用性。ICAL 的解决方案是：让大模型从次优的示范轨迹和人类反馈中自动生成可用于提示的抽象示例。具体流程包括：给定一个全新领域的嘈杂演示（例如一个任务的视频示范，可能包含低效动作或错误），首先由视觉-语言模型对该轨迹进行解析抽象。模型会产出一个通用的程序化描述，修正了示范中的低效步骤，并添加认知注释，标注出任务涉及的关系、对象状态变化、时间子目标和关键细节等。接下来，让真实机器人/代理尝试执行这份抽象计划，在执行过程中人类可以提供自然语言反馈纠正模型的错误理解或补充知识。模型根据反馈交互式地细化之前产生的抽象。最终得到的这些抽象示例（带有语言注释和优化后的步骤），被存储起来作为内存范例。在后续决策中，大模型可以将这些示例作为提示的一部分，从而显著提升对于类似任务的决策能力。ICAL 在三个具有挑战的环境中验证：TEACh对话式指令跟随、VisualWeb互联网多模态代理、Ego4D第一人称视频动作预测，都取得了新的SOTA性能。 核心创新：ICAL 将持续学习转化为持续积累提示示例的过程，而非传统的持续调权过程。这是一个范式转变：与其反复梯度更新模型参数，ICAL 让模型自己“理解反思”示范并生成抽象的知识总结，逐步丰富模型的提示库。这样的内省式示教机制是首次提出。具体创新点包括：(1) 自动抽象：利用预训练VLM从视觉示范中提取高层语义——不像以往只关注低级动作序列，ICAL 提取了任务因果关系、对象状态变化、时间逻辑、任务构造等四类认知抽象，这些抽象比原始示范更精炼、更具概括性。(2) 人机交互细化：ICAL 不仅依赖模型自我生成，还引入人类在模型执行时给出自然语言反馈，让模型迭代改进其抽象。这种循环使得次优示范（含错误或低效部分）也能通过反馈纠正而变得有价值，突破了以往示范学习要求专家示范的限制。(3) 提示记忆库：将生成的抽象示例存入一个不断扩展的库，并使用检索增强的大模型决策——模型在推理新任务时，会从库中取出相关示例放入提示，提高决策准确率。ICAL 相当于在持续学习过程中渐进地提升模型的上下文学习能力：模型见到的新任务越多，就自行总结出越多可泛化的知识片段，下次遇到类似任务时即使不调权也能通过提示完成。实验显示，ICAL 随着学习示例增多，模型性能持续提升，呈现出真正的“持续改进”特性。 与传统方法区别：传统持续学习多在参数空间操作，如EWC约束权重变化、经验回放存数据样本等。而 ICAL 完全绕开了参数遗忘问题：模型主体参数基本保持不变（或只在最后有小幅微调作为增强），因此谈不上灾难性遗忘。取而代之，ICAL 通过不断加入新的知识提示，让模型具备解决更多任务的能力。这种基于大模型提示的持续学习不同于以往任何持续学习范式。与知识蒸馏类方法相比，ICAL 并非让模型去逼近旧模型输出，而是生成更好的知识为己所用；与模块化或正则方法相比，它无需设计模型结构拆分或损失项，利用的是LLM/VLM自身强大的Few-shot学习能力和生成能力来“吸收”经验。在需要多模态理解和指令执行的任务中，ICAL 运用了视觉和语言的结合，这比单纯依赖视觉记忆（如CV领域一些记忆网络）要丰富，也比仅语言的持续学习（如对LLM增量训练新知识）更直观：ICAL 让模型直接看视频学，也听取人类语言，得到的是跨模态的知识。这些知识用自然语言+视觉标记来存储，具有很强的可解释性和可移植性。传统持续学习很难在不调整参数情况下大幅提高性能，而ICAL 展示了另一种可能：通过增强模型输入（上下文）而非改变模型本身来实现持续学习。 具身场景下的优势：ICAL 针对的场景包括家庭机器人任务（TEACh）、网络操作代理以及视频预测，都与具身智能密切相关。它的优势在于：对于复杂开放环境，很难人工定义明确的任务边界或提供足够训练数据，而ICAL 利用了现有大型模型，只需较少示范和交互即可让模型适应新任务。因此在具身智能典型的少样本、多样任务情况下，ICAL 能快速扩展技能。此外，它不要求持续占用机器人反复训练（除了必要的人机对话反馈），大部分“学习”都在模型的推理过程中完成（让模型生成抽象并评估）——这意味着机器人可以在线学习新任务而不中断服务，通过对话方式边干边学，正如人类新手在导师指导下学习新技能一样。对于任务模糊或长时间任务，ICAL 的抽象总结能力尤为关键：机器人可以从冗长的示范中提炼要点，避免执行时被次要细节干扰。这提高了机器人在复杂任务下的鲁棒性和泛化。实验证实，ICAL 在 TEACh 基准上将先前SOTA的目标达成率提升了12.6%，在VisualWebArena中成功率从14.3%提高到22.7%，在Ego4D动作预测上击败了Few-shot GPT-4V。更重要的是，性能提升是持续的：每当学习更多示例，模型表现就有所提高。因此在长期来看，ICAL 提供了一个持续增长智能体能力的途径，而无须反复训练模型参数，大大降低了具身人工智能系统维护的成本和风险（例如不会出现传统持续学习不慎遗忘必须人工纠正的问题）。这表明，充分利用大模型强大的自监督和内省能力，或许是具身智能持续学习的一条有效新路。 MLLM-CL: Continual Learning for Multimodal Large Language Models (Hongbo Zhao 等, 2025 年, arXiv) 方法简介：MLLM-CL 提出了一个多模态大模型（视觉-语言模型）持续学习的基准和方法。该工作将持续学习划分为两种情形：(1) 领域持续学习（Domain CL）：模型依次学习遥感、医学、科学、自动驾驶、金融等不同视觉问答领域的知识；(2) 能力持续学习（Ability CL）：模型顺序学习OCR识别、数学逻辑、视觉感知、GUI操作代理等不同能力。作者发现，传统增量学习范式（每次在上一个模型参数基础上微调新任务）不适用于多模态大模型：直接用上一任务的权重初始化下一任务，会损害模型对新任务的可塑性，导致次优结果。为解决这一任务冲突问题，MLLM-CL提出了MR-LoRA方法。具体来说，在每个新任务到来时，并不在原模型权重上继续微调，而是为该任务新建一个LoRA低秩适配器，从零随机初始化。这样每个领域/能力都有自己独立的LoRA模块，避免了继承上个任务权重带来的冲突，同时LoRA只引入极少参数，保证高效。在推理时，为了自动选择对应任务的LoRA，作者设计了一个基于大模型的路由器：利用多模态大模型本身处理复杂输入的能力，输入待测试样本后，由模型生成一个路由指令，选择最合适的专家LoRA来回答。这个路由器通过在持续学习每阶段后收集少量任务样本，对大模型进行轻量few-shot微调得到。总体而言，MR-LoRA包含多LoRA专家 + 大模型路由两部分。实验结果显示，在上述多个领域和能力的持续学习任务序列中，该方法在所有任务上的平均性能和最终性能都超越了现有方法，如直接微调、参数隔离、基于提示的方法等。 核心创新：MLLM-CL 的创新在于针对大模型的持续学习提出了避免遗忘的新范式。其一，每任务新LoRA的策略突破了以往串行微调“一脉相承”导致性能下降的窘境。这种做法类似“进渐网络（Progressive Networks）”，但由于LoRA参数规模小，可以在不剧增参数的情况下无限扩展到更多任务。各任务的LoRA彼此独立，保证旧任务参数不受新任务影响，从根本上杜绝了遗忘。同时，所有LoRA附加在同一个大模型上，共享其通用表示能力，又实现了知识迁移（因为基础模型权重保留了多任务共性表示）。其二，引入大模型驱动的路由机制，这是此前持续学习中少有探索的。传统任务识别通常依赖样本的特征相似度或任务ID，而该方法让多模态大模型读入输入后自主生成路由决策。由于大模型本身掌握高层语义和复杂推理，它可以比简单特征距离更精准地选出对应领域的LoRA专家。这保证了在推理阶段对于复杂多模态输入也能正确地匹配到相应能力模块。其三，该方法还建立了系统化的多模态持续学习基准（MLLM-CL基准），涵盖不同类型迁移（领域和能力）并提供了评价指标和数据集构建流程。这填补了多模态大模型持续学习缺少评测标准的空白，为后续研究提供了平台。 与传统方法区别：MLLM-CL 在设计上融合了参数高效微调和专家路由思想。与 EWC 等全模型正则不同，它不直接约束原模型参数，而是固定主干模型，只对每任务引入少量新参数，这避免了原模型权重冲突累积。与 PackNet 类似的方法相比，MR-LoRA 也采用了“每任务额外参数”的思路，但PackNet通过剪枝固定网络容量，MR-LoRA 则用可增添的LoRA实现弹性容量，理论上任务越多仅线性增加参数，无需预留容量。和提示学习（L2P、ModalPrompt 等）相比，提示方法在多模态大模型上效果有限，而且往往需要固定提示长度、可能仍有干扰；MR-LoRA 通过独立LoRA完全隔离任务，比软提示更干净利落。还有，许多多头网络或专家混合模型需要已知任务ID才能选专家，MR-LoRA 则通过路由器实现任务自动判别，不需要人工指示任务类别。可以说，它将模块化与任务识别两个问题一起解决了。在多模态场景下（例如同时应对图片文本、多任务问答），这种方法比单纯视觉或单纯语言的持续学习方法更复杂但也更全面地考虑了输入多样性和任务多样性。 具身场景下的优势：虽然 MLLM-CL 的实验主要是多模态问答和 GUI 任务，但其思想对许多具身智能应用同样有益。比如，一个家庭服务机器人具备多个视觉语言能力（识物、对话、读屏、算术等），MR-LoRA 可以让其持续添加新能力而旧能力不衰减：每项新技能加一个LoRA，机器人就掌握了新本领，又不会遗忘以前学过的（因为以前的LoRA保留原样）。这样的能力库扩展非常符合具身AI逐步进化的需求。其次，由于LoRA模块小，机器人可以在资源受限设备上部署多个技能专家，而不会像扩增整个模型那样内存爆炸。例如针对移动设备，把不同领域的视觉问答能力拆成多个LoRA加载，按需调用，远比加载多个大模型高效。再次，路由器机制使机器人在遇到新感知输入时能自动判断调用哪种能力，这对于多模态交互场景很关键——现实中用户不会明确告知机器人“这是一道医学问题”或“现在开始OCR任务”，机器人必须自适应切换内部技能。MR-LoRA 的路由方案正是训练机器人根据输入自行选择专家。作者实验也表明，在跨领域问答中，MR-LoRA 能正确选择相应LoRA模块，最终在所有领域上同时取得高精度，相比其它方法在后期任务训练后前期任务精度大幅下降，MR-LoRA 几乎零遗忘且新任务精度也高。这证明了其稳定性和塑造性兼顾的能力，非常适合需要终身学习的多才多艺型机器人。总之，MLLM-CL 为具身智能体集成多模态大模型指明了一条持续进化的道路：通过低秩模块化扩展和智能路由，实现持续学习众多技能而性能不减。 Task-Unaware Lifelong Robot Learning with Retrieval-based Weighted Local Adaptation (Pengzhi Yang 等, 2024 年, arXiv) 方法简介：此工作面向机器人开放环境，提出了无任务标识的终身学习框架，结合检索式记忆和局部微调来提升持续学习效果。在真实世界中，机器人遇到的新任务往往没有清晰的边界或ID，且无法存储海量以往数据。为此作者的方法在机器人学习过程中维护一个情景记忆（Episodic Memory），存储每个任务的一小部分关键示例。当机器人在序列学习新技能时，主要模型仍采用常规的基于经验回放的训练（比如采用一部分记忆样本进行练习，以降低遗忘）。然而仅靠有限回放仍难免遗忘一些早期技能。因此在测试执行阶段，如果机器人遇到类似以前学过却已部分遗忘的情境，该方法会从记忆中检索最相关的过往示例，并对当前策略进行快速局部微调（local fine-tuning），以恢复在该情境下的性能。为了提高这种局部微调的效率，作者引入选择性加权机制：首先让机器人在当前情境下执行若干回合，记录其性能（例如哪些步骤出错），然后将这些失败轨迹与记忆中检索到的示范进行比对，自动衡量哪部分旧示范对当前情况帮助最大，给予这些片段更高权重来指导微调 arxiv.org 。简言之，该框架模仿人类温习知识的过程：在需要时重点复习遗忘的难点，从而高效恢复技能。整个方法适用于没有明确任务划分的开放式场景，作者在 LIBERO等操作任务基准上测试了该方法，结果表明即使任务无明显边界，机器人也能持续学到新技能并在需要时重新找回旧技能，大幅优于不采用检索适应的基线。 核心创新：该方法的创新之处在于提出了任务无关的回顾与适应机制。传统持续学习通常假定任务边界明确（如任务开始和结束）以便采取对应策略，比如任务后冻结部分网络或保存样本。然而现实中机器人可能连续不断遇到各种挑战，无法提前知道哪些属于同一任务。本文通过检索式记忆绕过了显式的任务划分：无论当前遇到的情况是否是以前练过的任务，机器人都可以基于当前观测从记忆库检索类似经验。这赋予了模型一种情景感知能力，让它能够自行判定何时需要参考旧经验。其次，提出的Weighted Local Adaptation将元学习思想引入了持续学习测试阶段：不像传统只在训练阶段防遗忘，这里在测试阶段也运行一次短暂微调，相当于在机器人执行时刻临时提高旧技能专门性。这种做法以往少见，因为通常假设模型定型后就执行不变，但在终身学习背景下，适度的测试微调可以极大提升旧技能复现效果。通过限制微调只针对检索到的示例且有选择地重点学习难点，既保证了微调幅度受控不会破坏模型总体性能，又有针对性地弥补遗忘。第三，任务无关意味着模型无需任务标签输入或边界信号，这对开放环境学习非常关键。作者利用视觉和语言嵌入的一致性作为检索键值 arxiv.org ，避免了因持续学习造成的表征漂移（通过预训练模型确保不同阶段embedding空间一致）。这保证了即使模型学了新东西，仍能用统一标准去查找旧记忆，提高了检索可靠性。 与传统方法区别：相比 EWC等在训练时防遗忘的方法，该框架将遗忘补救延伸到了测试时刻：传统模型一旦训练完部署，如果遗忘了旧技能往往束手无策；而此方法允许机器人在执行某任务前“温习一遍”相关经验，相当于给模型一个自行恢复的机会。与 GEM 等在训练时用小样本回放不同，这里的记忆检索主要服务于执行阶段的微调，而非整个训练过程持续混入，因而不会显著增加训练难度，同时又充分利用了记忆在关键时刻的价值。和 iCaRL 直接根据记忆做最近邻推断不同，该方法选择的是微调模型，因此能适应当前状况的细微差别，而不是简单模式匹配，效果更强。在没有任务ID的情况下，很多传统方法如多头网络就无法应用，而本方法完全不依赖任务标签或边界，通过检索+微调实现了隐式的任务处理。可以将其视作结合了经验回放和快速自适应：既保留少量记忆（回放理念），又在需要时通过微调快速适应（元学习理念），融合了二者优点。 具身场景下的优势：对于在动态未知环境中工作的机器人来说，该方法提供了极高的灵活性。机器人无需预先知道将面对哪些任务，也不用在模型结构上做固定分配，它可以不断遇新仍保持从容：每当遗忘可能影响当前任务时，就从记忆中找回相关信息补强自己。这很像人类在现场快速翻阅笔记确认知识点，从而表现出色。尤其在长期部署中，机器人可能几天甚至几月不执行某项技能，这种情况下直接执行可能失败，但如果允许机器人在执行前复习一下过去案例，就能大幅提升成功率。作者的方法正是提供了这种快速热身功能，让机器人在现实使用中更可靠。其次，由于只存储挑选的一小部分示范数据，记忆库很轻便，不会像保存所有数据那样不现实。并且由于采用预训练embedding统一表示，不同时间学习的经验可以共同检索，这意味着机器人可以在跨环境、跨时间的情况下整合经验，非常符合具身智能需要持续集成多方信息的特性。最后，实验中该方法在没有任务边界信息的情况下，其性能超出了使用明确任务边界且大量回放的策略，体现出开放场景适应性。总之，此方法赋予机器人一种人类般的学习策略：即使偶尔遗忘细节，通过快速翻阅过往经历又能想起来，从而在不断变化的任务挑战中保持整体能力不退化，为具身智能的长期自主学习提供了新的思路。 方法演化趋势与对比 综上所述，2024–2025 年关于具身智能持续学习的方法展现出一些共同的发展趋势与对过去方法的根本区别： 从参数保护到知识重组：过去方法（EWC 等）通过阻止参数改动来防遗忘，但在机器人复杂任务中效果不佳。新方法更强调对知识的表达与重组，如通过前缀提示、技能库、知识空间等将知识单元化，避免直接在同一参数上博弈。这些机制允许共享与隔离并存：共享的是不同任务的共性（如PSPL原语、LEGION语言嵌入），隔离的是各任务独特部分（如每任务LoRA、每技能提示）。因此新方法能在不牺牲塑性前提下保持稳定性。 从任务清晰到任务模糊：许多新工作针对任务边界不明的现实。比如任务无关检索、CAMA 的置信度调整都不需要预先知道任务什么时候切换。相比之下，传统方法大多假定任务切换明确并提供信号，现实中难以满足。新方法通过连续监测模型行为或环境，相当于赋予模型自适应觉察能力，真正做到持续学习“终身运行”而非分阶段训练。 从被动防御到主动利用：以前方法把旧知识当需要保护的内容，新方法则倾向主动利用旧知识来帮助新学习。例如PSPL利用旧技能提示加速新技能习得、LEGION 用旧知识簇推断长序列任务、LEAGUE++ 复用已学技能解决新问题。这种正向迁移思路解决了传统方法虽然防遗忘但也不擅长迁移的问题，新方法在防止遗忘的同时显著提高了前向迁移性能，使机器人越学技能越多，解决新任务反而越快。 减少对存储和重放的依赖：传统方法如经验重放在机器人上往往不可行（存储无限增长且隐私风险）。许多新方法都强调“不依赖回放”：PSPL 无需回放旧经验、CAMA 无需存样本只存统计、LEGION 只以有限回放辅以知识空间、Hypernetwork 方法不重训旧示范。即便需要，也以小规模记忆或生成器代替全量存储（如Task-Unaware方法的小记忆+检索、本质上非常有限）。这使持续学习更能适应机器人存储和计算受限的条件。 引入大模型与多模态：与以往专注单一模型训练不同，新方法勇于将预训练大模型纳入持续学习框架，如利用LLM规划子任务（LEAGUE++）、用VLM自我生成知识（ICAL）、大模型本身作为路由器和骨干（MLLM-CL）。多模态信息（语言、视觉、触觉）也被融入，如LEGION用语言辅助任务编码、PSPL用文本和光流提示、ICAL用视频+语言反馈。这些让机器人更好地理解任务和环境，也提供了额外手段缓解遗忘（例如语言描述可作为语义锚点，帮助模型回忆对应技能）。这是持续学习与大模型、多模态技术的交叉融合，标志着具身智能持续学习进入一个更智能更复杂的阶段。 小结：传统持续学习方法在具身智能情境下面临诸多挑战，如参数共用导致遗忘、任务未知导致方法失效、资源有限导致回放不可行等。2024–2025 年的一系列新方法通过全新的机制——提示网络、非参知识库、超网络稳定、符号技能库、任务无关调整、大模型自监督等，成功地解决或缓解了这些问题。在机器人连续学习复杂技能、跨模态理解指令的任务中，这些方法展现出显著优于旧方法的性能，有的甚至实现了遗忘几乎为零且持续正迁移的效果。可以预见，未来具身智能持续学习将沿着模块化+共享、智能检索+自适应、融合大模型知识的路线继续发展，使智能体更接近人类的终身学习水平，在不断变化的世界中保持学习新知识的同时不忘记旧本领。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-16/","summary":"Avoid Catastrophy forget","title":"Bug Journal 2025-06-16"},{"content":"一、Vision-Language-Action (VLA) 模型 论文 目标 创新 平台 Code 算力 总结 UniAct 将 28 种机器人异构动作映射到统一“通用动作空间”，提升跨形态迁移 动作离散化＋对比约束学习，0.5 B 参数模型优于 7 B 基线 Open-X Embodiment、Libero、Droid；64×A100 训练 GitHub 64 × A100，10 天训练 UniAct 针对不同机器人动作空间异构的问题，提出“Universal Action Space”，把 28 种平台的演示映射到一组离散原子行为，使跨形态学习成为可能。该框架通过语义对比与动作重构双重约束，让 0.5 B 参数模型在多任务操控上超越 7 B 基线。实验表明，在 OXE 与 LIBERO 任务上显著提高数据效率与泛化性。 MoManipVLA 把固定底座 VLA 策略快速迁移到移动底座 双退火搜索选基座位姿＋SLSQP 精细优化 OVMM benchmark；Hexman Echo Plus + RM65 TBD 4×RTX 3090（作者建议） MoManipVLA 设计“双退火 + SLSQP”策略，把固定底座 VLA 生成的末端轨迹快速迁移到移动底座，实现零样本导航-操控一体。方法不改动原 VLA，只在推理阶段搜索最优基座位姿并微调手臂解算器。仿真与 Hexman + RM65 实验展示跨房间场景的 70 %→89 % 成功率提升。 CoT-VLA 为 VLA 引入显式visual chain-of-thought时序规划 先自回归预测未来视觉帧，再输出短动作序列 PyBullet 6DoF 手臂模拟 TBD 8×A100（7 B 模型预训练） CoT-VLA 将“视觉链式思考”引入 VLA：模型先自回归预测未来图像帧，作为显式视觉目标，再生成短动作序列。这样的分两步推理显著提升复杂操作的时间规划能力，并在 RT-X benchmark 刷新成绩。其 7 B 版本在无需额外模态标签下超越同规模基线。 SOLAMI 3D 虚拟角色“看-说-动”社交交互 合成 SynMSI 多模态对话 + 双塔解耦发声/动作 Oculus Quest 3 VR 前端；2×H800 推理 GitHub 32×V100 预训，16×V100 指令微调 SOLAMI 是首个端到端“社交” VLA 框架，可同时生成语音与动作驱动 3D 角色与人沉浸式互动。作者合成 6.3 K 多轮多模态对话数据 SynMSI 并预训练双塔模型，显著提升动作-语音一致性。用户 VR 研究验证，其角色在情感贴合度上优于 GPT-4o + 动画基线。 PVM Revisit 系统评测 PVM 预训练策略在机器人任务中的效果 提出 SlotMIM：在非对象中心数据上也保留 object-centric 表征 Franka Kitchen / Meta-World / Habitat GitHub 8×A100 训练 论文系统比较 DINO、iBOT、MAE 等预训练方法对机器人任务影响，发现“对象中心”特征是关键。为解决非对象中心数据劣化，提出 SlotMIM：语义瓶颈 + 跨视图一致约束诱导对象显式槽位。在 Franka Kitchen、Habitat 等 8 项任务全面超越现有 PVM。 Think Small, Act Big (PPL) 终身学习中复用“动作基元”避免遗忘 Primitive Prompt+Motion-Aware Query与 Diffusion Transformer MimicGen + LIBERO (sim)；Franka Panda TBD 4×A100（预训），1×A40（增量学习） Primitive Prompt Learning 首先在多技能库中学习可重用“动作基元”提示，再在终身阶段冻结旧提示、增量插入新提示，缓解灾难遗忘。模型利用光流-文本查询选择最相关基元，引导条件扩散 Transformer 产出动作。仿真与 Franka 实测均较 PEFT + 经验回放提升 20 % +。 Phoenix 失败后自我反思并细粒度修正动作 LLM 生成“语义-动作”双反思 → Diffusion Policy 校正 RoboMimic 仿真 + UR5 实机 GitHub 单张 RTX 4070 即可微调 Phoenix 通过 MLLM 生成“语义反思+动作反思”文本，先粗调运动指令，再用条件扩散策略做高频微调，实现失败后的自我纠正。框架把泛化压力转移到 LLM 层，低阶策略仅需少量新数据迭代。RoboMimic \u0026amp; UR5 显示跨任务鲁棒性提升 25 %。 OmniManip 物体中心交互基元 + VLM 推理，实现开箱即用操控 定义 canonical 空间 + 方向约束；零样本泛化 Franka Panda + RealSense GitHub TBD 4×A6000 训练 OmniManip 引入 “Object-Centric Interaction Primitive” 作为中间空间约束，让 VLM 只需输出 3D 关键点与方向即可驱动精密操控。通过零样本 IK + 轨迹优化，在 Franka 上完成 12 项复杂任务，平均成功率 78 %。方法避免昂贵 robot-Finetune，兼具成本与泛化优势。 Domain Discrepancy Mitigation 减小人演示与机器人视觉差 Vision Adapter 对齐嵌入；双塔 CLIP 训练 RLBench；xArm 7 GitHub 4×A6000 论文揭示人演示视频与机器人视觉存在形态与尺度差异，导致直接迁移失效。作者在视觉编码器尾部插入可学习适配器，并用跨域对比损失同时拉近语义与几何分布。RLBench 与 xArm7 实验显示在 unseen-object 设定下成功率翻倍。 Object-Centric Prompt-Driven (CrayonRobo) 用彩线标注关键方向减少冗余 四色线标 + GPT 选择最相关 3 条 SAPIEN + Franka GitHub \u0026gt;40 GB VRAM 研究用四色线条在图像中显式标注接触点与位姿方向，减少语言与视觉冗余。GPT 自动从 32 候选线中挑 3 条最相关提示，送入 CLIP-LLM 预测 SE(3) 接触位姿后由 IK 执行。模拟与 Franka 真实机结果优于端到端 VLM 8–12 pp。 Robotic Visual Instruction 用箭头/圆圈视觉语言代替冗余文本 手工 15 K 图像标注 RoVI 语言；VLM 直接生成代码 UR5 / XArm6 + SAPIEN GitHub TBD 1×A40 RoVI 提出箭头、圆圈、颜色、数字四元素的视觉指令语言，解决纯语音交互空间精度不足与嘈杂场景受限问题。作者手工标注 15 K 图像并用 VLM 生成任务代码，机器人可直接按图索骥。UR5 /X-Arm6 实验展示在无语音环境中完成多步装配。 RoboGround 结合语言先验做抓取/推动等 统一 Vision-Language-Action Prompt SAPIEN；Franka Panda GitHub 8×A100（训练） RoboGround 以 grounding-mask 作为视觉中介，将对象定位与策略网络分离；同时自动合成大规模仿真数据扩展训练域。结果在 see/unseen 物体抓取、推拉任务上均超越 End-to-End Diffusion Policy。 二、Policy / Diffusion 控制 论文 目标 创新 平台 Code 算力 总结 KStar Diffuser 双臂协作轨迹生成 物理关节动态图 + 可微前向运动学 Bimanual Franka (sim \u0026amp; real) TBD 8×A100 KStar Diffuser 用机器人双臂物理拓扑构造动态时空图并融入可微正运动学，引导扩散过程生成协调关节动作。此设计在 bimanual 夹取与对接任务上比 baseline Policy Diffusion 提升 17 %。引入 kinematic loss 使收敛速度加快 35 %。 RoboPEPP 视觉-关节姿态预训练 时序预测任务 + 姿态编码器 Isaac Gym 7DoF GitHub 4×A6000 RoboPEPP 把“遮盖-预测”自监督移植到机器人图像，强迫编码器重建被 Mask 关节嵌入，从而学到物理结构感知。微调后在多数据集姿态估计误差降低 30 %，对遮挡鲁棒性最强且推理耗时最低。 Lift3D Policy 将 2D 基础模型迁移到 3D 抓取 深度 Lift 模块 + Domain Adapt - GitHub 8×A100 Lift3D 先用任务感知 MAE 为 2D 基础模型注入深度重建能力，再通过坐标映射“抬升”到点云编码，构建显式 3D 表征。方法在 ManiSkill 等 3D 任务大幅超过纯 2D 预训和显式 3D CNN 基线。 PDFactor 三视角扩散场统一多任务策略 Tri-Perspective 视图条件扩散 Meta-World GitHub 8×A6000 PDFactor 以“鸟瞰-第一视角-自由视角”三视图为条件，学习统一 Policy Diffusion Field，同步处理抓取、转动、插配多任务。多视角条件显著提高迁移，CVPR 实验成功率相较单视角提升 22 %。 Two by Two 跨任务配对装配 Pairwise Object Assembly + Transformer SAPIEN GitHub 4×A100 作者发布含 517 物体对、18 类日常装配任务的大型 2BY2 数据集，并提出等变 SE(3) 两步姿态估计方法。新方法在所有装配任务上刷新 SOTA，并在真实机器人验证兼容性。 FlowRAM Region-Aware Mamba + Flow Matching 局部注意 + OT 监督 RLBench TBD 4×A40 FlowRAM 结合 Region-Aware Mamba 感知器与 Flow-Matching 生成器，统一视觉编码与动作扩散，提高采样效率。测试显示对 occlusion 与 multi-object 情况鲁棒性显著提升。 G3Flow 3D 语义流生成多场景抓放 TSDF + Diffusion Flow Habitat GitHub 8×A100 G3Flow 用 TSDF 构建稠密场景，再以生成式 3D 语义流预测目标-手序列，实现跨场景抓-放一体。Pose-aware 设计让训练迭代减少 40 %，零样本任务成功率提升 18 %。 DexHandDiff 自适应灵巧手规划 接触感知 diff + 物理约束 Shadow Hand TBD 8×A100 DexHandDiff 引入手-物接触显式编码与能量约束到扩散规划中，避免“漂浮抓取”幽灵态。框架在 Shadow Hand 抓转与重定位七项基准刷新记录。 Tra-MoE 多域轨迹预测条件策略 动态专家 gate + BC 初始化 ManiSkill2 TBD 4×RTX 3090 Tra-MoE 通过稀疏门控专家网络吸收多域外数据，预测任意目标轨迹，并以可学习 2D mask 对齐视觉观测指导策略。实验证明在 DomainGap20 % 情况下仍维持高成功率。 AffordDP 利用可迁移 affordance 的泛化策略 Affordance Mask + Diffusion Policy LIBERO TBD 4×A6000 AffordDP 把 3D 接触点 + 轨迹定义为可迁移 affordance，并在扩散采样中注入 6D 变换引导，支持 unseen 类别操控。与原 Diffusion Policy 比，新架构在真实实验 unseen 物体上提升 30 % 成功率。 三、Grasp 论文 目标 创新 平台 Code 算力 总结 UniGraspTransformer 通用抓取蒸馏 Transformer-Distil 模型简化训练 Shadow Hand (sim) GitHub 4×A40 提出通用 Transformer 抓取网络及简化蒸馏流程，既减低训练成本，又扩展到海量手型。实验证实缩短 40 % 训练时间同时保持高成功率。 DexGrasp Anything 面向任意物体的物理感知抓取 物理引擎约束 Embedding Franka + Mujoco GitHub 8×A100 DexGrasp Anything 在训练和采样阶段显式加入物理约束，并发布 3.4 M 抓姿-15 K 物体数据集。方法在多 benchmark 抓取精度全线领先。 ZeroGrasp 零样本形状重建驱动抓取 SDF ↔ Partial Depth BP Isaac Gym GitHub 4×A6000 ZeroGrasp 联合实时 3D 重建与抓姿预测，实现 zero-shot 抓取；采用 SDF 先验避免碰撞。近实时推理 (\u0026lt; 50 ms) 在模拟和 Franka 验证效果稳定。 四、Humanoid 论文 目标 创新 平台 Code 算力 总结 Humanoid Hiking 复杂崎岖地形步行 多技能 Curriculum + Terrain Adapt Unitree H1 TBD 8×A100 LEGO-H 框架通过层次 Transformer 预测未来局部目标，并以特权学习将视觉导航与步态控制整合，使模拟 humanoid 能走崎岖山路。跨多机器人形态验证了迁移与鲁棒性。 MobileH2R 合成数据学手递物 Unity-based Synthetic Human→Robot 数据 Clearpath Ridgeback + UR5 GitHub 4×A40 MobileH2R 用 Unity 合成高质量人-机器人交接数据，使移动底座在大空间内可靠收物。Sim-to-Real 迁移到 Ridgeback + UR5，达到 90 % 交接成功率。 五、3D Vision \u0026amp; Perception 论文 目标 创新 平台 Code 算力 总结 3D-MVP 用多视图 MAE 预训 3D 表征 分离视图编码 + Objaverse 预训 RVT + ManiSkill TBD 8×A100 3D-MVP 把 MAE 扩展到多视图 point-cloud，利用 RVT 对齐 voxel-level 特征；在多任务上全面提升 3D 理解与操控性能。 VidBot 2D 野外视频 → 零样本 3D 轨迹 SfM + 粗-细阶段扩散 Hexman Echo Plus TBD 4×A6000 VidBot 从网络 2D 视频自动恢复手轨迹与目标点，再通过粗-细扩散生成 3D 交互轨迹，实现零样本部署；在 13 项家庭任务上显著超越 SOTA。 Touch2Shape 触觉条件 3D 形状扩散 Vision-Touch 融合扩散 GelSight + iCub 手 TBD 4×A100 Touch2Shape 将 GelSight 触觉嵌入 3D Diffusion，结合 RL 引导探索，实现高保真局部细节重建；在触觉+视觉形状重建基准取得最佳分数。 六、Planning \u0026amp; Reasoning 论文 目标 创新 平台 Code 算力 总结 RoboBrain 从抽象到具体统一规划 层级 Brain 模型 + 递归搜索 SAPIEN (多机器人) GitHub 8×A100 RoboBrain 融合机器人与通用多模态数据，多阶段训练 MLLM，能处理长视频 + 高分辨率图像并输出操控计划，在多任务刷新成绩。 PhysVLM 让 VLM 知道“够不着” S-P Reachability Map 注入视觉编码 PyBullet + UR3/XArm6 GitHub 8×A800 × 48h PhysVLM 预先计算 Reachability Map 并注入 SigLip + Qwen 语义空间，LLM 可回答“到不了哪里”并辅助 VoxPoser 规划；零样本实机验证成功率最高。 RoboSpatial 训练 VLM 空间推理三视角 1 M 图 + 5 K 扫描数据集 Kinova Jaco + cuRobo GitHub 8×H100 × 20–40h ROBOSPATIAL 构造百万图像 + 5 K 3D 扫描数据集，以三视角问答方式训练 VLM 空间理解；Kinova 抓放实验超越 GPT-4o。 Tartan IMU 轻量惯导定位 F-model Transformer-Tiny + IMU 重加权 Tartan Drive Hugging Face TBD 单 RTX 2080 提出轻量级 Transformer 模型仅用 IMU 即可做定位，统一导航与姿态估计；在 TartanDrive 数据集上达成 SOTA。 Code-as-Monitor 视觉编程自动生成约束检测 LLM-to-Code + 生命周期触发 Amazon AWS 码实验室 TBD CPU-only CaM 将开集故障判定统一为约束求解问题，用 VLM 生成 Python 代码实时监控。系统在三模拟器+真实场景把成功率提高 28.7 %。 七、Video \u0026amp; Representation 论文 目标 创新 平台 Code 算力 总结 TASTE-Rob 任务导向手-物交互视频生成 三阶段：DynamiCrafter → MDM → SD-Adapter SAPIEN (sim) GitHub 8×A100 三阶段视频生成管线（DynamiCrafter→MDM→SD-Adapter）让语言-场景-手姿一致，可直接用作模仿学习数据；SAPIEN 验证动作可复现。 GraphMimic 视频 → Graph-to-Graph 生成策略 Skeleton-Graph VAE RoboMimic TBD 4×A6000 GraphMimic 把视频抽象为时序场景-动作图，并训练 Graph-to-Graphs 生成器预训练策略网络，在少量下游数据时显著提升。 八、Sim2Real \u0026amp; 机器人模型 论文 目标 创新 平台 Code 算力 总结 Prof. Robot 无自碰 \u0026amp; 无静态穿模可微渲染 带 Signed Distance 渲染器 Blender + PyTorch3D GitHub 1×A40 在可微渲染中加入 Eikonal 正则化学习碰撞分类器，实现无静态 / 自碰的梯度优化动作；对比 Dr.Robot 成本相当但碰撞率降低一半。 AutoURDF 无监督点云→URDF 模型 Cluster Registration + IK 估计 RealSense 点云 GitHub 1×RTX 3090 AutoURDF 通过点云聚类配准，无监督推断关节拓扑与参数，自动生成符合主流模拟器的 URDF；在多机器人扫描数据集精度领先。 九、基准与数据集 论文 目标 创新 规模 Code / 数据 算力 总结 RoboTwin 双臂数字孪生基准 生成式 Digital Twin 105 场景 × 10 K 轨迹 GitHub 数据预处理需 8×CPU RoboTwin 使用 3D 生成与 LLM 产出多样专家示范，建立双臂数字孪生评测；提供 105 场景、10 K 轨迹，填补双臂基准空白。 Pixel-aligned RGB-NIR Stereo 近红外+RGB 对齐立体数据 同轴 NIR Stereo 标定 50 K 对 TBD 无 提出同轴 RGB-NIR 立体系统并发布多光照数据集，示范两种融合方法显著改进暗光下深度与语义性能。 RoboSense 拥挤非结构环境 egocentric 感知 3D 相机 + 激光混合注释 150 K 帧，900 min 视频 GitHub 无 RoboSense 搭建 360° 相机-LiDAR 采集平台，发布 133 K 帧、1.4 M 3D box 的拥挤非结构环境数据，并定义近场匹配指标，覆盖 KITTI 270× 标注量。 ","permalink":"https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/","summary":"CVPR 2025 Robotics Paper Summary","title":"Bug Journal CVPR2025-Summary"},{"content":"快速文章阅读 prompt, 用于找到一个自己喜欢的题目\n这篇文章要做什么，目标是什么 动机是什么 数据是从哪里来的 算力要求多少 公开代码吗 For Robotics: 现在有这一篇文章： \u0026lt;文章标题\u0026gt; 请用中文回答我： 这篇文章要做什么，目标是什么 动机是什么 数据是从哪里来的 算力要求多少 公开代码吗 模拟环境用的是什么平台 现实环境用的是什么平台 ","permalink":"https://tzj2006.github.io/bugjournal/2025-06-14/","summary":"Prompt for fast paper read","title":"Bug Journal 2025-06-14"},{"content":"Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction CVPR 2025\nFrom 人大 \u0026amp; 上海AI lab\n要达成的事情：\n让机器人能够自我反思到底是哪里做得不好，然后自我调整\n动机：\n人类可以很自然地反思：为什么失败了，为什么机器人不行呢？\n模型实现方式：\n首先，这是一个“语言指导”的RL方法。\n对于这个方法，首先由 LLM 生成一个文字指令：\n比如：现在我要移动一个杯子，我要怎么做\n然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。\n如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。\n现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL\n最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。\n对于修正指令这个步骤，模型一共会输出两条语句，分别是 \u0026ldquo;语义上的反思\u0026rdquo; (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)\n最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。\n那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。\n结果：\n更强的学习能力，更强的泛化能力。\n计算要求：\n仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行\n虚拟环境为 RoboMimic 模拟器\n使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs\nPhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability CVPR 2025\nFrom 北京交通大学 \u0026amp; 广东技术师范大学\n目标： 告诉机器人什么位置它到不了\n动机： 有时候机器人不知道一个位置到不到得了，结果把自己搞坏了\n模型流程：\n首先离线计算什么位置是机械臂能达到的。 形成一个点云 (S-P Map)\n然后用 SigLip-400M 提取图像和点云的特征\n然后把这个 embedding 和文字的 embedding 混合之后\n通过 Qwen-2.5-Instruct-3B，生成一个回答给 VoxPoser 做。\n实验设计：\n仿真：基于 CUDA 11.x 的 PyBullet 环境，六种机器人（UR5、FR5、CR5、FRANKA、UR3、XArm6），共 1.3K 问答，零样本评测。\n实机：在 UR3、XArm6 真实平台上各 10 次零样本任务，评估任务成功率。\n评估指标：\nEQA-phys：基于 LLM 评分（5 分正确、1 分错误）计算平均分； RoboVQA-val / OpenEQA：标准 BLEU-4 与准确率； 任务规划：成功率。\n结果：\nS-P Map 在很多 LLM 上都有用\nPhysVLM-3B 效果平均最好\n数据集： Zero-shot\n算力要求：\n\u0026lt; 48h * 8 * A800\n代码：\n开源\nObject-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation CVPR 2025\nFrom PKU Agibot Lab\n目标： 帮助机器人排除冗余信息干扰\n动机： 语言和视频中冗余信息过多\n模型流程：\n在图片上加一些标记 分别是：\n接触点（蓝色） 末端执行器在接触时的 z 轴方向（红色） y 轴方向（绿色） 接触后移动方向（黄色） 这些标记可能来自 Grounding-Dino + SAM 或者 ChatGPT-4o 或者 手动标记\n标记方式如下：\n均匀生成 N(32) 条线，让 GPT 选择 3 条标上颜色\n然后把这个“增强”过后的信息输入给 CLIP,然后再让 LLM 输出\u0026quot;应该在哪里，以什么角度接触\u0026quot;\n对于这个信息，我们可以和GT 做 train\n最后，我们得到了这个位置和姿态的信息，我们就可以用IK求解路径了。\n结果：\n数据集：\n模拟环境：SAPIEN + PartNet-Mobility •\t平台：使用 SAPIEN（一个支持刚体与关节物体的物理模拟器），其提供高保真动力学与渲染接口，适合零样本机器人操控研究 ￼ ￼。 •\t资产集：加载 PartNet-Mobility 中的 1,500 多个关节化 CAD 模型，涵盖抽屉、门、笔记本电脑盖等常见家电与家具 ￼。 •\t飞行夹持器（Flying Gripper）：采用 SAPIEN 中的“飞行”版 Franka Panda Gripper，使得末端执行器可在自由空间中无碰撞地移动，以便专注测试模型对提示的理解能力 ￼。 •\t摄像机随机化：在每个场景随机采样摄像机视角（水平±45°、俯仰30°–60°），并在单帧中记录 RGB 图像与 3D 末端执行器位姿，用于提示生成与训练 ￼。 •\t数据采集：共采集 ~10,000 条训练样本，过程耗时约 6–8 小时；测试集与训练集按照固定比例拆分，模拟了 Seen/Unseen 物体类别的泛化能力评估 ￼ ￼。 现实机器人平台 •\t硬件平台：使用真实的 Franka Emika Panda 机械臂（7 自由度，集成高精度力矩传感器），配合标准的 RGB-D 摄像头（如 Intel RealSense D415）采集视觉输入 ￼。 •\t执行流程：将 CrayonRobo 在模拟环境中输出的 SE(3) 接触位姿与移动方向，通过 ROS + MoveIt! 的 IK 求解与笛卡尔轨迹规划一键下发真实机器人执行 ￼。 •\t测试任务：与模拟中一致，包括“拉抽屉”“开门”“掀笔记本盖”等单步原子操作，每个任务在 5–10 个不同初始姿态下重复试验。 •\t评价指标：以二值成功率衡量：当末端执行器按照预测方向将物体部件移动超过阈值（如 5 cm）即记为成功；平均成功率达到 74%（Seen）与 72%（Unseen） ￼。 算力要求： 未知 建议 \u0026gt; 40 GB VRAM\n代码： 开源\nCheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation CVPR 2025\nFrom PKU Agibot lab\n目标： 让机器人读取说明书之后根据说明书做出正确操作\n动机： 阅读说明书\n电子产品有时候视觉信息不够，很多有着相似外观的东西可能有着不同的功能。\n所以要读说明书\n模型流程：\nOCR → GPT-4o 提取图文信息 → GPT‑4o 生成操作步骤 → SoM、Grounding‑DINO、SAM 实现视觉对齐 → FoundationPose 匹配 CAD\n最后输出{部件，操作} pair, 最后使用 VoxPoser 进行操作。\n实验设置 模拟平台：SAPIEN 0.8 + Franka Panda 飞行夹具 ￼\n数据集： PartNet-Mobility CAD 模型； CheckManual 合成说明书（已公开，可下载使用） ￼\n评估指标： 任务完成率\n现实验证： Franka + RealSense 摄像头，完成单个用例的实物测试\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\nCode availability: 开源\n结果：\n总之有 manual 效果更好\nROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics CVPR 2025\nFrom OSU and NVIDIA\n目标：\n引导 VLM 2D \u0026amp; 3D 视觉，理解空间结构。\n动机：\nVLM 目前无法理解空间结构。 原因并非 VLM 不行，而是数据不够\u0026quot;spatial\u0026quot;\n模型实现细节：\n首先是数据收集：\n输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”\n同时，生成一个俯视图，来看看哪里适合放置一个物体。\n最后在看看这个物体是否适合被放在这个地方。\n对于物体的方位，每一次会从三个角度问问题：\n以机器人为中心的视角 (第一视角) 以物体为中心的视角 (第三视角) 以世界为中心的视角 (fix-cam)\n自我中心：“从您的视角看，书在电脑的左边吗？” 以物体为中心：“从电脑的视角看，书在电脑的左边吗？” 以世界为中心：“从海拔高度看，书在电脑的上方吗？” 这样的好处是可以让 VLM 有更强的空间理解\n结果：\n在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o\n算力要求：\n20-40h * 8 * H100\n模拟环境：\nROBOSPATIAL 数据集，这包括 ScanNet, Matterport3D, 3RScan, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。\n真实环境：\nKinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。 机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。\nRobotic Visual Instruction: A New Paradigm for Human-Robot Communication CVPR 2025\nFrom IC + 上海 AI lab + UCSD \u0026hellip;\n目标：\n更好的人机交互\n动机：\n语言有很多冗余信息，那在图片中增加信息不就行了？\n模型实现方式：\n机器人视觉指令 (RoVI) RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：\n箭头：指示运动方向和轨迹 圆圈：标记感兴趣的对象或动作目标 颜色：区分多个指令或动作步骤 数字：指示操作顺序 这种视觉语言具有以下几个优点：\n空间精确性：视觉标记精确地指示3D空间中的位置和路径 时间清晰性：顺序步骤被清晰地划分 直观设计：这些符号易于人类理解和创建 跨文化实用性：视觉指令超越语言障碍 RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。\n靠手动标记了 15K 图片。。。 工作量爆炸💥 而且以后也要人手标。。。\n🤔： 为什么不能自动标，难道作者没有想过这一点吗\n但总之，现在在这个图像的基础上，VLM 会帮忙生成：\n任务的文字描述 可以执行这些任务的代码 代码中包含：\n路径 起点，终点，过程点\n然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取\n算力要求：\nNvidia A40\n现实实验设置：\nUFACTORY X-Arm 6和UR5 两台机械臂 两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。 两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。\n模拟环境设置：\n使用了SAPIEN 作为模拟器。 SIMPLER 作为基础环境。\nTASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation CVPR 2025\nFrom Xiaoguang Han\u0026rsquo;s Lab at 港中深\n目标： 优化对齐视频-人手数据集\n动机： 现在有这些问题：\n视角不一致 动作语义无法对齐 手部姿态稳定性不高 这个模型想要解决这些问题 模型流程：\n数据集构建（Sec. 3）： •\t100,856 条 1–8 秒单动作视频，静态 1080p 视角，语义指令一一对应。 粗视频生成（Stage I – Coarse Action Planner）： •\t基于 DynamiCrafter（512 × 512 分辨率），语言指令＋静态环境图→粗交互视频； •\t微调参数：batch=16, lr=5×10⁻⁵, 30K steps。 姿态优化（Stage II – MDM Refinement）： •\t使用 Motion Diffusion Model 以 60 帧手部关键点序列为条件，保证握姿一致性； •\t训练设置：batch=64, lr=1×10⁻⁴, 500K steps; 推理 10-step DDIM denoise。 最终生成（Stage III – Frame-wise Adapter）： •\t将优化后手部姿态图像与环境图和语言指令一起接入 Stable Diffusion 2 Adapter，生成最终视频； •\t训练设置：batch=32, lr=5×10⁻⁵, 30K steps。 实验设置 •\t仿真验证：在 SAPIEN 模拟平台上，用 DynamiCrafter＋MDM + Adapter 生成视频，并通过 Figure 17 示范在虚拟机器人上的模仿效果。\n模型流程：\n第一阶段：Coarse Action Planner（粗动作生成） •\t目标：在给定语言指令（如“拿起杯子并倒入水”）和静态环境图（512×512 分辨率）条件下，生成一段粗略的、合理的手–物体交互视频序列（通常 16 帧）。 •\t模型架构：基于 Video Diffusion Model（VDM），如 DynamiCrafter，使用文本–图像条件扩散网络。 •\t训练细节： •\tBatch size = 16，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t推理时使用 50-step DDIM 采样，平衡生成质量与速度。 •\t输出：一段低分辨率、含大致手部运动轨迹和物体交互的粗视频，用于后续姿态细化。\n⸻\n第二阶段：Motion Diffusion Model (MDM) Refinement（姿态细化） •\t目标：针对第一阶段生成的粗视频，从中提取手部的 3D 关键点序列（60 帧），并利用扩散模型细化运动轨迹，提升抓握姿态的连贯性与自然度。 •\t模型架构：1D 时序扩散网络，对帧间手部关键点做去噪与重建。 •\t训练细节： •\tBatch size = 64，学习率 1×10⁻⁴； •\t训练步数 500K steps； •\t推理时使用 10-step DDIM，快速得到精细关键点序列。 •\t输出：一组平滑、符合物理约束的 3D 手部关键点轨迹，用以指导下一阶段的视频生成。\n⸻\n第三阶段：Frame-wise Adapter（帧级最终生成） •\t目标：将姿态细化后的关键点与原环境图、语言指令结合，生成最终高质量、手部姿态稳定的交互视频。 •\t模型架构：基于 Stable Diffusion 2，通过 Adapter 模块将手部姿态（以可视化关节点或姿态图形式）作为条件，连同环境图与文本，一起输入扩散模型。 •\t训练细节： •\tBatch size = 32，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t采用与 Stage I 相似的 DDIM 采样流程，保证画质与动作一致性。 •\t输出：分辨率可达 512×512 的连续视频帧序列，手–物体交互清晰、握姿自然，可直接用于机器人模仿学习。\nMitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation CVPR 2025\nFrom 港科广\n目标：\n缩小人机之间的 Gap\n动机：\n人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap\n问题：\n现在有这些 Gap:\n视觉环境差异 人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。\n形态差异： 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。\n尺度和视角： 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。\n解决这一差距的传统方法分为两大类，每类都有显著的局限性：\n预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。\n在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。\n模型实现方式：\nbasically, 就是说希望用一个 Adaptor 来 fill in the gap.\n把从 human demo pretrain embedding 转换成 robot demo embedding.\n对于任务感知也是如此。\n用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。\n结果：\nAlign 一下效果变好了\n算力要求：\n4 * Nvidia A6000\n模拟环境为 RLBench\n真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头\nMoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation CVPR 2025\nFrom 北邮 + 南阳理工 + 清华\n目标：\n导航 + 空间操作\n动机：\n虽然静态的操作已经没问题了，但是若是平台移动就不太好办。\n实现细节：\n这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分\nMoManipVLA使用以下方法实现这些优化问题：\n双退火搜索算法用于基座位置寻找优化 序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案 该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：\nRGB相机进行视觉感知 关节编码器进行本体感知（感知机器人自身位置） (optional) 深度感知以增强障碍物避免\n结果：\n模拟环境：\n模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。\n真实环境（机械臂）：\n在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。\n算力：\n4 * RTX 3090\nThink Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation CVPR 2025\nFrom 上交，复旦，上海 AI lab\n目标：\n如何避免灾难性遗忘\n动机：\n有些动作有相似之处，比如递筷子和递镊子有相似之处 那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？\n模型实现细节：\nPPL 框架的核心组件包括：\n输入编码器： 本体感觉编码器：处理机器人的关节状态和夹爪姿势 视觉编码器：处理场景的 RGB 图像 光流编码器：处理光流信息以捕获运动模式 文本编码器：处理任务的语言指令\n基元提示： 跨任务共享的基本运动模式的学习表示 注入到多头自注意力层的键和值中\n终身提示 (Lifelong Prompts)： 在终身学习期间为新任务学习的特定于任务的提示 与原始提示连接以自定义模型的行为\n运动感知提示查询 (Motion-Aware Prompt Query)： 结合光流和文本指令信息 用于确定不同原始提示的相关性\n扩散Transformer (Diffusion Transformer)： 基于条件输入和提示生成机器人动作\n算力：\n论文中没有直接说明具体的GPU类型和训练时间。\n模拟环境：\n模拟实验是在基于 MimicGen 和 LIBERO 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。\n机械臂：\n真实世界的实验是在 Franka Panda 机械臂上进行的。\n数据集：\n论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。\n获取方式：\n这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。\nGenerating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision CVPR 2025\nFrom Kyoto University\n这篇论文的标题是“从自我中心视角下的动作描述生成6自由度物体操作轨迹”，其核心目标是根据文本描述和初始视觉输入来生成物体在3D空间中的操作轨迹序列。\n目标： 从自我中心视频中提取6自由度（6DoF）物体操作轨迹。 基于这些提取的轨迹和相关的动作描述，开发能够根据文本动作描述生成物体操作轨迹的模型。\n动机： 开发能够协助人类活动的交互式机器人，其中一个关键能力是让机器人能够按人类指令操作工具和物体。 训练生成此类操作轨迹的模型需要大量多样且详细的物体操作演示，但这在实际中很难大规模收集。 利用日常工作场景中的视频来提取人类演示中的各种物体操作，是一种很有前景的解决方案。 现有的3D物体操作轨迹预测方法受限于昂贵的3D注释成本，难以处理多样化的操作动作。 现有数据集（如HOT3D）中的人类运动轨迹数据有限，不足以开发能够生成物体操作轨迹的模型。\n数据来源： 训练数据： 论文使用Ego-Exo4D数据集 [30] 来构建大规模训练数据集，包含6DoF物体操作轨迹以及图像、深度图和动作描述。这个过程是自动化的，不依赖于预设的相机外部参数。\n评估数据： 论文使用现有的HOT3D [3] 数据集进行评估。HOT3D是一个用于3D手和物体跟踪的自我中心视角数据集，通过光学标记和多红外OptiTrack相机记录，提供了精确的手和物体6DoF信息。\n算力要求： 论文提到模型训练了30个epoch，批处理大小为8。优化器使用了AdamW，基础学习率为2e-5（对于LLMs）和2e-4（对于其他参数）。 为了处理旋转连续性问题，每个角度（roll, pitch, yaw）都用[cos(θ), sin(θ)]表示。 虽然论文没有直接给出具体的算力消耗数字（例如所需的GPU数量或训练时间），但从训练epoch和批处理大小来看，以及使用了BLIP-2、VILA、PointLLM和MiniGPT-3D等大型视觉语言模型作为骨干网络，可以推断出这需要相当大的计算资源。这些模型本身就比较大，并且训练涉及处理大规模视频数据。\n公开代码： 论文在摘要中明确提到了数据集和代码的链接：https://biscue5.github.io/egoscaler-project-page/。\n模拟环境和现实环境平台： 论文没有提到使用了特定的模拟环境平台。 在现实环境方面，论文主要使用了自我中心视频数据。HOT3D [3] 数据集是通过Project Aria眼镜 [21] 和Quest 3 [65] 记录的，这些是现实世界中的可穿戴设备，用于捕捉自我中心视角下的活动。这表明他们的研究是基于真实世界中的人类活动数据。\nUniversal Actions for Enhanced Embodied Foundation Models CVPR 2025 From 清华\n这篇文章介绍的 UniAct 框架，目标是解决具身基础模型在处理异构动作数据时面临的挑战，并构建一个能够在通用动作空间中操作的框架。\n主要目标：\n构建通用动作空间： 学习一种能够捕捉不同机器人通用原子行为的动作空间，从而消除机器人之间因物理形态和控制接口差异造成的动作异构性。\n实现跨形态泛化： 使得具身基础模型能够有效利用跨领域数据，并在不同的机器人形态之间实现更好的泛化控制和适应能力。\n提高模型效率： 训练一个相对较小（0.5B 参数）但性能优于更大（14倍）现有模型的具身基础模型，证明通用动作的优势。\n动机： 数据异构性挑战： 现有的大型基础模型在自然语言处理和计算机视觉领域取得了巨大成功，主要得益于海量的、多样化的互联网数据。然而，将同样的方法应用于具身智能体时，面临一个显著的挑战：不同机器人收集的动作数据存在显著的异构性，因为它们有不同的物理形态和控制接口。这种异构性严重阻碍了跨领域数据共享和通用具身基础模型的发展。\n现有解决方案的局限性： 大多数现有方法要么强制性地将不同动作空间视为等效，采用统一的离散化或归一化技术，但这可能导致动作编码的物理意义冲突；要么试图设计一个适用于各种机器人系统的物理可解释动作空间，但这需要大量人工工程，且未能充分利用不同具身动作空间之间的内在联系。\n对通用代理的需求： 开发能够处理跨任务、跨环境和跨形态泛化的通用具身基础模型，是构建通用具身智能体的一个有前景的途径。 数据来源：\nUniAct-0.5B 模型在训练时整合了来自多个开源机器人数据集的示范数据。这些数据集包括：\nOpen-X Embodiment (OXE) Libero Droid 这些数据被标准化，以包含第三人称视角观察和语言指令，同时保留了动作的异构性。总共使用了来自 28 种不同机器人形态的约 100 万个示范数据进行训练。\n算力要求： UniAct-0.5B 的训练是在 64 块 A100 GPU 上进行的，并使用了 DeepSpeed 进行优化，持续了 10 天。\n公开代码： 是的，文章中提到了项目的项目页面，通常这意味着代码是公开的： 项目页面\nSOLAMI: Social Vision‑Language‑Action Modeling for Immersive Interaction with 3D Autonomous Characters CVPR 2025\nSOLAMI 这篇文章旨在介绍一个端到端的社交视觉-语言-动作（VLA）建模框架，用于与 3D 自动角色进行沉浸式交互。\n文章的目标是构建能够感知、理解并与人类互动的 3D 自动角色，使其具备类似于人类的社交智能，通过多模态响应（语音和动作）驱动角色进行社交互动。\n这项研究的动机在于，目前的字符代理在与用户交互时，主要限于文本或语音交互，缺乏更丰富的模态。在社交互动中，沉浸感越深，人类体验越好。因此，研究人员希望构建具有更丰富模态的 3D 自动角色。此外，多模态交互数据非常稀缺，难以获取，这也促使他们开发了数据合成方法。\n数据主要来源于以下几个方面：\n交互式多模态数据（SynMSI）： 这是一个合成的多模态社交互动数据集，通过自动化流程生成，利用了现有的文本-动作数据集、基于文本的角色扮演模型和语音合成方法。SynMSI 数据集包含 6.3K 多轮多模态对话项。 运动数据：为了进行预训练阶段的运动与文本对齐，以及生成多模态数据用于指令微调，研究人员收集了包含丰富社交动作的现有数据集，例如 HumanML3D (24K 运动-文本对)、Inter-X (20K 运动-文本对和 10K 两人运动对)，以及 DLP-MoCap (2K 运动-文本对)。\n语音数据： 用于预训练阶段的语音-文本对齐，使用了 CommonVoice (150K 语音-文本对)、AnyInstruct (200K 语音-文本对和 100K 语音到语音项)，以及通过文本到语音方法（Azure TTS 和 XTTS_v2）生成的合成语音数据 (60K 语音-文本对)。\n算力要求： 在预训练阶段，SOLAMI 使用了 32 块 V100 GPU 来训练模型，批处理大小为 256。 在指令微调阶段，SOLAMI 使用了 16 块 V100 GPU，批处理大小为 48。推理时，所有模型都部署在 2 块 H800 GPU 上，并采用 vLLM 框架和异步机制来提高性能并保持公平性。\n代码： 项目的 GitHub 链接\n模拟环境在文章中没有明确提及，但实验中提到了使用 VR 界面进行用户研究，其中用户可以与各种 3D 角色进行沉浸式交互。\n现实环境指的是 VR 界面，研究人员开发了一个基于 Oculus Quest 3 前端和后端服务的 VR 界面。前端实现用户与 3D 自动角色的沉浸式交互，后端由 2 块 H800 GPU 提供算力支持。在实际使用中，VR 头显捕获用户的语音和身体动作，并将其发送到后端计算节点。\nA Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning CVPR 2025 from HKU\n这篇文章主要研究了预训练视觉模型（PVMs）在机器人学习任务中的应用，特别是视觉运动控制和感知任务。文章的目标是找出最优的预训练方法和数据来源，以提高PVMs在机器人学习任务中的表现。\n动机 文章的动机源于当前PVMs在机器人学习任务中的应用存在一些问题和局限性。尽管PVMs在传统视觉任务中表现出色，但它们在机器人学习任务中的最优配置仍不清楚。文章通过系统性的评估发现，虽然某些PVMs（如DINO和iBOT）在视觉运动控制和感知任务中表现出色，但它们在非对象中心（NOC）数据上的表现会显著下降。这种下降与它们学习对象中心表示的能力减弱密切相关。\n数据来源 文章中使用的数据集包括：\n对象中心数据集：ImageNet 场景中心数据集：COCO 网络爬取数据：CC12M 以自我为中心的数据：Ego4D 这些数据集被用来评估PVMs在不同类型的数据上的表现。\n算力要求 由于PVMs的训练和评估需要大量的计算资源，文章中提到使用了8个A100 GPU进行训练。对于某些任务，如导航任务，需要大约400M到500M步的训练和512到320个并行环境，这对计算资源提出了极高的要求。\n代码公开情况 文章中提到，他们的代码和模型是公开可用的，链接\n模拟环境 文章中使用了多个模拟环境平台进行评估，包括：\nFranka Kitchen Meta-World Habitat（用于导航任务，包含HM3D和Gibson环境） 现实环境 虽然文章主要关注模拟环境中的评估，但提到PVMs在现实环境中的应用潜力。现实环境中的平台并未在文章中具体提及，但提到了多个现实环境中的机器人学习任务和应用。\n总结 总的来说，这篇文章通过系统性的评估和实验，提出了SlotMIM方法，以有效地从NOC数据中学习对象中心的表示，并在多个任务中取得了优于现有方法的性能。文章的研究为PVMs在机器人学习任务中的应用提供了新的见解和方法。\nOmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints 这篇文章介绍了一个名为 OmniManip 的新方法，旨在实现通用机器人操作。\n这篇文章要做什么，目标是什么？ 这篇文章提出了 OmniManip，这是一种开放词汇的操控方法，旨在弥合视觉-语言模型 (VLM) 的高层推理能力与低层精确操控之间的差距。其核心目标是开发一个通用的机器人操控系统，能够通过物体中心交互基元作为空间约束，在非结构化环境中执行各种操控任务，并具有强大的零样本泛化能力。\n动机是什么？ 开发能够在非结构化环境中进行通用机器人操控的系统是一个重大挑战。虽然 VLM 在高层常识推理方面表现出色，但它们缺乏精确操控任务所需的精细 3D 空间理解能力。现有的解决方案，例如在机器人数据集上微调 VLM，面临数据收集成本高昂和泛化性差的问题。通过将机器人动作抽象为交互基元并利用 VLM 定义空间约束，是解决这些挑战的动机。\n数据是从哪里来的？ 文章中提到，为了评估 OmniManip 在真实世界场景中的操控能力，他们设计了 12 个任务来评估模型的操控能力，这些任务涵盖了各种对象和复杂环境。虽然没有明确说明具体的数据集来源，但实验部分提到了通过 OmniManip 自动生成演示数据，并收集了每项任务 150 条轨迹用于训练行为克隆策略。\n算力要求多少？ 文章中没有直接给出具体的算力要求，但提及了多个 VLM 调用会带来计算挑战，即使进行了并行处理也是如此。这暗示了该系统可能需要较高的计算资源。\n公开代码吗？ 文章中没有明确提到代码是否公开。\n模拟环境用的是什么平台？ 文章中没有提到具体使用了哪个模拟环境平台。\n现实环境用的是什么平台？ 现实环境实验平台是基于 Franka Emika Panda 机器人臂搭建的，并配备了 UMI 机械手。感知方面，使用了两台 Intel RealSense D415 深度相机，一台安装在机械手上提供第一人称视角，另一台则放置在机器人对面提供第三人称视角。\nVidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic CVPR 2025\nFrom Technical University of Munich\n目标： 大规模网络视频人类样本学习 训练 家务机器人 模型\n动机： 机器人依赖实例教学，但是做家务没那么多教学\n模型流程：\n模型概览\nVidBot 通过三大模块——3D 可交互性提取、粗阶段 affordance 预测、细阶段轨迹生成——实现从“野外”RGB 视频到机器人可执行动作的零样本迁移。首先，它利用 Structure-from-Motion 和度量深度模型，从单目视频中恢复一致的 3D 手部轨迹与接触/目标点；然后，因式分解成粗预测网络 πc（提取高层次接触点与目标点）与细预测网络 πf（基于扩散生成精细轨迹），并在推理时引入多目标、法线与避碰等可微成本进行采样指导；最后，将生成的 3D 交互轨迹直接部署于多种机器人平台，实现开“即用”的零样本操控能力 ￼ ￼。\n⸻\n3D 可交互性提取管道 1.1 数据准备 •\t视频与语言输入：给定原始 RGB 图像序列 {Ĩ0,…ĨT} 及指令 l。 •\tSfM 与深度预测：使用 SfM 系统估计相机内参 K、无尺度位姿 {TWC} 及稀疏地标，再调用度量深度基模型（如 ZoeDepth）生成密集深度 {D̂t}，实现时序一致且度量尺度的重建 ￼； •\t手-物体分割与填充：借助手—物体检测（如 [72]）与分割模型（如 [94]）提取手部与接触物体掩码，再通过视频修复生成无手帧，以消除动态遮挡对优化的影响 ￼。\n1.2 姿态与尺度优化 •\t全局尺度校正：优化全帧尺度 sg，使稀疏地标深度与预测深度对齐； •\t位姿细化：联合优化每帧位姿 TWCi 与局部尺度 si，补偿 SfM 在手—物体动态区域的误差，实现一致的 3D 重建 ￼。\n1.3 交互表示提取 •\t手部中心轨迹：将优化后各帧手部中心点恢复至首帧坐标系，插值形成平滑交互轨迹 τ̂； •\t接触与目标点采样：在首帧均匀下采样手部中心生成接触点 ĉ，在末帧提取目标点 ĝ，用于监督模型的中间预测； •\t表示定义：最终得到的 3D affordance a = {c, τ}，其中 c∈ℝNc×3 为接触点集，τ∈ℝH×3 为轨迹序列 ￼ ￼。\n⸻\n粗—细分级 Affordance 学习 2.1 模型结构因式分解\n将 affordance 模型 π({Ĩ, D̃},l) 分解为： 1.\t粗阶段 πc：从 RGB-D 图像 Ĩ→D̃ 及指令 l 中预测目标点 g 和接触点 c； 2.\t细阶段 πf：在粗阶段输出 {g,c} 及场景上下文指导下，通过扩散模型生成精细轨迹 τ ； 整个流程利用测试时可微成本（多目标到达、碰撞避让等）对采样进行引导，增强与新场景/新形体的适应性 ￼ ￼。\n2.2 粗阶段：目标与接触点预测 •\t输入预处理：使用开集物体检测器裁剪关注物体区域的 RGB-D 图像； •\t网络架构：πc 由两支网络 πgoalc 与 πcontc 组成，分别输出目标和接触热图及（目标点）深度。 •\t融合视觉特征、RoI 池化后得到的物体嵌入、语言嵌入（CLIP 特征）和位置编码，通过 Perceiver 与 Transformer 生成每像素概率分布； •\t3D 投影：根据相机内参与热图深度，将像素坐标提升到三维空间，得到 g∈ℝNg×3 与 c∈ℝNc×3 ￼。\n2.3 细阶段：基于扩散的轨迹生成 •\t条件扩散模型：πf 采用 1D U-Net 架构，输入包含轨迹状态 τk、TSDF 空间特征（由 3D U-Net 从 RGB-D 构建的体素化 TSDF 地图提取）、语言与物体嵌入等； •\t正向/反向过程：遵循扩散概率模型，逐步加入高斯噪声，再由网络学习去噪，直接回归未加噪轨迹 τ̄0； •\t测试时引导：在每个去噪步骤中加入可微成本函数——多目标到达、法线一致、碰撞避让——通过扰动引导采样，提高交互轨迹在新场景/新形体下的合理性与可执行性 ￼ ￼。\n⸻\n输入与输出 •\t输入：首帧的“修复”RGB 图像 Ĩ̃0、对应度量深度 D̃0、裁剪的物体图像 Ĩ̃0o 及语言指令 l ￼。 •\t输出：3D 接触点集 c 和交互轨迹 τ ，直接可用于机器人控制模块执行操作 ￼。 算力要求： 没说\nCode availability: 暂时没有 (2025-06-11)\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-13/","summary":"Paper review of CVPR 2025","title":"Bug Journal 2025-06-13"},{"content":"Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction CVPR 2025\nFrom 人大 \u0026amp; 上海AI lab\n要达成的事情：\n让机器人能够自我反思到底是哪里做得不好，然后自我调整\n动机：\n人类可以很自然地反思：为什么失败了，为什么机器人不行呢？\n模型实现方式：\n首先，这是一个“语言指导”的RL方法。\n对于这个方法，首先由 LLM 生成一个文字指令：\n比如：现在我要移动一个杯子，我要怎么做\n然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。\n如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。\n现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL\n最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。\n对于修正指令这个步骤，模型一共会输出两条语句，分别是 \u0026ldquo;语义上的反思\u0026rdquo; (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)\n最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。\n那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。\n结果：\n更强的学习能力，更强的泛化能力。\n计算要求：\n仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行\n虚拟环境为 RoboMimic 模拟器\n使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs\nRobotic Visual Instruction: A New Paradigm for Human-Robot Communication CVPR 2025\nFrom IC + 上海 AI lab + UCSD \u0026hellip;\n目标：\n更好的人机交互\n动机：\n语言有很多冗余信息，那在图片中增加信息不就行了？\n模型实现方式：\n机器人视觉指令 (RoVI) RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：\n箭头：指示运动方向和轨迹 圆圈：标记感兴趣的对象或动作目标 颜色：区分多个指令或动作步骤 数字：指示操作顺序 这种视觉语言具有以下几个优点：\n空间精确性：视觉标记精确地指示3D空间中的位置和路径 时间清晰性：顺序步骤被清晰地划分 直观设计：这些符号易于人类理解和创建 跨文化实用性：视觉指令超越语言障碍 RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。\n靠手动标记了 15K 图片。。。 工作量爆炸💥 而且以后也要人手标。。。\n🤔： 为什么不能自动标，难道作者没有想过这一点吗\n但总之，现在在这个图像的基础上，VLM 会帮忙生成：\n任务的文字描述 可以执行这些任务的代码 代码中包含：\n路径 起点，终点，过程点\n然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取\n算力要求：\nNvidia A40\n现实实验设置：\nUFACTORY X-Arm 6和UR5 两台机械臂 两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。 两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。\n模拟环境设置：\n使用了SAPIEN 作为模拟器。 SIMPLER 作为基础环境。\nMitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation CVPR 2025\nFrom 港科广\n目标：\n缩小人机之间的 Gap\n动机：\n人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap\n问题：\n现在有这些 Gap:\n视觉环境差异 人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。\n形态差异： 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。\n尺度和视角： 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。\n解决这一差距的传统方法分为两大类，每类都有显著的局限性：\n预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。\n在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。\n模型实现方式：\nbasically, 就是说希望用一个 Adaptor 来 fill in the gap.\n把从 human demo pretrain embedding 转换成 robot demo embedding.\n对于任务感知也是如此。\n用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。\n结果：\nAlign 一下效果变好了\n算力要求：\n4 * Nvidia A6000\n模拟环境为 RLBench\n真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头\nMoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation CVPR 2025\nFrom 北邮 + 南阳理工 + 清华\n目标：\n导航 + 空间操作\n动机：\n虽然静态的操作已经没问题了，但是若是平台移动就不太好办。\n实现细节：\n这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分\nMoManipVLA使用以下方法实现这些优化问题：\n双退火搜索算法用于基座位置寻找优化 序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案 该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：\nRGB相机进行视觉感知 关节编码器进行本体感知（感知机器人自身位置） (optional) 深度感知以增强障碍物避免\n结果：\n模拟环境：\n模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。\n真实环境（机械臂）：\n在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。\n算力：\n4 * RTX 3090\nROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics CVPR 2025\nFrom OSU and NVIDIA\n目标：\n引导 VLM 2D \u0026amp; 3D 视觉，理解空间结构。\n动机：\nVLM 目前无法理解空间结构。 原因并非 VLM 不行，而是数据不够\u0026quot;spatial\u0026quot;\n模型实现细节：\n首先是数据收集：\n输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”\n同时，生成一个俯视图，来看看哪里适合放置一个物体。\n最后在看看这个物体是否适合被放在这个地方。\n对于物体的方位，每一次会从三个角度问问题：\n以机器人为中心的视角 (第一视角) 以物体为中心的视角 (第三视角) 以世界为中心的视角 (fix-cam)\n自我中心：“从您的视角看，书在电脑的左边吗？” 以物体为中心：“从电脑的视角看，书在电脑的左边吗？” 以世界为中心：“从海拔高度看，书在电脑的上方吗？” 这样的好处是可以让 VLM 有更强的空间理解\n结果：\n在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o\n算力要求：\n20-40h * 8 * H100\n模拟环境：\nROBOSPATIAL 数据集，这包括 ScanNet, Matterport3D, 3RScan, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。\n真实环境：\nKinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。 机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。\nThink Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation CVPR 2025\nFrom 上交，复旦，上海 AI lab\n目标：\n如何避免灾难性遗忘\n动机：\n有些动作有相似之处，比如递筷子和递镊子有相似之处 那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？\n模型实现细节：\nPPL 框架的核心组件包括：\n输入编码器： 本体感觉编码器：处理机器人的关节状态和夹爪姿势 视觉编码器：处理场景的 RGB 图像 光流编码器：处理光流信息以捕获运动模式 文本编码器：处理任务的语言指令\n基元提示： 跨任务共享的基本运动模式的学习表示 注入到多头自注意力层的键和值中\n终身提示 (Lifelong Prompts)： 在终身学习期间为新任务学习的特定于任务的提示 与原始提示连接以自定义模型的行为\n运动感知提示查询 (Motion-Aware Prompt Query)： 结合光流和文本指令信息 用于确定不同原始提示的相关性\n扩散Transformer (Diffusion Transformer)： 基于条件输入和提示生成机器人动作\n算力：\n论文中没有直接说明具体的GPU类型和训练时间。\n模拟环境：\n模拟实验是在基于 MimicGen 和 LIBERO 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。\n机械臂：\n真实世界的实验是在 Franka Panda 机械臂上进行的。\n数据集：\n论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。\n获取方式：\n这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。\nGenerating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision CVPR 2025\nFrom Kyoto University\n这篇论文的标题是“从自我中心视角下的动作描述生成6自由度物体操作轨迹”，其核心目标是根据文本描述和初始视觉输入来生成物体在3D空间中的操作轨迹序列。\n目标： 从自我中心视频中提取6自由度（6DoF）物体操作轨迹。 基于这些提取的轨迹和相关的动作描述，开发能够根据文本动作描述生成物体操作轨迹的模型。\n动机： 开发能够协助人类活动的交互式机器人，其中一个关键能力是让机器人能够按人类指令操作工具和物体。 训练生成此类操作轨迹的模型需要大量多样且详细的物体操作演示，但这在实际中很难大规模收集。 利用日常工作场景中的视频来提取人类演示中的各种物体操作，是一种很有前景的解决方案。 现有的3D物体操作轨迹预测方法受限于昂贵的3D注释成本，难以处理多样化的操作动作。 现有数据集（如HOT3D）中的人类运动轨迹数据有限，不足以开发能够生成物体操作轨迹的模型。\n数据来源： 训练数据： 论文使用Ego-Exo4D数据集 [30] 来构建大规模训练数据集，包含6DoF物体操作轨迹以及图像、深度图和动作描述。这个过程是自动化的，不依赖于预设的相机外部参数。\n评估数据： 论文使用现有的HOT3D [3] 数据集进行评估。HOT3D是一个用于3D手和物体跟踪的自我中心视角数据集，通过光学标记和多红外OptiTrack相机记录，提供了精确的手和物体6DoF信息。\n算力要求： 论文提到模型训练了30个epoch，批处理大小为8。优化器使用了AdamW，基础学习率为2e-5（对于LLMs）和2e-4（对于其他参数）。 为了处理旋转连续性问题，每个角度（roll, pitch, yaw）都用[cos(θ), sin(θ)]表示。 虽然论文没有直接给出具体的算力消耗数字（例如所需的GPU数量或训练时间），但从训练epoch和批处理大小来看，以及使用了BLIP-2、VILA、PointLLM和MiniGPT-3D等大型视觉语言模型作为骨干网络，可以推断出这需要相当大的计算资源。这些模型本身就比较大，并且训练涉及处理大规模视频数据。\n公开代码： 论文在摘要中明确提到了数据集和代码的链接：https://biscue5.github.io/egoscaler-project-page/。\n模拟环境和现实环境平台： 论文没有提到使用了特定的模拟环境平台。 在现实环境方面，论文主要使用了自我中心视频数据。HOT3D [3] 数据集是通过Project Aria眼镜 [21] 和Quest 3 [65] 记录的，这些是现实世界中的可穿戴设备，用于捕捉自我中心视角下的活动。这表明他们的研究是基于真实世界中的人类活动数据。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-12/","summary":"CVPR 2025 Robotics summary","title":"Bug Journal 2025-06-12"},{"content":"PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability CVPR 2025\nFrom 北京交通大学 \u0026amp; 广东技术师范大学\n目标： 告诉机器人什么位置它到不了\n动机： 有时候机器人不知道一个位置到不到得了，结果把自己搞坏了\n模型流程：\n首先离线计算什么位置是机械臂能达到的。 形成一个点云 (S-P Map)\n然后用 SigLip-400M 提取图像和点云的特征\n然后把这个 embedding 和文字的 embedding 混合之后\n通过 Qwen-2.5-Instruct-3B，生成一个回答给 VoxPoser 做。\n实验设计：\n仿真：基于 CUDA 11.x 的 PyBullet 环境，六种机器人（UR5、FR5、CR5、FRANKA、UR3、XArm6），共 1.3K 问答，零样本评测。\n实机：在 UR3、XArm6 真实平台上各 10 次零样本任务，评估任务成功率。\n评估指标：\nEQA-phys：基于 LLM 评分（5 分正确、1 分错误）计算平均分； RoboVQA-val / OpenEQA：标准 BLEU-4 与准确率； 任务规划：成功率。\n结果：\nS-P Map 在很多 LLM 上都有用\nPhysVLM-3B 效果平均最好\n数据集： Zero-shot\n算力要求：\n\u0026lt; 48h * 8 * A800\n代码：\n开源\nObject-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation CVPR 2025\nFrom PKU Agibot Lab\n目标： 帮助机器人排除冗余信息干扰\n动机： 语言和视频中冗余信息过多\n模型流程：\n在图片上加一些标记 分别是：\n接触点（蓝色） 末端执行器在接触时的 z 轴方向（红色） y 轴方向（绿色） 接触后移动方向（黄色） 这些标记可能来自 Grounding-Dino + SAM 或者 ChatGPT-4o 或者 手动标记\n标记方式如下：\n均匀生成 N(32) 条线，让 GPT 选择 3 条标上颜色\n然后把这个“增强”过后的信息输入给 CLIP,然后再让 LLM 输出\u0026quot;应该在哪里，以什么角度接触\u0026quot;\n对于这个信息，我们可以和GT 做 train\n最后，我们得到了这个位置和姿态的信息，我们就可以用IK求解路径了。\n结果：\n数据集：\n模拟环境：SAPIEN + PartNet-Mobility •\t平台：使用 SAPIEN（一个支持刚体与关节物体的物理模拟器），其提供高保真动力学与渲染接口，适合零样本机器人操控研究 ￼ ￼。 •\t资产集：加载 PartNet-Mobility 中的 1,500 多个关节化 CAD 模型，涵盖抽屉、门、笔记本电脑盖等常见家电与家具 ￼。 •\t飞行夹持器（Flying Gripper）：采用 SAPIEN 中的“飞行”版 Franka Panda Gripper，使得末端执行器可在自由空间中无碰撞地移动，以便专注测试模型对提示的理解能力 ￼。 •\t摄像机随机化：在每个场景随机采样摄像机视角（水平±45°、俯仰30°–60°），并在单帧中记录 RGB 图像与 3D 末端执行器位姿，用于提示生成与训练 ￼。 •\t数据采集：共采集 ~10,000 条训练样本，过程耗时约 6–8 小时；测试集与训练集按照固定比例拆分，模拟了 Seen/Unseen 物体类别的泛化能力评估 ￼ ￼。\n现实机器人平台 •\t硬件平台：使用真实的 Franka Emika Panda 机械臂（7 自由度，集成高精度力矩传感器），配合标准的 RGB-D 摄像头（如 Intel RealSense D415）采集视觉输入 ￼。 •\t执行流程：将 CrayonRobo 在模拟环境中输出的 SE(3) 接触位姿与移动方向，通过 ROS + MoveIt! 的 IK 求解与笛卡尔轨迹规划一键下发真实机器人执行 ￼。 •\t测试任务：与模拟中一致，包括“拉抽屉”“开门”“掀笔记本盖”等单步原子操作，每个任务在 5–10 个不同初始姿态下重复试验。 •\t评价指标：以二值成功率衡量：当末端执行器按照预测方向将物体部件移动超过阈值（如 5 cm）即记为成功；平均成功率达到 74%（Seen）与 72%（Unseen） ￼。\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\n代码： 开源\nCheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation CVPR 2025\nFrom PKU Agibot lab\n目标： 让机器人读取说明书之后根据说明书做出正确操作\n动机： 阅读说明书\n电子产品有时候视觉信息不够，很多有着相似外观的东西可能有着不同的功能。\n所以要读说明书\n模型流程：\nOCR → GPT-4o 提取图文信息 → GPT‑4o 生成操作步骤 → SoM、Grounding‑DINO、SAM 实现视觉对齐 → FoundationPose 匹配 CAD\n最后输出{部件，操作} pair, 最后使用 VoxPoser 进行操作。\n实验设置 模拟平台：SAPIEN 0.8 + Franka Panda 飞行夹具 ￼\n数据集： PartNet-Mobility CAD 模型； CheckManual 合成说明书（已公开，可下载使用） ￼\n评估指标： 任务完成率\n现实验证： Franka + RealSense 摄像头，完成单个用例的实物测试\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\nCode availability: 开源\n结果：\n总之有 manual 效果更好\nTASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation CVPR 2025\nFrom Xiaoguang Han\u0026rsquo;s Lab at 港中深\n目标： 优化对齐视频-人手数据集\n动机： 现在有这些问题：\n视角不一致 动作语义无法对齐 手部姿态稳定性不高 这个模型想要解决这些问题 模型流程：\n数据集构建（Sec. 3）： •\t100,856 条 1–8 秒单动作视频，静态 1080p 视角，语义指令一一对应。 粗视频生成（Stage I – Coarse Action Planner）： •\t基于 DynamiCrafter（512 × 512 分辨率），语言指令＋静态环境图→粗交互视频； •\t微调参数：batch=16, lr=5×10⁻⁵, 30K steps。 姿态优化（Stage II – MDM Refinement）： •\t使用 Motion Diffusion Model 以 60 帧手部关键点序列为条件，保证握姿一致性； •\t训练设置：batch=64, lr=1×10⁻⁴, 500K steps; 推理 10-step DDIM denoise。 最终生成（Stage III – Frame-wise Adapter）： •\t将优化后手部姿态图像与环境图和语言指令一起接入 Stable Diffusion 2 Adapter，生成最终视频； •\t训练设置：batch=32, lr=5×10⁻⁵, 30K steps。 实验设置 •\t仿真验证：在 SAPIEN 模拟平台上，用 DynamiCrafter＋MDM + Adapter 生成视频，并通过 Figure 17 示范在虚拟机器人上的模仿效果。\n模型流程：\n第一阶段：Coarse Action Planner（粗动作生成） •\t目标：在给定语言指令（如“拿起杯子并倒入水”）和静态环境图（512×512 分辨率）条件下，生成一段粗略的、合理的手–物体交互视频序列（通常 16 帧）。 •\t模型架构：基于 Video Diffusion Model（VDM），如 DynamiCrafter，使用文本–图像条件扩散网络。 •\t训练细节： •\tBatch size = 16，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t推理时使用 50-step DDIM 采样，平衡生成质量与速度。 •\t输出：一段低分辨率、含大致手部运动轨迹和物体交互的粗视频，用于后续姿态细化。\n⸻\n第二阶段：Motion Diffusion Model (MDM) Refinement（姿态细化） •\t目标：针对第一阶段生成的粗视频，从中提取手部的 3D 关键点序列（60 帧），并利用扩散模型细化运动轨迹，提升抓握姿态的连贯性与自然度。 •\t模型架构：1D 时序扩散网络，对帧间手部关键点做去噪与重建。 •\t训练细节： •\tBatch size = 64，学习率 1×10⁻⁴； •\t训练步数 500K steps； •\t推理时使用 10-step DDIM，快速得到精细关键点序列。 •\t输出：一组平滑、符合物理约束的 3D 手部关键点轨迹，用以指导下一阶段的视频生成。\n⸻\n第三阶段：Frame-wise Adapter（帧级最终生成） •\t目标：将姿态细化后的关键点与原环境图、语言指令结合，生成最终高质量、手部姿态稳定的交互视频。 •\t模型架构：基于 Stable Diffusion 2，通过 Adapter 模块将手部姿态（以可视化关节点或姿态图形式）作为条件，连同环境图与文本，一起输入扩散模型。 •\t训练细节： •\tBatch size = 32，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t采用与 Stage I 相似的 DDIM 采样流程，保证画质与动作一致性。 •\t输出：分辨率可达 512×512 的连续视频帧序列，手–物体交互清晰、握姿自然，可直接用于机器人模仿学习。\nVidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic CVPR 2025\nFrom Technical University of Munich\n目标： 大规模网络视频人类样本学习 训练 家务机器人 模型\n动机： 机器人依赖实例教学，但是做家务没那么多教学\n模型流程：\n模型概览\nVidBot 通过三大模块——3D 可交互性提取、粗阶段 affordance 预测、细阶段轨迹生成——实现从“野外”RGB 视频到机器人可执行动作的零样本迁移。首先，它利用 Structure-from-Motion 和度量深度模型，从单目视频中恢复一致的 3D 手部轨迹与接触/目标点；然后，因式分解成粗预测网络 πc（提取高层次接触点与目标点）与细预测网络 πf（基于扩散生成精细轨迹），并在推理时引入多目标、法线与避碰等可微成本进行采样指导；最后，将生成的 3D 交互轨迹直接部署于多种机器人平台，实现开“即用”的零样本操控能力 ￼ ￼。\n⸻\n3D 可交互性提取管道 1.1 数据准备 •\t视频与语言输入：给定原始 RGB 图像序列 {Ĩ0,…ĨT} 及指令 l。 •\tSfM 与深度预测：使用 SfM 系统估计相机内参 K、无尺度位姿 {TWC} 及稀疏地标，再调用度量深度基模型（如 ZoeDepth）生成密集深度 {D̂t}，实现时序一致且度量尺度的重建 ￼； •\t手-物体分割与填充：借助手—物体检测（如 [72]）与分割模型（如 [94]）提取手部与接触物体掩码，再通过视频修复生成无手帧，以消除动态遮挡对优化的影响 ￼。\n1.2 姿态与尺度优化 •\t全局尺度校正：优化全帧尺度 sg，使稀疏地标深度与预测深度对齐； •\t位姿细化：联合优化每帧位姿 TWCi 与局部尺度 si，补偿 SfM 在手—物体动态区域的误差，实现一致的 3D 重建 ￼。\n1.3 交互表示提取 •\t手部中心轨迹：将优化后各帧手部中心点恢复至首帧坐标系，插值形成平滑交互轨迹 τ̂； •\t接触与目标点采样：在首帧均匀下采样手部中心生成接触点 ĉ，在末帧提取目标点 ĝ，用于监督模型的中间预测； •\t表示定义：最终得到的 3D affordance a = {c, τ}，其中 c∈ℝNc×3 为接触点集，τ∈ℝH×3 为轨迹序列 ￼ ￼。\n⸻\n粗—细分级 Affordance 学习 2.1 模型结构因式分解\n将 affordance 模型 π({Ĩ, D̃},l) 分解为： 1.\t粗阶段 πc：从 RGB-D 图像 Ĩ→D̃ 及指令 l 中预测目标点 g 和接触点 c； 2.\t细阶段 πf：在粗阶段输出 {g,c} 及场景上下文指导下，通过扩散模型生成精细轨迹 τ ； 整个流程利用测试时可微成本（多目标到达、碰撞避让等）对采样进行引导，增强与新场景/新形体的适应性 ￼ ￼。\n2.2 粗阶段：目标与接触点预测 •\t输入预处理：使用开集物体检测器裁剪关注物体区域的 RGB-D 图像； •\t网络架构：πc 由两支网络 πgoalc 与 πcontc 组成，分别输出目标和接触热图及（目标点）深度。 •\t融合视觉特征、RoI 池化后得到的物体嵌入、语言嵌入（CLIP 特征）和位置编码，通过 Perceiver 与 Transformer 生成每像素概率分布； •\t3D 投影：根据相机内参与热图深度，将像素坐标提升到三维空间，得到 g∈ℝNg×3 与 c∈ℝNc×3 ￼。\n2.3 细阶段：基于扩散的轨迹生成 •\t条件扩散模型：πf 采用 1D U-Net 架构，输入包含轨迹状态 τk、TSDF 空间特征（由 3D U-Net 从 RGB-D 构建的体素化 TSDF 地图提取）、语言与物体嵌入等； •\t正向/反向过程：遵循扩散概率模型，逐步加入高斯噪声，再由网络学习去噪，直接回归未加噪轨迹 τ̄0； •\t测试时引导：在每个去噪步骤中加入可微成本函数——多目标到达、法线一致、碰撞避让——通过扰动引导采样，提高交互轨迹在新场景/新形体下的合理性与可执行性 ￼ ￼。\n⸻\n输入与输出 •\t输入：首帧的“修复”RGB 图像 Ĩ̃0、对应度量深度 D̃0、裁剪的物体图像 Ĩ̃0o 及语言指令 l ￼。 •\t输出：3D 接触点集 c 和交互轨迹 τ ，直接可用于机器人控制模块执行操作 ￼。 算力要求： 没说\nCode availability: 暂时没有 (2025-06-11)\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-11/","summary":"Find Robotics in CVPR 2025","title":"Bug Journal 2025-06-11"},{"content":"CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models Currently on Arxiv.\nFrom Nvidia, Stanford, and MIT\n动机：\n为什么不能和 Language 一样，对 Vision 也做 CoT 呢\n实现方式：\n先生成实现这个目标的话图像应该怎么变 然后再生成动作\n实验设计：\n使用 LIBERO 四个任务组（Spatial, Object, Goal, Long），每组 10 个任务，各 50 条示范轨迹；评价指标为成功率（500 次试验，3 个随机种子）\n真实机器人：Bridge-V2（WidowX 机械臂，45k 语言标注轨迹）和 Franka-Tabletop（Panda 机械臂，10–150 条示范），分别测试视觉泛化、运动泛化、语义泛化和语言对齐四类场景；指标为成功率\n对比模型 •\tDiffusion Policy：从头训练的扩散策略，各场景均需单独训练； •\tOcto：通用 VLA，预训练于 OpenX，无 VLM 初始化； •\tOpenVLA：基于预训练 VLM 的开源 VLA； •\tSUSIE：两阶段图像编辑 + 目标条件策略。 在 LIBERO 中，CoT-VLA 平均成功率达 81.13%，高于 OpenVLA 的 76.5% 和 Diffusion Policy 的 72.4%\nRobotic Control via Embodied Chain-of-Thought Reasoning Currently on Arxiv\nFrom UCB, UWarsaw, Stanford\n动机：\n泛化能力\n实现方式：\n多步文本推理 + 框选关键物品位置 + LLM CoT 生成 \u0026ndash;\u0026gt; OpenVLA 完成所有步骤\n实验设计：\n两种设计：\n一样的物品/一样的指令/一样的视角 不同的物品/不同的指令/不同的视角 Robo-DM: Data Management For Large Robot Datasets ICRA 2025 BestPaper on Robot Learning\nfrom UCB \u0026amp; Google Deepmind\n做数据库的\n以前的数据都没压缩过，太大了; 存储，传输成本也高 这样的话加载也会很慢 这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\n他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\n最后把信息都通过 EBML file 存储\n为什么选择 EBML 呢?\n因为：\n支持嵌套结构 是自包含的，更方便复用 支持流处理，不需要一次性全部导入到内存中 支持自动时间同步 对于视频，主要选择了.H264 来压缩，显著降低了文件大小\n最后这个数据集又小又快\nAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview ICRA 2025 Best Paper on Robot Learning Finalist\nfrom Google Deepmind\n机器人打乒乓球\n打乒乓球要又快又准。所以是理想的机器人测试器\n对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\n首先用模仿学习做 base, 然后强化学习训练\n分层控制则是类似 MOE 的思路，每一个子网络都“学一种打球技术”\n然后让主网络来“选择一种打球技术”。\n控制频率：50HZ.\n并且主网络还会在 Validation 的时候 Continue Learning\n比喻：当机器人发现正手比反手好得分，那机器人就会偏向于打更多正手\n$H(s, a) = H_{offline}(s, a) + \\alpha * [R(s, a) - R_{expected}(s, a)]$\n在这里，$H(s,a)$ 是现在的偏好 $H_{offline}(s,a)$ 是训练时的偏好 $R(s,a)$ 是当前环境的奖励 $R_{expected}$ 是预期获得的奖励\n结果：\n此处，B 指 Beginner, I 指 Intermediate, A 指 Expert\n训练难度：\n2.4 Billion steps on 6k Parallel Simulators 训练出了正反手\n每一个操作需要训练 300 - 1200 Million Steps.\n但是推理难度很低，只需要一个 CPU 的 3ms CPU time 即可完成\n最终能实现 50Hz 的推理速度\nNo Plan but Everything Under Control: Robustly Solving Sequential Tasks with Dynamically Composed Gradient Descent ICRA 2025 Best Paper on robot learning finalist\nFrom University of Berlin\n部分现有方法用的是 planning 来做的机器人 manipultaion task.\n就是说比如会找到一个机器人的起点和终点，然后通过一些算法从起点移动到终点\n这样的算法会通过一些数据来训练\n但是人类在做这些 task 的时候并不会有一个 planning, 那如何不训练做这些 task 呢？\n既然现在 Gradient Descent 这么强，能不能考虑直接用 Gradient Desent 来解决这个问题呢？\n可以的：但是和传统的 Gradient Desnet 不一样的点在于：传统的 Gradient Desent 会把所有的 Gradient 全部加在一起，但是对于现在的 task, 不一定要找全局最优解，可以找当前“做哪个分解动作”最优。\n比如：现在可以让机械臂向某个轴的某个方向移动，或者让机械臂把物品抓起来。\n缺点：这个方法必须要对于每一个 task 都设计一个新的 Gradient 方向。\n优点：这个方法可以避免一些不必要的移动，并且可以根据当前状态来调整他的策略。\n实验设计：在“block world”模拟环境中和现实中推拉抽屉。\n在现实世界中会对于这个环境给予一个 干扰，看看这个模型的抗干扰能力如何。\n比较的模型是：ICRA 2020: Online replanning in belief space for partially observable task and motion problems\nPolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation ICRA 2025 Best Paper in Field and Service Robotics\n使用了多种 Modality 来增强模型的 Manipulation 能力\n使用了包括：视觉，听觉，触觉 \u0026hellip; 等 modality 的能力\n实验设计：\n耐用性 成功率 首先和一个其他的商业模型做对比，然后发现耐用性更高\n然后和自己做消融实验做对比，然后发现模态越多，效果越好\nHuman-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition ICRA 2025 Best Paper on Human-Robot Interaction\nFrom 上交 \u0026amp; UIUC\n提出了一种更高效的数据收集 \u0026amp; 训练的方式\n核心在于 Diffusion Policy 的应用。\n最开始的时候，会采集一些数据用于最初的训练\n然后在接下来的训练中，机器人会一步一步 take control, 这时人类只需要做一个大致的动作就可以了。\ne.g.\n以“拿起杯子并放到指定位置”（Pick-and-Place）任务为例：\n阶段一：接近杯子 人类操作：你只需要做一个简单的“向前移动”的手势。 智能体接管：智能体理解你的意图是“去拿杯子”，于是它会自主地、平滑地控制机械手移动到杯子正上方，并摆好最佳的抓取姿势。 关键节点 1 到达：机械手已经就位，悬停在杯子上方。第一个子任务“接近杯子”已完成。此时，智能体停下来，因为它不知道你接下来是想抓取，还是想调整位置，或是想取消任务。它在等待你的下一个指令。\n阶段二：抓取杯子 人类操作：你做一个“抓握”的手势。 智能体接管：智能体接收到“抓取”指令，于是它会自主执行精确的抓取动作，以最稳定的方式合拢机械手，握紧杯子。 关键节点 2 到达：杯子已经被成功拿起。第二个子任务“抓取杯子”已完成。现在，智能体又停下来了。它知道手里拿着杯子，但它不知道你想把杯子放到哪里去。\n阶段三：移动到目标位置并释放 人类操作：你做一个指向目标位置的“移动”手势。 智能体接管：智能体理解意图，自主地将拿着杯子的手移动到目标位置上方，然后等待你最后的指令。 人类操作：你做一个“松开”的手势。 智能体接管：智能体平稳地释放杯子。任务完成。\nVisual Chain-of-Thought Prompting for Knowledge-based Visual Reasoning AAAI 2024\nFrom MIT, UCLA, CMU\n可以作为以后的 baseline\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-10/","summary":"paper review","title":"Bug Journal 2025-06-10"},{"content":"Robo-DM: Data Management For Large Robot Datasets ICRA 2025 BestPaper on Robot Learning\nfrom UCB \u0026amp; Google Deepmind\n做数据库的\n以前的数据都没压缩过，太大了; 存储，传输成本也高 这样的话加载也会很慢 这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\n他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\n最后把信息都通过 EBML file 存储\n为什么选择 EBML 呢?\n因为：\n支持嵌套结构 是自包含的，更方便复用 支持流处理，不需要一次性全部导入到内存中 支持自动时间同步 对于视频，主要选择了.H264 来压缩，显著降低了文件大小\n最后这个数据集又小又快\nAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview ICRA 2025 Best Paper on Robot Learning Finalist\nfrom Google Deepmind\n机器人打乒乓球\n打乒乓球要又快又准。所以是理想的机器人测试器\n对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\n首先用模仿学习做 base, 然后强化学习训练\n分层控制则是类似 MOE 的思路，每一个子网络都“学一种打球技术”\n然后让主网络来“选择一种打球技术”。\n控制频率：50HZ.\n并且主网络还会在 Validation 的时候 Continue Learning\n比喻：当机器人发现正手比反手好得分，那机器人就会偏向于打更多正手\n$H(s, a) = H_{offline}(s, a) + \\alpha * [R(s, a) - R_{expected}(s, a)]$\n在这里，$H(s,a)$ 是现在的偏好 $H_{offline}(s,a)$ 是训练时的偏好 $R(s,a)$ 是当前环境的奖励 $R_{expected}$ 是预期获得的奖励\n结果：\n此处，B 指 Beginner, I 指 Intermediate, A 指 Expert\n训练难度：\n2.4 Billion steps on 6k Parallel Simulators 训练出了正反手\n每一个操作需要训练 300 - 1200 Million Steps.\n但是推理难度很低，只需要一个 CPU 的 3ms CPU time 即可完成\n最终能实现 50Hz 的推理速度\nNo Plan but Everything Under Control: Robustly Solving Sequential Tasks with Dynamically Composed Gradient Descent ICRA 2025 Best Paper on robot learning finalist\nFrom University of Berlin\n部分现有方法用的是 planning 来做的机器人 manipultaion task.\n就是说比如会找到一个机器人的起点和终点，然后通过一些算法从起点移动到终点\n这样的算法会通过一些数据来训练\n但是人类在做这些 task 的时候并不会有一个 planning, 那如何不训练做这些 task 呢？\n既然现在 Gradient Descent 这么强，能不能考虑直接用 Gradient Desent 来解决这个问题呢？\n可以的：但是和传统的 Gradient Desnet 不一样的点在于：传统的 Gradient Desent 会把所有的 Gradient 全部加在一起，但是对于现在的 task, 不一定要找全局最优解，可以找当前“做哪个分解动作”最优。\n比如：现在可以让机械臂向某个轴的某个方向移动，或者让机械臂把物品抓起来。\n缺点：这个方法必须要对于每一个 task 都设计一个新的 Gradient 方向。\n优点：这个方法可以避免一些不必要的移动，并且可以根据当前状态来调整他的策略。\n实验设计：在“block world”模拟环境中和现实中推拉抽屉。\n在现实世界中会对于这个环境给予一个 干扰，看看这个模型的抗干扰能力如何。\n比较的模型是：ICRA 2020: Online replanning in belief space for partially observable task and motion problems\nPolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation ICRA 2025 Best Paper in Field and Service Robotics\n使用了多种 Modality 来增强模型的 Manipulation 能力\n使用了包括：视觉，听觉，触觉 \u0026hellip; 等 modality 的能力\n实验设计：\n耐用性 成功率 首先和一个其他的商业模型做对比，然后发现耐用性更高\n然后和自己做消融实验做对比，然后发现模态越多，效果越好\nHuman-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition ICRA 2025 Best Paper on Human-Robot Interaction\nFrom 上交 \u0026amp; UIUC\n提出了一种更高效的数据收集 \u0026amp; 训练的方式\n核心在于 Diffusion Policy 的应用。\n最开始的时候，会采集一些数据用于最初的训练\n然后在接下来的训练中，机器人会一步一步 take control, 这时人类只需要做一个大致的动作就可以了。\ne.g.\n以“拿起杯子并放到指定位置”（Pick-and-Place）任务为例：\n阶段一：接近杯子 人类操作：你只需要做一个简单的“向前移动”的手势。 智能体接管：智能体理解你的意图是“去拿杯子”，于是它会自主地、平滑地控制机械手移动到杯子正上方，并摆好最佳的抓取姿势。 关键节点 1 到达：机械手已经就位，悬停在杯子上方。第一个子任务“接近杯子”已完成。此时，智能体停下来，因为它不知道你接下来是想抓取，还是想调整位置，或是想取消任务。它在等待你的下一个指令。\n阶段二：抓取杯子 人类操作：你做一个“抓握”的手势。 智能体接管：智能体接收到“抓取”指令，于是它会自主执行精确的抓取动作，以最稳定的方式合拢机械手，握紧杯子。 关键节点 2 到达：杯子已经被成功拿起。第二个子任务“抓取杯子”已完成。现在，智能体又停下来了。它知道手里拿着杯子，但它不知道你想把杯子放到哪里去。\n阶段三：移动到目标位置并释放 人类操作：你做一个指向目标位置的“移动”手势。 智能体接管：智能体理解意图，自主地将拿着杯子的手移动到目标位置上方，然后等待你最后的指令。 人类操作：你做一个“松开”的手势。 智能体接管：智能体平稳地释放杯子。任务完成。\n概念理解\nauto regressive Auto regressive 是一种生成方式。可以从前一个数生成下一个数。 更准确地说，$X_{t_i} = a_1X{t_1} + a_2X{t_2} + \\dots + a_{t_{i-1}}X{t_{i-1}}$.\nteacher forcing Teacher forcing 是指在训练的过程中，把真实信息放入训练\nPi 0\nflow matching\n连续的动作和离散的动作有什么区别\nGemini diffusion\n已加入 waitlist 正在测试 LLaDA 开源 Diffusion model. 目前该模型仍然无法加入 Chain of Thought 问题是：没有 Chain of Thought 的模型显著没有加入 Chain of Thought 的模型强 但是现在有一个叫做 Diffusion of Thought 的方法可以加入类似的东西\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-09/","summary":"\u003ch2 id=\"robo-dm-data-management-for-large-robot-datasets\"\u003eRobo-DM: Data Management For Large Robot Datasets\u003c/h2\u003e\n\u003cp\u003eICRA 2025 BestPaper on Robot Learning\u003c/p\u003e\n\u003cp\u003efrom UCB \u0026amp; Google Deepmind\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e做数据库的\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e以前的数据都没压缩过，太大了; 存储，传输成本也高\u003c/li\u003e\n\u003cli\u003e这样的话加载也会很慢\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"1749461291726\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461291726.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\u003c/p\u003e\n\u003cp\u003e他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461487133\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461487133.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后把信息都通过 EBML file 存储\u003c/p\u003e\n\u003cp\u003e为什么选择 EBML 呢?\u003c/p\u003e\n\u003cp\u003e因为：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e支持嵌套结构\u003c/li\u003e\n\u003cli\u003e是自包含的，更方便复用\u003c/li\u003e\n\u003cli\u003e支持流处理，不需要一次性全部导入到内存中\u003c/li\u003e\n\u003cli\u003e支持自动时间同步\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对于视频，主要选择了.H264 来压缩，显著降低了文件大小\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461874888\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461874888.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461827040\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461827040.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后这个数据集又小又快\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461984813\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461984813.png\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"achieving-human-level-competitive-robot-table-tennis-a-comprehensive-overview\"\u003eAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview\u003c/h2\u003e\n\u003cp\u003eICRA 2025 Best Paper on Robot Learning Finalist\u003c/p\u003e\n\u003cp\u003efrom Google Deepmind\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e机器人打乒乓球\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e打乒乓球要又快又准。所以是理想的机器人测试器\u003c/p\u003e\n\u003cp\u003e对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\u003c/p\u003e","title":"Bug Journal 2025-06-09"},{"content":"Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation 发表时间：15 Jun 2024\n动机 当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\n问题 1：学习难度高（需要从高维图像中学全局策略）； 问题 2：泛化差（模型可能过拟合于训练视角或场景）； 问题 3：sample efficiency 差（训练数据需求量大）\n作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的， 也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\n主要论点 提出 Local Policy Networks（LPN）： 将策略函数设计为一组 局部策略（local policy heads）； 每个 head 只负责“在自己 patch 上预测动作”； 用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）； 最终策略通过对多个 local head 输出聚合（weighted sum）得到。\n模型流程图 简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\n实验 setting: 使用数据集：RT-1（Robotics Transformer 1）:\nGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper） 数据来源 真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集 数据规模 ~130k 条实际机器人操作轨迹，覆盖 700+ 种任务 采样频率 每条轨迹包含约 50–100 帧关键帧（图像 + 动作） 场景 家庭式办公环境（桌面、水槽、地面） 物体 80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等） 语言指令 每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型） 对于每一条指令：\nRGB 图像 来自机器人头部相机的视角（尺寸通常为 240×320） 深度图（可选） 当前状态 如 gripper pose（位置 + 朝向） 语言指令 例如 “pick up the red apple and place it in the bowl” 动作标签 6-DoF 末端动作（位置增量、旋转、夹爪开合） 时间戳 当前帧在轨迹中的位置 成功标志 是否完成任务（某些版本包含） 文章通过 “加热”在数据集中的抓取位置 来 train heatmap.\n在 heatmap 中取出关键点\n之后通过神经网络预测每一个关键点的移动，加权得到机械臂移动的方向。\n通过模仿学习来算 Loss, 然后训练。\nCoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models 发表时间：13 Mar 2024\n动机 当前机器人 manipulation 研究大多集中在：\n高层任务规划（如使用 LLMs 推理“该做什么”）， 或低层控制（通过 imitation learning 或 RL 获取 end-effector 动作序列）。 然而，低层控制模块在真实世界中很难泛化，因为：\n缺乏对物体几何和功能部分的理解， 容易对新的任务和场景失效。 因此，本文希望结合 VLMs 的常识知识和几何建模能力，以“空间约束”的形式桥接语言与机器人执行之间的鸿沟，实现可泛化的低层 manipulation 能力。\n核心思想 Zero-shot! GPT-4V is all you need!\n模型流程图 首先分割场景，识别场景中有多少物体。\n简单来说就是：让 GPT 决定怎么操作这个物体。 比如说，要抓哪里，要怎么抓，物体的姿态是什么，\u0026hellip;\n最后使用一个传统路径规划算法来达成上述所有条件 一旦条件达成，任务也就完成了\n结果 MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment 发表时间: 24 Mar 2024\n动机 之前的模拟环境都太简单了，设计一个复杂的\n主要论点 设计了一个复杂的模拟环境:\n更多更复杂场景，更真实的物理引擎，更好的 Reward\nAny-point Trajectory Modeling for Policy Learning 发表时间：28 Dec 2023\n动机 数据不够用啦，我要从视频里学\n本文旨在从无动作标签的视频中提取出用于控制的轨迹，从而辅助策略学习\n主要论点 作者提出了 Any-point Trajectory Modeling (ATM) 框架：\n第一阶段：使用无动作标签的视频和现有视频追踪器，从视频中提取任意点的轨迹（即二维运动轨迹），训练一个轨迹预测模型（Track Transformer）。 第二阶段：使用轨迹预测模型，在给定任务语言描述和当前图像的情况下预测未来的轨迹，并用这些轨迹作为中间目标（subgoal）指导策略学习，从少量动作演示中学习有效的策略。 这不就是昨天的General Flow吗\n模型流程图 和昨天的那篇文章的区别在于:\n这篇不需要 Ground Truth 数据，只是视频就行，而那一篇需 如何做到的？答:用了一个 LLM Tracker 来跟踪 这篇文章是对\u0026quot;运动最显著\u0026quot;的 K(32) 个点算运动向量，那个是对每一个像素算运动向量 MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning 发表时间: 19 Oct 2024\n动机 做 RL, 还是得泛化\n过去的 embedding 太 General 了。\n思路是：如果 embedding 不分任务注意所有细节, 反而做不好 这里的泛化能忽略掉任务无关的信息\n总之，这里使用了一个 MoE 网络来处理输入。\n模型流程图 至于这个 Perturbation.. 流程如下:\n[输入图像 I] ↓ [Encoder 输出特征 z] ↓ [策略网络 → 计算 loss] ↓ [反向传播：计算 ∇_z L] ↓ [构造 δ = ε · normalized gradient] ↓ [扰动特征 z + δ → 再送策略网络训练] ↓ [更新 encoder + expert 参数] 总之就是训练的时候把 z 往成功的方向“推一下” “引导视觉 encoder 学会放大那些能带来任务成功的区域”\n结果 模拟环境：\nDMC (DeepMind Control Suite) → 如：Walker、Cheetah、Finger、Cartpole 等控制任务 Meta-World → 多任务机器人操作环境（push、reach、pick-place 等） RLBench → 视觉+控制为主的仿真机械臂任务，如插电源、开抽屉等\n真实世界任务：\n在完成 Task 的时候会干扰一下不让它完成这个 Task.\n但是最后还是成功完成了。\nTake away MoE 是什么呢？其实就是多个动态加权平均的网络。 什么是动态加权平均呢？ 就是权重是通过 $SoftMax(MLP)$ 算出来的 这样每次每个网络加权平均的权重就会不同。\nDemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025\n动机 Robust，还得是Robust。\n为什么不加数据(ο´･д･)??\n过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\n那怎么办呢(ο´･д･)??\n很简单，纯计算模拟不就是了。\n主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\n这样就不需要实际操作，但是可以直接获得大量数据。\n模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\n思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\n机器人一共有两段动作\n一段是碰到物体前的动作\n一段是碰到物体后的动作\n对于第一段，直接用一个变换矩阵变换\n对于第二段，直接规划一个新路径 (use RRT-Connect)\n现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\n如果可以用的话\n然后通过模拟环境生成这个路径的图像\n实验设定 虚拟环境：1 条 GroundTruth\n真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\n一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\n结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 + mask + 指令 + robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM + Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove 一个这样的力反馈手套\n可以链接到灵巧手上，然后展示反馈物体的力\n盲抓分辨物体\n盲眼抓杯子\nReactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation 发表时间：23 Apr 2025\n动机 人有触觉，为什么机器人不行？\n所以我们为机器人加上了触觉。\n并且用触觉来优化模型.\n可以是，触觉数据怎么来呢？🤔\n在机器人手上加上触觉组件就好了\n模型流程 方法很简单：\nimage -\u0026gt; 正常的 CV Encoder -\u0026gt; Touch Encoder -\u0026gt; (Action -\u0026gt; Touch Encoder) * N -\u0026gt; image2 \u0026hellip;\n实现效果 Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control 发表时间：June 23 2025\n动机 现在机器人的表情不够生动，希望能生动一点\n用一段语音输入进机器人\n为什么选语音？\n因为语音有语气, 语气中可能有细微的表情差别\n“我没事\u0026hellip;” “我没事！”和 “我?没事” 是有区别的\n这更适合模型生成表情.\n而现有机器人的表情做得不好，所以这个团队自己做了一个可以做更丰富表情的机器人脸\n实现方式 第一部分，有一个带有丰富语气的语音数据集 + 对应的 3D 人脸建模数据\n第二部分，为了训练模型 -\u0026gt; 电机； 随机生成了 5000 个 3D 建模 让电机去拟合这个表情\n↑以上过程都是在虚拟环境中训练完，然后搬到真实环境做的。\nTwo by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation 发表时间: 9 Apr 2025\n动机 机器人泛化能力和精确对齐的能力不行\n一一对应的插入 task 做不好\n模型流程图 一言以蔽之：先找插座位置，再预测插件位置\n这里的位置是真正的位置，分别是 Tran(三位坐标系中的位置) \u0026amp; Rot (三个轴旋转的角度)\n这样就可以计算 loss 了\n之后第二部分就是把刚才得到的 embedding 和插件的 embedding 乘起来 之后还是 predict Tran \u0026amp; Rot 算 loss.\n注：所有训练都是在虚拟环境完成，真实环境仅有 validation.\nGrasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping 发表时间: ICRA 2025\n动机 以前的方法都是以机器人为中心。 没有考虑到机器人和物块之间的相关性\n动机2 之前方法都太慢了\n动机 3 如果我们可以根据 observation 来推测出机械臂抓起物块的时刻的姿态 那我们就可以用 IK 等算法来计算这个抓取路径\n那我们就不需要数据中的路径信息了，只需要两帧，一帧开始，一帧结束\n模型流程图 先训练 Robot Encoder，用手部多配置点云对做 point-level contrastive learning，使手部特征具有结构一致性； 训练 CVAE：把训练好的 hand embedding 和 object embedding（经过 Transformer 融合）作为条件； 将 grasp pose 的手点云 输入到 CVAE 编码器，学习一个 latent 抓取表示； 用 CVAE 解码器重建抓取交互矩阵 D(R,O)，这是对 hand-object grasp 状态的结构表示； 用重建误差 + KL loss 训练整个模型。 结果 缺陷 有很多模型都只考虑了开始点和结束点的信息，没有经过 Motion Planning 这样的话如果遇到障碍物就容易出问题\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-06/","summary":"\u003ch2 id=\"leveraging-locality-to-boost-sample-efficiency-in-robotic-manipulation\"\u003eLeveraging Locality to Boost Sample Efficiency in Robotic Manipulation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2406.10615\"\u003e发表时间：15 Jun 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003e当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\u003c/p\u003e\n\u003cp\u003e问题 1：学习难度高（需要从高维图像中学全局策略）；\n问题 2：泛化差（模型可能过拟合于训练视角或场景）；\n问题 3：sample efficiency 差（训练数据需求量大）\u003c/p\u003e\n\u003cp\u003e作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的，\n也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e提出 Local Policy Networks（LPN）：\n将策略函数设计为一组 局部策略（local policy heads）；\n每个 head 只负责“在自己 patch 上预测动作”；\n用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）；\n最终策略通过对多个 local head 输出聚合（weighted sum）得到。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749017331301\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749017331301.png\"\u003e\u003c/p\u003e\n\u003cp\u003e简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\u003c/p\u003e\n\u003ch4 id=\"实验-setting\"\u003e实验 setting:\u003c/h4\u003e\n\u003cp\u003e使用数据集：RT-1（Robotics Transformer 1）:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-markdown\" data-lang=\"markdown\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据来源\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据规模\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e~130k 条实际机器人操作轨迹，覆盖 700+ 种任务\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e采样频率\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每条轨迹包含约 50–100 帧关键帧（图像 + 动作）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e场景\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e家庭式办公环境（桌面、水槽、地面）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e物体\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e语言指令\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型）\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e对于每一条指令：\u003c/p\u003e","title":"Bug Journal 2025-06-06"},{"content":"DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025\n动机 Robust，还得是Robust。\n为什么不加数据(ο´･д･)??\n过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\n那怎么办呢(ο´･д･)??\n很简单，纯计算模拟不就是了。\n主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\n这样就不需要实际操作，但是可以直接获得大量数据。\n模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\n思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\n机器人一共有两段动作\n一段是碰到物体前的动作\n一段是碰到物体后的动作\n对于第一段，直接用一个变换矩阵变换\n对于第二段，直接规划一个新路径 (use RRT-Connect)\n现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\n如果可以用的话\n然后通过模拟环境生成这个路径的图像\n实验设定 虚拟环境：1 条 GroundTruth\n真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\n一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\n结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 + mask + 指令 + robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM + Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove 一个这样的力反馈手套\n可以链接到灵巧手上，然后展示反馈物体的力\n盲抓分辨物体\n盲眼抓杯子\nReactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation 发表时间：23 Apr 2025\n动机 人有触觉，为什么机器人不行？\n所以我们为机器人加上了触觉。\n并且用触觉来优化模型.\n可以是，触觉数据怎么来呢？🤔\n在机器人手上加上触觉组件就好了\n模型流程 方法很简单：\nimage -\u0026gt; 正常的 CV Encoder -\u0026gt; Touch Encoder -\u0026gt; (Action -\u0026gt; Touch Encoder) * N -\u0026gt; image2 \u0026hellip;\n实现效果 Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control 发表时间：June 23 2025\n动机 现在机器人的表情不够生动，希望能生动一点\n用一段语音输入进机器人\n为什么选语音？\n因为语音有语气, 语气中可能有细微的表情差别\n“我没事\u0026hellip;” “我没事！”和 “我?没事” 是有区别的\n这更适合模型生成表情.\n而现有机器人的表情做得不好，所以这个团队自己做了一个可以做更丰富表情的机器人脸\n实现方式 第一部分，有一个带有丰富语气的语音数据集 + 对应的 3D 人脸建模数据\n第二部分，为了训练模型 -\u0026gt; 电机； 随机生成了 5000 个 3D 建模 让电机去拟合这个表情\n↑以上过程都是在虚拟环境中训练完，然后搬到真实环境做的。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-05/","summary":"\u003ch2 id=\"demogen-synthetic-demonstration-generation-for-data-efficient-visuomotor-policy-learning\"\u003eDemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2502.16932\"\u003e发表时间：24 Feb 2025\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003eRobust，还得是Robust。\u003c/p\u003e\n\u003cp\u003e为什么不加数据(ο´･д･)??\u003c/p\u003e\n\u003cp\u003e过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\u003c/p\u003e\n\u003cp\u003e那怎么办呢(ο´･д･)??\u003c/p\u003e\n\u003cp\u003e很简单，纯计算模拟不就是了。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\u003c/p\u003e\n\u003cp\u003e这样就不需要实际操作，但是可以直接获得大量数据。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749026226827\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749026226827.png\"\u003e\u003c/p\u003e\n\u003cp\u003e一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\u003c/p\u003e\n\u003cp\u003e思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\u003c/p\u003e\n\u003cp\u003e机器人一共有两段动作\u003c/p\u003e\n\u003cp\u003e一段是碰到物体前的动作\u003c/p\u003e\n\u003cp\u003e一段是碰到物体后的动作\u003c/p\u003e\n\u003cp\u003e对于第一段，直接用一个变换矩阵变换\u003c/p\u003e\n\u003cp\u003e对于第二段，直接规划一个新路径 (use RRT-Connect)\u003c/p\u003e\n\u003cp\u003e现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\u003c/p\u003e\n\u003cp\u003e如果可以用的话\u003c/p\u003e\n\u003cp\u003e然后通过模拟环境生成这个路径的图像\u003c/p\u003e\n\u003ch4 id=\"实验设定\"\u003e实验设定\u003c/h4\u003e\n\u003cp\u003e虚拟环境：1 条 GroundTruth\u003c/p\u003e\n\u003cp\u003e真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\u003c/p\u003e\n\u003cp\u003e一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\u003c/p\u003e\n\u003ch4 id=\"结果\"\u003e结果\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749093303146\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749093303146.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749093327428\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749093327428.png\"\u003e\u003c/p\u003e\n\u003ch4 id=\"vs-roboground\"\u003eV.S. RoboGround\u003c/h4\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003cstrong\u003e方面\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eDemoGen\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eRoboGround\u003c/strong\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e目标问题\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e数据高效、空间泛化性差的视觉模仿学习\u003c/td\u003e\n          \u003ctd\u003e多任务泛化能力差、语义-空间信息连接弱\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e核心思想\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e从少量人类演示中\u003cstrong\u003e合成大量视觉演示数据\u003c/strong\u003e用于模仿学习\u003c/td\u003e\n          \u003ctd\u003egrounding mask（掩码）作为embedding增强泛化\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e数据生成方式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像\u003c/td\u003e\n          \u003ctd\u003e构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e人类演示\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e一条\u003c/td\u003e\n          \u003ctd\u003e在仿真中自动生成，无需真实 rollouts\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e任务表征形式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e(图像帧, 末端动作)\u003cstrong\u003e对\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e图像 + mask + 指令 + robot state\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e中间表示\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eNone（直接预测动作）\u003c/td\u003e\n          \u003ctd\u003e掩码（mask）作为空间引导\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e依赖模型\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eImmitation Learning\u003c/td\u003e\n          \u003ctd\u003e利用 VLM + Grounded Perceiver 构建 mask-guided policy\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e泛化方式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e利用空间重定向与图像合成覆盖更多初始状态\u003c/td\u003e\n          \u003ctd\u003e通过 grounding masks 和多样 instruction 提升语义-空间泛化\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"doglove-dexterous-manipulation-with-a-low-cost-open-source-haptic-force-feedback-glove\"\u003eDOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"1749095773362\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749095773362.png\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-06-05"},{"content":"Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation 发表时间：15 Jun 2024\n动机 当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\n问题 1：学习难度高（需要从高维图像中学全局策略）； 问题 2：泛化差（模型可能过拟合于训练视角或场景）； 问题 3：sample efficiency 差（训练数据需求量大）\n作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的， 也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\n主要论点 提出 Local Policy Networks（LPN）： 将策略函数设计为一组 局部策略（local policy heads）； 每个 head 只负责“在自己 patch 上预测动作”； 用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）； 最终策略通过对多个 local head 输出聚合（weighted sum）得到。\n模型流程图 简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\n实验 setting: 使用数据集：RT-1（Robotics Transformer 1）:\nGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper） 数据来源 真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集 数据规模 ~130k 条实际机器人操作轨迹，覆盖 700+ 种任务 采样频率 每条轨迹包含约 50–100 帧关键帧（图像 + 动作） 场景 家庭式办公环境（桌面、水槽、地面） 物体 80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等） 语言指令 每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型） 对于每一条指令：\nRGB 图像 来自机器人头部相机的视角（尺寸通常为 240×320） 深度图（可选） 当前状态 如 gripper pose（位置 + 朝向） 语言指令 例如 “pick up the red apple and place it in the bowl” 动作标签 6-DoF 末端动作（位置增量、旋转、夹爪开合） 时间戳 当前帧在轨迹中的位置 成功标志 是否完成任务（某些版本包含） 文章通过 “加热”在数据集中的抓取位置 来 train heatmap.\n在 heatmap 中取出关键点\n之后通过神经网络预测每一个关键点的移动，加权得到机械臂移动的方向。\n通过模仿学习来算 Loss, 然后训练。\nCoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models 发表时间：13 Mar 2024\n动机 当前机器人 manipulation 研究大多集中在：\n高层任务规划（如使用 LLMs 推理“该做什么”）， 或低层控制（通过 imitation learning 或 RL 获取 end-effector 动作序列）。 然而，低层控制模块在真实世界中很难泛化，因为：\n缺乏对物体几何和功能部分的理解， 容易对新的任务和场景失效。 因此，本文希望结合 VLMs 的常识知识和几何建模能力，以“空间约束”的形式桥接语言与机器人执行之间的鸿沟，实现可泛化的低层 manipulation 能力。\n核心思想 Zero-shot! GPT-4V is all you need!\n模型流程图 首先分割场景，识别场景中有多少物体。\n简单来说就是：让 GPT 决定怎么操作这个物体。 比如说，要抓哪里，要怎么抓，物体的姿态是什么，\u0026hellip;\n最后使用一个传统路径规划算法来达成上述所有条件 一旦条件达成，任务也就完成了\n结果 MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment 发表时间: 24 Mar 2024\n动机 之前的模拟环境都太简单了，设计一个复杂的\n主要论点 设计了一个复杂的模拟环境:\n更多更复杂场景，更真实的物理引擎，更好的 Reward\nAny-point Trajectory Modeling for Policy Learning 发表时间：28 Dec 2023\n动机 数据不够用啦，我要从视频里学\n本文旨在从无动作标签的视频中提取出用于控制的轨迹，从而辅助策略学习\n主要论点 作者提出了 Any-point Trajectory Modeling (ATM) 框架：\n第一阶段：使用无动作标签的视频和现有视频追踪器，从视频中提取任意点的轨迹（即二维运动轨迹），训练一个轨迹预测模型（Track Transformer）。 第二阶段：使用轨迹预测模型，在给定任务语言描述和当前图像的情况下预测未来的轨迹，并用这些轨迹作为中间目标（subgoal）指导策略学习，从少量动作演示中学习有效的策略。 这不就是昨天的General Flow吗\n模型流程图 和昨天的那篇文章的区别在于:\n这篇不需要 Ground Truth 数据，只是视频就行，而那一篇需 如何做到的？答:用了一个 LLM Tracker 来跟踪 这篇文章是对\u0026quot;运动最显著\u0026quot;的 K(32) 个点算运动向量，那个是对每一个像素算运动向量 MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning 发表时间: 19 Oct 2024\n动机 做 RL, 还是得泛化\n过去的 embedding 太 General 了。\n思路是：如果 embedding 不分任务注意所有细节, 反而做不好 这里的泛化能忽略掉任务无关的信息\n总之，这里使用了一个 MoE 网络来处理输入。\n模型流程图 至于这个 Perturbation.. 流程如下:\n[输入图像 I] ↓ [Encoder 输出特征 z] ↓ [策略网络 → 计算 loss] ↓ [反向传播：计算 ∇_z L] ↓ [构造 δ = ε · normalized gradient] ↓ [扰动特征 z + δ → 再送策略网络训练] ↓ [更新 encoder + expert 参数] 总之就是训练的时候把 z 往成功的方向“推一下” “引导视觉 encoder 学会放大那些能带来任务成功的区域”\n结果 模拟环境：\nDMC (DeepMind Control Suite) → 如：Walker、Cheetah、Finger、Cartpole 等控制任务 Meta-World → 多任务机器人操作环境（push、reach、pick-place 等） RLBench → 视觉+控制为主的仿真机械臂任务，如插电源、开抽屉等\n真实世界任务：\n在完成 Task 的时候会干扰一下不让它完成这个 Task.\n但是最后还是成功完成了。\nTake away MoE 是什么呢？其实就是多个动态加权平均的网络。 什么是动态加权平均呢？ 就是权重是通过 $SoftMax(MLP)$ 算出来的 这样每次每个网络加权平均的权重就会不同。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-04/","summary":"\u003ch2 id=\"leveraging-locality-to-boost-sample-efficiency-in-robotic-manipulation\"\u003eLeveraging Locality to Boost Sample Efficiency in Robotic Manipulation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2406.10615\"\u003e发表时间：15 Jun 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003e当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\u003c/p\u003e\n\u003cp\u003e问题 1：学习难度高（需要从高维图像中学全局策略）；\n问题 2：泛化差（模型可能过拟合于训练视角或场景）；\n问题 3：sample efficiency 差（训练数据需求量大）\u003c/p\u003e\n\u003cp\u003e作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的，\n也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e提出 Local Policy Networks（LPN）：\n将策略函数设计为一组 局部策略（local policy heads）；\n每个 head 只负责“在自己 patch 上预测动作”；\n用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）；\n最终策略通过对多个 local head 输出聚合（weighted sum）得到。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749017331301\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749017331301.png\"\u003e\u003c/p\u003e\n\u003cp\u003e简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\u003c/p\u003e\n\u003ch4 id=\"实验-setting\"\u003e实验 setting:\u003c/h4\u003e\n\u003cp\u003e使用数据集：RT-1（Robotics Transformer 1）:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-markdown\" data-lang=\"markdown\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据来源\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据规模\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e~130k 条实际机器人操作轨迹，覆盖 700+ 种任务\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e采样频率\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每条轨迹包含约 50–100 帧关键帧（图像 + 动作）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e场景\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e家庭式办公环境（桌面、水槽、地面）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e物体\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e语言指令\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型）\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e对于每一条指令：\u003c/p\u003e","title":"Bug Journal 2025-06-04"},{"content":"Catch It! Learning to Catch in Flight with Mobile Dexterous Hands 发表时间: 16 Sep 2024\n动机 Basically, 这是之前那篇 DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。\n模型流程图 这是真机器人的样子： 一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。\n训练的过程分为两步：\n第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。 第二步是微调灵巧手让手抓住这个物体。\n最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。\n解决的难点 部署到真机器人上 一步到位 end-to-end 效果没那么好 抓不住从未见过的物体 还需要解决的难点 从未见过的物体还是不好抓 仍然没有考虑材质之类的问题 还是无法在当物体在空中时就判断物体的形状 Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own 发布时间: 4 Oct. 2023\n动机 现在机器人训练要 $10^6$ 级别的数据，这要的时间太长了。 反观人类，人类不需要这么多数据。 这可能是因为人类在训练之前就知道什么能做什么不能做。\n那么，我们能不能输入一个 policy 给机器人让它也知道什么能做什么不能做呢？\n主要论点 训练方式：Reinforcement Learning\n并非模仿学习\n作者提出了一个新框架：\nRLFP（Reinforcement Learning with Foundation Priors），其中融合三种先验： 策略先验（Policy Prior）：告诉 agent “该怎么做”。 价值先验（Value Prior）：评估当前状态“是否更接近成功”。 成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 他们在此框架下构建了具体算法 FAC（Foundation-guided Actor-Critic），把三类先验引入到 Actor-Critic 的学习流程中，并在真实机器人与模拟任务中验证了该方法的有效性。\n模型流程图 策略先验（Policy Prior）：告诉 agent “该怎么做”。 这会生成一个策略分布。 这个策略分布会作为一个 KL 正则项。 希望 Actor 生成的策略和这个策略分布不会差太远\n价值先验（Value Prior）：评估当前状态“是否更接近成功”。 这会作为 Reward 的一部分。 告诉 Robot 它是不是更接近成功了。\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 这也会作为 Reward 的一部分。 告诉 Rotbot 它是不是成功了。\n实现细节 在现实中： 策略先验（Policy Prior）：告诉 agent “该怎么做”。 本文使用了 GPT-4V 来实现这个功能：\n对于每一个单独的任务，需要重新写一个这样的 prompt, 但是模板都是一样的。\n模板如下：\nYou are a helpful robot programming assistant. Here is an image of the current environment, which includes: \u0026lt;Task Instruction\u0026gt; Your job: \u0026lt;Task Instruction\u0026gt; You may only use the following primitive skills: \u0026lt;Primitive Skills List\u0026gt; \u0026lt;Image Input\u0026gt; Please write a Python code to solve this task, however, you can only write code in this format: \u0026lt;Code Format\u0026gt; e.g. (根据文章推测的 example prompt):\nYou are a helpful robot programming assistant. Here is an image of the current environment, which includes: - A plastic bottle with a green cap (the bottle is fixed to the table) - A pink plate nearby Your job: Help a robot arm **unscrew the bottle cap** and **place it on the pink plate**. You cannot lift the bottle. You must rotate the cap **anticlockwise** to unscrew it. You may only use the following primitive skills: # Primitive Skills: # 1. move_to x y z —— move the gripper to position (x, y, z) # 2. grasp —— close the gripper to grasp # 3. release —— open the gripper # 4. rotate_anticlockwise —— rotate the gripper anticlockwise (90°) # 5. rotate_clockwise —— rotate the gripper clockwise (90°) # 6. reset —— move back to the home position \u0026lt;input image\u0026gt; Please write a Python function `code_policy()` that returns a plan list using the above skills. Be sure to: - Estimate the coordinates from the image (roughly) - Include comments to explain each step - Output only the code block and nothing else Your format should be like this: def code_policy(): plans = [ \u0026#39;move_to 0.5 0.0 0.26\u0026#39;, \u0026#39;grasp\u0026#39;, \u0026#39;rotate_anticlockwise\u0026#39;, \u0026#39;move_to 0.75 0.0 0.06\u0026#39;, \u0026#39;release\u0026#39; ] return plans Now write the code: 价值先验（Value Prior）：评估当前状态“是否更接近成功”。 使用了一个 Pretrain LLM 来 “判断好不好” VIP: Universal Visual Reward and Representation via Value-Implicit Pretraining\nVIP 是一个使用大规模离线机器人/视频数据集，目标是 通过一个 image 得到一个方程 $V(O_{t_i})$, 越大表示越成功。\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 在现实中使用 GPT-4V 来判断这个任务是否完成。\n这有浇花的时候没判断成功 (3 success in all 4 tasks)。\n浇花和狡猾脚滑谐音，做不对是正常的\n在虚拟环境中 策略先验（Policy Prior）：告诉 agent “该怎么做”。 为了证明 Policy 不需要固定的形式\n使用了 \u0026ldquo;a diffusion-based policy prior, following the UniPi [25] pipeline\u0026rdquo;\n先用扩散模型生成一个完成任务的视频，再通过一个逆动力学模型把视频帧之间的状态变化转化为动作。\n为了效率起见，使用了 开源视频扩散模型 Seer [26] 预生成视频，然后离线训练（distill）出一个策略模型（policy network）\n然而，因为模拟环境图像质量比现实差，所以生成的视频效果也不好。\n所以用了 10个视频 fine-tune 了一下。\n价值先验（Value Prior）：评估当前状态“是否更接近成功”。 Same set up.\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 这里有 Ground Truth 了，就不用 GPT-4V 了 但是，为了模拟现实中 GPT-4V 的情况，加入了一些噪声。 加入方法如下： 训练一个模型，从状态 + label 预测是否成功 这个模型不是 100% 准确。\n结果 现实世界一个小时后： 模拟世界:\nAblation study\nTake away run on 3090 GPU\nGeneral Flow as Foundation Affordance for Scalable Robot Learning 发表时间: 21 Jan 2024\n动机 一言以蔽之：机器人如何从感知（图像）中知道：“我该操作哪儿”和“怎么操作”？\n当前机器人操作学习普遍依赖：\n大量手工收集的数据； 手动定义的 affordance； 复杂的模仿学习或强化学习流程 而现在的数据：\n泛化能力差 没有统一，可以拓展的，自动的，包含语义的 embedding 主要论点 General Flow（GF） —— 一种结构化、密集的视觉场表示，表征“像素应如何在操作中流动”。\n这个和 NVIDIA DLSS 中的 OPTICAL FLOW 很类似，只不过加入了语义信息。\n模型流程图 ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors 发表时间: 30 Apr 2025\n动机 机器人操作策略泛化能力差, 能不能用 mask 的办法让机器人操作策略的泛化能力更强。\n主要论点 构建一个自动化数据生成流水线，合成高复杂度、多样化语言指令的数据集（112K 指令，24K 演示）;\n利用 GLaMM 模型和 SAM 架构生成目标对象与放置区域的精细分割 mask; 将这些 masks 融合进策略网络\n模型流程图 Part 1 数据集 现在的数据集不够好，我要弄一个新的数据集。 我有一个虚拟环境，这个环境里面有一些物体。\n那我能不能写一个脚本来自动设计一个数据集。\n为了给测试用的模型增加难度，我要在环境中添加一些相似的物体。\n那就可以在图片中找一些相似的物体出来，最好是有一项特征(如颜色)完全一致\n这时候 GPT 可以帮忙\nGPT 对这些物体有一定理解 (3视图，材质，颜色)\nGPT 的这些理解也可以加入进来帮我挑选要放入那些物体。\n有了环境信息还不够，我还要一个 Language Instruction.\n首先，我可以根据位置信息自动生成一些 rule-based Instruction: 比如把 A 移动到 B的右边\u0026hellip;\n那如果要一些更 abstract 的 Instruction, 那我可以用 GPT 生成一个 Language Instruction.\n比如说 水果 -\u0026gt; making jam.\nPart 2 New method 左边的部分和那天的 SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation 很像，都是通过一个 LLM + SAM 获取起始点和终点的信息。\n而右边的部分就是通过 强调起始点和终点 的 attention 来增强起点和终点权重。\n更详细地：通过增加了两个 Query, 分别只和起点和终点做 attention 来增强。\n最后通过一个 transformer decoder 输出离散的 action。\n训练是通过模仿学习，最小化和样本之间的差距。\n效果 计算复杂度 8 * 4090 GPU Approx. 5 days.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-03/","summary":"\u003ch2 id=\"catch-it-learning-to-catch-in-flight-with-mobile-dexterous-hands\"\u003eCatch It! Learning to Catch in Flight with Mobile Dexterous Hands\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2409.10319\"\u003e发表时间: 16 Sep 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003eBasically, 这是之前那篇 \u003cem\u003e\u003ca href=\"https://arxiv.org/abs/2310.08809\"\u003eDexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands\u003c/a\u003e\u003c/em\u003e 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1748922875088\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-03/1748922875088.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是真机器人的样子：\n一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1748922862160\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-03/1748922862160.png\"\u003e\u003c/p\u003e\n\u003cp\u003e训练的过程分为两步：\u003c/p\u003e\n\u003cp\u003e第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。\n第二步是微调灵巧手让手抓住这个物体。\u003c/p\u003e\n\u003cp\u003e最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。\u003c/p\u003e\n\u003ch4 id=\"解决的难点\"\u003e解决的难点\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e部署到真机器人上\u003c/li\u003e\n\u003cli\u003e一步到位 end-to-end 效果没那么好\u003c/li\u003e\n\u003cli\u003e抓不住从未见过的物体\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"还需要解决的难点\"\u003e还需要解决的难点\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e从未见过的物体还是不好抓\u003c/li\u003e\n\u003cli\u003e仍然没有考虑材质之类的问题\u003c/li\u003e\n\u003cli\u003e还是无法在当物体在空中时就判断物体的形状\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"reinforcement-learning-with-foundation-priors-let-the-embodied-agent-efficiently-learn-on-its-own\"\u003eReinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2310.02635\"\u003e发布时间: 4 Oct. 2023\u003c/a\u003e\u003c/p\u003e","title":"Bug Journal 2025-06-03"},{"content":"CLIP 这是 CLIP 的结构，主要是两个部分，一个是 text encoder, 用于得到 text embedding, 一个是 image encoder, 用于得到 image encodding.\nText encoder 是一个 text transformer, Image encoder 是一个 ResNet50 / ViT.\n输入数据是一张图片和它的 alternative text.\n训练的逻辑也不难理解：\n现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text, 现在我对这 N 个 pair 做两两配对。\n如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近 如果不属于同一个 pair, 那么我希望他们的 embedding 更远\n同时，我希望这个embedding的距离是有意义的，越相近的离得越近，越不同的离得越远。\n这时候我们就可以用 Cosine Similarity Loss 来比较两个 embedding 之间的距离。\nThat\u0026rsquo;s it.\n这里有个 assumption, 就是说，虽然我的数据质量不怎么样，但是我有很好的数量。\n对于每一个patch, 我有整整 32k 个图文 pair, 加起来一共 1B 个 True/False pair, 那我一定是可以学到一些东西的。\n但是这里也就是它的局限所在：32k 个图文 pair 要放到非常大量的 GPU 中才能 work, 这时设备之间的通信就成为了最大的效率瓶颈。\n但是虽然说这个模型 train 起来非常复杂，但是其实这个模型不算太大，单 GPU 就足以 inference.\n代码也非常简洁，一个简单的实现如下:\nimport clip import torch from PIL import Image device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;, device=device) image = preprocess(Image.open(\u0026#34;test.png\u0026#34;)).unsqueeze(0).to(device) # CLIP.png为本文中图一，即CLIP的流程图 text = clip.tokenize([\u0026#34;car\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;leaf\u0026#34;]).to(device) # 将这三句话向量化 with torch.no_grad(): logits_per_image, logits_per_text = model(image, text) probs = logits_per_image.softmax(dim=-1).cpu().numpy() print(\u0026#34;Label probs:\u0026#34;, probs) BLIP 这是 BLIP，大部分和之前一样：\n现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text, 现在我对这 N 个 pair 做两两配对。\n如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近 如果不属于同一个 pair, 那么我希望他们的 embedding 更远\n但是，现在我增加了一个部分：除了原本的 Contrastive learning 之外，我还要做一个图片和文字之间的 cross-attention.\n另外，原本的数据里有很多噪音。\n现在我已经初步 train 好一个图文匹配模型了。\n那我们默认在这个模型中图文匹配比较好的，就是数据中“高质量”的部分。\n这时我们再引入一个图生文模型，让模型自己学习这些“高质量”数据，然后覆盖“低质量数据”。\n这样就可以提高数据的整体质量。\nQ-Former 是轻量、任务相关、可控制的视觉语义提取器。\nSigLIP AlexNet AlexNet就是最开始的 CNN 网络\nResNet ResNet引入残差的概念，不再让 CNN 学习原始表示，而是让 CNN 学习不同层之间的差 同时，有些层的结果可以越过中间某些层直接去往更深的层。 这让更深的网络成为了可能。\nDenseNet DenseNet 则是再进一步，DenseNet 会让每层之间形成两两连接，使得网络效果更好。\nTake Away 为什么要用 Cosine Similarity:\n可大可小，既可以拉进，又可以推远；重要的是大小是有意义的，越大代表越不相近，越小代表越相近。\n单塔模型和双塔模型：\n单塔模型就是一个输入的模型；双塔输出就是有两个输入的模型。 或者说，如果一个 embedding 只过一遍模型，那就是双塔模型。 如果一个 embedding 可能要过多遍一个模型，那就是单塔模型。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-02/","summary":"Review of Three base VLA models and three basic CNNs.","title":"Bug Journal 2025-06-02"},{"content":"主流视觉-文本多模态模型技术分析 近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。\nCLIP (OpenAI, 2021) 整体架构设计： CLIP 采用 双编码器架构 ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到相同维度的向量空间lightly.ai。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型lightly.ai。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到共享的多模态嵌入空间lightly.ai。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。 模态对齐方式： CLIP通过对比学习实现视觉-语言对齐。训练时，模型给定一批图文对，学习预测哪张图像与哪段文本匹配lightly.ai。具体而言，CLIP使用 对称的跨模态对比损失 （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度lightly.ai。这种训练使图像和文本编码器产生的特征在共享空间中成对靠近lightly.ai。 输入 token 表达统一： CLIP并未显式统一图像和文本的输入表示格式。 图像和文本各有独立的token化和编码流程 ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征lightly.ai。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过独立编码+对齐空间的方式，规避了直接将图像作为序列token处理的不统一问题。 损失函数与训练策略： 损失采用 对比学习的InfoNCE损失 （实现为带温度系数的归一化softmax交叉熵）research.google。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了大批量训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化lightly.ai。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应lightly.ai。CLIP从随机初始化开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型lightly.ai。 数据集及伪标签： CLIP在一个超大规模的图文配对数据集上预训练，包含约4亿对图像-文本arxiv.org（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖自然语言的弱监督arxiv.org。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。 六大难点应对： 模态对齐困难： CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远lightly.ai。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。 token格式不统一： 采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对lightly.ai。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。 语义粒度不匹配： CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如Fine-Grained CLIP等正是受限于CLIP在局部语义对齐上的不足lightly.ailightly.ai。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。 多模态上下文保持： CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 多模态上下文 （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。 训练数据稀缺： CLIP通过大规模弱标注数据从根本上缓解了数据稀缺的问题arxiv.org。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征research.google。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。 计算开销高： 训练CLIP确实需要巨大的算力和显存，但相对来说，其双塔架构使训练可并行展开，推理时也可分别预编码图文后做相似度计算，具有一定的效率优势lightly.ai。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化lightly.ai。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现lightly.ai。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。 参考： CLIP 的论文arxiv.org详细描述了其对比预训练方法，OpenAI 的博客也提供了概述lightly.ai。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库huggingface.co。\nALIGN (Google, 2021) 整体架构设计： ALIGN（ A Large-scale ImaGe and Noisy-Text embedding ）延续了与CLIP相同的双编码器对比学习架构research.google。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：EfficientNet-L2卷积网络作为图像编码器，BERT-Large作为文本编码器，并均从随机初始化开始训练research.google。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。 模态对齐方式： ALIGN采用对比损失（normalized softmax）来训练，使匹配的图文对嵌入向量接近，不匹配的远离research.google。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以批为单位的跨模态对比训练，使模型学到强大的图文对齐表示。 输入 token 表达统一： ALIGN同样没有将图像直接离散为token序列，而是通过双通道处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出embedding空间实现统一表示research.google。 损失函数与训练策略： 使用 对比学习损失 （InfoNCE变体），在大批量上训练模型research.google。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 最小程度的过滤 ，通过数据规模来弥补噪声research.google。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。 数据集及伪标签： ALIGN的亮点在于使用了超过10亿对图像-Alt文本的超大规模数据集research.google。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 无需人工标注 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN 放宽过滤标准 ，只做了基于频率的简单过滤，最终得到约18亿对图文数据research.google。这些文本描述可能包含噪声甚至与图像无关，但研究表明 规模弥补质量 ：如此海量的数据使模型学到泛化的视觉语言表示research.google。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题research.google。 六大难点应对： 模态对齐困难： ALIGN证明了数据规模在对齐中的重要作用。通过十倍于CLIP的数据规模和强大的对比损失，模型学到了稳健的跨模态对齐能力research.googleresearch.google。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型research.google。 token格式不统一： 与CLIP类似，ALIGN通过双编码器架构回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可research.google。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。 语义粒度不匹配： ALIGN的训练目标依旧作用在 全局图像-句子层面 ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对细粒度语义的不匹配没有特殊解决方案。 多模态上下文保持： ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力research.google，未涉及多模态对话等情境。因此，ALIGN在多轮交互或长上下文问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。 训练数据稀缺： ALIGN的策略是极端扩增数据规模以消除数据稀缺瓶颈research.google。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型research.google。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 弱标注的大数据 。 计算开销高： 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展research.google。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源research.google。尽管计算开销高昂，ALIGN通过 冻结架构复杂性 （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，优先扩大数据规模比增加模型复杂度更有效research.googleresearch.google。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。 参考： ALIGN 的研究细节发表于 ICML 2021research.google。Google Research 官方博客提供了对ALIGN的通俗描述research.googleresearch.google。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。\nBLIP (Salesforce, 2022) 整体架构设计： BLIP（ Bootstrapping Language-Image Pre-training ）提出了一种 多模态混合编码-解码架构（MED） ，旨在同时支持视觉-语言的理解和生成任务lightly.ai。具体而言，BLIP的模型包含三个不同模式的子模型：① 单模态编码器 ：对图像或文本单独编码，用于提取各自模态的表示，并采用图文对比损失（ITC）对齐两种模态的全局特征lightly.ai；② 图像引导的文本编码器 ：在文本Transformer中引入跨模态注意力，让文本编码能够利用图像特征，训练时使用 图文匹配（ITM）损失来判断给定图文是否匹配lightly.ai；③ 图像引导的文本解码器 ：使用因果自注意（单向）以实现文本生成，能够在给定图像的条件下生成描述或回答，训练时使用语言模型（LM）损失lightly.ai。这三部分共享同一个视觉编码器（ViT）和大部分文本编码-解码参数，仅在自注意力层是否双向/单向上有所区别lightly.ai。这种设计使一个模型即可兼顾判别任务 （如检索、VQA判断）和 生成任务 （如图像描述）。 模态对齐方式： BLIP结合多种目标实现模态对齐和融合：(1) 图文对比学习（ITC）让图像和文本的全局表示对齐，获得CLIP类似的跨模态嵌入空间lightly.ai；(2) 图文匹配（ITM）通过引入交叉注意力的文本编码器，对图文对的匹配与否进行二分类训练，从而细粒度地对齐图像内容和文本语义lightly.ai；(3) 图像条件文本生成（LM损失）使模型学会在视觉条件下生成合适的语言输出。这三种损失共同训练，迫使模型不同层面地对齐：既有 全局对齐 （ITC保证embedding空间一致），又有 局部对齐 （ITM通过注意力机制关注图像局部来判断匹配），还有跨模态 生成对齐 （LM确保图像信息融入生成过程）。尤其ITM子任务，需要模型理解图像细节与文本句子细微差异，提高了模态对齐的精细程度。 输入 token 表达统一： BLIP并未将图像直接当作序列token交给文本Transformer，而是采用部分共享参数的编码器-解码器架构来统一多模态信息。图像首先由视觉Transformer编码为一组图像embedding序列，文本则通过词嵌入得到文本token表示。在图文交互阶段，图像embedding通过跨模态注意力机制供文本编码器/解码器读取，从而在Transformer中融合lightly.ai。由于文本端Transformer参数在编码和解码模式下共享（仅注意力方向不同），图像信息可以以一致的方式融入文本token处理流程lightly.ai。简言之，BLIP通过在Transformer中引入图像作为钥匙/值的跨注意力，将图像内容注入文本token序列的处理，使两种模态的信息在Transformer中统一表达和交互。 损失函数与训练策略： BLIP在预训练阶段联合优化三种损失lightly.ai：图文对比损失、图文匹配损失、语言建模损失。一次训练迭代中，通过共享的图像编码器提取视觉特征后，分别送入上述三种模式的网络计算损失lightly.ai。为提高效率，BLIP 让文本编码器和解码器共享参数 （仅自注意力模块不同），这样图像特征可一次读取，多头任务不会成倍增加参数lightly.ai。训练策略上，BLIP先在大规模图文数据上这样多任务预训练，然后可以微调到下游具体任务。由于同时优化多目标，需平衡各损失的权重，论文中选择了适当的超参使模型在理解和生成性能上均有提升。此外，为了利用噪声较大的网络数据，BLIP设计了 两阶段Bootstrapping策略 ：首先用预训练好的模型作为图像描述生成器（Captioner）为图像生成候选描述，然后用一个筛选器（Filter）剔除不匹配的图文对，再将清洗/补充后的数据用于进一步训练lightly.ai。这种“Captioner+Filter”流程有效降低了训练数据中的噪声，并引入了合成的伪标签描述，提高了数据利用率arxiv.org。 使用的数据集及伪标签： BLIP的预训练使用了数百万规模的公众图文对数据（如COCO Caption、Visual Genome等常用数据集的组合，以及从网上爬取的图文对）arxiv.org。相对于CLIP/ALIGN那样十亿级的弱标注数据，BLIP使用的数据规模较小但质量更高（经过一定清洗）。为了进一步扩充数据，BLIP引入 伪标签机制 ：利用自己模型生成图像描述（Captioner生成synthetic captions），再通过训练好的匹配模型（Filter）过滤噪声lightly.ai。结果是，原本嘈杂的网络图文数据被“自举”出较为可靠的新图文对，从而缓解了高质量标注数据不足的问题arxiv.org。这一过程中生成的图像描述相当于 合成标签 ，极大丰富了训练语料。在预训练后，BLIP在下游如Flickr30K检索、COCO描述、VQA等数据集上进行微调或直接评估，均取得领先性能arxiv.org。值得一提的是，BLIP的整个预训练不依赖外部标注工具（如不使用额外OCR或检测模型），完全通过多任务训练和自举数据来提高性能。 六大难点应对： 模态对齐困难： BLIP通过多重训练目标从不同层次对齐图像和文本。ITC损失提供全局嵌入对齐，ITM损失迫使模型关注细粒度关联来判断真伪配对，LM损失则确保图像信息能融入自然语言生成lightly.ailightly.ai。此外，BLIP在训练中让图像参与文本Transformer的注意力计算，直接在模型内部融合模态，这比单纯对比学习的对齐更深入。综合来看，BLIP有效缓解了模态对齐难的问题，使模型不仅对整体匹配敏感，也能对局部语义对齐做出正确判断。 token格式不统一： BLIP没有一刀切地将图像转为离散token，而是采用跨注意力融合策略保持各模态表征方式的优势。图像以连续向量形式存在，通过跨模态注意力供文本Transformer使用，实现类似“在Transformer中把图像当成一串记忆token”的效果。这种方法避免了人为定义图像token格式的问题，由模型自学怎样将图像特征融入文本语境。因此，BLIP在不显式统一输入格式的情况下，通过模型结构实现了功能上的token统一处理。 语义粒度不匹配： BLIP在架构上引入了细粒度语义对齐机制。ITM子任务要求模型判别图文是否匹配，这通常取决于对图像细节和文本词语是否对应的判断（例如一句描述中某个细节是否在图中存在）。模型通过跨注意力可以聚焦图像的局部区域来对应文本片段，从而解决了图像区域与文本词汇粒度不对齐的问题。在生成阶段，图像引导的解码器也能描述具体对象或属性，实现细粒度描述lightly.ai。因此BLIP比CLIP这类全局对齐模型更好地兼顾了细粒度的语义对齐。 多模态上下文保持： 虽然BLIP主要针对单幅图像与单段文本的配对任务，但其架构天然适用于 多轮交互的扩展 。因为BLIP的文本Transformer本质是一个语言模型，经过适当调整可以接受前文对话作为文本输入，然后结合图像生成回答。事实上，BLIP的设计理念已被后续多模态对话模型（如 InstructBLIP 等）继承，用于处理多轮对话。不过BLIP原始模型并未显示多轮对话训练。在单轮情景下，BLIP利用Transformer的长序列能力，一定程度上可以处理 更长的文本上下文 （如图像说明+问题一起作为前缀，然后生成回答）。因此在上下文保持上，BLIP通过Transformer结构具备了潜在优势，但需通过特定训练来充分发挥。 训练数据稀缺： BLIP以 自举（Bootstrapping）的方式缓解数据不足。面对高质量标注数据有限的问题，BLIP使用初步模型为大量未标注图像生成描述（相当于自动标注），再过滤噪声后加入训练arxiv.org。这种方式有效放大了训练集规模且成本低，因为生成伪标签比人工标注快得多。结果，BLIP无需像CLIP那样依赖上亿数据，就能取得优异表现arxiv.org。此外，多任务联合训练也提高了数据利用效率：同一数据同时为对比、匹配、生成三种任务服务，信息提取更加充分。总之，BLIP通过模型自生成数据+多任务学习 ，成功在有限数据下逼近甚至超越了依赖海量数据的方法。 计算开销高： BLIP的模型大小适中（基于ViT-B/16等视觉主干，文本部分与BERT-base级别相当），但同时优化三种目标确实增加了训练复杂度。不过，通过 参数共享 （文本编码器和解码器共享大部分参数）lightly.ai和 模块复用 （同一个视觉编码器和Transformer用于多任务），BLIP将训练开销控制在可接受范围。相较于为理解和生成训练两个模型，BLIP训练单个模型完成两类任务，实际上 节省了总体算力 。当然，多任务训练需要更长时间收敛，但Salesforce的实验表明收益是值得的arxiv.org。在推理阶段，BLIP可以根据任务切换模式，例如执行检索时只用编码器部分，做描述时用编码-解码器，全模型参数无需全部参与，从而 推理开销也相对可控 。综上，BLIP通过架构设计在性能和计算成本之间取得了平衡，使得大型多模态模型的训练变得更加高效。 参考： BLIP的论文发表在 ICML 2022arxiv.orgarxiv.org。官方代码已开源在 GitHubarxiv.org（salesforce/BLIP仓库），提供了预训练模型和下游任务的fine-tune实现，方便复现论文结果。\nBLIP-2 (Salesforce, 2023) 整体架构设计： BLIP-2的核心思想是利用现有的预训练模型来高效构建多模态模型arxiv.org。它冻结了图像编码器（如ViT系列）和大型语言模型（LLM，如OPT、Flan-T5等），通过引入一个轻量级的Query Transformer（Q-Former）将二者连接起来arxiv.org。架构上包括：冻结的视觉编码器-\u0026gt; Q-Former -\u0026gt; 冻结的文本生成模型。Q-Former本质是一个Transformer模块，接受视觉特征作为输入，输出一组固定数量的查询向量lightly.ai。这组向量经过投影后，作为虚拟的“视觉token”，嵌入到LLM的输入序列中，从而让LLM能够接收图像信息lightly.ai。由于LLM参数冻结，BLIP-2主要训练Q-Former和少量连接层。 模态对齐方式： BLIP-2将跨模态对齐的主要难点转移到Q-Former上。它采用两阶段训练lightly.ai：第一阶段，让Q-Former结合冻结视觉编码器进行 图文表示学习 （类似BLIP-1的方法，使用图文对比或匹配损失），使Q-Former学会提取与文本语义相关的视觉概念lightly.ai。第二阶段，将训练好的Q-Former输出连接到冻结的LLM输入embedding，利用图文对话/描述数据训练生成任务，使整个系统能够端到端地产生对应输入图像的文本lightly.ai。在对齐过程中，Q-Former充当中介：它一头通过跨注意力读取视觉特征，另一头输出的查询向量要能和LLM的语义空间对接。因此，通过专门设计的损失（如阶段一的对比/ITC+ITM，阶段二的语言建模），BLIP-2成功将视觉空间对齐到语言空间。直观来说， Q-Former学会生成“描述图像的语言向量” ，这些向量插入LLM提示中后，LLM即可理解并基于图像内容作出回答。 输入 token 表达统一： 在BLIP-2中，输入给LLM的是标准的文本token序列，但其中混入了由图像生成的 特殊嵌入向量 。具体实现是：LLM的词表中引入若干个保留位置，用来放置Q-Former生成的视觉查询向量（不一定真的映射为离散token，而是直接作为embedding）lightly.ailightly.ai。因此，从LLM角度看，它接收到了一串长度为N（固定）的“视觉token”嵌入，后面可能跟随文本token，例如问题或提示语。通过这种方式，图像信息被格式统一地并入LLM的输入序列，就好像视觉也被表示成了一组特殊的单词embedding。值得注意的是，这里的视觉token并非通过人工词典获得，而是Q-Former自由学习产生的向量。不过，对于LLM来说，无论是真实文字embedding还是视觉embedding，它都一视同仁地通过自注意力机制处理。这实现了在架构上的 输入统一 ：图像被转换成等价于文本embedding的形式，与文本共同作用于下游生成。 损失函数与训练策略： BLIP-2采用 分阶段训练策略 。阶段一使用与BLIP类似的目标（ITC对比学习、ITM匹配等）训练Q-Former，使其能够对齐视觉和文本表示lightly.ai。阶段二则固定视觉编码器和Q-Former不变（或仅微调Q-Former），仅训练将Q-Former输出喂入LLM后的生成能力lightly.ai。阶段二通常采用 语言模型损失 ：给定图像和（可选的）文本提示，让LLM输出描述或答案，与GT文本计算交叉熵损失，从而调整Q-Former和连接层使LLM的输出正确。在这一阶段，LLM本身参数冻结，所以训练信号主要作用于Q-Former，使其输出的视觉查询能被LLM高效利用。此外，BLIP-2可能使用了混合数据训练策略：既包含纯图文对话数据，也包含传统图文描述数据，以增强模型泛化能力。总结来说，第一阶段注重表示对齐，第二阶段注重生成对接arxiv.org。两个阶段结合，使模型以较低的训练成本达到对大语言模型“喂图”的效果。 使用的数据集及伪标签： BLIP-2所使用的数据包括现成的大规模图文对以及 对话式多模态数据 。阶段一使用的数据类似BLIP-1，例如COCO Caption、Visual Genome Caption以及LAION-400M等开放图文集，用于学习跨模态表示。阶段二则需要图像输入/文本输出的监督数据，如VQA问答、图像描述，以及自制的指令数据集等。由于BLIP-2本身是在2023年提出，可能利用了当时兴起的多模态指令数据（例如由GPT生成的对话）来增强模型的对话能力。关于伪标签，BLIP-2相比BLIP-1更少需要合成描述，原因是它直接利用预训练的LLM已经具备生成流畅文本的能力。相反，BLIP-2更关注 如何高效利用预训练资源 。它没有从零开始生成伪标签，而是通过降低训练需求（冻结大模型）来避免需要海量新标注数据arxiv.org。因此，除非为了特定任务，BLIP-2通常不依赖额外的伪标注数据。不过，在一些研究和开源实现中，会将BLIP-2作为基础，再用GPT-4生成的指令数据进行微调（如InstructBLIP），那属于后续fine-tuning阶段。总的来说，BLIP-2本身强调 利用已有数据与模型 ，而非采集新数据，这是一种不同于以往“大规模爬取”的范式。 六大难点应对： 模态对齐困难： BLIP-2的巧妙之处在于借助预训练模型降低对齐门槛：视觉编码器（如CLIP的ViT）本身已具有与文本对齐的表示能力，LLM则有强大的语言理解和生成能力。Q-Former经过专门训练，学习如何从图像提取出能解释文本的关键视觉概念lightly.ai。它将视觉信号压缩成几十个查询向量，使之恰好能被LLM理解。通过两阶段训练，BLIP-2成功将视觉信息嵌入LLM的上下文中，实现 模态隐式对齐 。尤其第二阶段训练，让模型生成正确描述，确保了视觉表示和语言表示在语义空间上对齐，以至于LLM可以将来自图像的embedding视作自身词汇的一部分。这解决了LLM未看过图像的难题，将跨模态对齐转换为一个中等规模Transformer训练就完成了arxiv.org。因此，BLIP-2在对齐上绕过了直接训练巨型多模态模型的难关，以更低成本达到对齐效果。 token格式不统一： BLIP-2通过 Query Transformer输出固定长度视觉token向量 ，使得图像信息以接近文本token的形式输入LLMlightly.ai。这些视觉token不是离散符号，但在Transformer中发挥的作用与普通词嵌入相同。LLM在位置嵌入上也不区分它们，这样视觉和文本序列实际上融合为一个统一的序列处理lightly.ai。因此，虽然没有显式定义图像的词汇表，BLIP-2达成了功能上的token格式统一：模型把连续视觉特征转换为离散的若干embedding插入序列。LLM可以像处理句子一样处理“图像句子”。这一设计继承了Flamingo等模型的思路，但更轻量（因为只有Q-Former承担额外计算）。 语义粒度不匹配： BLIP-2输出的视觉token向量本质上可以被视为图像的 语义摘要 。Q-Former通过训练，会针对文本任务提取图像中与语义相关的细粒度信息。例如，若任务是描述图像，Q-Former会聚焦显著对象和属性；若任务是回答问题，Q-Former会提取与问题相关的视觉线索。这种机制使图像的大量低层次像素信息被压缩，仅保留语义层面的关键内容，从而匹配LLM处理的语言粒度（概念级别）lightly.ai。因此，语义粒度的鸿沟通过Q-Former的提炼得到弥合——图像的细节被提升到语义概念后才提供给LLM。实践证明，BLIP-2能够让LLM正确识别图中具体对象并生成相应描述，说明语义层次基本匹配了语言空间arxiv.org。当然，如果图像中有非常细微的局部信息，固定数量的查询可能略有不足，但总体上BLIP-2在保持主要语义同时过滤冗余细节方面是成功的。 多模态上下文保持： BLIP-2本身不直接处理多轮对话，但由于它的输出接口是对接LLM，而LLM天然支持长上下文对话，因此BLIP-2具备扩展为多模态对话的潜力。事实上，将BLIP-2生成的视觉token视为对话的一部分，就可以实现 ChatGPT+图像 的效果。BLIP-2的论文主要评估的是单轮任务（如VQA回答、图像描述），但把它用于对话时，可以每次在提示中加入视觉token并配合已有的聊天上下文，LLM即可持续参考视觉信息进行对话。这意味着BLIP-2间接实现了 视觉上下文在多轮对话中的保持 ：视觉token可以在对话prompt中重复出现或被引用，使LLM记住之前提到的图像要点。不过，在一个会话过程中，BLIP-2通常针对每张新图像各自运行一次，不会像Flamingo那样显式处理多张图共同存在的情况。因此严格来说，BLIP-2原生支持 单图上下文保持 ，多图或连续对话需借助LLM的记忆机制来维系。 训练数据稀缺： BLIP-2的策略是 以预训练模型替代海量数据 。因为直接训练一个看图的LLM需要海量图文数据，但BLIP-2通过使用预先训练好的ViT和LLM，将主要学习任务转为训练Q-Former。Q-Former的参数规模（约1.9亿）远小于LLM，所需训练数据也相对少lightly.ai。实验表明，在已有的大模型基础上，只需在相对有限的图文数据上微调，就能达到甚至超过训练80亿参数模型（如Flamingo）的效果arxiv.org。这等于用模型知识弥补了数据量不足。此外，BLIP-2本身利用了BLIP-1时期的图文数据清洗经验，挑选高质量数据进行两阶段训练，以较小的数据量取得高性能arxiv.org。因此，对研究者而言，BLIP-2降低了训练多模态模型的数据门槛——无需爬取上亿样本，有几百万高质量样本配合预训练模型就够用。 计算开销高： 相比从头训练一个多模态Transformer（参数往往数十亿），BLIP-2的训练开销显著降低。冻结LLM和视觉编码器意味着大部分参数不需要反向传播更新，只训练Q-Former等少部分参数，使内存和算力需求下降。作者报告，BLIP-2仅有少量可训练参数却超越了一些体量大几十倍的模型arxiv.org。同时，由于分阶段训练，第一阶段可在相对小模型上完成，第二阶段虽然用LLM但只进行embedding层和Q-Former的调优，计算效率高。综合来看，BLIP-2通过迁移学习和 参数高效微调 ，极大缓和了算力需求。这也体现在推理阶段：因为LLM冻结且对话时只需将视觉token拼接输入，不增加额外推理步骤，实时性有保障。当然，BLIP-2依赖的LLM本身推理开销不低（如果LLM很大），但相较于训练一个同等大小的多模态模型，BLIP-2的总计算代价小得多。因此，在算力有限的环境下，BLIP-2提供了一种实用可行的多模态方案。 参考： BLIP-2的论文在 arXiv 发布arxiv.orgarxiv.org（ICLR 2023），详细介绍了其两阶段训练方法和在零样本VQA等任务上的性能。代码已开源在 GitHub（salesforce/LAVIS库中提供了BLIP-2实现）。BLIP-2的效果也推动了许多衍生工作（如开放对话系统 MiniGPT-4 等），这些都建立在BLIP-2提供的视觉-语言接口之上。\nGIT (Microsoft, 2022) 整体架构设计： GIT（ Generative Image-to-text Transformer ）尝试将视觉-语言任务完全统一到一个生成式Transformer框架下ar5iv.labs.arxiv.org。其架构极为简洁： 一个图像编码器 + 一个文本解码器 ，二者共同组成一个端到端的序列到序列模型ar5iv.labs.arxiv.org。图像编码器提取图像特征（采用预训练的CLIP视觉Transformer或自训练的ViT等，输出二维特征序列ar5iv.labs.arxiv.org），然后通过线性层投影并加上位置嵌入，作为文本解码器的跨注意力键值输入ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。文本解码器是标准的Transformer解码架构（多层自注意力+交叉注意力），以语言模型方式生成文本ar5iv.labs.arxiv.org。不同于许多早期方法，GIT不使用任何物体检测器或OCR模型来预处理图像，也不引入额外的多模态编码器，一切融合在Transformer解码器中完成ar5iv.labs.arxiv.org。这种纯粹的“图像到文本”架构使模型在预训练和微调阶段的结构完全一致，能够方便地泛化到各种以文本为输出的视觉任务。 模态对齐方式： GIT没有采用显式的对比对齐或ITM损失，而是通过单一的语言建模任务隐式地实现模态对齐ar5iv.labs.arxiv.org。在预训练时，模型接收图像并 直接生成整段描述文本 （或回答），训练目标是最小化生成文本与真实文本之间的交叉熵损失ar5iv.labs.arxiv.org。这种方式迫使图像编码器提取的特征必须包含生成正确文本所需的所有信息，同时解码器的交叉注意力会学习将文本词汇与相应的图像区域关联，以便正确生成。这意味着图像和文本的对齐并不是通过拉近embedding距离实现的，而是在Transformer解码过程中，通过注意力权重对齐：模型只有在正确对齐图像内容与生成词语时才能取得低损失。例如，当解码器生成单词“狗”时，跨模态注意力会自然地关注图像中狗所在的特征区域，从而将视觉语义与该单词绑定。经过大规模训练后，这种注意力驱动的软对齐形成模型内隐的模态对齐机制。值得一提的是，作者在预训练时 扩充了任务种类 ，不仅包括图像描述，还有图像问答等，这些任务都要求正确关联图像和文本才能解答，从而进一步强化了模态对齐ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。 输入 token 表达统一： GIT通过 将图像特征序列拼接进Transformer解码器的输入 ，实现了一种隐式的token统一表示ar5iv.labs.arxiv.org。具体而言，图像编码器输出经过投影变换后，作为一组“图像token”（连续向量）排列在Transformer解码器的输入序列最前ar5iv.labs.arxiv.org。紧随其后的是文本的\u0026lt;BOS\u0026gt;标记和需要生成的文本token（初始化为待预测状态）。在Transformer内部，采用一个特别的序列到序列注意力掩码ar5iv.labs.arxiv.org：文本token可以看见所有图像token和之前的文本token，而图像token之间也可以相互看到（便于图像特征全局建模）ar5iv.labs.arxiv.org。这样，Transformer解码器实际上同时处理了图像token和文本token的序列。对模型而言，图像token与普通文本embedding在同一计算图中，只是通过mask控制了注意力方向。通过这种机制，GIT无需修改Transformer结构，就实现了 图像+文本统一序列建模 ：图像被视作解码开始时的一段前缀序列。这保证了图像信息能够像前文一样参与生成过程，从而让图像上下文与文本自然融合。另外，这种方法也不需要离散化图像，只要提供足够的图像token分辨率，模型就能以连续表示处理视觉信息。 损失函数与训练策略： GIT采用 纯粹的自回归语言模型损失 。给定图像（以及可选的提示文本），让模型生成目标文本序列，计算标准的交叉熵损失来训练ar5iv.labs.arxiv.org。在预训练期间，为了让模型适应多样任务，训练数据中包含了各种形式：图像标题生成、图像问答（在这种情况下，会在图像token后加入问题文本作为前缀，然后生成答案）等ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。例如，对于VQA，输入序列是「\u0026lt;img\u0026gt;\u0026hellip;\u0026lt;img\u0026gt; 问题：\u0026hellip; 答案：」，模型学习在看到“问题”后生成正确“答案”ar5iv.labs.arxiv.org。这种统一的语言模型策略使预训练和下游任务能够共享同一套参数和目标，不需要为不同任务切换架构或损失函数。此外，作者强调扩大预训练数据和模型规模对性能至关重要ar5iv.labs.arxiv.org。他们使用了比以往更大规模的图文数据，以及训练了不同尺寸的模型（从Base到巨型）进行对比，在多个任务上取得新的SOTAar5iv.labs.arxiv.org。训练策略上没有使用教师模型或多阶段训练，而是一阶段大一统模型学尽可能多的任务。这种“无技巧（no bells and whistles）”的方法充分依赖海量数据和模型容量来获得性能ar5iv.labs.arxiv.org。 使用的数据集及合成数据： GIT的预训练数据非常广泛和庞大。微软在论文中没有公布确切的数据量，但提到**“扩大了预训练数据规模”ar5iv.labs.arxiv.org。推测他们使用了公共的大型图文数据集合集（如COYO、LAION等），以及内部收集的数据，包括图片描述和问答标注。此外，他们还将视频字幕数据扩充到模型中，使模型能处理视频（选帧作为序列的一部分）ar5iv.labs.arxiv.org。在下游微调时，GIT在12个具有挑战性的基准上测试，包括COCO、nocaps（开放词汇描述）、VizWiz（盲人拍照求助）、TextCaps（需要OCR的图片描述）、多种VQA和视频caption等arxiv.org。令人瞩目的是，GIT在TextCaps数据集上首次超越了人类表现arxiv.org，说明模型学会了相当程度的场景文本识别和理解——这归功于预训练涵盖了带文字的图像以及无需OCR模块的端到端学习。GIT并未借助合成的伪标签数据；相反，它直接在真实任务数据上大规模训练**。例如，为了让模型具备OCR能力，他们可能在预训练中加入了带文字的图像及其文字描述（如OCR-VQA等），让模型自己去学习文字区域的特征提取ar5iv.labs.arxiv.org。因此，GIT更多是通过多任务训练覆盖各种模态难点，而不是通过额外生成数据来弥补。当然，训练这样一个模型本身需要巨量的数据，但微软具备这样的资源优势。 六大难点应对： 模态对齐困难： GIT选择了端到端生成作为对齐手段。由于模型只能通过正确生成文本来降低损失，它被迫在内部对齐图像与文本。例如，Transformer解码器的交叉注意力会在训练中自动调整，使得每个生成的词与相应的图像内容关联。这种隐式对齐不需要额外的对比损失，却在模型Attention权重中形成了图像区域-文本词汇的映射关系。再加上GIT预训练涵盖问答等任务，模型学会在回答问题时关注相关图像部分，在描述时依照图像内容组织语言——这些都属于模态对齐的体现。可见，尽管没有显式对齐Loss，GIT通过任务驱动对齐实现了高质量的模态对齐ar5iv.labs.arxiv.org。模型的成功表明，只要任务设计合理，生成式训练本身就能让模型学会跨模态对齐。 token格式不统一： GIT通过序列到序列Transformer架构，巧妙地让图像和文本“同列于一个序列”。图像编码器输出一系列向量，这些向量在解码器里被视作一段上下文序列ar5iv.labs.arxiv.org。这样，虽然图像不是离散单词，但在Transformer看来，它们只是前若干个特殊的输入embedding。后续文本token可以自然地参考这些图像embedding，就如同参考句首提供的提示一样。这个设计避免了需要定义图像词典或修改模型输入结构，使 格式统一的问题迎刃而解 。换言之，Transformer模型对图像和文本一视同仁，只是通过mask控制依赖关系ar5iv.labs.arxiv.org。因此，GIT内部已经实现了对不同模态信息的格式融合，不存在单独处理再对齐的问题。 语义粒度不匹配： GIT直接使用CNN/ViT提取图像特征，并通过Transformer将其转换为语言。没有显式区域级别的对齐机制，但Transformer的交叉注意力可以细粒度地处理图像patch与词的关系。例如，模型在生成某个名词时，会极大地注意对应物体的那些视觉token，实现类似局部对齐的效果。这相当于让细粒度对齐在注意力机制中自发完成。此外，作者使用了一个trick：他们用对比学习预训练好的图像编码器ar5iv.labs.arxiv.org，保证图像特征本身具有较高级的语义表示能力（对比预训练会让相同类别/语义的图像特征聚类）。这意味着图像特征一开始就带有一定的语义概括性，减少了视觉低层细节与语言高级概念的不匹配ar5iv.labs.arxiv.org。因此，在语义粒度上，GIT通过预训练的视觉语义特征+解码器注意力两方面，较好地解决了粒度差异问题。模型的OCR能力说明它可以从小区域拼写出单词，说明精细粒度也能捕获；而在描述整图时又能抓大放小，生成整体语义，这体现了粒度上的灵活性。 多模态上下文保持： GIT的设计初衷不在对话，而在统一各种 静态视觉任务 。因此原版GIT不具备多轮对话记忆。然而，它提供了 统一的生成框架 ，理论上可以扩展对话：只需在输入序列中加入之前对话的文本，即可将历史作为上下文。而图像如果需要在对话中反复参考，可以在每轮答复时都把同样的图像token放入输入。但这会受到模型最大序列长度限制。微软没有在论文中报道对话实验，但在VQA任务里，GIT通过将“问题”作为前缀文本与图像共同输入ar5iv.labs.arxiv.org来回答，已经体现了处理图文混合上下文的能力。对于多张图像，GIT可以一次编码多张图的特征串联作为更长的图像token序列，只是论文未深入探索。这种架构天然支持多模态上下文的扩展，但需要注意计算成本会随序列长度增长。在视频场景中，作者已经验证了能处理多帧（通过给每帧加上时间嵌入再串联）ar5iv.labs.arxiv.org。所以GIT显示出一定的上下文扩展性，但要真正保持多轮对话语境，可能还需在生成策略上做些改动（如引入特殊标记区分说话人等）。总的说来，GIT为多模态上下文提供了一个统一容器，但对话管理不在其预训练范围内，需要额外设计。 训练数据稀缺： GIT依赖大规模多样化数据取得成功。它的理念是与其设计复杂模型，不如用简单模型配合巨量数据ar5iv.labs.arxiv.org。虽然作者未公开数据细节，但可以推测其使用近十亿级别的图文对进行训练（极可能包括微软内部的ALIGN-类数据或JFT系列）。通过大量数据，GIT在各任务上都达到新的高度arxiv.org。对于普通研究者而言，如此数据难以获得。但GIT证明，大模型+大数据可以在无需额外标注和复杂技巧的情况下解决很多问题。因此，GIT没有使用伪标签，它体现的是另一种思路： 以规模取胜 。这在一定程度上回避了数据稀缺，因为一旦数据够多，很多小数据集的问题都变得可以零样本解决ar5iv.labs.arxiv.org。此外，统一模型能跨任务共享知识：例如在描述任务学到的知识对VQA有帮助，这其实提高了每条数据的利用率。这种多任务迁移也缓和了单任务数据不足的情况。因此，虽然GIT本身消耗了巨大数据，但相对于分别训练多个任务专用模型，其综合效率反而更高。 计算开销高： 训练一个像GIT这样的模型（尤其是大尺寸版本）需要相当高的计算投入。微软通过大规模并行和分布式训练完成了这一过程。幸运的是，GIT架构简单统一，没有多分支，这使并行效率较高。模型参数虽多，但Transformer易于在GPU/TPU上加速。而且作者在论文中提供了不同模型规模的对比如Base、Large、Huge等ar5iv.labs.arxiv.org。在实际应用中，可根据算力选择较小的模型进行fine-tune。推理方面，由于没有双塔或额外模块，GIT生成一次回复需要完整地跑Transformer，对于长序列仍较耗时。但没有交叉模块切换开销。值得注意的是，GIT证明了 统一模型减少了重复计算 ：比起每个任务训练不同模型，一个预训练模型fine-tune各任务总计算量更小ar5iv.labs.arxiv.org。同时，它也展示了Transformer在CV任务中的威力，使GPU上的Transformer算力得以充分利用，不像以前CNN+RNN需要异构处理。所以总体看，GIT的 训练成本虽然高，但回报是一个通用模型 。随着算力的提升，这种“大一统预训练”将变得越来越现实。 参考： GIT论文发表于 2023 年CVPRarxiv.org（OpenReview提供了审稿意见）。论文附带的代码已在GitHub开源arxiv.org（microsoft/GenerativeImage2Text），方便社区使用。有关GIT的更深入讲解，可参考微软研究博客和OpenAI笔记等资源对比GIT与同类模型的设计理念。\nFlamingo (DeepMind, 2022) 整体架构设计： Flamingo是DeepMind提出的一种 少样本视觉语言模型 ，它将预训练的视觉编码器和预训练的大型语言模型结合，通过插入跨模态注意力层实现图文融合lilianweng.github.iolilianweng.github.io。具体来说，Flamingo采用了CLIP的ViT作为图像编码器（提取每张图像的一组视觉特征），采用类似GPT-3风格的大型Transformer作为文本生成模型（如Chinchilla 70B）lilianweng.github.io。在两者之间，Flamingo引入一个 Perceiver Resampler模块 ，将任意长度的视觉特征压缩成固定数量的 视觉tokens （如每张图像压缩成N≈64个token）lilianweng.github.io。然后，在语言模型的每层若干位置，插入“门控跨注意力层”，让文本流在生成过程中可以多次访问这些视觉tokenslilianweng.github.io。这些跨注意力层在语言模型层之间交织，使模型在生成每个词时，都能参考图像信息。值得强调的是，Flamingo在训练时 冻结了原有的语言模型和视觉编码器权重 ，只训练中间的新组分（包括Perceiver和跨模态层）lilianweng.github.io。这种设计确保了预训练模型的语言和视觉知识被最大程度保留，同时通过新组件实现模态融合。 模态对齐方式： Flamingo的模态对齐依赖于 预训练模型的知识+少量新的连接参数 。图像编码器CLIP本身已提供高质量的视觉表示，语言模型也有丰富的语言常识。Flamingo只训练连接部分，通过自回归语言模型目标来让视觉信息对接语言输出lilianweng.github.io。训练过程中，模型读取一串交织的图像和文本（例如一个网页内容，其中有文字和插入的图片），试图按照出现顺序预测下一个文本tokenlilianweng.github.io。这隐含地要求模型学会对齐：当遇到需要描述图像的地方，就必须利用视觉tokens提供的信息来正确地产生文字。因此，Flamingo没有明确的对比或匹配损失，而是在 序列建模过程中完成对齐 。尤其得益于CLIP提供的视觉特征空间和语言模型的语义空间都非常成熟，跨注意力层只需学会将二者关联即可。例如，Flamingo使用一个门控机制控制每个跨注意力头对视觉的依赖程度，这保证了模型不会过度依赖或忽略视觉信息，而是渐进式地融合lilianweng.github.io。经过训练，Flamingo实现了图文对齐，以至于在推理时，可以在看到图像后正确地继续对话生成相关文本。这种对齐能力在它的few-shot学习中表现突出：只需给出少量图文示例，模型就能对新图像输出合理描述或回答，表明模态对齐已经内化在模型中了lilianweng.github.io。 输入 token 表达统一： Flamingo通过 对文本序列进行特殊标记和掩码 ，实现了对图像和文本交替输入的统一处理lilianweng.github.io。他们在训练语料的文本中插入特殊标记 \u0026lt;image\u0026gt;代表图像占位符，当遇到该标记时，模型会取下一张图像的视觉tokens作为输入lilianweng.github.io。在Transformer内部，通过设计注意力mask，使得文本token只能看见最近一次出现的图像tokens以前的文本，而不能看见更早图像，以此处理多图场景lilianweng.github.io。同时，由于视觉tokens长度固定，每当有图像时，就把那N个视觉tokens嵌入序列。这样，整个输入序列可能形如：“文本段1 \u0026lt;image\u0026gt; 文本段2 \u0026lt;image\u0026gt; 文本段3\u0026hellip;”。对于Transformer来说，\u0026lt;image\u0026gt;标记只是一种指示，它实际会被替换为N个视觉embedding。最终，模型看到的是一个混合序列，其中既有文本token embedding也有视觉token embedding。Flamingo的跨注意力层保证文本可以从视觉embedding汲取信息lilianweng.github.io。总之，Flamingo实现了 在同一序列中交织图像和文本 ：在位置编码上，文本和图像embedding各据其位，模型通过mask确保因果关系正确lilianweng.github.io。这种方式处理输入使得模型能够自然地接受任意交替的多模态输入，而不需要显式地将图像转成离散标签或one-hot表示。 损失函数与训练策略： Flamingo以自回归下一个词预测作为唯一的训练目标lilianweng.github.io。训练数据是精心构造的 多模态序列 ：DeepMind构建了一个名为“M3W”（MassiveWeb）的大型数据集，从网络抓取包含图像和文字的网页片段共4300万条lilianweng.github.io。这些数据被处理成长度为256的token序列（其中可能包含最多5张图像）lilianweng.github.io。此外，Flamingo还混合了传统的 图文对数据 （如ALIGN的1.8亿图文对）和 视频-文本数据 （如从视频中抽帧及对应描述）进行训练lilianweng.github.iolilianweng.github.io。整个训练在不同数据源上采用 分布式多任务训练 ：每个batch随机抽取来自网页、多图文对、视频的样本分别计算NLL损失，再按设定权重求和优化lilianweng.github.io。这样的策略使模型同时适应多种输入形式。训练中需要注意各数据集的权重分配，作者采用均衡采样避免小数据集被忽略，同时也调整过不同任务损失的比重lilianweng.github.io。最后，通过大量算力（语言模型80B参数，加上新插入层）训练，Flamingo可以在不微调的情况下实现few-shot学习，即给定少数示例即可在16个下游任务中取得接近或超过有监督SOTA的成绩lilianweng.github.io。模型也支持进一步微调，但由于参数量巨大且新的门控层较敏感，微调需要小心调参。不过，一旦训练完成，Flamingo在多模态few-shot方面展示了卓越的能力。 使用的数据集及伪标签： 如上所述，Flamingo的主要预训练数据包括三个部分：网页多模态语料M3Wlilianweng.github.io、 图片-文本对数据 （如ALIGN 1.8B对、LAION等）lilianweng.github.io、 视频-文本数据 （如Instagram/Twitter短视频说明等，文中代号LTIP和VTP）lilianweng.github.io。M3W的构建无需人工标注，纯粹爬取网页，这可以视为引入了 大量弱监督数据 。那些网页上的文本并非专门描述图像，但模型会通过上下文学习其中关联。这有点类似伪标签，因为并非每句话都准确描述对应图像，但模型会自己找关系。另外，Flamingo并未使用生成模型来自行生产描述，它依赖真实世界的数据多样性。值得注意的是，Flamingo训练所需的监督非常少，几乎全是弱标注或无标注数据。few-shot能力使它在下游不需要大规模微调数据。因此，Flamingo充分体现了用海量弱标注数据替代高质量标注的理念。没有迹象表明Flamingo使用了由其他模型生成的伪标签数据；它更像是把互联网当作最大的标注来源，在文本和图像并存的自然场景中学习。 六大难点应对： 模态对齐困难： Flamingo借助冻结的CLIP提供良好的图文先验表示，并通过少量参数训练将其输出嵌入语言模型上下文。这意味着视觉和语言模态的大体对齐已经由CLIP和预训练LM保证，Flamingo只需学习 在具体上下文中关联 。通过跨注意力层，Flamingo学会在需要时提取视觉token信息用于生成下一个词，从而实现对齐。其few-shot性能表明，训练后模型能够快速对齐新任务的图文语义，这得益于大量多样化训练让对齐泛化良好lilianweng.github.io。换言之，Flamingo用数据多样性+强大基础模型平稳地度过了模态对齐难关。 token格式不统一： Flamingo直接在Transformer中处理交替的多模态序列，将图像表示为固定长度token插入序列，这相当于统一了输入格式lilianweng.github.io。虽然图像token不是离散符号，但它们像文本token一样有自己的位置，与前后文本共同组成序列输入Transformer。同时，引入 \u0026lt;image\u0026gt;标记作为占位符，使文本流认识到何处有图像lilianweng.github.io。这种方案无需对图像进行离散化编码，而是用连续向量表示并通过mask和标记融入序列，实现了格式统一。实验证明，这样模型可以灵活处理任意交替顺序的图文输入，这正是统一输入格式带来的好处。 语义粒度不匹配： Flamingo利用CLIP的高层视觉特征（ViT-L/14等）作为输入，这些特征本身具有较丰富的语义信息（CLIP已对齐过标签文本）。再通过Perceiver压缩，Flamingo获得一组紧凑的视觉tokens，每个可能聚合了图像若干部分信息lilianweng.github.io。这会损失一些低层细节，但保留主要语义，匹配语言模型处理的概念粒度。对于非常细的细节，如图像中的文字或小目标，Flamingo如果训练数据涵盖这类任务也能捕捉（但Flamingo主要没专门练OCR类任务，表现可能一般）。总体而言，Flamingo的设计旨在 抓主要语义 ：用几百个视觉token代表整张图lilianweng.github.io。语言模型生成注重全局语义和上下文，微观细节在few-shot场景下可能需要提示引导才能关注。不过，通过web数据训练，Flamingo也学习了不少细节（如定位照片里的物体等）。因此，它在语义粒度上采取以语义为主，细节为辅的策略，符合few-shot应用的需求。 多模态上下文保持： 这是Flamingo最大的强项之一。模型专门设计来处理 任意长度的交互式多模态上下文 。通过mask策略，Flamingo可以应对多张图和多段文本交替：保证每段文本只能看最近的图像，从而按顺序关联图像和文字lilianweng.github.io。这使模型在一个序列中可以包含多轮图文对话——实际上Flamingo天生就是支持图文混合对话的。训练中它看过网页内容的多次图文交替，因此对多模态上下文延续性有经验lilianweng.github.io。few-shot推理时，可以先给几个示例（图+问+答），模型就能在持续的多模态对话下发挥作用lilianweng.github.io。这种能力是一般模型不具备的。因此Flamingo很适合多轮对话、讲故事等需要保持上下文的场景。需要注意长序列涉及的内存和计算成本，但Flamingo通过稀疏注意力等优化应对。总的来说，Flamingo在多模态上下文保持方面达到了当时的新高度，真正实现了在Transformer中融合长上下文的多模态信息。 训练数据稀缺： Flamingo通过大规模弱标注数据和 多数据源混合 ，在没有显式人工标注的情况下取得了卓越性能lilianweng.github.io。它所需的只是网络上已有的大量图文并茂内容，而不需要额外的人工作答或描述数据（除了验证集）。这证明了利用海量的非结构化数据也能训练出强大的多模态模型。few-shot学习的优势在于，模型可以适应新任务而不需要对每个任务都有成千上万标注数据。Flamingo在16个任务上的结果显示，即使这些任务的数据对模型来说是新的，它依然靠few-shot提示达到不错效果lilianweng.github.io。这极大缓解了对监督数据的需求。因此，Flamingo的方案是 用预训练+提示学习替代下游数据 。当然，预训练本身用了43M网页和十亿级对，耗资巨大，但都是低成本获取的数据。可以说，它把收集标注的钱换成了算力钱。一旦模型训练完毕，同样权重可以few-shot解决多个任务，再也不需要逐个任务大量标注了。 计算开销高： Flamingo包含一个80B规模的语言模型（如Chinchilla 70B）和一系列新插入的层，总参数量非常高，训练消耗巨大的TPU/GPU资源。这显然是非常高的计算开销。然而，Flamingo通过 冻结大模型 ，大幅减少了需要更新的参数量lilianweng.github.io。仅训练新加的几千万参数，使得训练收敛更快、更稳定，同时避免灾难性遗忘。此外，相比从零训练80B多模态模型，这种“夹心”微调的成本要低得多。推理阶段，Flamingo的计算与一个同等大小的LM相当，外加一些跨注意力计算，可以在多卡并行生成。在few-shot时，不需要反复fine-tune，从而节省了针对每个任务微调的算力。因此，对于拥有训练超大模型能力的团队来说，Flamingo的 性价比反而不错 ：用额外\u0026lt;1B参数的代价，把一个纯语言模型变成了多模态模型。总之，Flamingo依然属于算力投入极高的模型，但在架构上做了取舍，通过参数冻结和高并行设计，把这笔开销控制在可能范围，并用其泛化能力回收了在多个任务上的成本。 参考： Flamingo的论文（Alayrac et al. 2022）可在arXiv获取lilianweng.github.iolilianweng.github.io。其中详述了模型架构和训练数据构成。DeepMind未公开Flamingo的代码，但有社区复现项目（如lucidrains的PyTorch实现）。Lilian Weng的博客对Flamingo进行了通俗讲解lilianweng.github.io。Flamingo在Few-shot VQA等任务上的表现促使后续多模态聊天模型（如OpenAI的GPT-4V）采用类似思想。\nGPT-4V (OpenAI, 2023) 整体架构设计： GPT-4V是GPT-4模型的视觉增强版本，能够接受图像和文本输入，输出文本en.wikipedia.org。虽然OpenAI并未公开GPT-4V的具体架构和参数en.wikipedia.org，“V”版的实现大致可推测为在GPT-4大型Transformer架构中融合了视觉处理模块。很可能GPT-4V采用了单一Transformer模型来同时处理图像和文本：图像通过一个卷积或ViT编码器提取特征，然后以某种形式馈入Transformer。例如，有推测称GPT-4V使用类似Flamingo的方法——一个 预训练的ViT作为图像编码器 ，将其输出作为额外的输入embedding，通过新添的跨注意力机制注入到原GPT-4的Transformer中lilianweng.github.io。也有可能GPT-4V将图像编码为若干“视觉token”直接拼接到文本token序列中处理（类似BLIP-2/GIT那样）。不管实现细节如何，GPT-4V的架构原则应是 在不大幅改变GPT-4语言能力的前提下，赋予其视觉输入通路 。因此，它很可能保留了GPT-4的大部分层和参数，仅在输入嵌入层或中间插入层增加视觉接口，使模型能够在Self-Attention中同时考虑图像和文本信息。作为一个多模态LLM，GPT-4V仍以Transformer为核心en.wikipedia.org。 模态对齐方式： GPT-4V在开发过程中应该经历了 大量多模态预训练和对齐调优 。预训练阶段，模型接受图文混合数据，学习以生成下一个token为目标（无论下一个是文字还是需要根据图像产生的文字）。这种训练会驱动模型自动建立图像与文本语义的映射关系。由于GPT-4本身非常强大，GPT-4V可能仅需较少的额外数据就能学会模态对齐。然而OpenAI可能使用了多种辅助手段：包括 对比损失 （确保图像相关的文本embedding靠近）或者多模态一致性约束等，但具体未知。可以肯定的是，GPT-4V经过了 强化学习人类反馈（RLHF）的对齐环节en.wikipedia.org：人工反馈不仅针对文本回答质量，也包括对视觉理解正确性的评价。这种人工调教确保模型在视觉问答中对齐人类期望。例如，人类监督会奖赏模型正确描述图像、严惩胡编乱造，从而促使模型更好地学习视觉-语言对齐关系。总的说来，GPT-4V的模态对齐来自两部分 ：一是模型大规模多模态训练的自我监督对齐（让模型预测正确的多模态输出而被迫对齐），二是 对抗性和人类反馈微调 （纠正不准确的对齐，如图像内容误解）以达成人类满意的对齐度。最终结果是GPT-4V在各种视觉描述、问答任务上表现出强大的理解力和对齐度，甚至可以准确解释复杂图片、阅读图中文字并将之融入答案——这说明其视觉语义已与语言很好地结合。 输入 token 表达统一： 从用户接口看，GPT-4V接受的输入是图像（像素形式）和文本，自然语言以token形式进入，图像则以文件上传形式进入API。但在模型内部，必须将图像转化为与文本token可交互的表示。根据业界经验，GPT-4V可能采用两种方式之一：其一， 离线视觉编码+前缀嵌入 。即通过一个CNN/ViT将图像转成一串embedding，然后在Transformer输入端用特殊标记占位，将这些embedding作为“视觉前缀”插入。这类似BLIP-2和GIT的策略，让视觉embedding在Transformer序列中，与后续文本共同处理。其二， 中途插入跨模态层 ，即模型运行过程中，当需要处理图像时，调用一个微型视觉Transformer将图像转成键值供专门的跨注意力层使用（类似Flamingo做法）。无论哪种，最终效果是 模型看到了一系列向量表示，部分来自图像，部分来自文本 ，并通过统一的注意力机制处理它们。因此，GPT-4V实现了 视觉信息向等价文本向量的转换 ：这些向量可能没有离散token对应，但Transformer无差别对待它们，把它们当作上下文的一部分。OpenAI也定义了GPT-4V的token计费方式：图像按一定像素大小折算成若干token成本platform.openai.com, 这暗示他们内部将图像信息映射为了固定数量的embedding，相当于一些token。这与输入统一表示的思路一致。此外，GPT-4V支持在对话中多次输入图像，模型通过聊天记忆可以连续参照多幅图像。这种灵活性也表明输入的图像已经嵌入Transformer上下文，模型可以在内部“记住”它，就像记住前文一样。因此可以说，GPT-4V在实现上做到了图像和文本输入的格式统一，至少从Transformer的视角来看是一致的序列信息流。 损失函数与训练策略： GPT-4V的训练包括两个阶段： 预训练（Self-Supervised）和对齐微调（Supervised + RLHF） en.wikipedia.org。预训练损失是标准的 因果语言建模损失 ，扩展到多模态场景，即给定之前的文本token和图像embedding，预测下一个文本tokenen.wikipedia.org。这一步可能使用了大量图文对数据和合成任务数据，让模型具备基础视觉理解和描述能力。接下来，OpenAI对GPT-4V进行了 监督微调 ，包括让模型跟随指令、可靠回答问题、避免不当输出等。这一步使用有人类标注答案的图像问答数据和对话数据，损失为交叉熵对标参考答案。最后还有 RLHF阶段 ，通过人类反馈训练一个奖励模型，对模型回答质量评分，再用策略梯度或近端策略优化调整模型参数，使之产生更符合人类期望的回答en.wikipedia.org。在RLHF中，人类会比较两版对同一图像问题的回答优劣，以训练奖励模型。这确保GPT-4V不仅正确，还要解释清楚、详尽并遵守安全守则。训练策略方面，GPT-4V很可能采用了 混合训练 ：例如让模型在大约80%时间学习纯文本任务（以不损害其语言能力），20%时间学习带图像的任务，以逐渐融合视觉能力而不遗忘语言能力。这符合OpenAI对GPT-4统一多模态模型的描述，称其在巨量算力下进行预测性能的平稳扩展arxiv.org。因此，GPT-4V训练过程相当复杂，但核心损失仍是让模型预测正确的输出序列（文本），只是过程中施加了各种人类知识和偏好约束。 使用的数据集及伪标签： OpenAI未公开GPT-4V使用的数据细节en.wikipedia.org。推测其预训练数据包含 互联网爬取的大规模图文对 （如可能使用LAION、ALIGN数据，或者自建的10亿级别数据集），涵盖多样领域。还可能有 OCR场景数据 （扫描文档及文本）、 图表数据 、网页截图和说明等，因为GPT-4V表现出识别文档、读表格、看图编程等广泛能力。监督微调阶段，他们可能编纂了一个多模态指令数据集，类似InstructGPT，但带图像：比如让标注员提供图像并提问，写出高质量参考答案。这部分数据可能较小（数万到数十万对），但涵盖不同任务（描述、定位、分类、推理等）。此外，社区猜测OpenAI可能利用GPT-4自身生成了一部分训练数据（即“判师”策略），但官方未证实。相较于开源做法（如LLaVA用GPT-4生成对COCO的问答作为训练集），OpenAI有资源直接人工标注，所以GPT-4V的关键数据更可能是人类精标而非伪标签。唯一确定的是，GPT-4V 融合了多源数据 ：文本数据（与GPT-4共享）、图像+文本数据，以及人类反馈数据en.wikipedia.org。这种多阶段、多样本训练使模型具有极其广泛的视觉语言知识。基于效果推断，GPT-4V肯定见过各种真实世界图像场景，包括照片、插画、截图、漫画等，也了解了不少常见视觉任务的问答格式。这正是其在未知图片上一样游刃有余的原因。 六大难点应对： 模态对齐困难： GPT-4V可被视为目前模态对齐最成功的例子之一。OpenAI通过 统一模型训练+精细对齐调优 ，使得GPT-4V在视觉和语言之间建立了深度联系。模型能将图像中的元素转换成文字描述或用于推理，说明跨模态概念高度统一。例如，它可以看图进行幽默理解、数学分析，这意味着不仅低层语义对齐，高层推理也对齐了。相比CLIP等需下游配对的模型，GPT-4V内部产生了 端到端的对齐 ：一幅图像输入，其内部生成的表征能直接触发与之对应的知识和词汇。RLHF过程中，人类引导模型关注正确区域、忠实描述，进一步强化了 精准对齐 （比如不编造不存在的物体）。因此，对以前悬而未决的模态对齐难题，GPT-4V以超级规模训练+人类校正的方法给出了答案：几乎可以对任意复杂图文实现正确对齐。 token格式不统一： GPT-4V在接口上依然区分图像上传和文本输入，但在模型内部已经实现了格式统一。如上推测，图像被编码成embedding插入Transformer，相当于模型看到的是统一的向量序列，其中没有本质区别区分来源。OpenAI甚至提供了一个token折算方法来计价图像，这暗示他们定义了一个统一本质的token空间包含图像platform.openai.com。GPT-4V也许没有明确的视觉词表，但通过扩展embedding层，模型接受了一批额外的向量（视觉patch的embedding或者Resampler输出）作为“视觉token”。Transformer处理自注意力时，对这些向量和普通文字embedding执行相同的矩阵计算。因此可以说，GPT-4V在实现上 消除了模态输入格式差异 ，达到了真正的多模态Transformer形态。这也是为什么用户能对它自由提问“图中有什么字”或“这个人是谁”，模型像读文字一样“读”图。这种统一在OpenAI的技术报告中虽未明说，但从其行为特征和架构趋势能推断出来。 语义粒度不匹配： GPT-4V展现出处理各种粒度语义的能力，从辨认具体细节（如图中小字、微小物品）到理解抽象场景（如人物关系、场景氛围）。这表明模型采用了高分辨率的视觉表征和 强大的分层理解 。一种可能方式是多级特征：基础ViT提供细粒度patch特征，然后Transformer多层逐步汇总，像人类视觉系统一样先看细节再理解整体。此外，OpenAI可能特意在训练集中加入了一些需要细粒度识别的任务（OCR、细分类），迫使模型关注局部细节。同时，大语言模型部分拥有强大的上下文推理能力，能从细节推导整体意义。这两方面结合，使GPT-4V能较好地弥合视觉像素级信息与语言概念级信息之间的鸿沟。例如，对一张复杂的漫画，模型既能识别面部表情这样的细节，又能归纳出搞笑之处这样的高层语义。可以认为，GPT-4V通过多尺度注意力解决了语义粒度不匹配：低层注意力抓取细节，高层Self-Attention整合语义，并在输出时选择恰当的语言粒度表述。 多模态上下文保持： GPT-4V本质是ChatGPT的扩展版，因而天然具备对话上下文记忆能力。用户可以在一次对话中连续上传多张相关图像并配以提问，模型能够参考对话历史和所有已提供的图像信息来回答。比如，用户先上传一家谱照片问“这是谁？”，再上传另一张照片问“他和前面那人是什么关系？”，GPT-4V可以基于前文记忆，将两图人物联系起来回答。这说明模型内部对多轮图像和文本都建立了表示，并通过对话状态维持了跨轮次的多模态上下文。OpenAI很可能在微调阶段加入了这类多轮、多图对话的数据，使模型学会使用 \u0026lt;image_n\u0026gt;引用之前的图像。在推理实现上，ChatGPT系统会给每张图一个编号，将其embedding保存在对话状态，后续提问如果引用，模型就会重新利用。这种机制虽未明示，但从体验上看GPT-4V确实支持相当长的多模态对话。因此，它在多模态上下文保持上达到了目前最强水平：既能处理长文本对话，又能记忆多张图像的内容并综合推理。这一能力是之前模型（如Flamingo）few-shot模拟的更高级形式，因为GPT-4V经过明确的对话格式训练和强化，对话管理更加可靠。 训练数据稀缺： 对于普通研究者来说，高质量大规模多模态数据稀缺是难点，但OpenAI通过自身积累和合作，可能获取了十分丰富的数据。GPT-4V可以被视为以数据和算力硬碰硬解决问题的典型。它用规模（模型、数据）换性能，不太依赖小技巧。值得注意的是，虽然OpenAI未公布数据，但推测很大一部分来自现有开放数据（LAION、COCO、Visual Genome等）以及定制采集的数据（比如购买版权图片、内部生成的数据等）。此外，人类标注在对齐阶段起了决定性作用，这是另一种形式的数据： 专家知识数据 。OpenAI投入了大量人力去微调模型的行为，使得最终模型的能力远超仅靠原始数据训练的版本。这相当于通过人类反馈来弥补数据集不足之处——对于一些模型自己难以领会的任务，人类示范和偏好指导提供了额外信息。这种做法开创了用少量高质量人工数据引导海量机器学习的范式。简而言之，GPT-4V应对数据稀缺的方案在于： 一手抓“大”（扩展预训练数据广度），一手抓“精”（收集人类高质量指令/反馈数据） 。两者结合，使模型既见多识广，又合乎人意。 计算开销高： GPT-4V毫无疑问是在极其庞大的算力支持下训练的。传闻GPT-4基础模型参数在数千亿以上，训练消耗数千万美元级别GPU成本。加入视觉模态后，训练复杂度进一步提高。不过OpenAI通过一些工程手段控制了成本：据报道，他们使用了训练性能预测方法，在较小模型上估计大模型表现，从而少走弯路arxiv.org。另外采用混合精度、模型并行、流水线并行等技术提高效率。模型结构上，使用统一Transformer而非多分支，可以充分利用成熟的Transformer优化器和加速器。这些都帮助缓解了计算压力。在推理阶段，GPT-4V同样需要强大算力支持，但OpenAI通过托管API方式，用优化过的推理服务器提供服务，单次调用成本对于终端用户来说隐藏在付费中。可以说，GPT-4V目前的计算开销不是一般机构能承担的，但它也展示了高投入带来高性能的路线。随着硬件进步和可能的压缩蒸馏技术，未来GPT-4V的成本有望下降。就当前而言，OpenAI通过自身资源攻克了这一难题，对外提供一个无需本地计算就能调用的强大多模态模型，这在客观上绕过了许多用户对算力的需求。 参考： GPT-4 的技术报告en.wikipedia.orgen.wikipedia.org提到其多模态能力和训练方法，但未披露细节。维基百科也指出OpenAI未公布GPT-4的架构和数据en.wikipedia.org。尽管如此，我们可以参考类似的研究（如Google PaLM-E、DeepMind Flamingolilianweng.github.io）来推测GPT-4V的设计思路。OpenAI的GPT-4发布博客openai.com和官方FAQ也提供了一些线索（如图像计费折算）。目前没有公开的GPT-4V代码或模型，但已有一些开源项目（如MiniGPT-4、LLaVA等）尝试复现其部分功能，可供了解实现原理。总的来说，GPT-4V代表了当前多模态模型技术的前沿，将视觉和语言能力融合达到了前所未有的高度。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-01/","summary":"\u003ch1 id=\"主流视觉-文本多模态模型技术分析\"\u003e主流视觉-文本多模态模型技术分析\u003c/h1\u003e\n\u003cp\u003e近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。\u003c/p\u003e\n\u003ch2 id=\"clip-openai-2021\"\u003eCLIP (OpenAI, 2021)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e整体架构设计：\u003c/strong\u003e CLIP 采用 \u003cstrong\u003e双编码器架构\u003c/strong\u003e ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到\u003cstrong\u003e相同维度\u003c/strong\u003e的向量空间\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到\u003cstrong\u003e共享的多模态嵌入空间\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模态对齐方式：\u003c/strong\u003e CLIP通过\u003cstrong\u003e对比学习\u003c/strong\u003e实现视觉-语言对齐。训练时，模型给定一批图文对，学习\u003cstrong\u003e预测哪张图像与哪段文本匹配\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。具体而言，CLIP使用 \u003cstrong\u003e对称的跨模态对比损失\u003c/strong\u003e （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。这种训练使图像和文本编码器产生的特征在共享空间中\u003cstrong\u003e成对靠近\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e输入 token 表达统一：\u003c/strong\u003e CLIP并未显式统一图像和文本的输入表示格式。 \u003cstrong\u003e图像和文本各有独立的token化和编码流程\u003c/strong\u003e ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过\u003cstrong\u003e独立编码+对齐空间\u003c/strong\u003e的方式，规避了直接将图像作为序列token处理的不统一问题。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e损失函数与训练策略：\u003c/strong\u003e 损失采用 \u003cstrong\u003e对比学习的InfoNCE损失\u003c/strong\u003e （实现为带温度系数的归一化softmax交叉熵）\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了\u003cstrong\u003e大批量\u003c/strong\u003e训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,2023%2C%20in%20their\"\u003elightly.ai\u003c/a\u003e。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=insensitive%20to%20the%20capacity%20of,the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。CLIP从\u003cstrong\u003e随机初始化\u003c/strong\u003e开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据集及伪标签：\u003c/strong\u003e CLIP在一个超大规模的图文配对数据集上预训练，包含约\u003cstrong\u003e4亿对图像-文本\u003c/strong\u003e\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖\u003cstrong\u003e自然语言的弱监督\u003c/strong\u003e\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e六大难点应对：\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003e模态对齐困难：\u003c/em\u003e  CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003etoken格式不统一：\u003c/em\u003e  采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e语义粒度不匹配：\u003c/em\u003e CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如\u003cstrong\u003eFine-Grained CLIP\u003c/strong\u003e等正是受限于CLIP在局部语义对齐上的不足\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=SigLIP%3A%20Optimising%20the%20loss%20function,for%20better%20scaling\"\u003elightly.ai\u003c/a\u003e。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e多模态上下文保持：\u003c/em\u003e CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 \u003cstrong\u003e多模态上下文\u003c/strong\u003e （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e训练数据稀缺：\u003c/em\u003e CLIP通过\u003cstrong\u003e大规模弱标注数据\u003c/strong\u003e从根本上缓解了数据稀缺的问题\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e计算开销高：\u003c/em\u003e 训练CLIP确实需要巨大的算力和显存，但相对来说，其\u003cstrong\u003e双塔架构\u003c/strong\u003e使训练可并行展开，推理时也可分别预编码图文后做相似度计算，\u003cstrong\u003e具有一定的效率优势\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,Training%E2%80%9D%2C%20propose%20to\"\u003elightly.ai\u003c/a\u003e。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=implementation%20is%20numerically%20unstable%2C%20and,additional%20bias%20terms%2C%20and%20calculations\"\u003elightly.ai\u003c/a\u003e。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e参考：\u003c/strong\u003e CLIP 的论文\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e详细描述了其对比预训练方法，OpenAI 的博客也提供了概述\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库\u003ca href=\"https://huggingface.co/docs/transformers/en/model_doc/clip#:~:text=CLIP%20uses%20an%20image%20encoder,the%20same%20number%20of\"\u003ehuggingface.co\u003c/a\u003e。\u003c/p\u003e\n\u003ch2 id=\"align-google-2021\"\u003eALIGN (Google, 2021)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e整体架构设计：\u003c/strong\u003e ALIGN（ \u003cstrong\u003eA Large-scale ImaGe and Noisy-Text embedding\u003c/strong\u003e ）延续了与CLIP相同的\u003cstrong\u003e双编码器对比学习架构\u003c/strong\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：\u003cstrong\u003eEfficientNet-L2卷积网络\u003c/strong\u003e作为图像编码器，\u003cstrong\u003eBERT-Large\u003c/strong\u003e作为文本编码器，并均从随机初始化开始训练\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模态对齐方式：\u003c/strong\u003e ALIGN采用\u003cstrong\u003e对比损失（normalized softmax）\u003cstrong\u003e来训练，使匹配的图文对嵌入向量接近，不匹配的远离\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以\u003c/strong\u003e批为单位的跨模态对比\u003c/strong\u003e训练，使模型学到强大的图文对齐表示。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e输入 token 表达统一：\u003c/strong\u003e ALIGN同样没有将图像直接离散为token序列，而是通过\u003cstrong\u003e双通道\u003c/strong\u003e处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出\u003cstrong\u003eembedding空间\u003c/strong\u003e实现统一表示\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e损失函数与训练策略：\u003c/strong\u003e 使用 \u003cstrong\u003e对比学习损失\u003c/strong\u003e （InfoNCE变体），在\u003cstrong\u003e大批量\u003c/strong\u003e上训练模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 \u003cstrong\u003e最小程度的过滤\u003c/strong\u003e ，通过\u003cstrong\u003e数据规模\u003c/strong\u003e来弥补噪声\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据集及伪标签：\u003c/strong\u003e ALIGN的亮点在于使用了\u003cstrong\u003e超过10亿对图像-Alt文本\u003c/strong\u003e的超大规模数据集\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 \u003cstrong\u003e无需人工标注\u003c/strong\u003e 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN \u003cstrong\u003e放宽过滤标准\u003c/strong\u003e ，只做了基于频率的简单过滤，最终得到约\u003cstrong\u003e18亿对\u003c/strong\u003e图文数据\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20this%20work%2C%20we%20follow,text%20pairs\"\u003eresearch.google\u003c/a\u003e。这些文本描述可能包含噪声甚至与图像无关，但研究表明 \u003cstrong\u003e规模弥补质量\u003c/strong\u003e ：如此海量的数据使模型学到泛化的视觉语言表示\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e六大难点应对：\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003e模态对齐困难：\u003c/em\u003e ALIGN证明了\u003cstrong\u003e数据规模\u003c/strong\u003e在对齐中的重要作用。通过\u003cstrong\u003e十倍于CLIP的数据规模\u003c/strong\u003e和强大的对比损失，模型学到了稳健的跨模态对齐能力\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,L2\"\u003eresearch.google\u003c/a\u003e。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003etoken格式不统一：\u003c/em\u003e 与CLIP类似，ALIGN通过\u003cstrong\u003e双编码器架构\u003c/strong\u003e回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e语义粒度不匹配：\u003c/em\u003e ALIGN的训练目标依旧作用在 \u003cstrong\u003e全局图像-句子层面\u003c/strong\u003e ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对\u003cstrong\u003e细粒度语义\u003c/strong\u003e的不匹配没有特殊解决方案。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e多模态上下文保持：\u003c/em\u003e ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e，未涉及多模态对话等情境。因此，ALIGN在\u003cstrong\u003e多轮交互\u003c/strong\u003e或\u003cstrong\u003e长上下文\u003c/strong\u003e问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e训练数据稀缺：\u003c/em\u003e ALIGN的策略是\u003cstrong\u003e极端扩增数据规模\u003c/strong\u003e以消除数据稀缺瓶颈\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 \u003cstrong\u003e弱标注的大数据\u003c/strong\u003e 。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e计算开销高：\u003c/em\u003e 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。尽管计算开销高昂，ALIGN通过 \u003cstrong\u003e冻结架构复杂性\u003c/strong\u003e （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，\u003cstrong\u003e优先扩大数据规模\u003c/strong\u003e比增加模型复杂度更有效\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e参考：\u003c/strong\u003e ALIGN 的研究细节发表于 ICML 2021\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20,We\"\u003eresearch.google\u003c/a\u003e。Google Research 官方博客提供了对ALIGN的通俗描述\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。\u003c/p\u003e","title":"Bug Journal 2025-06-01"},{"content":"主要动机 目前有三大挑战：数据稀缺、泛化能力不足，以及在复杂物理环境中效果不好 作者希望通过引入大规模语言及VLA模型在自然语言处理和计算机视觉中的成功经验解决这个问题.\n主要论点 在预训练的视-语言模型（VLM，本文采用 PaliGemma）基础上，增加一个“动作专家”（action expert），通过条件流匹配（diffusion policy）生成高频、连续的动作序列（每秒可达50 Hz）\n模型流程图 收集数据 -\u0026gt; 训练$\\phi_0$ -\u0026gt; Zero-Shot/微调/Fine-tune\n数据来自7种不同的机器人，68个不同的任务，总计 10k小时。\n每一个单独机械臂的自由度在 6-7 (有些机器人有多机械臂)\n输入有 3 块，分别是：Image, Language, and State\nImage 用 400M Pretrained VIT SigLip 得到 embedding, 一共会有up to 3 个 Image， 所以最会有 3 个 embedding (没有就填 0)\nLanguage 用 2.6B Pretrained LLM Gemma, 得到 embedding\n最后是关节信息，最多会有 18 个(没有就填 0)。\n之后运用 Diffusion Policy 来生成每一步的动作\n生成的是 $p(A_t|O_t)$, 即，在给定条件(环境下)，每一个动作的概率。\n这时给一个 chunk 内的真实动作加噪音，然后让模型学习如何去噪。\n方式如下：\n随机采一个噪声级别 $\\tau$，采一个高斯噪声 $\\varepsilon$； 构造带噪动作块 $A_t^\\tau = \\tau A_t + (1 - \\tau)\\varepsilon$； 用观测 $o_t$ 与 $A_t^\\tau$ 输入网络，预测去噪速度场 $v_\\theta(A_t^\\tau, o_t) \u0026lt;-\u0026gt; \\varepsilon - A_t$； 以 $\\left| v_\\theta - (\\varepsilon - A_t) \\right|^2$ 作为监督信号； 推理的时候就会把这个过程分成 10-20步，每一步去噪一点点，最终得到一个动作 注：\u0026ldquo;这个过程\u0026quot;指的是随机生成一个噪声，然后从这个噪声去噪的过程\nCode Inference def create_trained_policy( train_config: _config.TrainConfig, # 训练配置，包含模型定义、数据配置等 checkpoint_dir: pathlib.Path | str, # 检查点目录：存放已训练模型参数和归一化统计信息的路径 *, repack_transforms: transforms.Group | None = None, # 可选的“重打包”预处理组——在所有其他 transform 之前应用 sample_kwargs: dict[str, Any] | None = None, # 传递给 policy.sample_actions 的参数字典 default_prompt: str | None = None, # 默认提示词，如果输入数据中没有 prompt，则注入该默认值 norm_stats: dict[str, transforms.NormStats] | None = None, # 归一化统计信息（均值、方差或分位数），若未提供则从 checkpoint 中加载 ) -\u0026gt; _policy.Policy: \u0026quot;\u0026rdquo;\u0026quot; 从训练好的检查点创建并返回一个可交互的 Policy 对象。\nArgs: train_config: 用于创建模型和数据流水线的训练配置。 checkpoint_dir: 存储模型参数和归一化信息的目录路径。 repack_transforms: （可选）在所有其他数据变换之前应用的变换组。 sample_kwargs: （可选）调用 sample_actions 方法时使用的关键字参数。 default_prompt: （可选）注入到输入数据中的默认提示词。 norm_stats: （可选）归一化统计信息；如果未提供，会尝试从 checkpoint 加载。 \u0026quot;\u0026quot;\u0026quot; # 如果外部没有传入 repack_transforms，则使用一个空的 transforms.Group repack_transforms = repack_transforms or transforms.Group() # 下载 checkpoint_dir = download.maybe_download(str(checkpoint_dir)) # 怀疑是这个地方卡住了，正在测试 logging.info(\u0026quot;Loading model...\u0026quot;) # 从 checkpoint 的 params 文件中恢复模型参数，并用 jnp.bfloat16 精度加载到模型中 model = train_config.model.load( _model.restore_params(checkpoint_dir / \u0026quot;params\u0026quot;, dtype=jnp.bfloat16) ) # 使用训练配置中的 data 部分构建数据流水线（包括 asset 路径、transform 定义等） data_config = train_config.data.create(train_config.assets_dirs, train_config.model) # 如果调用方未提供归一化统计信息，则从 checkpoint 中加载 if norm_stats is None: # 确保 data_config 中配置了 asset_id，否则无法定位归一化文件 if data_config.asset_id is None: raise ValueError(\u0026quot;Asset id is required to load norm stats.\u0026quot;) # 从 checkpoint_dir/assets/\u0026lt;asset_id\u0026gt; 文件夹加载归一化统计信息 norm_stats = _checkpoints.load_norm_stats(checkpoint_dir / \u0026quot;assets\u0026quot;, data_config.asset_id) # 构造并返回 Policy 对象 return _policy.Policy( model, # 定义输入端的 transform 流水线 transforms=[ *repack_transforms.inputs, # 首先应用重打包变换 transforms.InjectDefaultPrompt(default_prompt), # 注入默认 prompt（如有） *data_config.data_transforms.inputs, # 然后是数据阶段的预处理（如裁剪、编码） transforms.Normalize(norm_stats, # 使用加载的统计信息做归一化 use_quantiles=data_config.use_quantile_norm), *data_config.model_transforms.inputs, # 最后是模型期望的输入 transform（如维度调整、拼接） ], # 定义输出端的 transform 流水线，用于将模型输出反向映射回原始数据格式 output_transforms=[ *data_config.model_transforms.outputs, # 模型输出后先做反向 transform（如反维度调整） transforms.Unnormalize(norm_stats, # 反归一化 use_quantiles=data_config.use_quantile_norm), *data_config.data_transforms.outputs, # 数据阶段的后处理（如解码、去补齐） *repack_transforms.outputs, # 最后应用重打包的输出变换 ], sample_kwargs=sample_kwargs, # 传给 sample_actions 的运行时参数 metadata=train_config.policy_metadata, # 附带的元数据 ) 创新点 VLM+流匹配的融合：首次将预训练视觉-语言骨干与流匹配（flow matching）动作生成相结合，实现高频连续动作预测。 跨平台预训练：采用跨样本（cross-embodiment）训练，将来自单臂、双臂及移动操纵器的多样化数据统一到同一模型中。 两阶段训练配方：借鉴大规模语言模型的“预训练–后训练”流程，预训练学会恢复与泛化行为，后训练习得高效、精炼策略。 动作专家模块：在 Transformer 上增设专门处理机器人状态与动作的子网络，相当于一种混合专家（mixture-of-experts）设计，提高对连续动作的建模能力 解决的难点 数据稀缺：以往专用策略仅依赖于任务特定的少量数据，难以涵盖错误恢复或未见场景；π0 通过多任务多平台数据缓解了此问题。 泛化与鲁棒性：先前的自回归离散动作方法（如 OpenVLA）不支持高频动作分块，难以处理精细操控；π0 的流匹配架构可生成连续、高精度动作，提升了对复杂任务的适应能力。 多阶段任务：传统方法往往针对单一任务设计，难以扩展到折叠衣物、装箱等涉及多步骤和语义推理的场景；π0 可直接通过语言提示或与高层策略结合，完成复杂多阶段流程 还需要解决的难点 预训练数据组成与加权：如何选择和加权最有助于下游任务的数据仍然未知。 任务可靠性：部分下游任务（尤其与预训练差异大者）仍存在不稳定性，需要更多高质量后训练数据。 跨领域通用性：尚不清楚该框架能否推广到更异质的机器人领域（如自主驾驶、步态运动等）。 资源需求：大规模预训练对算力和示教数据的需求极高，实际部署成本仍是瓶颈 Take away tmux CUDA_VISIBLE_DEVICES=num pdb\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-31/","summary":"\u003ch4 id=\"主要动机\"\u003e主要动机\u003c/h4\u003e\n\u003cp\u003e目前有三大挑战：数据稀缺、泛化能力不足，以及在复杂物理环境中效果不好\n作者希望通过引入大规模语言及VLA模型在自然语言处理和计算机视觉中的成功经验解决这个问题.\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e在预训练的视-语言模型（VLM，本文采用 PaliGemma）基础上，增加一个“动作专家”（action expert），通过条件流匹配（diffusion policy）生成高频、连续的动作序列（每秒可达50 Hz）\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1748596153733\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-31/1748596153733.png\"\u003e\u003c/p\u003e\n\u003cp\u003e收集数据 -\u0026gt; 训练$\\phi_0$ -\u0026gt; Zero-Shot/微调/Fine-tune\u003c/p\u003e\n\u003cp\u003e数据来自7种不同的机器人，68个不同的任务，总计 10k小时。\u003c/p\u003e\n\u003cp\u003e每一个单独机械臂的自由度在 6-7 (有些机器人有多机械臂)\u003c/p\u003e\n\u003cp\u003e输入有 3 块，分别是：Image, Language, and State\u003c/p\u003e\n\u003cp\u003eImage 用 400M Pretrained VIT SigLip 得到 embedding, 一共会有up to 3 个 Image， 所以最会有 3 个 embedding (没有就填 0)\u003c/p\u003e\n\u003cp\u003eLanguage 用 2.6B Pretrained LLM Gemma, 得到 embedding\u003c/p\u003e\n\u003cp\u003e最后是关节信息，最多会有 18 个(没有就填 0)。\u003c/p\u003e\n\u003cp\u003e之后运用 Diffusion Policy 来生成每一步的动作\u003c/p\u003e\n\u003cp\u003e生成的是 $p(A_t|O_t)$, 即，在给定条件(环境下)，每一个动作的概率。\u003c/p\u003e\n\u003cp\u003e这时给一个 chunk 内的真实动作加噪音，然后让模型学习如何去噪。\u003c/p\u003e\n\u003cp\u003e方式如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e随机采一个噪声级别 $\\tau$，采一个高斯噪声 $\\varepsilon$；\u003c/li\u003e\n\u003cli\u003e构造带噪动作块 $A_t^\\tau = \\tau A_t + (1 - \\tau)\\varepsilon$；\u003c/li\u003e\n\u003cli\u003e用观测 $o_t$ 与 $A_t^\\tau$ 输入网络，预测去噪速度场 $v_\\theta(A_t^\\tau, o_t) \u0026lt;-\u0026gt; \\varepsilon - A_t$；\u003c/li\u003e\n\u003cli\u003e以 $\\left| v_\\theta - (\\varepsilon - A_t) \\right|^2$ 作为监督信号；\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e推理的时候就会把这个过程分成 10-20步，每一步去噪一点点，最终得到一个动作\n\u003cem\u003e注：\u0026ldquo;这个过程\u0026quot;指的是随机生成一个噪声，然后从这个噪声去噪的过程\u003c/em\u003e\u003c/p\u003e","title":"Bug Journal 2025-05-31"},{"content":"Docker 的安装和调试 Docker相当于一台虚拟机。安装之后就可以在这台虚拟机上跑代码了。\n安装方式：\n首先上 Dockerhub 挑选一个心仪的 docker, 下面以 nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 为例：\n然后运行以下代码：\ndocker run -it -rm\\ --name \u0026lt;your-instance-name\u0026gt; \\ --network host \\ nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 \\ /bin/bash 注：这里的 -it 指的是打开一个可交互界面，-rm 指的是用后删除\n这时候就会自动下载 docker 并打开一个 bash 来用。\n现在你会发现这个虚拟机里面什么都没有，所以就需要 apt-get install\n另外，如果你的宿主机器的根目录比较小，想要挂载一个硬盘的话，就在 docker run 中间加上：\n-v /path/to/large/storage:/somepath \\ 这样就可以在 somepath 下挂载这个硬盘了。\n注：不能挂载在根目录下，必须挂载在一个文件夹下\n这里在测试的时候建议加上 -rm,这样不会产生很多个休眠中的 docker 但是在要频繁使用的时候不建议使用 -rm, 而是就让 docker休眠就好。\n一些比较常用的 docker 指令： # 启动 docker docker start \u0026lt;容器ID或名字\u0026gt; # 关闭 docker docker stop \u0026lt;容器ID或名字\u0026gt; # 重启 docker docker restart \u0026lt;容器ID或名字\u0026gt; # 删除容器 docker rm \u0026lt;容器ID或名字\u0026gt; # 进入容器 docker exec -it \u0026lt;容器ID或名字\u0026gt; bash # 查看正在运行的容器 docker ps # 查看所有容器（包括停止的） docker ps -a # 列出本地镜像 docker images # 删除镜像 docker rmi \u0026lt;镜像ID或名字\u0026gt; # 挂载目录 docker run -v /host/path:/container/path # 增加环境变量 docker run -e HTTP_PROXY=http://localhost:10086 代理的使用 这里使用的是 xray。\nxray 是这样运行的：\n./xray run -c config.json 运行之后你就可以看到哪个端口放开了，就可以在哪个端口上使用代理,比如 port: 10086。\n这时候如果你想使用代理就需要：\nexport HTTP_PROXY=http://localhost:10086 export HTTP_PROXY=http://localhost:10086 这样你的下载就会走代理辣。\n非常重要 (大坑) apt-get install 对代理的要求较高，没那么稳定的代理会很稳定的挂，报 Error 503 Service Unavailable. 这时候就直接换清华源就行了，别使用代理了，等之后下别的再用。\n代理的使用 之后就是装 git, 装conda, 装 python \u0026hellip;\n装 git:\napt update \u0026amp;\u0026amp; apt-get install git 装 conda:\napt update \u0026amp;\u0026amp; apt install -y curl cd /Path curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p /Path/miniconda3 /Path/miniconda3/bin/conda init source ~/.bashrc 装 python:\nconda --version conda create -n \u0026lt;yourname\u0026gt; python=\u0026lt;yourversion\u0026gt; -y conda activate \u0026lt;yourname\u0026gt; 注意：不要在~/.bashrc 中添加 source ~/.bashrc，而是就运行一遍 source ~/.bashrc 就可以了\nPDF压缩 gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/default -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf input.pdf ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-30/","summary":"Docker 的安装和调试","title":"Bug Journal 2025-05-30"},{"content":"DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 动机 现有机器人在静态任务（比如说开门，拿方块，玩魔方）上已经做得很好了，但是在动态任务(比如接住一支笔)上做得不行。 所以现在希望能够解决：\u0026ldquo;灵巧手动态抛接物体\u0026rdquo; 这个问题。\n主要论点 作者提出一个新的强化学习框架 LTC (Learning-based Throwing-Catching) 来操控灵巧手完成抛接动作\n利用压缩后的点云特征感知物体； 基于PPO算法的Actor-Critic策略学习； 引入Lyapunov稳定性准则 和 Intrinsic Advantage 提高捕捉稳定性与学习效率； 模型流程图 PointCloud V2 获取物体点云 -\u0026gt; PCA 压缩点云信息 后面K-Means 优化的线索\n输入观察得到的信息concat 上点云输入\n然后使用 PPO 算法优化\n简单来说，Actor 负责做一个动作，Critic 负责判断这个动作好不好，PPO则会让策略和策略之间的连贯性更强。\n另外，为了增加系统的稳定性，作者引入了一个Lyapunov 函数（经典控制理论中用来衡量系统稳定性）来让这个系统更加稳定。\n这时候有 3 个值：第一个是 原本 PPO 算法中算出来的值，第二个是 Critic 对于动作价值的预测值，第三个是Lyapunov 函数的值\n最后通过加权平均，得到最后的A_all 用于优化 actor.\n创新点 首个实现任意物体灵巧抛接的学习方法，尤其在手部侧握极不稳定条件下成功； 引入Lyapunov稳定性引导的优势估计，显著提升捕捉的稳定性； 点云 + PCA 压缩特征用于泛化物体类型，训练期间加入物理属性随机扰动； 提出混合优势估计，结合 PPO 优势、Lyapunov 稳定优势、Intrinsic 优势； 通过仿真-现实迁移设计，包括扰动鲁棒性验证和实际机器人平台部署规划。 解决的难点 引入系统稳定性约束，避免学习出高回报但不稳定的动作策略； 物体点云感知 + 训练过程中的域随机化增强泛化能力； 之前方法的缺点 大多为静态任务（如抓握、拼积木、开门）； RL方法在动态任务中效果极差（之前成功率几乎为0）； 缺乏对动态稳定性的建模或对多物体、多姿态的泛化能力。 还需要解决的难点 对复杂形状和姿态扰动的鲁棒性仍有限； 仅仅在模拟环境中验证； 复杂场景（如多机器人配合抛接）。 Take away 说到动机，想问一下学长为什么想做具身智能呢？ Mojuco 引擎是一个可以运行的虚拟环境\n对于这一段的 reinforcement learning, 这里的意思是：S 是机器人的状态，A 是机器人可以做的动作的集合，p 是环境，R 是奖励函数，$\\rho_0$是一个随机的初始状态分布，$\\gamma$是奖励因子：最终的奖励类似：$R = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} \\dots$ 而现在我们要做的是:选择一些动作让 $E[R]$ 尽可能大。\n在这里，PPO 看起来很复杂，但是其实不难：\n简单来说，Actor 负责做一个动作，Critic 负责判断这个动作好不好，PPO则会让策略和策略之间的连贯性更强。\n这里的“连贯性更强指的是”：新旧策略在面对同一个 state 的情况下做某一个 action 的概率之间的比值不会超过$(1 - \\theta, 1 + \\theta)$\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-29/","summary":"Paper review 2025-05-29","title":"Bug Journal 2025-05-29"},{"content":"SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation 发表时间：24 Jan 2025\n主要论点 这篇文章提出了一种新的模仿学习框架 SKIL（Semantic Keypoint Imitation Learning），通过结合视觉基础模型自动提取“语义关键点”（semantic keypoints），使得机器人在仅有少量示范的条件下，依然能完成具有泛化能力和复杂步骤的操作任务（如挂毛巾、折叠布料等）。该方法显著降低了训练所需的数据量，并且在未见过的物体与场景中也表现出优越的泛化能力。\nHow can we reduce sample complexity to enable robots to learn data-efficient and generalizable manipulation tasks?\n模型流程图 创新点 自动语义关键点提取： 借助如 DiFT 等视觉基础模型与 SAM，对参考图像中的目标区域进行聚类，自动生成语义关键点，不需要人工标注或专门训练。 语义关键点描述符（descriptor）设计： 结合相似度向量（cosine similarity）与 3D 坐标构建表示，每个关键点携带语义和空间信息。 结合 Diffusion Policy 输出动作序列： 使用 Transformer 编码器和扩散模型作为动作头，实现连续动作输出。 支持跨主体学习（Cross-embodiment）： 提出的 SKIL-H 模块允许利用人类视频（无动作标签）进行辅助训练，提高数据效率和泛化性。 Ensemble 推理策略： 在推理阶段进行关键点子集 dropout 与多次采样求中位数，从而降低视觉匹配误差带来的动作抖动。\nbb*SKIL-H: ** 可以把第三人称视角人的视频转换为辅助训练的数据集\n使用一个 frozen 的通用关键点检测器（如 SAM-Track 或 VIT tracker），输出每一帧的人体相关关键点（如手、手指等）。 画出这些关键点之间的轨迹 将人类演示中关键点的表示映射到机器人操作空间中的目标关键点 直接用 transformer 编码 解决的难点 利用视觉大模型提取对任务有语义意义的关键点，极大降低了状态空间维度； 通过高质量、稀疏但语义强的关键点建模，提升少样本学习能力； 使用 Transformer + Diffusion Policy 构建策略网络，强化连续动作输出的能力； 提供了一种利用人类演示辅助学习的方法，不依赖标注动作。 之前方法的缺点 慢：比如ACT要收集上万个数据; GenGP 用的是完整的语意场，有太多信息了 只预测关键帧，而不是连续的动作 (注：我认为这里说不定关键帧可能好一点) 预测连续动作的模型没有 generalizability 还需要解决的难点 关键点提取质量依赖于视觉基础模型能力： 如在 “Bulb Assembly” 等精度要求极高的任务中，DiFT 模型提取的关键点不够精确，导致失败。 忽略环境障碍与场景信息： 当前关键点只从目标对象上提取，无法感知障碍物等环境元素，可能导致安全问题。 固定视角与姿态： 当前工作主要依赖固定的第三视角 RGBD 摄像头，泛化到第一视角或动态视角仍有挑战。 动作表示维度有限： 尚未充分拓展到高自由度控制（如仿人手）、复杂轨迹规划等更广泛的应用场景。 泛化问题：这个模型并不是 zero-shot,而是每个 task 都单独 train 了一个 model\nTake away semantic keypoints: 语义关键点 指的是一个物体和操作相关的哪一个部分。比如杯子上的把手\ncosine similarity 可以获得不同关键点之间的相似度，相当于某种位置信息；之后可以把真正的位置信息也嵌入进去。\nDiffusion Policy: 这种方法使得 multi-model 变得可行\noff-the-shelf tracking models: 约等于已经开源的动作跟踪实现\n数据集 Meta World\nDATA SCALING LAWS IN IMITATION LEARNING FOR ROBOTIC MANIPULATION 发表时间：24 Oct 2024 修改于：12 Feb 2025\n主要论点 这篇文章研究了模仿学习中的数据规模定律(data scaling laws) 在机器人操作任务中的适用性，核心问题是：\n在不同环境和不同物体下增加演示数据量，是否能提升策略的泛化能力，进而使单任务策略在新环境和新物体上零样本部署成为可能？\n作者通过在真实世界收集超 4 万条人类演示、1.5 万次机器人 rollout，基于 Diffusion Policy 训练策略，在多个任务上发现：\n泛化性能随着训练环境/物体/环境-物体组合数量近似呈幂律增长，即：$y = \\alpha x ^ \\beta$, 其中，y 是用 normalized score 来衡量的泛化性能，x 是样本的多样性。 同一个物体或环境上多收集数据的效果远远不如增加多样性； 实证证明仅需 32 个不同环境-物体组合、每个 50 个演示，就能训练出成功率超 90% 的策略。 模型流程图 创新点 首次系统性提出并验证模仿学习中机器人操控的“scaling law” 测试了 环境泛化 + 物体泛化； 提出了一个高效数据采集策略：以环境和物体的多样性优先，不盲目增加演示数量； 在多项任务上（Pour Water、Mouse Arrangement、Fold Towels、Unplug Charger）进行大规模真实机器人验证； 对视觉模型和动作模型的扩展进行了 ablation study，验证视觉编码器更关键。 解决的难点 数据量大 还需要解决的难点 只验证了四个任务，任务种类仍有限，无法保证适用于所有任务 只使用了 Diffusion Policy，未评估不同学习算法对 scaling law 的依赖性差异 未来还将验证 Reinforcement Learning 的 scaling law. Take away 为了获得更好的泛化能力，采集更多不同的环境 ($# \u0026gt; 16$) \u0026gt; 在同一个环境中堆砌多个物体样本\nLearning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving 发表时间：22 Jan 2025\n主要论点 这篇文章提出了一种新颖的自我训练方法 LEPA（Learning to Plan before Answering），训练 LLM 在生成具体解答前，先生成高层次的抽象“计划”。这些“计划”是通用的元知识（meta-knowledge），可以指导模型更有效地推理和解题。\n具体流程如下： 1.\t模型先生成anticipatory plan（预测性计划）：对问题的大致解题策略。 2.\t再基于这个计划生成具体解答。 3.\t如果解答错误，模型会进行 自我反思（self-reflection），调整 plan 并重新解题，直到成功或达到最大尝试次数。 4.\t最终用 plan + solution 对模型进行监督微调（Supervised Fine-Tuning）。\n模型流程图 创新点 引入 anticipatory plan：首次在自我训练中加入计划作为元知识，帮助 LLM 更清晰地组织解题路径。 计划 + 解答联合训练：不仅学习答案，还学习如何规划解题路径，提高泛化性。 自我反思机制：错误时能分析原因并优化 plan，不再依赖外部标签。 信息隔离设计：计划不能包含具体答案细节，避免 LLM 投机取巧。 可与 RL 兼容扩展：初步展示了 LEPA 与 RL（REINFORCE）结合后的性能提升。 之前方法的缺点 没有引导模型形成通用的解题策略 → 泛化能力差； 错误的修改方式（如修改最终答案而非解题路径）容易产生假阳性答案； 缺少系统性规划与反思机制 。 解决的难点 自我训练生成的数据质量不足、泛化能力差，尤其是在复杂的数学和推理任务上表现不佳。\n之前的方法无法抽象出问题：比如，在做物理题时先抽象出公式，然后再代数运算。\n还需要解决的难点 计划与答案不一致问题：计划可能无法完全约束模型生成的推理步骤； 复杂度与推理选择权平衡：对于简单问题，可能不需要计划，LEPA 可能浪费计算； 计划的质量仍依赖 LLM 自身能力 ，弱模型难以产生有用计划； RL 优化仍是初步探索，与更强 RL 算法结合仍是未来方向； Take away Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving 主要论点 本文提出一种针对四足机器人“难以在仿真中准确建模”的目标（如总电池功耗）进行优化的数据驱动微调方法。核心思想是：\n先用预训练策略在现实中采集数据； 训练一个“测量模型”来预测那些仿真环境中缺失或误差较大的目标（如总电流）； 将这个模型集成到仿真中，作为奖励函数优化新策略； 通过仿真+现实评估的迭代更新，逐步提升策略在真实世界中的表现。 他们以“降低四足机器人总功耗”为目标，在 Unitree Go1 上进行实证研究，结果显示电池功耗减少了 24-28%。\n模型流程图 创新点 数据驱动的测量模型作为奖励代理：直接从现实数据学习一个预测真实目标的模型（如总电流），替代传统的机械功或热功率等代理指标。 目标不可建模时的微调框架：将测量模型集成进仿真中，使得原本难以仿真的目标也可以用于训练优化。 层级策略选择机制：候选策略先在仿真中初选，再在现实中精评，保留最优用于下次迭代。 迭代式收集数据 + 微调：不断积累现实数据、优化测量模型、提升策略性能，达到持续优化的效果。 解决的难点 传统 sim-to-real 方法依赖物理仿真器（如 MuJoCo），但这些仿真器通常无法准确建模：\n电池电流/电压（尤其是 PMSM 电机复杂控制） 步态噪声 电机过热等非刚体物理特性 之前方法的缺点 使用代理目标 (如机械功) 不够准确，甚至误导优化过程； 现实直接训练 RL 效率低，只适用于简单任务； 以往方法不支持针对新目标进行微调（例如预训练时没有考虑节能目标，无法直接迁移）。 还需要解决的难点 需大量真实数据才能收敛（约 38 万个样本）； 测量模型存在分布偏移（Out-of-Distribution）问题； 实验仅在 Unitree Go1 和平地环境下进行，缺乏跨机器人、复杂地形验证； 没有测试对其他 hard-to-simulate 目标（如声音、热量）的迁移适应性。 Take away Revisit when doing a robot dog related task.\n关注文章的 Motivation\n关注文章的 Motivation 是怎么和方法联系起来的\n关注文章是如何卖出去的\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-28/","summary":"2025-05-28 论文阅读笔记","title":"Bug Journal 2025-05-28"},{"content":"Today\u0026rsquo;s problem 2894. Divisible and Non-divisible Sums Difference\nIntuition We are given an integer n and a divisor m, and we want to compute the difference between:\nThe sum of numbers from 1 to n that are not divisible by m. The sum of numbers from 1 to n that are divisible by m. This means we want to partition the numbers 1 to n into two groups based on divisibility by m, sum each group, and return the difference.\nApproach We can solve this problem using two methods:\nMethod 1: Formula-Based Use the formula for the sum of the first n natural numbers: n * (n + 1) // 2 to get the total sum. Count how many numbers from 1 to n are divisible by m: k = n // m. The divisible numbers are: m, 2m, ..., km, and their sum is m * (1 + 2 + ... + k) = m * (k * (k + 1) // 2). Subtract the divisible sum from the total to get the sum of non-divisible numbers, then subtract. Method 2: Brute-Force Iteration Iterate from 1 to n. If the number is divisible by m, add it to num2. Otherwise, add it to num1. Return the difference num1 - num2. Complexity Time complexity:\nMethod 1: $O(1)$ (constant time using formulas) Method 2: $O(n)$ (linear time iteration) Space complexity:\nBoth methods: $O(1)$ (only a few variables used) Code class Solution: def differenceOfSums(self, n: int, m: int) -\u0026gt; int: # Method 1: Formula-Based total_sum = n * (n + 1) // 2 k = n // m divisible_sum = m * (k * (k + 1) // 2) return total_sum - divisible_sum class Solution: def differenceOfSums(self, n: int, m: int) -\u0026gt; int: # Method 2: Brute-Force Iteration num1, num2 = 0, 0 for i in range(1, n + 1): if i % m == 0: num2 += i else: num1 += i return num1 - num2 Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-27/","summary":"Traverse and Mathmatics!","title":"LeetCode Daily Question 2025-05-27"},{"content":"模板 请仔细阅读这篇文章，并告诉我： 1. 这篇文章的动机是什么，要解决什么问题 2. 这篇文章大概讲了什么 3. 这篇文章的创新点是什么 4. 这篇文章解决了什么问题，之前的人为什么不能解决 5. 这篇文章还有什么问题没解决 6. 这篇文章有什么需要我注意的点 7. 这篇文章是如何做实验的，setting 是什么 8. 这篇文章的算力要求是多少，多少卡运行了多久，用了什么数据集，是不是可以公开获取的，模型代码呢，能不能公开获取 如果这篇文章提出了一个模型，那请告诉我： 1. 这个模型的输入是什么 2. 输出是什么 3. 输入和输出数据经过了什么处理 4. 这个模型是如何处理输入和输出数据的 动机 主要论点 模型流程图 创新点 解决的难点 之前方法的缺点 还需要解决的难点 Take away ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-27/","summary":"A template for reading AI papers","title":"Bug Journal 2025-05-27"},{"content":"Today\u0026rsquo;s problem 2131. Longest Palindrome by Concatenating Two Letter Words\nIntuition There are in total two ways to form a palindrome.\na string that has an inverse string in the list a string that is a palindrome itself. In this case, the string that is palindrome can only exisit in the middle of the palindrome. Approach Therefore, we can use a hash to solve this problem. Note that we will first run test 1 before test 2. If there is an inverse string in the list, then put that string and the current string into the list.\nThen test whether this string is a palindrome itself.\nComplexity Time complexity: $O(N)$, N is the length of words.\nSpace complexity: $O(N)$, N is the length of words.\nCode class Solution: def longestPalindrome(self, words: List[str]) -\u0026gt; int: cnt = Counter(words) ans = 0 sp = 0 for word, t in cnt.items(): # print(word, t) if word[0] == word[1]: ans += (t - t % 2) sp |= (t % 2) else: ans += min(t, cnt[word[::-1]]) return (ans + sp) * 2 Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-25/","summary":"Hash!","title":"LeetCode Daily Question 2025-05-25"},{"content":"Today\u0026rsquo;s problem 2942. Find Words Containing Character\nIntuition Do what the question ask.\nApproach Do what the question ask, find the string in every word in words array.\nComplexity Time complexity: $O(N \\times M)$, N is the length of words array, M is the length of each word. Space complexity: $O(N \\times M)$, N is the length of words array, M is the length of each word. Code class Solution: def findWordsContaining(self, words: List[str], x: str) -\u0026gt; List[int]: ans = [] for i, word in enumerate(words): if x in word: ans.append(i) return ans Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-24/","summary":"Do what the question ask","title":"LeetCode Daily Question 2025-05-24"},{"content":"Today\u0026rsquo;s problem 3068. Find the Maximum Sum of Node Values\nImportant: all the methods below are based on this fact: xor even times equals xor zero times. Method 1: Tree DP Intuition and Approach In this problem, if we only consider one direction, e.g., from root to leaf, then the process will not have after effect (later decisions will not affect previous ones). Therefore, we can use DP to solve this problem.\nThe hardest part is the definition of the dp. As we have a prerequisite of a direction, a better way to define the dp formula is to exclude the effect of current node. Also, for each node, there are two status, as described above, each node can either xor odd times or even times.\nTherefore, we have our DP definition. $dp[x][0/1]$ means the largest value the children of x can achieve when the node x is changed (1) or unchanged (0).\nNow, for each child c of node x, we can do two operations: either do xor for both node x and c, or do not do xor for neither x nor c.\nThe dp formula of these two operations will be: (Note: the priority of $\\oplus$ is lower than $+$, so it is very important to add a parentheses.)\nDo the xor operation $dp[x][0] = max(dp[x][0] + dp[c][0] + nums[c], dp[x][0] + dp[c][1] + (nums[c] \\oplus k))$. $dp[x][1] = max(dp[x][1] + dp[c][0] + nums[c], dp[x][1] + dp[c][1] + (nums[c] \\oplus k))$. NOT do the xor operation $dp[x][0] = max(dp[x][1] + dp[c][1] + nums[c], dp[x][1] + dp[c][0] + (nums[c] \\oplus k))$. $dp[x][1] = max(dp[x][0] + dp[c][0] + nums[c], dp[x][0] + dp[c][0] + (nums[c] \\oplus k))$. Note that the dp[x][0] and dp[x][1] should be renewed at the same time.\nMoreover, another important thing is the initialization of the dp array. For all $dp[x][1]$, we will give it a value of $-inf$, so that we can avoid the case when c is a leaf node and the number is $\\oplus$ with k contributes to the $dp[x]$ array.\nThe final result will be $max((dp[0][0] + nums[0]), (dp[0][1] + (nums[0] ^ k)))$\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp = [[0 for _ in range(2)] for _ in range(n)] for i in range(n): dp[i][1] = -10_000_000_000 edge = [[] for _ in range(n)] for x,y in edges: edge[x].append(y) edge[y].append(x) def dfs(x, fa): for to in edge[x]: if to == fa: continue dfs(to, x) c0 = max(dp[to][0] + nums[to], dp[to][1] + (nums[to] ^ k)) c1 = max(dp[to][0] + (nums[to] ^ k), dp[to][1] + nums[to]) dp[x][0], dp[x][1] = max(dp[x][0] + c0, dp[x][1] + c1), max(dp[x][1] + c0, dp[x][0] + c1) dfs(0,-1) return max((dp[0][0] + nums[0]), (dp[0][1] + (nums[0] ^ k))) Method 2: Tree DP with better memory Intuition and Approach In the previous code, we find that the $dp[x]$ will only use two times. Once in calculating the result of $dp[x]$, once in calculating the result of $dp[fa]$.\nTherefore, we can return the value of $dp[x][0]$ and $dp[x][1]$ to avoid the extra space of the dp array.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(1)$. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) edge = [[] for _ in range(n)] for x,y in edges: edge[x].append(y) edge[y].append(x) def dfs(x, fa): dp0,dp1 = 0,-1e9 for to in edge[x]: if to == fa: continue c0, c1 = dfs(to, x) dp0, dp1 = max(dp0 + c0, dp1 + c1), max(dp0 + c1, dp1 + c0) return max(dp0 + nums[x], dp1 + (nums[x] ^ k)), max(dp0 + (nums[x] ^ k), dp1 + nums[x]) return dfs(0,-1)[0] Important: all the methods below are based on this fact: there are always a path between two nodes on a tree. Therefore, we can $\\oplus$ all the nodes on this path, resulting the $\\oplus$ of any two nodes on the tree. Method 3: DP without tree Intuition and Approach For each node, we have two status, whether to $\\oplus$ k or not. Therefore, the definition of the DP array will be: $dp[i][0/1]$ means whether there are odd (1) or even (0) $\\oplus$ k operations when traversing to the ith node.\nWe then have the formular:\nWhen this node $\\oplus$ with k: $dp[i][0] = max(dp[i-1][0] + nums[i], dp[i-1][1] + (nums[i] ^ k))$ When this node do not $\\oplus$ with k: $dp[i][1] = max(dp[i-1][1] + nums[i], dp[i-1][0] + (nums[i] ^ k))$ Note that there are always even $\\oplus$ operations, so the answer would be $dp[n-1][0]$.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp = [[0 for _ in range(2)] for _ in range(n)] dp[0][0] = nums[0] dp[0][1] = (nums[0] ^ k) for i in range(1, n): dp[i][0] = max(dp[i-1][0] + nums[i], dp[i-1][1] + (nums[i] ^ k)) dp[i][1] = max(dp[i-1][0] + (nums[i] ^ k), dp[i-1][1] + nums[i]) return dp[-1][0] Method 4: DP without tree with better memory Intuition and Approach Same as Method 2, we also find out that the dp[i] formular only use twice. In this case, we can use two variables instead of the whold array to have a better memory usage.\nAlso, the $max$ operations is too slow in python, so a better way is to use if else equations instead of max.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(1)$. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp0, dp1 = 0, -10_000_000_000 for i in range(n): a = nums[i] b = a ^ k new_dp0 = dp0 + a if dp0 + a \u0026gt; dp1 + b else dp1 + b new_dp1 = dp0 + b if dp0 + b \u0026gt; dp1 + a else dp1 + a dp0, dp1 = new_dp0, new_dp1 return dp0 Method 5: Greedy algorithm Intuition and Approach Another way to look at this method without of tree is using greedy algorithm. Because we know that we can $\\oplus$ k as long as we can find a pair of nodes, we can use greedy algorithm to find the pairs that has the most differences after $\\oplus$ k.\nThat is, we can first $\\oplus$ every element with k, calculating the difference between the new array and the previous array, then find all the pairs that has a difference that is larger than zero, then we get our answer.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: ans = sum(nums) diff = [(x ^ k) - x for x in nums] cnt,l,r = 0,inf,-inf for x in diff: if x \u0026gt; 0: cnt += 1 if x \u0026lt; l: l = x ans += x else: if r \u0026lt; x: r = x if cnt % 2 == 1: ans += max(-l, r) return ans I don\u0026rsquo;t know why using sort to do greedy algorithm is so neat and fast. Just as the one in the official solution.\nAdvertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-23/","summary":"5 Solutions in one question!","title":"LeetCode Daily Question 2025-05-23"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/zero-array-transformation-iii/\nIntuition This question requires us to find the number of sections that is \u0026ldquo;useful\u0026rdquo;, or in other words, the smallest number of sections that is enough to make the array a zero array.\nThe key to solve this question is to change a view of how we look at this problem: if we take each element in the array seperately, then we can use greedy algorithm to solve this problem.\nThe thing is, if we look at each element seperately, i.e., to make the entire array a zero array, we must make each element zero.\nIn this case, for each element, the best way is to find a section that contains this elemnt, while it has a further tail. This is because a furtuer tail means to cover more elements in the future, which will be always better compared with the sections that has a shorter tail.\nTherefore, now we need a data structure to store the current \u0026ldquo;farest tail\u0026rdquo;. This data structure need to add element dynamically and delete the largest item, when a heap will be the best way to store the \u0026ldquo;tail\u0026rdquo;.\nApproach Therefore, we can form our algorithm.\nFirst, we need to sort the array using the left end of the query as keyword. In this case, we can find which queries has a left end that is to the left of our current index. Then we need to create a difference array to deal with the section add operation; a heap to store the right end of the queries; and an index to show where we are currently at when we traverse all the queries. The next step is to traverse the number array: for each element in the number array, we first need to push all the queries that has the left end that is less the current index. This makes all the elements in the heap potentially available to use to decrease the current element. Then we will deal with the current element, finding all the available queries for the current element, then deal with the section decrease operation. (In this case, we ensure every operation is valid by checking that the endpoint of the heap top is larger or equal to the index, so that the left end of the array will be less or equal to the current index, and the right end of the array will be larger or equal to the current index. Therefore, we guarentee that the operation is valid). Finally, if the number is still larger than 0, we will return -1; otherwise, we will return the remaining element in h, which is all the unused elements. Complexity Time complexity: $O(N \\times log(m))$\nSpace complexity: $O(N + M)$\nCode class Solution: def maxRemoval(self, nums: List[int], queries: List[List[int]]) -\u0026gt; int: queries.sort(key = lambda x: x[0]) diff = [0] * (len(nums) + 1) h = [] idx, now = 0,0 for i in range(len(nums)): while idx \u0026lt; len(queries) and queries[idx][0] \u0026lt;= i: heappush(h, -queries[idx][1]) idx += 1 now += diff[i] while h and now \u0026lt; nums[i] and -h[0] \u0026gt;= i: now += 1 diff[-heappop(h) + 1] -= 1 if now \u0026lt; nums[i]: return -1 return len(h) For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-22/","summary":"Greedy Algorithm, look at each element seperately!","title":"LeetCode Daily Question 2025-05-22"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-smallest-number-from-di-string/\nIntuition This problem let us find a sequence of numbers under some constrains.\nFirst we observe that the N, the length of the sequence, is very small. Thereofre, we can use a dfs to traverse all possible situations and get the result.\nAnother way to do that is using a stack. When the array is increasing, the smallest way to get a valid array is to traverse all the numbers, filling in the smallest number possible. When the array is decreasing, becuase we still want the smallest array, we need to put the smallest number available, which should be also larger to the next number. In this case, the smallest number we can put at current place is the position + the number of consequtive \u0026lsquo;D\u0026rsquo;s afterward. Therefore, we can use a stack to temporarily sotre the number we are traversing, and add it back to the current array when we meet a \u0026lsquo;I\u0026rsquo; or reach to the end of the array.\nApproach For Solution 1, the dfs solution, all we need to do is to traverse all the solutions and get the first one that fullfills the requirement.\nFor Solution 2, the stack solution, when we meet \u0026lsquo;I\u0026rsquo;, we can put \u0026ldquo;idx + 1\u0026rdquo; to our current array, then fill all the elements in a stack into our current array in reverse order, then flush the stack. When we meet \u0026lsquo;D\u0026rsquo;, we can put \u0026ldquo;idx + 1\u0026rdquo; to our stack for our future use.\nTrick DFS: Because we are required to find the smallest valid sequence, so the first sequence that is not None is our target. This means that we can return this answer as soon as we get a valid result.\nStack: Here I intentionally add a \u0026ldquo;D\u0026rdquo; to the end of the sequence. Intuitively speaking, the last element is the largest element in the sequence, so to put it into our current sequence, it requires a \u0026ldquo;D\u0026rdquo; operation. In this case, if the original last character is \u0026lsquo;I\u0026rsquo;, then we can directlly put the largest number to the end of our original sequence, which is the same as a \u0026lsquo;D\u0026rsquo; operation. This is because the \u0026lsquo;I\u0026rsquo; operation will flush the stack, so there will only be one element in the stack, making it the same whether adding as a normal sequence or a inverted sequence. If the original last character is \u0026lsquo;D\u0026rsquo;, then the largest character should be at the position whether the consequtive sequence of \u0026lsquo;D\u0026rsquo; starts. In this case, this means that there should be a \u0026lsquo;D\u0026rsquo; operation to put this number into the right position. Though this trick makes the code more tidy and elegant, it sacrifices readability, which is not encouraged.\nComplexity Time complexity for dfs solution: $O(N!)$, N is the length of the sequence.\nTime complexity for stack solution: $O(N)$, N is the length of the sequence.\nSpace complexity for dfs solution: $O(N)$, N is the length of the sequence.\nSpace complexity for stack solution: $O(N)$, N is the length of the sequence.\nCode class Solution: def smallestNumber(self, pattern: str) -\u0026gt; str: arr = [] n = len(pattern) + 2 def dfs(arr): if len(arr) == n - 1: return arr now = len(arr) - 1 res = None if pattern[now] == \u0026#39;I\u0026#39;: for i in range(arr[now] + 1, n): if i not in arr: res = dfs(arr + [i]) if res is not None: return res else: for i in range(1, arr[now]): if i not in arr: res = dfs(arr + [i]) if res is not None: return res return res for i in range(1, n): ans = dfs([i]) if ans is not None: return \u0026#39;\u0026#39;.join(map(str, ans)) # return ans return None class Solution: def smallestNumber(self, pattern: str) -\u0026gt; str: arr = [] n = len(pattern) + 2 def dfs(arr): if len(arr) == n - 1: return arr now = len(arr) - 1 res = None if pattern[now] == \u0026#39;I\u0026#39;: for i in range(arr[now] + 1, n): if i not in arr: res = dfs(arr + [i]) if res is not None: return res else: for i in range(1, arr[now]): if i not in arr: res = dfs(arr + [i]) if res is not None: return res return res for i in range(1, n): ans = dfs([i]) if ans is not None: return \u0026#39;\u0026#39;.join(map(str, ans)) # return ans return None For more solutions, please visit My blog.\n","permalink":"https://tzj2006.github.io/leetcode/2025-02-18/","summary":"DFS beats 100% and O(N) stack with trick","title":"LeetCode Daily Question 2025-02-18"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/set-matrix-zeroes/\nIntuition This question asks us to change the rows and columns to 0 if there exists an 0 in the row or column. Therefore, we can store the rows and columns and then change all these rows and colums to zero.\nApproach Store all the columns and rows that contains 0 Change all these columns and rows Complexity Time complexity: $O(N \\times M)$, N is the length of the array, M is the width of the array.\nSpace complexity: $O(N \\times M)$, N is the length of the array, M is the width of the array.\nCode class Solution: def setZeroes(self, matrix: List[List[int]]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify matrix in-place instead. \u0026#34;\u0026#34;\u0026#34; change_row_idx = set([]) change_col_idx = set([]) # Note that here I use set to avoid recording the same row or column multiple times. for i in range(len(matrix)): for j in range(len(matrix[0])): if matrix[i][j] == 0: change_row_idx.add(i) change_col_idx.add(j) for i in range(len(matrix)): for j in range(len(matrix[0])): if i in change_row_idx or j in change_col_idx: matrix[i][j] = 0 For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-21/","summary":"Do what the question asks!","title":"LeetCode Daily Question 2025-05-21"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/zero-array-transformation-i/\nIntuition This problem means to decrease one in a range (l, r) for each query. To deal with the change of a range, we can consider prefix sum. Note that the question says \u0026ldquo;Select a subset of indices\u0026rdquo;, this means that we do not necessarily need to minus 1 for all indices in the range. In this case, because we only care whether the final array is a zero array or not, so instead of testing whether the final array is zero or not, we can test whether the final array is less or equal to zero or not becuase of the subset mentioned in the question.\nApproach For each query, we can add 1 to the difference array at l and add -1 to the difference array at r + 1. Then when we calculate the final answer, we can use the prefix sum to add them up and get the change of the array. Finally, when we want to know whether the final array is zero array or not, we can add the difference array to the original array and test whether each index is less or equal to zero or not to get the answer.\nComplexity Time complexity: $O(N + M)$, N is the length of the original array, M is the length of the query.\nSpace complexity: $O(N)$.\nCode class Solution: def isZeroArray(self, nums: List[int], queries: List[List[int]]) -\u0026gt; bool: diff = [0] * (len(nums) + 1) for l, r in queries: diff[l] -= 1 diff[r + 1] += 1 for i in range(len(nums)): if i \u0026gt; 0: diff[i] += diff[i-1] if nums[i] + diff[i] \u0026gt; 0: # print(i, nums[i], diff[i]) return False return True For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-20/","summary":"Use Chafen!","title":"LeetCode Daily Question 2025-05-20"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/type-of-triangle\nIntuition Do what the question asks.\nApproach Do what the question asks.\nComplexity Time complexity: $O(1)$\nSpace complexity: $O(1)$\nCode class Solution: def triangleType(self, nums: List[int]) -\u0026gt; str: nums.sort() if nums[0] + nums[1] \u0026lt;= nums[2]: return \u0026#34;none\u0026#34; elif nums[0] == nums[1] == nums[2]: return \u0026#34;equilateral\u0026#34; elif nums[0] == nums[1] or nums[1] == nums[2]: return \u0026#34;isosceles\u0026#34; return \u0026#34;scalene\u0026#34; For more Solutions, please visit my website.\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-19/","summary":"Do what the question ask!","title":"LeetCode Daily Question 2025-05-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/painting-a-grid-with-three-different-colors/\nIntuition In this question, we find that m is relatively small compared with n. As $m \\le 5$, and $n \\le 1000$. Then we may consider to enumerate all solutions for a column, then elaborate it to the whole matrix. When elaborating it to the whole matrix, we figure out a thing: whatever a column is painted, the only influence is its next column, while future columns will not influence previous columns. This makes DP possible.\nApproach Therefore, here is our approach:\nFirst, we need to find out how many valid patterns are there in a column. Therefore, we can perform a dfs to search for all possible combinations. Second, we need to know which two patterns can be in adjcent columns, so we enumerate through each pair of patterns, and then test whether they can be in adjcent rows or not. Third, we use DP to elaborate from one column to the next. In this case, the DP formular will be: $DP[col][case_x] = \\sum DP[col-1][case_y]. \\forall \\text{casex and casey can be in two adjcent columns}$. Finally, all we need to do is to add up all the cases of the final column of DP to get our answer. Complexity Time complexity: $O(3^{2m} \\times n)$\nSpace complexity: $O(3^{2m})$\nCode class Solution: def colorTheGrid(self, m: int, n: int) -\u0026gt; int: pat = [] col = [0, 1, 2] def dfs(x, s): if x == m: pat.append(s) return for i in col: if x == 0 or s[x - 1] != i: dfs(x + 1, s + [i]) dfs(0, []) # till this step, we find all valid patterns for a column and store it in the pattern list. l = len(pat) valid = [[True for _ in range(l)] for _ in range(l)] for i in range(l): for j in range(i + 1, l): for k in range(m): if pat[i][k] == pat[j][k]: valid[i][j] = False break # till this step, we find all the pattern pairs that is valid. dp = [[0 for _ in range(l)] for _ in range(n)] mod = 1_000_000_007 for i in range(l): dp[0][i] = 1 # for column 0, each pattern is valid. for i in range(1, n): for x in range(l): for y in range(x + 1, l): if valid[x][y]: dp[i][x] = (dp[i][x] + dp[i-1][y]) % mod dp[i][y] = (dp[i][y] + dp[i-1][x]) % mod # we elaborate to the next column according to the DP formula. ans = 0 for i in range(l): ans = (ans + dp[-1][i]) % mod # finally, we add up all the answers. return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-05-18/","summary":"First DFS then DP!","title":"LeetCode Daily Question 2025-05-18"},{"content":"Mac 监控： 磁盘信息：\nbrew install smartmontools smartctl -a disk0 效果： CPU GPU占用信息：\nbrew install macmon macmon 效果展示： ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-17/","summary":"\u003cp\u003eMac 监控：\n磁盘信息：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ebrew\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esmartmontools\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esmartctl\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e \u003cspan class=\"n\"\u003edisk0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e效果：\n\u003cimg alt=\"1\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-17/1.png\"\u003e\u003c/p\u003e\n\u003cp\u003eCPU GPU占用信息：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ebrew install macmon\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emacmon\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e效果展示：\n\u003cimg alt=\"2\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-17/2.png\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-05-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/letter-tile-possibilities/description/\nIntuition In this question, we want to know how many different tiles we can generate.\nApproach Therefore, we can use backtracking to enumerate all the solutions.\nComplexity Time complexity: $O(2^N)$, N is the length of the sequence.\nSpace complexity: $O(N)$, N is the length of the sequence.\nCode class Solution: def numTilePossibilities(self, tiles: str) -\u0026gt; int: counter = defaultdict(int) for ch in tiles: counter[ch] += 1 def dfs(counter): total = 0 for ch in counter: if counter[ch] == 0: continue # Choose character total += 1 counter[ch] -= 1 total += dfs(counter) counter[ch] += 1 # backtracking return total return dfs(counter) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-17/","summary":"\u003col start=\"1079\"\u003e\n\u003cli\u003eLetter Tile Possibilities\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-the-lexicographically-largest-valid-sequence/description/\nIntuition This question requires you to find a solution according to the requirements.\nApproach Note that $N \\le 20$, so we can use a brute search to find the answer.\nComplexity Time complexity: $O(N^N)$, N is the same definition as the question.\nSpace complexity: $O(N)$, N is the same definition as the question.\nCode Normal dfs solution class Solution: def dfs(self, pos, n, vis, pls): if pos == 2 * n - 1: return pls # if we enumerate to the end of the sequence, then we can return the answer. if pls[pos] != 0: return self.dfs(pos + 1, n, vis, pls) # if we place a number in the current position, then we can move to the next position. for i in range(n, 1, -1): # we enumerate from large to small so that we can get the largest sequence. if vis[i]: continue # if we use this number, then pass. if pos + i \u0026lt; 2 * n - 1 and pls[pos + i] == 0: pls[pos] = i pls[pos + i] = i vis[i] = True # put the number into the slot if it is available. ret = self.dfs(pos + 1, n, vis, pls) if ret is not None: return ret vis[i] = False pls[pos] = 0 pls[pos + i] = 0 if vis[1]: return None vis[1] = True pls[pos] = 1 ret = self.dfs(pos + 1, n, vis, pls) if ret is not None: return ret vis[1] = False pls[pos] = 0 # special check for 1 becuase 1 only puts into one slot. return None def constructDistancedSequence(self, n: int) -\u0026gt; List[int]: pls = [0] * (2 * n - 1) # pls is the sequence that we place numbers vis = [0] * (n+1) # visit is the sequence we test whether a number exists in the current sequence or not. return self.dfs(0, n, vis, pls) Faster solution for future use Note that the solution will not change when we input the same number, therefore, we can just store the answer we get and output it for every query.\nclass Solution: def constructDistancedSequence(self, n: int) -\u0026gt; List[int]: ans = [ [1], [2,1,2], [3,1,2,3,2], [4,2,3,2,4,3,1], [5,3,1,4,3,5,2,4,2], [6,4,2,5,2,4,6,3,5,1,3], [7,5,3,6,4,3,5,7,4,6,2,1,2], [8,6,4,2,7,2,4,6,8,5,3,7,1,3,5], [9,7,5,3,8,6,3,5,7,9,4,6,8,2,4,2,1], [10,8,6,9,3,1,7,3,6,8,10,5,9,7,4,2,5,2,4], [11,9,10,6,4,1,7,8,4,6,9,11,10,7,5,8,2,3,2,5,3], [12,10,11,7,5,3,8,9,3,5,7,10,12,11,8,6,9,2,4,2,1,6,4], [13,11,12,8,6,4,9,10,1,4,6,8,11,13,12,9,7,10,3,5,2,3,2,7,5], [14,12,13,9,7,11,4,1,10,8,4,7,9,12,14,13,11,8,10,6,3,5,2,3,2,6,5], [15,13,14,10,8,12,5,3,11,9,3,5,8,10,13,15,14,12,9,11,7,4,6,1,2,4,2,7,6], [16,14,15,11,9,13,6,4,12,10,1,4,6,9,11,14,16,15,13,10,12,8,5,7,2,3,2,5,3,8,7], [17,15,16,12,10,14,7,5,3,13,11,3,5,7,10,12,15,17,16,14,9,11,13,8,6,2,1,2,4,9,6,8,4], [18,16,17,13,11,15,8,14,4,2,12,2,4,10,8,11,13,16,18,17,15,14,12,10,9,7,5,3,6,1,3,5,7,9,6], [19,17,18,14,12,16,9,15,6,3,13,1,3,11,6,9,12,14,17,19,18,16,15,13,11,10,8,4,5,7,2,4,2,5,8,10,7], [20,18,19,15,13,17,10,16,7,5,3,14,12,3,5,7,10,13,15,18,20,19,17,16,12,14,11,9,4,6,8,2,4,2,1,6,9,11,8] ] return ans[n - 1] ","permalink":"https://tzj2006.github.io/leetcode/2025-02-16/","summary":"This is an NP Complete question","title":"LeetCode Daily Question 2025-02-16"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/sort-colors/\nIntuition In this case, we know that there are only three elements in the list, so we can use bucket sort to solve this problem.\nApproach All we need is to use a bucket to calculate the number of times each number exists, then we put these numbers into the array.\nComplexity Time complexity: $O(N)$, N is the length of the array. Space complexity: $O(Num)$, Num is the number of different numbers. Code class Solution: def sortColors(self, nums: List[int]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; # nums.sort() cnt = [0,0,0] for num in nums: cnt[num] += 1 cnt[1] += cnt[0] cnt[2] += cnt[1] cur = 0 for i in range(len(nums)): while i \u0026gt;= cnt[cur]: cur += 1 nums[i] = cur ","permalink":"https://tzj2006.github.io/leetcode/2025-05-17/","summary":"\u003col start=\"75\"\u003e\n\u003cli\u003eSort Colors\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-05-17"},{"content":"Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (ACT Algorithm) 主要论点 ALOHA 是一个开放源代码的低成本双臂远程操作硬件系统，整体成本低于 20,000 美元，主要由现成的机器人组件和少量 3D 打印部件组成。该系统支持精细、动态和接触丰富的任务，如穿拉链、乒乓球颠球和链条组装等。\n为了应对模仿学习中政策误差累积的问题，研究人员提出了 ACT 算法。该算法基于 Transformer 架构，采用条件变分自编码器（CVAE）框架，通过预测动作序列（即“动作块”）而非单步动作，减少了有效的预测范围，从而提高了学习效率和稳定性。\n在六个现实世界的精细操作任务中，如打开透明调味杯盖和插入电池等，ALOHA 系统仅通过约 10 分钟的示范数据（约 50 次演示）就实现了 80% 至 90% 的成功率，展示了其在低成本硬件上的高效学习能力。\n创新点 低成本高性能：ALOHA 系统在成本控制的同时，仍能执行复杂的双臂操作任务，降低了高精度机器人研究的门槛。\n动作块预测 ：ACT 算法通过预测动作序列，减少了政策误差的累积，提高了模仿学习的稳定性和效率。\n快速学习能力：系统仅需少量的演示数据即可学习复杂任务，展示了高效的学习能力。\n解决的难点 高精度双臂系统价格昂贵，限制研究和数据获取 。\n模仿学习中长序列预测误差累积问题严重 。\n设计低于 $20,000 的双臂系统 ALOHA + 高效 Transformer 模仿学习方法 ACT。\n通过动作 chunking 预测提升长序列动作稳定性。\n还需要解决的难点 硬件精度限制 ：尽管系统成本低廉，但硬件精度的限制可能影响在更复杂任务中的表现。\n泛化能力：系统在面对未见过的任务或环境时的泛化能力仍需进一步提升。\n实时性能 ：在实际应用中，如何确保系统的实时响应能力和稳定性是一个挑战。\nAutoregressive Action Sequence Learning for Robotic Manipulation 主要论点 论文的核心思想是将机器人动作表示为序列数据，并通过自回归序列建模生成动作序列。为此，作者提出了两项关键技术：\nChunking Causal Transformer (CCT)：该模型扩展了传统因果变换器的单步预测能力，支持在一个步骤中预测多个动作“块”。这种方法提高了对不同控制频率任务的适应性，并通过减少自回归步骤提高了效率 Autoregressive Policy (ARP)：基于CCT，作者设计了ARP架构，用于生成混合动作序列，解决多种机器人操作任务。该架构在Push-T、ALOHA和RLBench等多种机器人操作环境中进行了评估，结果显示ARP作为通用架构，在所有测试基准中匹配或超越了特定环境下的最新技术，同时在计算和参数规模上更为高效 简而言之，CCT 可以预测未来的多个动作 创新点 多动作块预测机制：CCT模型引入了预测多个动作块的能力，使其能够处理不同类型和频率的动作数据，提高了模型的灵活性和效率\n混合动作序列设计：通过将不同类型的动作（如关节位置、2D像素坐标和末端执行器姿态）混合在一个序列中，并为每种动作类型使用不同的块大小，增强了模型对复杂任务的适应能力\n通用策略架构：ARP架构作为一个通用的策略架构，在多个不同的机器人操作环境中表现出色，显示出其广泛的适用性和高效性\n解决的难点 自回归策略效率低: 传统每次只预测一个动作的自回归方法效率低，难以适配高频任务。\n动作混合表示困难：连续值与离散值混合表示在序列学习中不易统一建模。\n还需要解决的难点 动作数据的异质性 ：机器人动作数据通常包括连续值和离散值，如何有效地将这些异质数据表示为序列，并进行建模，是一个挑战。\n高频控制任务的建模 ：现有的自回归架构在处理高频控制任务时存在限制，如何扩展模型以支持高频控制任务，需要进一步研究\n混合动作序列的生成与优化 ：在生成包含多种动作类型的混合序列时，如何确保各动作类型之间的协调性和整体序列的最优性，是一个需要解决的问题\nπ0: A Vision-Language-Action Flow Model for General Robot Control 主要论点 π₀模型的核心是将预训练的视觉-语言模型（VLM）与流匹配架构相结合，形成一个统一的视觉-语言-动作（VLA）模型，用于通用机器人控制。该模型通过在多个灵巧机器人平台（包括单臂、双臂和移动操纵器）的大型多样化数据集上进行训练，学习从视觉和语言输入到动作输出的映射关系。\n模型的训练分为两个阶段：\n预训练阶段 ：在大规模多样化的数据集上进行训练，学习通用的感知和语言理解能力。 微调阶段：在特定任务的高质量数据上进行微调，以提高在特定任务上的性能。 该模型在多个任务上进行了评估，包括折叠衣物、清洁桌子和组装盒子等，展示了其在零样本学习、语言指令遵循和新技能获取方面的能力。\n创新点 流匹配架构 ：引入流匹配技术生成连续的动作分布，适用于高频率和灵巧的任务。\n跨机器人平台训练 ：结合多种机器人类型的数据进行训练，使模型能够适应不同的机器人配置和动作表示。\n高效推理机制 ：模型设计允许高效的推理过程，通过缓存和重用注意力键值对来减少计算量，适应实时控制的需求。\n解决的难点 VLA模型对高频、连续动作建模困难。\n泛化能力差，多机器人平台适应性弱。\n引入流匹配（flow matching）机制，生成连续动作分布，替代离散token生成方式。\n在多机器人、多任务、多平台上训练实现跨平台泛化。\n还需要解决的难点 数据的多样性和质量：虽然模型在多个平台和任务上进行了训练，但如何进一步提高数据的多样性和质量，以增强模型的泛化能力，仍是一个挑战。\n高频动作控制的稳定性 ：在高频率控制任务中，如何确保模型生成的动作序列的稳定性和准确性，需要进一步研究。\n模型的可扩展性和部署 ：如何将该模型部署到实际的机器人系统中，并确保其在不同硬件平台上的性能和效率，是实现其实际应用的关键。\nFAST: Efficient Action Tokenization for Vision-Language-Action Models (Chunking) 主要论点 传统的VLA模型在处理连续的机器人动作信号时，通常采用逐维、逐时间步的简单分箱（binning）策略进行离散化。然而，这种方法在面对高频率、精细操作任务时表现不佳，主要原因在于连续动作之间的强相关性导致模型难以有效学习。\n为解决这一问题，作者提出了FAST（Frequency-space Action Sequence Tokenization）方法，其核心思想包括：\n离散余弦变换（DCT） ：将连续的动作序列转换到频域，捕捉动作信号的主要频率成分，从而减少时间上的冗余信息。（关键帧技术） 量化与字节对编码（BPE） ：对DCT系数进行量化，并采用BPE进行压缩，生成信息密度更高的离散动作标记序列。 此外，作者还推出了 FAST+ ，一个在100万个真实机器人动作轨迹上训练的通用动作标记器，能够适用于多种机器人类型和控制频率的动作序列\n创新点 频域压缩的动作离散化：首次将DCT应用于机器人动作序列的离散化，有效减少了时间上的冗余信息，提高了模型对高频率动作的学习能力。\n通用动作标记器FAST+ ：通过在大规模、多样化的机器人动作数据上训练，FAST+实现了对不同机器人平台和任务的广泛适应性，减少了对特定任务手工设计标记器的需求。\n显著提升训练效率**** ：与传统的扩散模型相比，采用FAST的自回归VLA模型在训练时间上减少了多达5倍，同时在多个任务上达到了相当甚至更优的性能。\n解决的难点 传统动作token推理慢 。\n无法处理高频控制任务和长序列建模 。\n提出 DCT + BPE 的动作频域压缩方法（FAST），大幅压缩动作序列长度，提升效率。\n预训练通用tokenizer（FAST+）跨机器人平台迁移能力强。\n还需要解决的难点 高频动作的精确重建 ：虽然FAST在压缩动作序列方面表现出色，但在某些需要高精度控制的任务中，如何确保压缩后的动作序列能够准确还原原始动作，仍需进一步研究。\n（可不可以设置不同的专家模型，交由模型来判断应该使用原始序列还是压缩后的序列）\n与其他模型架构的兼容性 ：FAST主要与自回归VLA模型结合使用，其在其他类型的模型架构（如非自回归模型）中的表现和适应性尚待探索。\n实时控制的延迟问题 ：在实际机器人控制中，动作的生成和执行需要满足实时性要求，FAST在实际部署中可能面临延迟带来的挑战。\nπ0.5: a Vision-Language-Action Model with Open-World Generalization (More data, multimodel) Basically, 更强的π0。\n更强的 model, 更多的数据\n更强的开放世界泛化能力\nOpenVLA: An Open-Source Vision-Language-Action Model (远程连接 + Chunking + Dino) 主要论点 OpenVLA 是一个拥有 70 亿参数的开源 VLA 模型，基于 Llama 2 语言模型，并结合了 DINOv2 和 SigLIP 的预训练视觉特征。该模型在 Open X-Embodiment 数据集中的 97 万个真实机器人操作轨迹上进行了训练，涵盖了多种机器人形态、任务和场景。OpenVLA 能够直接控制多种机器人，并通过参数高效的微调方法快速适应新的机器人配置\n创新点 开源性与可访问性 ：OpenVLA 是首个完全开源的 VLA 模型，提供了模型检查点、微调笔记本和 PyTorch 训练代码，支持在 Open X-Embodiment 数据集上进行大规模训练。\n融合视觉编码器：模型采用了融合 DINOv2 和 SigLIP 特征的视觉编码器，结合了空间和语义信息，以增强模型对视觉输入的理解能力。\n高效的动作离散化方法：OpenVLA 通过将连续的机器人动作映射到离散的标记上，并使用语言模型的分词器进行处理，提高了模型的训练和推理效率。\n强大的泛化能力 ：在 29 个任务和多种机器人形态上，OpenVLA 的任务成功率比封闭模型 RT-2-X（55B 参数）高出 16.5%，同时参数数量减少了 7 倍。\n解决的难点 缺乏可公开访问、端到端训练的开源VLA模型 。\n跨机器人统一控制策略训练成本高 。\n基于 Llama2 + DINOv2 构建的70亿参数模型开源并附完整工具链。\n使用统一token表示动作（通过BPE），促进模块化、可复用性和可迁移性。\n还需要解决的难点 对未见机器人形态的泛化能力有限 ：OpenVLA 在预训练数据中未包含的机器人形态上，零样本泛化能力有限，需要通过微调适应新的机器人配置。\n动作离散化的精度问题 ：尽管采用了高效的动作离散化方法，但在某些需要高精度控制的任务中，如何确保离散化后的动作能够准确还原原始动作，仍需进一步研究。\n实时控制的计算需求：在实际机器人控制中，动作的生成和执行需要满足实时性要求，OpenVLA 在实际部署中可能面临计算资源和延迟的挑战。\nHi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models (LLM分层) 主要论点 Hi Robot 系统采用了两层策略结构，分别对应心理学中丹尼尔·卡尼曼提出的“系统1”（快速、直觉反应）和“系统2”（慢速、深度推理）模型：\n高层策略（System 2） ：利用预训练的视觉-语言模型（VLM），对复杂的自然语言指令进行解析，结合视觉观察，生成一系列中间步骤的低层语言命令。这一层具备推理能力，能够处理多阶段任务，并根据用户的实时反馈进行调整。 低层策略（System 1） ：基于 π₀ 模型，执行高层策略生成的原子级命令。该层专注于具体动作的执行，如“抓取杯子”，并能根据实时的视觉和状态信息进行快速反应。 该系统在三种不同的机器人平台上进行了测试，包括单臂、双臂和双臂移动机器人，任务涵盖清理杂乱的桌面、制作三明治和杂货购物等，展示了其在处理复杂任务和适应用户反馈方面的能力。\n创新点 分层架构设计：通过将任务解析与动作执行分离，Hi Robot 能够更有效地处理复杂指令和动态反馈，提高了系统的灵活性和适应性。\n“自言自语”机制 ：高层策略在生成低层命令前，会进行内部的语言推理过程，类似于人类在执行复杂任务前的思考过程，增强了系统的推理能力。\n实时反馈整合：系统能够在任务执行过程中接受并理解用户的实时语言反馈，如“那不是垃圾”，并据此调整当前的行为策略。\n广泛的任务适应性 ：通过在多种机器人平台和任务上的测试，验证了该系统在处理多样化任务和环境中的泛化能力。\n解决的难点 单一层次VLA模型无法处理长时序、开放式自然语言指令 。\n缺乏对人类实时反馈的理解和响应能力 。\n引入分层策略（System 1/2）模拟人类推理流程，实现任务分解与动作控制解耦。\n支持语言实时中断和调整（如用户打断、重说等）。\n还需要解决的难点 高层与低层策略的协同：确保高层生成的命令能够被低层准确执行，尤其是在面对未见过的任务或环境时，仍需进一步优化两层之间的接口和协同机制。\n实时性与计算资源的平衡：高层策略的推理过程可能带来计算延迟，如何在保证系统实时响应的同时，维持高层策略的推理深度，是一个需要权衡的问题。\n数据多样性与泛化能力 ：尽管系统在多个任务上表现良好，但在面对极端或未见过的指令和环境时，其泛化能力仍需通过更多样化的数据训练进行提升。\nDiffusion Policy: Visuomotor Policy Learning via Action Diffusion (Diffusion Policy) 主要论点 传统的机器人策略学习方法在处理多模态动作分布和高维动作空间时常面临挑战。为此，作者提出了 Diffusion Policy ，将机器人视觉-动作策略表示为条件去噪扩散过程（Conditional Denoising Diffusion Process）\n该方法的核心思想是：\n训练阶段：在动作空间中添加噪声，并训练模型预测如何从噪声中恢复出原始动作序列。 推理阶段：从随机噪声开始，逐步去噪，生成符合当前视觉观察的动作序列。 通过这种方式，Diffusion Policy 能够有效建模多模态动作分布，适应高维动作空间，并在多个机器人操作任务中表现出色。\n创新点 多模态动作建模能力 ：Diffusion Policy 能够自然地处理多种可能的动作路径，适应复杂任务中的多样性。 高维动作空间适应性：通过在整个动作序列上进行建模，方法在处理高维动作空间时表现稳定，避免了传统方法在高维空间中的不稳定性。 稳定的训练过程：相比于传统的能量模型（Energy-Based Models），Diffusion Policy 避免了对归一化常数的估计，提升了训练的稳定性。 闭环控制机制：结合递归视野控制（Receding Horizon Control），实现了在任务执行过程中的动态重规划，提高了系统的鲁棒性。 解决的难点 能量模型训练不稳定、归一化常数难估计 。\n无法处理多模态动作空间（例如多个解法、连续空间）。\n用扩散模型建模动作序列的分布，天然适配多模态分布。\n在多个真实世界任务中验证其高鲁棒性与学习稳定性。\n还需要解决的难点 推理速度 ：由于扩散过程需要多步去噪，推理速度较慢，可能限制了在实时控制任务中的应用。\n与其他模型的集成 ：如何将 Diffusion Policy 与其他感知或语言模型有效集成，以处理更复杂的任务，仍需进一步研究。\nRT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE 主要论点 RT-1 模型的核心目标是实现一个通用的机器人控制策略，能够处理多种任务、对象和环境。为此，研究团队收集了一个包含 13 台机器人、历时 17 个月、涵盖 700 多个任务、共计 13 万个操作示例的大规模数据集。这些数据包括机器人在实际环境中执行任务的图像序列、自然语言指令和对应的动作序列。\nRT-1 的架构包括以下关键组件：\n图像编码器 ：使用预训练的 EfficientNet-B3 模型处理输入图像，并通过 FiLM 层融合语言指令，提取与任务相关的视觉特征。 TokenLearner 模块 ：对图像特征进行压缩，生成一组紧凑的 token，提高模型的推理效率。 Transformer 模型 ：接收图像和语言的 token 输入，输出离散化的动作 token，控制机器人的执行。 在训练过程中，RT-1 通过模仿学习（Imitation Learning）方法，从收集的示例中学习任务的执行策略。模型能够以每秒 3 次的频率进行闭环控制，直到任务完成或达到预设的时间步数。\n创新点 大规模多任务学习 ：RT-1 在一个包含 13 万个示例、700 多个任务的大规模数据集上进行训练，展示了 Transformer 架构在机器人控制中的强大能力。\n统一的输入输出表示 ：将图像、语言指令和动作统一表示为 token 序列，使得模型能够处理多模态输入，并生成相应的动作输出。\n高效的推理机制：通过 TokenLearner 模块对图像特征进行压缩，显著提高了模型的推理速度，满足实时控制的需求。\n强大的泛化能力 ：RT-1 在未见过的任务、环境和对象上表现出色，展示了其在零样本学习和迁移学习方面的潜力。\n解决的难点 大规模跨任务学习难以整合语言、视觉、动作三模态数据 。\n模型在未见任务和环境上的泛化能力弱 。\n在 130,000+ 真实机器人操作轨迹上训练的 Transformer 模型。\nTokenLearner + FiLM结构实现多模态融合，提升跨任务泛化。\n还需要解决的难点 对新任务的泛化能力有限：尽管 RT-1 在多种任务上表现良好，但在面对完全未见过的任务时，其泛化能力仍有待提升。\n对复杂操作的适应性：当前模型主要针对相对简单的操作任务，对于需要高精度和复杂操作的任务，其性能尚未验证。\n实时性与计算资源的平衡：虽然模型在推理速度上有所优化，但在资源受限的实际部署环境中，如何进一步提高实时性仍是一个挑战。\nOpen-TeleVision: Teleoperation with Immersive Active Visual Feedback 主要论点 Open-TeleVision 系统结合了 VR 设备与机器人控制，允许操作者通过 VR 头显实时感知机器人的立体视觉环境，并将自身的手臂和手部动作映射到机器人上，实现如同“身临其境”的操作体验。\n系统的核心组件包括：\n主动视觉反馈 ：机器人头部配备可动的立体 RGB 摄像头，能够根据操作者的头部运动调整视角，提供实时的第一人称3D 观察。 动作映射机制 ：通过逆运动学（IK）算法和 dex-retargeting 技术，将操作者的手部关键点转换为机器人关节角度，实现精确的动作控制。 远程操作能力 ：系统支持通过互联网进行远程控制，操作者无需与机器人处于同一地点。 在实验中，研究团队使用该系统在两个不同的人形机器人（Unitree H1 和 Fourier GR-1）上完成了包括罐头分类、罐头插入、毛巾折叠和物品卸载等四项长时序、精细操作任务，并成功部署了模仿学习策略。\n创新点 沉浸式第一人称视角：通过主动立体视觉技术，操作者能够以第一人称视角直观感知机器人周围环境，增强了空间感知能力和操作**直觉。\n高精度动作映射：结合逆运动学和 dex-retargeting 技术，实现了操作者动作到机器人动作的高精度映射，支持多指灵巧手的控制。\n远程操作与数据采集 ：系统支持远程操作，操作者可以跨地域控制机器人，并收集高质量的操作数据，促进模仿学习的发展。\n解决的难点 传统远程遥操作缺乏空间沉浸感与操作精度 。\n低质量遥操作数据不利于模仿学习 。\n主动式立体视觉反馈 + VR 映射手部运动实现高保真远程控制。\n为高质量模仿学习数据采集（精细双臂、多指操作）提供有效工具。\n还需要解决的难点 my Questions 如何评估一个机械臂的能力呢？ 高频动作，怎么样算高频呢？\n为什么说机器的精度很重要，到底有多重要呢？\n输出给机器臂的值可以是连续的而不是离散的吗？对应 pi 0\n在不同机器人上训练的难点是？\n为什么需要端到端模型呢？\n机器人上的设备能支持多大的模型运行呢？\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-14/","summary":"Summary of Robotics papers","title":"Bug Journal 2025-05-25"},{"content":" 如何控制机器人的位置？机器人控制算法的输出是什么？是机械臂的位置，速度，加速度的值吗。另外，动作分布 50Hz 的意思是？机械臂的移动是一个分布吗 多模态数据对齐的时候有什么难点，需要网络有什么样的特性呢？ 一般来说泛化能力是如何实现的，加入噪声吗？ 想法：可不可以用视频数据做一个增强：比如用 LLM 总结视频里的手都做了什么，然后以这个总结为 prompt 告诉机械臂要做什么\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-09/","summary":"遇到的问题","title":"Bug Journal 2025-05-09"},{"content":"Citation Content mainly from here: 具身智能基础技术路线.\nPart1: 场景理解 这一部分主要是机器人对环境输入的理解\n希望让机器人识别出环境中比较重要的部分， 比如：要抓起的物件\n检测分割 检测分割的算法如：SAM, Open-VOC Detection 可以比较好地检测和分割输入中需要的部分和其他部分\n多模态分割 多模态大模型可以理解更加复杂的语言，比如“汽车旁的穿蓝色衣服的男人”， 并且可以做到像素级别。\nprompt 的多样化可以让分割更有针对性，模型可以更好地理解和定位\nPart2: 数据收集和引导 视频学习 优点：不需要遥控，数据量更广\n缺点：需要让机器学习视频中的动作，训练难度更高\nVR遥控 优点：可以让人手模拟，并且可以“手把手”教\n缺点：数据量少，需要每一种新情况都需要手动模拟\n环境模拟 优点：环境搭建完成之后就有无限的数据集\n缺点：环境搭建复杂，并且无法模拟特殊情况\n动作执行 生成式模仿学习 把每一个时间点的信息丢进去学习\nDiffusion policy 在每一个时间点之间做 diffusion\nAffordance 输入每一个 region可以被如何操作：比如输入一个瓶子可以被夹起来\n这样的话就不再是low level joint, 而是控制器实现的目标\nQ\u0026amp;A from LLM 将 prompt 拆解，并且可以在文本中提取出机器人最后要做的事情\nLanguage Correction 在人类观察的时候用语言/语音帮助机器人更好地完成任务\n世界模型 预测下一步会发生什么\n我对具身智能的理解 实际上是一个 Agent, 输入是用户的指令以及整个环境。\n输出是对环境做出一些改变，比如让机器人举起一个杯子。\n所以有这些需要做的：识别输入，做出反应，强化学习\nRandom Thoughts 新的LLM生成方式：token -\u0026gt; 大纲 -\u0026gt; 句子 -\u0026gt; 文章 pyramid\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-08/","summary":"初识具身智能","title":"Bug Journal 20250508"},{"content":"Talk 6: scDesign3: Semi-synthetic Negative \u0026amp; Positive Control 数据不够的时候需要一些 simulated data\n那这时候 simlator 就需要 interpretable \u0026amp; realistic (real data characteristic \u0026amp; contains ground truth data)\ne.g. single cell RNA simulator -\u0026gt; 考虑 gene gene correlation -\u0026gt; 考虑别的 cell types and omics -\u0026gt; RNA count to RNA read\n这个可以用来给数据预处理+降噪\nTalk 7: spacial omics data 理解 low-rank property of Hi-C chromatin contact maps.\n数据上有 3 维（question what type of data / or do you have ground truth of the location of the spatial data? are the model predicting the location or the gene expression or both of them? ）\n但是问题是这个数据很 sparce. e.g. 5kb 的数据就已经是丢失了 99.5%的数据了\n所以就用了一些技术来降噪\u0026amp;还原\n之前的 AI模型：训练完了之后就没有用生物相关的信息来预测我们想要的\n现在的 AI 模型，可以自己思考来相处一个 hypothsis，并且验证他\ne.g. 用 interatctions\n要解决的问题：\nblack box ML models, overwhelming hypothesis, false positive\nML model -\u0026gt; intreatcion 可能的值\n现在提出一个预测的可能的可行性\n这个值是 false positive / total accept 值越低越好\n那如何预测 false positive 呢？这样，对于每一个 feature, 分开预测\n然后再liangliang 预测，然后看结合了之后是好了还是坏了\n还有一个办法，就是用元数据 + 假数据\n如果元数据和假数据得出来的差距大，那就说明这个元数据没什么用\n否则就说明这个有用\nIntegration of histology and multi-plex for understanding pancreatic cancer.\nimage are usually 2D, that is not connected to other tissues.=m telling us there is not spatial relationship.\nquestion is every slice a slice near another.\nSpacial transcriptomics platforms (PASTA))\n太稀疏啦\n怎么办呢？用 model concentrate 喽\n但是这些 model 并不一定准（你想，要用 500gene去补 20k,太难了）\n并且这些 model 没有用 cell type / pathway 信息\npathway loss + gene similarity + pathway expression + location (这不就是 STHD吗)\n但是这里预测的不是 cell type,是 gene expression\nTalk 8: AI in medication AI 可以很好的数据，但是要求很高\n所以要用很聪明的方法使用这些 AI\n另外，预测出了结果之后要做什么呢？\nAI 还可以用来提供数据\n生成式 AI VAE \u0026amp; Transformer (需要有 meaningful order/sequence)\nDiffusion model: forward (加噪音) and backward (去噪音) （不需要 data 是 ordered, 但是 sampling 需要很多计算资源）\n如果 black box prediction model 不正确，我应该如何 make a valid inference?\nEnd to end scalable integrative analysis ","permalink":"https://tzj2006.github.io/bugjournal/2025-03-28/","summary":"\u003ch1 id=\"talk-6-scdesign3-semi-synthetic-negative--positive-control\"\u003eTalk 6: scDesign3: Semi-synthetic Negative \u0026amp; Positive Control\u003c/h1\u003e\n\u003cp\u003e数据不够的时候需要一些 simulated data\u003c/p\u003e\n\u003cp\u003e那这时候 simlator 就需要 interpretable \u0026amp; realistic (real data characteristic \u0026amp; contains ground truth data)\u003c/p\u003e\n\u003cp\u003ee.g. single cell RNA simulator -\u0026gt; 考虑 gene gene correlation -\u0026gt; 考虑别的 cell types and omics -\u0026gt; RNA count to RNA read\u003c/p\u003e\n\u003cp\u003e这个可以用来给数据预处理+降噪\u003c/p\u003e\n\u003ch1 id=\"talk-7-spacial-omics-data\"\u003eTalk 7: spacial omics data\u003c/h1\u003e\n\u003cp\u003e理解 low-rank property of Hi-C chromatin contact maps.\u003c/p\u003e\n\u003cp\u003e数据上有 3 维（question what type of data / or do you have ground truth of the location of the spatial data? are the model predicting the location or the gene expression or both of them? ）\u003c/p\u003e","title":"Bug Journal 2025-03-28"},{"content":"MCBIOS Conference Day 1:\nTalk 1: sharing data: https://datacommons.cancer.gov/ 介绍了很多数据：特点是比较多，比较新，并且user-friendly. 还有 NIH founding 可以 use start-up server.\nhttps://computational.cancer.gov/ 介绍了很多模型：都是用来预处理数据的\n比如有一个 AI-based toolbox (类似scanpy) 可以预处理所有数据\n还有 Automated Data Collection\nlink TBD\nLLM 翻译诊断结果\nTalk 2: Write code with Github Copilot. 你能用这个干嘛 当然是写代码啦，还能干嘛（\n好处是可以直接在你的 IDE 里面生成(虽然现在 ChatGPT.app 也可以了，(反正都是一家的[doge])\n比如说你可以先写一段注释来让 Copilot 生成你想要的代码\n然后选中这段代码并且点击旁边的小星星来让 Copilot 更改这段代码\n用于重复的项目效果更佳。比如分离数据集\n修bug还挺好用的(虽然有时候越修越多[doge])\n修改代码的语言：比如把 R code 换成 python code.\n还可以 explain what the code is doing (by using /explain).\n用 /doc 来写注释\n要不要用这个 想用就用，只是这个效果不一定好罢了[doge]\nTalk 3: Graph is a link between spatial omics applications and pixel graph | cell graph | spot graph\n有 graph 就有 matrix\nfrequency 的大小决定了是否 pattern（？）\nspatial variable gene thereshold\nTalk 4: Some random ideas 结合两个 LLM, e.g. Gemini \u0026amp; ChatGPT\n在小程序中内置一个 LLM 让它当 agent.\nTalk 5: scPerb style vector 可以说是一个 noise, 然后我们希望一个 neural network 能学习到这个 noise. 并且这个学习的过程是 cell type specific 的。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-03-27/","summary":"\u003cp\u003eMCBIOS Conference Day 1:\u003c/p\u003e\n\u003ch1 id=\"talk-1-sharing-data\"\u003eTalk 1: sharing data:\u003c/h1\u003e\n\u003ch2 id=\"httpsdatacommonscancergov\"\u003e\u003ca href=\"https://datacommons.cancer.gov/\"\u003ehttps://datacommons.cancer.gov/\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e介绍了很多数据：特点是比较多，比较新，并且user-friendly. 还有 NIH founding 可以 use start-up server.\u003c/p\u003e\n\u003ch2 id=\"httpscomputationalcancergov\"\u003e\u003ca href=\"https://computational.cancer.gov/\"\u003ehttps://computational.cancer.gov/\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e介绍了很多模型：都是用来预处理数据的\u003c/p\u003e\n\u003cp\u003e比如有一个 AI-based toolbox (类似scanpy) 可以预处理所有数据\u003c/p\u003e\n\u003cp\u003e还有 Automated Data Collection\u003c/p\u003e\n\u003cp\u003elink TBD\u003c/p\u003e\n\u003cp\u003eLLM 翻译诊断结果\u003c/p\u003e\n\u003ch1 id=\"talk-2-write-code-with-github-copilot\"\u003eTalk 2: Write code with Github Copilot.\u003c/h1\u003e\n\u003ch2 id=\"你能用这个干嘛\"\u003e你能用这个干嘛\u003c/h2\u003e\n\u003cp\u003e当然是写代码啦，还能干嘛（\u003c/p\u003e\n\u003cp\u003e好处是可以直接在你的 IDE 里面生成(虽然现在 ChatGPT.app 也可以了，(反正都是一家的[doge])\u003c/p\u003e\n\u003cp\u003e比如说你可以先写一段注释来让 Copilot 生成你想要的代码\u003c/p\u003e\n\u003cp\u003e然后选中这段代码并且点击旁边的小星星来让 Copilot 更改这段代码\u003c/p\u003e\n\u003cp\u003e用于重复的项目效果更佳。比如分离数据集\u003c/p\u003e\n\u003cp\u003e修bug还挺好用的(虽然有时候越修越多[doge])\u003c/p\u003e\n\u003cp\u003e修改代码的语言：比如把 R code 换成 python code.\u003c/p\u003e\n\u003cp\u003e还可以 explain what the code is doing (by using /explain).\u003c/p\u003e","title":"Bug Journal 2025-03-27"},{"content":"Today\u0026rsquo;s Problem https://leetcode.com/problems/check-if-grid-can-be-cut-into-sections\nIntuition This question asks about merging sections. In this case, if we smash it into 1D array, it just means \u0026ldquo;Is there more than two gaps inside the section?\u0026rdquo;\nApproach Therefore, we can sort the list, and then iterate the whole list and see whether there is a gap between the section we already iterated and the new section. If there is, then add 1, else, merge this new section to our old section. The thing is that you need to do it twice.\nComplexity Time complexity: $O(N)$, N is the length of rectangles. Space complexity: $O(1)$. Code class Solution: def checkValidCuts(self, n: int, rectangles: List[List[int]]) -\u0026gt; bool: N = len(rectangles) def get_res(a,b): rectangles.sort(key = lambda x: (x[a], x[b])) gapCnt,maxPos,l = 0,1,0 while(l \u0026lt; N): while(l \u0026lt; N and rectangles[l][a] \u0026lt; maxPos): maxPos = max(maxPos, rectangles[l][b]) l += 1 if l == N: break else: gapCnt += 1 maxPos = rectangles[l][b] # print(a,l) if gapCnt \u0026gt; 1: return True return False return get_res(0,2) or get_res(1,3) ","permalink":"https://tzj2006.github.io/leetcode/2025-03-25/","summary":"\u003col start=\"3394\"\u003e\n\u003cli\u003eCheck if Grid can be Cut into Sections\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-03-25"},{"content":"Intuition In this question, we find that if we flip a coin twice, then it is the same as flipping it zero times. Moreover, the only three ways to flip a coin is: Flip this coin Flip the coin before Flip the coin before this coin This means that if we pass this coin, we can no longer flip this coin. Approach In this case, we can iterate from front to end and flip every coin that is 0, and check whether the whole array is 1 at the end. Why is this method correct then? First we know that we cannot flip a coin after we pass this coin.\nThis means that we must flip this coin. If we do not flip this coin, then this coin will remain 0, which does not satisfy the quetion. Therefore, this step is required, missing this step will not give us the array we need.\nWe can also prove that this will lead us to the result for every array that can achieve this step. Because you have to flip this coin no matter what operation you did.\nComplexity Time complexity: $O(N)$, N is the length of the array. Space complexity: $O(1)$. Code class Solution: def minOperations(self, nums: List[int]) -\u0026gt; int: cnt = 0 for i in range(len(nums) - 2): if nums[i] == 0: cnt += 1 nums[i] = 1 nums[i + 1] = 1 - nums[i + 1] nums[i + 2] = 1 - nums[i + 2] if nums[-1] == 1 and nums[-2] == 1: return cnt return -1 ","permalink":"https://tzj2006.github.io/leetcode/2025-03-19/","summary":"\u003col start=\"3191\"\u003e\n\u003cli\u003eMinimum Operations to Make Binary Array Elements Equal to One I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-03-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-punishment-number-of-an-integer/description/\nIntuition Note that all the combinations of a number that is less than $1000^2$ is $2^5 = 32$. Which means that if we use a dfs to decide whether to choose to break from one interval or not cost at most 32 for one number.\nSince we only have $N \\leq 1000$, we can solve it with brute method.\nApproach Use a dfs to check whether it cawn be a punishment number or not.\nComplexity Time complexity: $O(N\\times 2^{log(N)})$, N is the same representation as the description.\nSpace complexity: $O(1)$.\nCode class Solution: def punishmentNumber(self, n: int) -\u0026gt; int: def check(x, now, s, nows, cnt): if now == 0: return (s + nows) == x if s \u0026gt; x: return False flag = check(x, now // 10, s, nows + now % 10 * (10 ** cnt), cnt + 1) if flag: return True flag |= check(x, now // 10, s + nows, now % 10, 1) return flag ans = 0 for i in range(n + 1): if check(i, i*i, 0, 0, 0): ans += i * i return ans class Solution: def punishmentNumber(self, n: int) -\u0026gt; int: punishmentNumbers = [0, 1, 9, 10, 36, 45, 55, 82, 91, 99, 100, 235, 297, 369, 370, 379, 414, 657, 675, 703, 756, 792, 909, 918, 945, 964, 990, 991, 999, 1000] ans = 0 for x in punishmentNumbers: if x \u0026gt; n: return ans ans += x * x return ans Result of normal solution: Result of fastest solution: ","permalink":"https://tzj2006.github.io/leetcode/2025-02-15/","summary":"\u003col start=\"2698\"\u003e\n\u003cli\u003eFind the Punishment Number of an Integer\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-15"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/product-of-the-last-k-numbers/description/\nIntuition Situation 1: if there is no 0: In this case, we can just use a prefix multiplication to do what the question asks. Situation 2: if there is 0: Then it would be 0! Therefore, all we need is to just check whether there is a 0 in the last k numbers of the stream. If yes, them just return 0.\nApproach Use an array to store the multiplication prefix. Check whether there is a zero in the last k streams. Complexity Time complexity: $O(Q)$, Q means the number of operations.\nSpace complexity: $O(Q)$, Q means the number of operations.\nCode class ProductOfNumbers: def __init__(self): self.q = [] self.mul = 1 def add(self, num: int) -\u0026gt; None: self.mul *= num self.q.append(self.mul) if num == 0: self.q = [] self.mul = 1 def getProduct(self, k: int) -\u0026gt; int: if k \u0026gt; len(self.q): return 0 if k == len(self.q): return self.mul return self.mul // self.q[-k - 1] # Your ProductOfNumbers object will be instantiated and called as such: # obj = ProductOfNumbers() # obj.add(num) # param_2 = obj.getProduct(k) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-14/","summary":"\u003col start=\"1352\"\u003e\n\u003cli\u003eProduct of the Last K Numbers\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-14"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-operations-to-exceed-threshold-value-ii/description\nIntuition Do what the question ask using heap.\nApproach Get the minimum two from the list using heap. Put back value $2 * min(x, y) + max(x, y)$ to the heap. If the value you get is all larger or equal to k, then it is all done. Complexity Time complexity: $O(N\\times log(N))$, N is the length of the sequence.\nSpace complexity: $O(1)$, by using heapify, there is no external storage.\nCode class Solution: def minOperations(self, nums: List[int], k: int) -\u0026gt; int: ans = 0 heapify(nums) x = heappop(nums) while(len(nums) \u0026gt; 0 and x \u0026lt; k): y = heappop(nums) nx = min(x, y) * 2 + max(x, y) heappush(nums, nx) x = heappop(nums) ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-13/","summary":"\u003col start=\"3066\"\u003e\n\u003cli\u003eMinimum Operations to Exceed Threshold Value II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-13"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/max-sum-of-a-pair-with-equal-sum-of-digits/description\nIntuition In this question, we can simulate the process described in the question, and then get the answer.\nApproach First write a count function that counts the sum of every integer. Use a dictionary to store the top two values. Compare the current number with two numbers stored in the dictionary. Update ans (initialized by -1), note that we need at least two numbers in the dictionary before we can update the answer. Complexity Time complexity: $O(Nlog(M))$, N is the length of the sequence, M is the maximum number.\nSpace complexity: $O(N)$.\nCode class Solution: def maximumSum(self, nums: List[int]) -\u0026gt; int: def cnt(x): res = 0 while(x \u0026gt; 0): res += x % 10 x //= 10 return res dic = dict() ans = -1 for x in nums: nx = cnt(x) if nx in dic: if x \u0026gt; dic[nx][0]: dic[nx][1] = dic[nx][0] dic[nx][0] = x if dic[nx][1] \u0026gt; 0: ans = max(ans, dic[nx][0] + dic[nx][1]) elif x \u0026gt; dic[nx][1]: dic[nx][1] = x ans = max(ans, dic[nx][0] + dic[nx][1]) else: dic.update({nx: [x, 0]}) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-12/","summary":"\u003col start=\"2342\"\u003e\n\u003cli\u003eMax Sum of a Pair With Equal Sum of Digits\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/remove-all-occurrences-of-a-substring/description/\nIntuition In this case, we need to delete all the occurance of \u0026ldquo;part\u0026rdquo; in \u0026ldquo;s\u0026rdquo;. Therefore, we can check whether \u0026ldquo;s\u0026rdquo; contains \u0026ldquo;part\u0026rdquo;. Then we can delete it from the string.\nApproach Method 1. You can use a stack to do that. When you detect that your stack input a string that is the same to \u0026ldquo;part\u0026rdquo;, then we can delete the string from the stack.\nMethod 2. You can use the python function to find and delete \u0026ldquo;part\u0026rdquo; from the original string S.\nComplexity Time complexity: $O(N\\times M)$, N is the length of s, M is the length of part.\nSpace complexity: $O(1)$\nCode class Solution: def removeOccurrences(self, s: str, part: str) -\u0026gt; str: st = [] N = len(part) for ch in s: st.append(ch) if len(st) \u0026gt;= N: flag = True for i in range(1, N + 1): if st[-i] != part[-i]: print(st[-i], part[-i]) flag = False break if flag: for i in range(N): st.pop() return \u0026#39;\u0026#39;.join(st) Real Python class Solution: def removeOccurrences(self, s: str, part: str) -\u0026gt; str: while part in s: s = s.replace(part,\u0026#34;\u0026#34;,1) return s ","permalink":"https://tzj2006.github.io/leetcode/2025-02-11/","summary":"\u003col start=\"1910\"\u003e\n\u003cli\u003eRemove All Occurrences of a Substring\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-11"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/clear-digits/description/\nIntuition We need to pop the character before the digit in this question.\nApproach Therefore, all we need is just to utilize a stack.\nComplexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, N is the length of the string.\nCode class Solution: def clearDigits(self, s: str) -\u0026gt; str: st = [] for ch in s: if \u0026#39;0\u0026#39; \u0026lt;= ch and ch \u0026lt;= \u0026#39;9\u0026#39;: st.pop() else: st.append(ch) return \u0026#39;\u0026#39;.join(st) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-10/","summary":"\u003col start=\"3174\"\u003e\n\u003cli\u003eClear Digits\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-10"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-number-of-bad-pairs/description/\nIntuition In this question, we find that we need to find the pairs that has the same distance as the difference of nums.\nIn this case, if we distract the distance between these two numbers, then they would be the same.\nThat is: $j - i = nums[j] - nums[i] \\Longrightarrow nums[j] - j = nums[i] = i$.\nThen the question would be easy: we just subtract the index of every number in the list, and then found how many pairs of i,j in the nums array that has the same number.\nWe then subtract these counts from the total counts of answer.\nApproach Count all pairs of i,j; that is, $N \\times (N - 1)$, N is the length of the array. Subtract all the nums[i] by i. Count how many pairs of i,j has the same number. Subtract these i,j pairs from the original answer. Complexity Time complexity: $O(N)$. N is the length of the array.\nSpace complexity: $O(N)$. N is the length of the array.\nCode class Solution: def countBadPairs(self, nums: List[int]) -\u0026gt; int: nums = [nums[i] - i for i in range(len(nums))] cnt = Counter(nums) N = len(nums) ans = N * (N - 1) // 2 for v in cnt.values(): ans -= v * (v - 1) // 2 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-09/","summary":"\u003col start=\"2364\"\u003e\n\u003cli\u003eCount Number of Bad Pairs\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-09"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/design-a-number-container-system/description/\nIntuition In this question, we need to find the smallest index of the number in the list. Note that the index may change when dealing with the list. Because we need the smallest index, so we need a sorted datastructure. Trick: We can get a lazy tag that stores the number of each index, so that we can pop only when we find our current answer does not fullfill the requirement.\nApproach Store a dictionary that stores the sorted sequence of the numbers and the index. Store a dictionary of index and numbers pair. Check whether the answer is valid in the find function. Complexity Time complexity: $O(Q\\ times log(N))$, Q is the time of query, N is the size of the dictionary.\nSpace complexity: $O(Q)$, Q is the time of query.\nCode class NumberContainers: def __init__(self): self.lst = dict() self.idx = dict() def change(self, index: int, number: int) -\u0026gt; None: if number not in self.lst: self.lst.update({number: []}) heappush(self.lst[number], index) self.idx.update({index: number}) def find(self, number: int) -\u0026gt; int: if number not in self.lst: return -1 while self.lst[number]: currIndex = self.lst[number][0] if self.idx[currIndex] != number: heappop(self.lst[number]) else: return currIndex return -1 # Your NumberContainers object will be instantiated and called as such: # obj = NumberContainers() # obj.change(index,number) # param_2 = obj.find(number) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-08/","summary":"\u003col start=\"2349\"\u003e\n\u003cli\u003eDesign a Number Container System\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-number-of-distinct-colors-among-the-balls/description\nIntuition All we need to know is how many colors left in the list. Then we can store how many balls do one color have. If there are 0 balls left, then col_cnt -= 1. If there appears a new color, then col_cnt += 1.\nApproach Store a buket for every ball and every color. Change the color of one ball. If there are 0 balls left, then col_cnt -= 1. If there appears a new color, then col_cnt += 1. Complexity Time complexity: $O(N)$, N is the length of the query.\nSpace complexity: $O(N)$, N is the length of the query.\nPotential follow up question Now I want to change the color of a section? For example, now the imput change into (x,y,z), changing the color of the balls from x to y (inclusive) to z. Then tell me how many balls have distinct colors?\nCode class Solution: def queryResults(self, limit: int, queries: List[List[int]]) -\u0026gt; List[int]: col = 0 ans = [] visCol = dict() balCol = dict() for (x, y) in queries: if x in balCol: visCol[balCol[x]] -= 1 if visCol[balCol[x]] == 0: col -= 1 balCol.update({x: y}) if (y not in visCol) or (visCol[y] == 0): col += 1 visCol.update({y: 1}) else: visCol[y] += 1 ans.append(col) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-07/","summary":"\u003col start=\"3160\"\u003e\n\u003cli\u003eFind the Number of Distinct Colors Among the Balls\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-07"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/tuple-with-same-product/description/\nIntuition In this question, all we need to do is to find the tuple that the numbers in the tuple has the same multiplication value. For example, if there are n pairs of numbers that has the same multiplication value, then the result would be $(^2_n) \\times 4$. This is because we can pick any two pair from the set and form a tuple.\nThe question was, why can you prove that this two pairs are distinct?\nThis is because the original array is distinct. This means that there are no duplicated numbers in the original array. Therefore, if $a \\times b = c \\times d$, we know that $a \\ne c$, then we can now that $b \\ne d$.\nApproach Therefore, all we need to do is to iterate through the whole list and find all tuples that has the same multiplication.\nTrick Among four dictionaries, defaultdict, Counter, dict, and {}, dict has the fastest speed.\nComplexity Time complexity: $O(N ^ 2)$, N is the length of the array.\nSpace complexity: $O(N)$, we need to store the whole array.\nCode class Solution: def tupleSameProduct(self, nums: List[int]) -\u0026gt; int: nums.sort() cnt = dict() N = len(nums) for i in range(N): for j in range(i + 1, N): tmp = nums[i] * nums[j] if tmp not in cnt: cnt[tmp] = 1 else: cnt[tmp] += 1 ans = 0 for v in cnt.values(): ans += 4 * (v) * (v - 1) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-06/","summary":"\u003col start=\"1726\"\u003e\n\u003cli\u003eTuple with Same Product\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-06"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-one-string-swap-can-make-strings-equal/description\nIntuition Do what the question ask.\nApproach Do what the question ask.\nCheck that whether there are 0 or exactly 2 position that is different. Check that whether swapping can solve the question. Complexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(1)$.\nCode class Solution: def areAlmostEqual(self, s1: str, s2: str) -\u0026gt; bool: fst = -1 sec = -1 for i in range(len(s1)): if s1[i] != s2[i]: if fst == -1: fst = i elif sec == -1: sec = i else: return False if fst == -1 and sec == -1: return True if fst == -1 or sec == -1: return False if s1[fst] == s2[sec] and s1[sec] == s2[fst]: return True return False ","permalink":"https://tzj2006.github.io/leetcode/2025-02-05/","summary":"\u003col start=\"1790\"\u003e\n\u003cli\u003eCheck if One String Swap Can Make Strings Equal\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-05"},{"content":". \u0026ndash;\u0026gt; find all characters except \u0026lsquo;\\n\u0026rsquo;.\n^ \u0026ndash;\u0026gt; find all the start of the string\n$ \u0026ndash;\u0026gt; find all the end of the string.\n[\u0026hellip;] find a character inside the [].\n[^] find characters not inside the [].\n\\. find the special characters.\n\\d all numbers \u0026lt;=\u0026gt; [0-9]\n\\D all not numbers \u0026lt;=\u0026gt; [^0-9]\n\\s all space characters\n\\S all not space characters\n\\w all letter, num, and . [a-z-A-Z0-9].\n\\W all not letter num and _.\n\\b find side.\nfind find at least once ? find 0 times or once.\n{n} exact n times.\n{n,} at least n times.\n{n, m} at least n times, at most m times.\n| mean or.\n(\u0026hellip;) find all.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-02-04/","summary":"\u003cp\u003e. \u0026ndash;\u0026gt; find all characters except \u0026lsquo;\\n\u0026rsquo;.\u003c/p\u003e\n\u003cp\u003e^ \u0026ndash;\u0026gt; find all the start of the string\u003c/p\u003e\n\u003cp\u003e$ \u0026ndash;\u0026gt; find all the end of the string.\u003c/p\u003e\n\u003cp\u003e[\u0026hellip;] find a character inside the [].\u003c/p\u003e\n\u003cp\u003e[^] find characters not inside the [].\u003c/p\u003e\n\u003cp\u003e\\. find the special characters.\u003c/p\u003e\n\u003cp\u003e\\d all numbers \u0026lt;=\u0026gt; [0-9]\u003c/p\u003e\n\u003cp\u003e\\D all not numbers \u0026lt;=\u0026gt; [^0-9]\u003c/p\u003e\n\u003cp\u003e\\s all space characters\u003c/p\u003e\n\u003cp\u003e\\S all not space characters\u003c/p\u003e\n\u003cp\u003e\\w all letter, num, and \u003cem\u003e. [a-z-A-Z0-9\u003c/em\u003e].\u003c/p\u003e","title":"Bug Journal 2025-02-04"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-ascending-subarray-sum/\nIntuition Same as the problem yesterday, the only difference is changing count to sum.\nApproach Same as the problem yesterday, the only difference is changing count to sum.\nComplexity Time complexity: $O(N)$, N is the length of the array.\nSpace complexity: $O(1)$\nCode class Solution: def maxAscendingSum(self, nums: List[int]) -\u0026gt; int: ans, tmp, pre = nums[0], nums[0], nums[0] for num in nums[1::]: if num \u0026gt; pre: tmp += num else: ans = max(ans, tmp) tmp = num pre = num return max(tmp, ans) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-04/","summary":"\u003col start=\"1800\"\u003e\n\u003cli\u003eMaximum Ascending Subarray Sum\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-04"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/longest-strictly-increasing-or-strictly-decreasing-subarray/\nIntuition Do what the question ask.\nApproach Do what the question ask.\nComplexity Time complexity: $O(N)$, N is the length of the array.\nSpace complexity: $O(1)$.\nCode class Solution: def longestMonotonicSubarray(self, nums: List[int]) -\u0026gt; int: cntI, cntD = 1,1 ans = 1 pre = nums[0] for x in nums[1::]: if x \u0026gt; pre: cntI += 1 ans = max(ans, cntD) cntD = 1 elif x \u0026lt; pre: cntD += 1 ans = max(ans, cntI) cntI = 1 else: ans = max(ans, cntI, cntD) cntI = 1 cntD = 1 pre = x return max(ans, cntI, cntD) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-03/","summary":"\u003col start=\"3105\"\u003e\n\u003cli\u003eLongest Strictly Increasing or Strictly Decreasing Subarray\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-03"},{"content":"Why not use Apple to run LLM: https://github.com/user-attachments/assets/e03bd9e6-0174-44b0-8c99-9a1ab88eeef2\n","permalink":"https://tzj2006.github.io/bugjournal/2025-02-02/","summary":"Speed of M4 Pro","title":"Bug Journal 2025-02-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-array-is-sorted-and-rotated/description/\nIntuition If it is shifted, then it must contain a full original list if we concate two nums together.\nTrick Append the original list to the back of itself it is somehow a ring.\nApproach Concat nums at the end of itself. Then test whether there are at least N non decreasing numbers. Complexity Time complexity: $O(N)$, N is the length of the array\nSpace complexity: $O(1)$\nCode class Solution: def check(self, nums: List[int]) -\u0026gt; bool: n = len(nums) nums += nums cnt = 1 pre = nums[0] for num in nums[1::]: if num \u0026gt;= pre: cnt += 1 else: cnt = 1 pre = num if cnt \u0026gt;= n: return True return False ","permalink":"https://tzj2006.github.io/leetcode/2025-02-02/","summary":"\u003col start=\"1752\"\u003e\n\u003cli\u003eCheck if Array Is Sorted and Rotated\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/divide-nodes-into-the-maximum-number-of-groups/description/\nIntuition The key of this question is $|y - x| = 1$. We know that for a step further, the parity must change. Therefore, when we encounter a case when the parity has a problem (for example, a loop with three nodes and three edges), we would return -1. Otherwise, all we can do is to iterate the starting point of the graph to find which point is the best starting point.\nTherefore, we can have our approach:\nWe can use a dfs to find whether there is a parity issue or not. We can use a bfs to find the starting point. Questions: why using bfs to find the starting point?\nThis is because we need to add a point to the next group if it has an edge connecting the current point and the next point.\nApproach We can use a dfs to find whether there is a parity issue or not. We can use a bfs to find the starting point. Complexity Time complexity: $O(N^2)$\nSpace complexity: $O(N^2)$\nCode class Solution: def magnificentSets(self, n: int, edges: List[List[int]]) -\u0026gt; int: vis = [0] * (n + 1) bvis = [0] * (n + 1) e = [[] for _ in range(n + 1)] for x,y in edges: e[x].append(y) e[y].append(x) ans = 0 clock = 0 def bfs(x): nonlocal clock clock += 1 bvis[x] = clock q = deque([(x,1)]) res = 1 while(len(q) \u0026gt; 0): now, dis = q.popleft() res = max(res, dis) for to in e[now]: if bvis[to] == clock: continue bvis[to] = clock q.append((to, dis + 1)) return res cur = 0 def dfs(x): nonlocal cur cur = max(cur, bfs(x)) # print(cur) tmp = 0 for to in e[x]: if vis[to] == 0: if vis[x] == 0: print(\u0026#34;Warning!\u0026#34;, x) vis[to] = -vis[x] tmp += dfs(to) else: if vis[to] != -vis[x]: return -1 return tmp for i in range(1, n + 1): if vis[i] == 0: cur = 0 vis[i] = 1 if dfs(i) \u0026lt; 0: return -1 # print(\u0026#34;out: \u0026#34;, cur) ans += cur # print(bfs(5)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-30/","summary":"\u003col start=\"2493\"\u003e\n\u003cli\u003eDivide Nodes Into the Maximum Number of Groups\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-30"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/special-array-i/description/\nIntuition Do what the question ask.\nApproach Iterate through the array, then check whether the two number has the are both odd or even or not.\nComplexity Time complexity: $O(N)$\nSpace complexity: $O(1)$\nCode class Solution: def isArraySpecial(self, nums: List[int]) -\u0026gt; bool: for i in range(1, len(nums)): if (nums[i] - nums[i-1]) % 2 == 0: return False return True ","permalink":"https://tzj2006.github.io/leetcode/2025-02-01/","summary":"\u003col start=\"3151\"\u003e\n\u003cli\u003eSpecial Array I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-01"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/making-a-large-island/description/\nIntuition In this problem, we need to flip a 0 into a one to calculate the largest connected block. Now, if we can calculate the size of the connected block in the 4 directions of a 0 in the grid, then we just need to add them up and we are all set.\nApproach First we need to calculate the size of each connected block and give a label to each connected block so that we are not adding the same connected block twice.\nThen we iterate all the 0, flip its result is the sum of the unique connected blocks around it in 4 directions.\nComplexity Time complexity: $O(N^2)$\nSpace complexity: $O(N^2)$\nCode class Solution: def largestIsland(self, grid: List[List[int]]) -\u0026gt; int: islandCount = [0,0] dx = [0,0,1,-1] dy = [1,-1,0,0] n = len(grid) m = len(grid[0]) def dfs(x, y, cnt): grid[x][y] = cnt islandCount[cnt] += 1 for i in range(4): nx = x + dx[i] ny = y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or grid[nx][ny] != 1: continue dfs(nx, ny, cnt) cnt = 1 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 1: cnt += 1 islandCount.append(0) dfs(i, j, cnt) ans = max(islandCount) for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 0: tmp = 1 vis = set([]) for k in range(4): nx = i + dx[k] ny = j + dy[k] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or grid[nx][ny] in vis: continue tmp += islandCount[grid[nx][ny]] vis.add(grid[nx][ny]) ans = max(ans, tmp) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-31/","summary":"\u003col start=\"827\"\u003e\n\u003cli\u003eMaking A Large Island\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-31"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/redundant-connection/description/\nIntuition Note that in a ring, every node have more than one index. Therefore, if we delete all the index that has one index, the remaining points will be a ring.\nApproach Note that the label starts from 1 and ends at n, you would like to decrease index by one.\nComplexity Time complexity: $O(N)$\nSpace complexity: $O(N)$\nCode class Solution: def findRedundantConnection(self, edges: List[List[int]]) -\u0026gt; List[int]: N = len(edges) du = [0] * (N) E = [[] for _ in range(N)] for x,y in edges: x -= 1 y -= 1 du[x] += 1 du[y] += 1 E[x].append(y) E[y].append(x) q = deque([]) for i in range(N): if du[i] == 1: q.append(i) # print(du) while(len(q) \u0026gt; 0): x = q.popleft() du[x] = 0 for to in E[x]: if du[to] \u0026gt; 0: du[to] -= 1 if du[to] == 1: q.append(to) # print(du) loop = set([]) for i in range(N): if du[i] \u0026gt; 0: loop.add(i) for i in range(N - 1, -1, -1): x,y = edges[i] if x - 1 in loop and y - 1 in loop: return [x, y] return None ","permalink":"https://tzj2006.github.io/leetcode/2025-01-29/","summary":"\u003col start=\"684\"\u003e\n\u003cli\u003eRedundant Connection\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-29"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-number-of-fish-in-a-grid/description/\nIntuition Find the size of the connected blocks.\nApproach Iterate through the grid to add the size to a connected block, then find the maximum size of the connected block.\nComplexity Time complexity: $O(N\\times M)$, we will visit every point exactly once.\nSpace complexity: $O(1)$, if you do not count the original grid.\nCode class Solution: def findMaxFish(self, grid: List[List[int]]) -\u0026gt; int: ans = 0 dx = [0,0,1,-1] dy = [1,-1,0,0] for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 0: continue tmp = grid[i][j] grid[i][j] = 0 q = deque([(i,j)]) while len(q) \u0026gt; 0: x,y = q.popleft() for k in range(4): nx = x + dx[k] ny = y + dy[k] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= len(grid) or ny \u0026gt;= len(grid[0]) or grid[nx][ny] == 0: continue tmp += grid[nx][ny] grid[nx][ny] = 0 # print(i,j,nx, ny, tmp) q.append((nx, ny)) ans = max(ans, tmp) print(grid) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-28/","summary":"\u003col start=\"2658\"\u003e\n\u003cli\u003eMaximum Number of Fish in a Grid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-28"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/course-schedule-iv/description/\nIntuition In this problem, we need to find whether a point is the father of another point or not. In this case, we can simply use one set to store all the fathers of a point and path that set to all its children.\nApproach Use a dfs to iterate all the points. Create a set for every point, then pass it to its children. Complexity Time complexity: $O(N^3 + Q)$. We would at visit each edge at most once. The passing of a set is $O(N)$. So the final time complexity would be $O(N^3 + Q)$.\nSpace complexity: $O(N^2) + Q$. We need to store a set for every point and we also need to store the answer.\nCode class Solution: def checkIfPrerequisite(self, numCourses: int, prerequisites: List[List[int]], queries: List[List[int]]) -\u0026gt; List[bool]: edges = [[] for _ in range(numCourses)] prereq = [set([_]) for _ in range(numCourses)] for x, y in prerequisites: edges[y].append(x) def dfs(x): for to in edges[x]: if len(prereq[to]) == 1: dfs(to) prereq[x] = prereq[x] | prereq[to] for i in range(numCourses): dfs(i) ans = [] for x, y in queries: if x in prereq[y]: ans.append(True) else: ans.append(False) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-27/","summary":"\u003col start=\"1462\"\u003e\n\u003cli\u003eCourse Schedule IV\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-27"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-employees-to-be-invited-to-a-meeting/description/\nIntuition Here we have a directed graph with a ring. To get the ring, we can use topological sort. That is, to find the point which has index 0, and delete the point and its corresponding point.\nBy doing so, the remaining points in the graph are all in a ring.\nIn this problem, there are two ways to put everyone in a seat:\na ring where everyone has its favorite person on his left hand site. when two person are each other\u0026rsquo;s favorite person, they themselves can form a complete ring while people who like them can form a list that points to them. Such structure is special because everyone can find his favorite person without forming a complete ring. Therefore, there could be multiple structures in the room. Approach Use topological sort to find all the rings in the graph. Find all the special case when two people are each others\u0026rsquo; favorite. Return the max size of a ring or return the max size of that multiple structures. Complexity Time complexity: $O(N)$\nSpace complexity: $O(N)$\nCode class Solution: def maximumInvitations(self, favorite: List[int]) -\u0026gt; int: N = len(favorite) du = [0] * N l = [1] * N for x in favorite: du[x] += 1 q = deque([]) for i in range(N): if du[i] == 0: q.append((i, 1)) while(len(q) \u0026gt; 0): x, leng = q.popleft() to = favorite[x] du[to] -= 1 l[to] = max(l[to], leng + 1) if du[to] == 0: q.append((to, leng + 1)) vis = [0] * N def dfs(i): to = favorite[i] vis[i] = 2 if vis[to] == 2: return 1 return dfs(to) + 1 ans = 0 res = 0 for i in range(N): if du[i] != 0 and vis[i] == 0: tmp = dfs(i) # print(i, tmp) if tmp == 2: # print(i, favorite[i], l[i], l[favorite[i]]) res += l[i] + l[favorite[i]] else: ans = max(ans, tmp) return max(ans, res) ","permalink":"https://tzj2006.github.io/leetcode/2025-01-26/","summary":"\u003col start=\"2127\"\u003e\n\u003cli\u003eMaximum Employees to Be Invited to a Meeting\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-26"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/make-lexicographically-smallest-array-by-swapping-elements/description/\nIntuition In this question, we are doing a sorting process that only difference less than limit can swap. Therefore, there forms \u0026ldquo;groups\u0026rdquo;. In a group, two sequential number has a difference less than limit. In this case, if we sort the numbers in a group, then we are all done. Approach We need to get the groups. We need to sort the array first. Then if the difference between two numbers are bigger than limit, then it would belong to two different groups. Then we sort the result for each group. Complexity Time complexity: $O(N\\times log(N))$, N is the length of the array.\nSpace complexity: $O(N)$\nCode class Solution: def lexicographicallySmallestArray(self, nums: List[int], limit: int) -\u0026gt; List[int]: sorted_nums = [] for idx, x in enumerate(nums): sorted_nums.append((x, idx)) sorted_nums.sort() # First we sort the array groups = [] tmp = [] for i in range(len(nums)): if i \u0026gt; 0 and sorted_nums[i][0] - sorted_nums[i-1][0] \u0026gt; limit: tmp.sort() groups.append(tmp) tmp = [] tmp.append(sorted_nums[i][1]) tmp.sort() groups.append(tmp) # Then we form groups idx = 0 pos = 0 ans = [0] * len(nums) for i in range(len(nums)): if pos == len(groups[idx]): pos = 0 idx += 1 ans[groups[idx][pos]] = sorted_nums[i][0] pos += 1 # Then we sort the groups return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-25/","summary":"\u003col start=\"2948\"\u003e\n\u003cli\u003eMake Lexicographically Smallest Array by Swapping Elements\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-25"},{"content":"Some times we want to install a jupyter kernel for our server. Then we would like to use ipykernel\npip install ipykernel ipython kernel install --user --name= To know the PID of a jupyter notebook, you can use:\nimport os os.getpid() ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-24/","summary":"ipykernel, PID","title":"Bug Journal 2025-01-24"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-eventual-safe-states/description/\nNote Outgoing edges means any edge of this point, even if this point connects to itself.\nIntuition The only points that are terminate are the points that have no edges.\nThen we have to find which point connect to these terminate points.\nTherefore, we can use recursive (or any LIFO algorithms) to solve this question.\nApproach Use a DFS. If we find a point that has no outgoing edges, then its a terminate point. If we find a self-loop, all points in the loop are not safety. If a point only connects to safty points, then it is safety. Complexity Time complexity: $O(N)$, all points will be visited only once. Space complexity: $O(N)$. Code class Solution: def eventualSafeNodes(self, graph: List[List[int]]) -\u0026gt; List[int]: n = len(graph) safety = [-1] * n vis = [0] * n ans = [] def dfs(x): if safety[x] != -1: return safety[x] if vis[x] == 1: safety[x] = 0 return 0 vis[x] = 1 if len(graph[x]) == 0: safety[x] = 1 return 1 res = 0 for to in graph[x]: res += dfs(to) if res == len(graph[x]): safety[x] = 1 else: safety[x] = 0 return safety[x] for i in range(n): if(dfs(i) == 1): ans.append(i) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-24/","summary":"\u003col start=\"802\"\u003e\n\u003cli\u003eFind Eventual Safe States\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-24"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-servers-that-communicate/\nIntuition Do what the question ask.\nApproach First count the number of computers in each row and each column. Then count whether a computer has another computer that has the same row or column with it.\nComplexity Time complexity: $O(NM)$, N, M, are the length and the width of the grid.\nSpace complexity: $O(NM)$\nCode class Solution: def countServers(self, grid: List[List[int]]) -\u0026gt; int: cntR = [0] * len(grid) cntC = [0] * len(grid[0]) for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j]: cntR[i] += 1 cntC[j] += 1 ans = 0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] and (cntR[i] \u0026gt; 1 or cntC[j] \u0026gt; 1): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-23/","summary":"\u003col start=\"1267\"\u003e\n\u003cli\u003eCount Servers that Communicate\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-23"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/map-of-highest-peak/description/\nIntuition Because the maximum absolute value of the height difference between two adjacent grid is 1, and the height of water gird is 0. This means that the answer is just the manhattan distance to the nearest water grid.\nApproach Use a BFS to find the nearest manhattan idstance to a water grid.\nComplexity Time complexity: $O(NM)$, N, M are the length and width of the grid.\nSpace complexity: $O(NM)$\nCode class Solution: def highestPeak(self, isWater: List[List[int]]) -\u0026gt; List[List[int]]: q = deque() ans = [[2005 for _ in range(len(isWater[0]))] for _ in range(len(isWater))] for i in range(len(isWater)): for j in range(len(isWater[0])): if isWater[i][j] == 1: q.append((i,j)) ans[i][j] = 0 dx = [0,0,1,-1] dy = [1,-1,0,0] while q: x,y = q.popleft() for i in range(4): nx = x + dx[i] ny = y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= len(isWater) or ny \u0026gt;= len(isWater[0]) or ans[nx][ny] \u0026lt;= ans[x][y] + 1: continue ans[nx][ny] = ans[x][y] + 1 q.append((nx, ny)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-22/","summary":"\u003col start=\"1765\"\u003e\n\u003cli\u003eMap of Highest Peak\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-22"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/grid-game/description/\nIntuition This is a special case where there are only two rows. Moreover, it is also important to note that all number in the grid.\nSince the robot cannot go back whenever we choose to go down, this means that for both robots, there is only one chance to go to the second row.\nIf robot one goes to the second row at index $k$, what will happen?\nNow, in this case, the only numbers not 0 are the numbers that has index less than k in the second row and the numbers that has index more than k in the first row.\nTherefore, to maximize the result for robot 2, it gets to choose to get the numbers in the first row or in the second row because it cannot get back to the first row when it choose to get to the second row.\nApproach Now, all we have to calculate is the sum of all the numbers after index k in the first row, and the sum of all the numbers before index k in the second row.\nTrick Now we have a trick of prefix sum to solve this problem.\nThe sum of all numbers after index in the first row k can be calculated by the sum of all numbers after index k - 1, by subtracting $grid[k][0]$. The sum of all numbers before index in the second row k can be calculated by the sum of all numbers after index k - 1, by adding $grid[k][1]$. The required sum of the first row is always decreasing, while the required sum of the second row is always increasing. Therefore, when $max(sum1, sum2) \u0026gt; presentAns$, we can break the loop as now sum2 \u0026gt; sum1 and will continue increase. Therefore, the answer will not decrease anymore. (Here sum1 means the required sum of row1, sum2 means the required sum of row2, and presentAns means the answer we get at present point when we iterate to index k).\nComplexity Time complexity: $O(N)$, N is the length of the gird.\nSpace complexity: $O(1)$, we only store a few variables.\nCode class Solution: def gridGame(self, grid: List[List[int]]) -\u0026gt; int: x,y = sum(grid[0][1:]), 0 ans = x for i in range(1, len(grid[0])): x -= grid[0][i] y += grid[1][i - 1] if ans \u0026gt;= max(x,y): ans = max(x,y) else: return ans return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-21/","summary":"\u003col start=\"2017\"\u003e\n\u003cli\u003eGrid Game\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-21"},{"content":"Today\u0026rsquo;s Problem https://leetcode.com/problems/first-completely-painted-row-or-column/description\nIntuition Do what the question ask.\nTrick We can store a count array for each row and column, so that we can know how many block painted in any row or column.\nApproach Store the position of each number in the gird. Add one count in each row and column every print. If the print lead to a row or column that is all painted, then output i. Complexity Time complexity: $O(NM)$, N,M are the length and width of the grid.\nSpace complexity: $O(NM)$, we need to store the index of each number.\nCode class Solution: def firstCompleteIndex(self, arr: List[int], mat: List[List[int]]) -\u0026gt; int: col = [0] * (len(arr) + 1) row = [0] * (len(arr) + 1) for i in range(len(mat)): for j in range(len(mat[0])): col[mat[i][j]] = j row[mat[i][j]] = i cntR,cntC = [0] * len(mat), [0] * len(mat[0]) for i, x in enumerate(arr): cntR[row[x]] += 1 cntC[col[x]] += 1 if cntR[row[x]] == len(mat[0]) or cntC[col[x]] == len(mat): return i return -1 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-20/","summary":"\u003col start=\"2661\"\u003e\n\u003cli\u003eFirst Completely Painted Row or Column\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-20"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/trapping-rain-water-ii/description/\nIntuition In yesterday\u0026rsquo;s problem, we talked about the situation when we may consider using graph based methods to solve problems.\nNow we can apply that criteria to this question: in this question, we found that the water can flow from four dirctions, which means that there are aftereffects.\nTherefore, we need to apply graph based methods.\nFor each block, how many water it can store depends on the height difference between it and its lowest neighbor (image a wood bucket with edge heights you may see in psychology classes).\nTo clearify, here, water wall can also be s type of wall that contributes to the height.\nTo know the height of the walls, we first need to genereate a wall. But where is it?\nThe first wall you may consider is the outmost edge of the graph (such as the one shown in case two where stores water using the outmost edge).\nThen we can find the lowest place on the wall to create more walls inside. That is, if its neighbor is higher than the point on the wall, then the point inside will become a new componenet of the wall. Otherwise, that inside point can store enough water to create a water wall as high as the current point.\nBecause we are using the lowest place on the wall, so all other parts of the wall would be higher or equal to the point, which means that the height of the wall is the upper bond of how many water can be stored inside the wall.\nApproach In this case, we can use a priority queue to find the point of the wall efficiently.\nThen follow the algorithm described above:\nCreate the initial wall\nloop:\nfind the lowest point on the wall\ncreate new walls or new water walls\nend loop\nsum up all addition height of water walls\nComplexity Time complexity: $O(NM\\times log(NM))$\nSpace complexity: $O(NM)$\nCode class Solution: def trapRainWater(self, heightMap: List[List[int]]) -\u0026gt; int: if len(heightMap) \u0026lt; 3 or len(heightMap[0]) \u0026lt; 3: return 0 dx = [0,0,1,-1] dy = [1,-1,0,0] vis = [] n, m = len(heightMap), len(heightMap[0]) q = [] for i in range(n): heappush(q, (heightMap[i][0], i, 0)) heappush(q, (heightMap[i][-1], i, m - 1)) vis.append((i, 0)) vis.append((i, m-1)) for i in range(1, m - 1): heappush(q, (heightMap[0][i], 0, i)) heappush(q, (heightMap[-1][i], n - 1, i)) vis.append((0, i)) vis.append((n-1, i)) vis = set(vis) ans = 0 while len(q) \u0026gt; 0: h, x, y = heappop(q) for i in range(4): nx, ny = x + dx[i], y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or (nx,ny) in vis: continue if heightMap[nx][ny] \u0026lt; h: ans += h - heightMap[nx][ny] heappush(q, (max(h, heightMap[nx][ny]), nx, ny)) vis.add((nx, ny)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-19/","summary":"\u003col start=\"407\"\u003e\n\u003cli\u003eTrapping Rain Water II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-cost-to-make-at-least-one-valid-path-in-a-grid/\nIntuition Consider a question:\nHere, there is a graph, each edge has a value 1 or 0, and you should travel from point 0 to point N, what is the shortest path?\nIn this case, you would quickly think of graph algorithms such as Dijkstra, SPFA, or even BFS.\nBut what if I tell you that the question above is the exact same question as what we are solving this quesion? Can you quickly think of the transition between the setting in the question and the setting in this more simple version?\nNow, some of you may think that this problem may be DP question: we have a grid, we may be able to write a DP formular\u0026hellip;\nBut wait, the most important prerequisite of DP is aftereffect. To run a DP, you must make sure that there is not aftereffect. In our situation, because we may need to go from right to left, from down to top, so aftereffect exists. Therefore, we cannot use DP in this question.\nApproach Now, to transfer our question to the question above, we only need to iterate through the graph and create an edge between a point to its neighbor, if this is the neighbor it is pointing at, then the value of the edge will be 0, otherwise it would be 1.\nSome of you may consern the correctness of this solution, as there is also a limitation that \u0026ldquo;You can modify the sign on a cell one time only\u0026rdquo;.\nHowever, the situation is, this graph has not negative edges, which means that your result will always increase if you go through more points. Therefore, you will not even vist the same point more than 1 time, so it is impossible for the solution you get to change the sign of a cell more than 1 time.\nNow, run your Dijkstra (or other shortest path algorithms), and you are all set!\nComplexity Time complexity: $O(N\\times M\\times log(N\\times M))$\nSpace complexity: $O(N\\times M)$\nCode class Solution: def minCost(self, grid: List[List[int]]) -\u0026gt; int: dx = [0,0,1,-1] dy = [1,-1,0,0] n,m = len(grid), len(grid[0]) edges = [[] for i in range(n * m)] def cordinate2d21d(x,y): return x * m + y for i in range(len(grid)): for j in range(len(grid[0])): pos = cordinate2d21d(i, j) for idx in range(4): nx = i + dx[idx] ny = j + dy[idx] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m: continue npos = cordinate2d21d(nx, ny) if idx + 1 == grid[i][j]: edges[pos].append([npos, 0]) else: edges[pos].append([npos, 1]) dis = [inf] * (n * m) dis[0] = 0 q = [(0,0)] while q: d, x = heappop(q) for to, v in edges[x]: if d + v \u0026lt; dis[to]: dis[to] = d + v heappush(q, (dis[to], to)) return dis[cordinate2d21d(n-1, m-1)] ","permalink":"https://tzj2006.github.io/leetcode/2025-01-18/","summary":"\u003col start=\"1368\"\u003e\n\u003cli\u003eMinimum Cost to Make at Least One Valid Path in a Grid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-18"},{"content":"Part Ollama Sometimes someone may already open an ollama server on the HPC. In this case, we need to open a new ollama server, otherwise both of use will run at a slower speed.\nIn this case, what can we do?\nOpen a new personal port!\ne.g:\nenv OLLAMA_MODELS=/orange/qsong1/zt81.duke/Models OLLAMA_HOST=127.0.0.1:11451 ollama serve ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-17/","summary":"Ollama","title":"Bug Journal 2025-01-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/neighboring-bitwise-xor/description/\nIntuition Based on the information of yesterday\u0026rsquo;s problem, we know that $a\\space xor\\space a = 0$. Therefore, $\\large{XOR}_{i=0}^{n} \\small derived[i]$ has to be 0, because all the $original$ offsets. Here, n means the last index of the sequence. Now lets prove that if $\\large{XOR}_{i=0}^{n} \\small derived[i] = 0$ enables us to create the whole $original$ sequence. Let $original[0]=0$, $original[k] = original[k-1]\\space xor\\space derived[k]$. Now all we need to prove is $original[n]\\space xor\\space original[0]=derived[n]$, that is, $original[0] = original[n]\\space xor\\space derived[n]$. According to the formular above, $original[n] = \\large{XOR}_{i=0}^{n-1} \\small derived[i]\\space xor\\space original[0]$ Because $\\large{XOR}_{i=1}^{n} \\small derived[i] = 0$, so $original[n]\\space xor\\space derived[n] = original[0]\\space xor\\space \\large{XOR}_{i=0}^{n} \\small derived[i] = 0 = original[0]$. Therefore, this sequence of $original$ is valid. Trick Now we want to know whether the sequence itself has a xorsum 0 or not. Now, because it is a binary sequence, we can put all 0s together and 1s together, so that now by xor all the 0 and 1s, we find that the result is just the count of 1s. If the count is odd, then the result would be 1, otherwise it would be 0. Therefore, the easiest way to solve this question is to sum up everything in the sequence and check whether it is odd or even. Approach Sum up everything in the sequence and check whether it is odd or even.\nComplexity Time complexity: $O(N)$, N is the length of the sequence.\nSpace complexity: $O(1)$, no other variables stored.\nCode class Solution: def doesValidArrayExist(self, derived: List[int]) -\u0026gt; bool: return (sum(derived) \u0026amp; 1) == 0 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-17/","summary":"\u003col start=\"2683\"\u003e\n\u003cli\u003eNeighboring Bitwise XOR\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/bitwise-xor-of-all-pairings/description/\nIntuition According to the question, the formular of the output would be: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} \\large{XOR}{\\small j = 1}^{\\small m} nums1[i]\\space xor \\space nums2[j] $$ where $\\large{XOR}$ means the operation that xor from $i$ to $n$. Here, $n$ means the length of $nums1$, and $m$ means the length of $nums2$. According to the properties of xor, xor satisfies the law of commutation and the law of association (more information can be seen here), so we can change the formular to: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} (nums1[i]^m) \\space xor \\space \\large{XOR}{\\small j = 1}^{\\small m} (nums2[j]^n) $$ According to the property that $A\\space xor A = 0$, we now know that the result would be: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} (nums1[i]^{m % 2}) \\space xor \\space \\large{XOR}{\\small j = 1}^{\\small m} (nums2[j]^{n % 2}) $$\nApproach Iterate two arrays and apply the formular above.\nComplexity Time complexity: $O(N + M)$, N is the length of nums1, M is the length of nums2.\nSpace complexity: $O(1)$, no more space is used.\nCode class Solution: def xorAllNums(self, nums1: List[int], nums2: List[int]) -\u0026gt; int: ans = 0 if len(nums1) % 2 == 1: for x in nums2: ans ^= x if len(nums2) % 2 == 1: for x in nums1: ans ^= x return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-16/","summary":"\u003col start=\"2425\"\u003e\n\u003cli\u003eBitwise XOR of All Pairings\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-16"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimize-xor/description/\nXOR This is the True-False Diagram for $XOR$: In this diagram, we can find that if A and B is the same, then $A\\space XOR\\space B = 0$; else, $A\\space XOR\\space B = 1$.\nIntuition For the first requirement in the question, \u0026ldquo;same number of set bits\u0026rdquo; means \u0026ldquo;same number of bit 1 in the number\u0026rdquo;.\nWhy? This is because the number of 0 in a number can be infinity, while only the number of bit 1 is finite.\nTherefore, we need to count how many 1s are there in number2.\nNow, based on the $XOR$ Diagram we know that to make a number after doing $XOR$, we need to put a one in the same position where number1 has a 1, so that we can decrease it to 0 after doing $XOR$.\nIn this case, we want the \u0026ldquo;decreased\u0026rdquo; 1s from top to down to minimize the result.\nIf there are more 1s in number2 than number1, we would have to add new 1s to the result.\nIn this case, we want to \u0026ldquo;add\u0026rdquo; 1s from down to top to minimize the result.\nApproach Use $bit_count()$ function to count the 1s in num2. Iterate from top to down to decrease 1. Iterate from down to top to add 1. Complexity Time complexity: $O(log_2(N))$, N means the number.\nSpace complexity: $O(1)$, only some variables are stored\nCode class Solution: def minimizeXor(self, num1: int, num2: int) -\u0026gt; int: cnt,ans = num2.bit_count(), 0 for i in range(31, -1, -1): if cnt \u0026gt; 0 and (num1 \u0026amp; (1 \u0026lt;\u0026lt; i)) \u0026gt; 0: ans += (1 \u0026lt;\u0026lt; i) cnt -= 1 for i in range(31): if cnt \u0026gt; 0 and (ans \u0026amp; (1 \u0026lt;\u0026lt; i)) == 0: ans += (1 \u0026lt;\u0026lt; i) cnt -= 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-15/","summary":"\u003col start=\"2429\"\u003e\n\u003cli\u003eMinimize XOR\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-15"},{"content":"Part module load When you want to use something on the server, please first check whether it is on the server or not.\nUse this command:\nmodule avail Then load using this command:\nmodule load [your model name] Important:\nYou can always email rescomputing@duke.edu to get support.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-14/","summary":"\u003ch2 id=\"part-module-load\"\u003ePart module load\u003c/h2\u003e\n\u003cp\u003eWhen you want to use something on the server, please first check whether it is on the server or not.\u003c/p\u003e\n\u003cp\u003eUse this command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emodule avail\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen load using this command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emodule load \u003cspan class=\"o\"\u003e[\u003c/span\u003eyour model name\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eImportant:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou can always email \u003ca href=\"rescomputing@duke.edu\"\u003erescomputing@duke.edu\u003c/a\u003e to get support.\u003c/p\u003e","title":"Bug Journal 2025-01-14"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-prefix-common-array-of-two-arrays/description/\nIntuition In this question, the only three ways that the answer will increase one is:\n$A[i] == B[i]$. $A[i]$ appears in $B$. $B[i]$ appears in $A$. Approach Iterate through the Array, then check for those three situation. Note that situation 1 conflicts with situation 2 \u0026amp; 3. That is, if cnt is add by 1 through situation 1, then situation 2 \u0026amp; 3 will not increase cnt. But situation 2 \u0026amp; 3 could increase cnt. Complexity Time complexity: $O(N)$, N is the length of the list.\nSpace complexity: $O(N)$, because we need to store a set.\nCode class Solution: def findThePrefixCommonArray(self, A: List[int], B: List[int]) -\u0026gt; List[int]: ans = [] cntA = set([]) cntB = set([]) cnt = 0 for i in range(len(A)): if A[i] == B[i]: cnt += 1 else: if A[i] in cntB: cnt += 1 if B[i] in cntA: cnt += 1 cntA.add(A[i]) cntB.add(B[i]) ans.append(cnt) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-14/","summary":"\u003col start=\"2657\"\u003e\n\u003cli\u003eFind the Prefix Common Array of Two Arrays\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-13"},{"content":"When facing this problem, you can stop using GPU and change to CPU and check the error.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-12/","summary":"\u003cp\u003eWhen facing this problem, you can stop using GPU and change to CPU and check the error.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250112224658415\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-01-12GPUError.jpg\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-01-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-length-of-string-after-operations/\nIntuition In this question, each character is independent. Therefore, we can deal with one character a time. If one character has odd count, than we can delete from left to right and remain 1 character at the end. Otherwise, we will left 2 characters at the end. Approach Therefore, all we need to do is the count each character, and then test whether it has odd count or even count.\nComplexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, I stored a counter.\nCode class Solution: def minimumLength(self, s: str) -\u0026gt; int: cnt = Counter([ch for ch in s]) ans = 0 for x,v in cnt.items(): if v % 2 == 0: ans += 2 else: ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-13/","summary":"\u003col start=\"3223\"\u003e\n\u003cli\u003eMinimum Length of String After Operations\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-13"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-a-parentheses-string-can-be-valid/\nIntuition To make a parentheses string, we need to find a \u0026lsquo;(\u0026rsquo; for every \u0026lsquo;)\u0026rsquo; on its left side, and a \u0026lsquo;)\u0026rsquo; for every \u0026lsquo;(\u0026rsquo; on its right hand side. We can fulfill the first requirement first as we are iterating from left to right.\nWhen we encounter a \u0026lsquo;)\u0026rsquo;, there are three ways to find him a \u0026lsquo;(\u0026rsquo;, one is a \u0026lsquo;(\u0026rsquo; that is already existing, another is to find him a \u0026lsquo;)\u0026rsquo; that is unlocked, and the last way is to turn him into a \u0026lsquo;(\u0026rsquo; if it is unlocked itself.\nNow, assume that we have find all \u0026lsquo;)\u0026rsquo; a \u0026lsquo;(\u0026rsquo;, then we now need to find all \u0026lsquo;(\u0026rsquo; a \u0026lsquo;)\u0026rsquo;. Now, the only way to find a \u0026lsquo;(\u0026rsquo; on its right side is to find a \u0026lsquo;(\u0026rsquo; that is unlocked, because all \u0026lsquo;)\u0026rsquo; is matched with a \u0026lsquo;(\u0026rsquo;.\nApproach First of all, not that is the length is odd, then it could never be a parentheses string, so please just return False.\nWe can store two arrays, one $anyBracket$ that stores the index all unlocked brackets, and another $openBracket$ stroing all \u0026lsquo;(\u0026rsquo;.\nNow we iterate the whole string from left to right, here are some situations we would meet:\nThis is a unlocked bracket: Then we could put it into our $anyBracket$ stack. This is a \u0026lsquo;(\u0026rsquo;: Then we could put it into our $openBracket$ stack. This is a \u0026lsquo;)\u0026rsquo;: Then we need to find him a \u0026lsquo;(\u0026rsquo;. First we would like to find him a \u0026lsquo;(\u0026rsquo; in our $openBracket$ stack, which will also finish the task that helps a \u0026lsquo;(\u0026rsquo; to find a \u0026lsquo;)\u0026rsquo;. Then if our $openBracket$ stack is empty, then we will find him a \u0026lsquo;(\u0026rsquo; in our $anyBracket$ stack by either a \u0026lsquo;(\u0026rsquo; or a \u0026lsquo;)\u0026rsquo; that is unlocked. If both our $openBracket$ stack and our $anyBracket$ stack are empty, then return False, because we cannot find a \u0026lsquo;(\u0026rsquo; for him. Now we might left some \u0026lsquo;(\u0026rsquo; that is unmatched.\nThen we can iterate every \u0026lsquo;(\u0026rsquo; in our $openBracket$ stack to find whether there is a \u0026lsquo;(\u0026rsquo; or \u0026lsquo;)\u0026rsquo; in our $anyBracket$ stack that has a larger index than our current \u0026lsquo;(\u0026rsquo;.\nIf there is, then we successfully find him a \u0026lsquo;)\u0026rsquo;, congratulations! Otherwise we cannot find him a \u0026lsquo;)\u0026rsquo;, which leads to return False Now if there are even number in our $anyBracket$ stack (which will always be the case because we have already did the singularity test above), please return True, then you are all set!\nImportant Trick Why we need a stack for $openBracket$ and $anyBracket$? Because in this situation, a \u0026lsquo;)\u0026rsquo; will always match to the nearest \u0026lsquo;(\u0026rsquo; on its left hand site, which means we need a FIFO (First in First out) data structure to get \u0026ldquo;the nearest object\u0026rdquo;. Complexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, we stored the indexs of the brackets.\nCode class Solution: def canBeValid(self, s: str, locked: str) -\u0026gt; bool: if len(s) % 2 == 1: return False openBracket = [] anyBracket = [] for i in range(len(s)): if locked[i] == \u0026#39;0\u0026#39;: anyBracket.append(i) else: if s[i] == \u0026#39;(\u0026#39;: openBracket.append(i) else: if len(openBracket) \u0026gt; 0: openBracket.pop() elif len(anyBracket) \u0026gt; 0: anyBracket.pop() else: return False if len(anyBracket) \u0026lt; len(openBracket): return False idx = len(anyBracket) - 1 for i in range(len(openBracket) - 1, -1, -1): if anyBracket[idx] \u0026lt; openBracket[i]: return False idx -= 1 return idx % 2 == 1 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-12/","summary":"\u003col start=\"2116\"\u003e\n\u003cli\u003eCheck if a Parentheses String Can Be Valid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-k-palindrome-strings/description/\nIntuition A palindrome can have at most one character with an odd count. Therefore, to create $k$ palindrome strings, there must be at most $k$ characters in string $s$ with an odd count.\nAdditionally, since there are at most 26 different characters, if $26 \\leq k$, the result must be true. However, if the length of $s$ is less than $k$, it would be false.\nTo prove this, let the number of characters with odd counts be $cntO$, and let the count of all remaining characters be $2 \\times cntE$.\nIf all character counts are even, we can always create palindrome strings as long as $k \\leq N$, where $N$ is the total length of $s$. This is because we can place one character on the leftmost side of a palindrome string and its duplicate on the rightmost side, preserving the palindrome structure.\nSince $k \\leq N$, it follows that $k \\leq cntO + 2 \\times cntE$. Thus, $k - cntO \\leq 2 \\times cntE$. This implies that all characters with odd counts can be used to form $cntO$ palindrome strings.\nNow, we have already proved that if all character counts are even, we can always create palindrome strings as long as $k \\leq N$, where $N$ is the total length of $s$. So in this case, if $cntO \\le k$, the result would be ture, otherwise, it would be false.\nFinally, the question reduces to the proposition that $k \\leq N$ when all characters have even counts, which is always true.\nTrick Since there are at most 26 different characters, if $26 \\leq k$, the result must be true.\nApproach Now we only need to calculate the occurence of every character and test whether the odd-count characters are less or equal to $k$ or not.\nComplexity Time complexity: $O(N)$, while N is the length of the string.\nSpace complexity: $O(1)$, while the count of 26 characters are stored.\nCode class Solution: def canConstruct(self, s: str, k: int) -\u0026gt; bool: cnt = [0] * 26 if len(s) \u0026lt; k: return False if k \u0026gt; 25: return True # The code that makes the code run very fast. for ch in s: cnt[ord(ch) - ord(\u0026#39;a\u0026#39;)] += 1 x = 0 for i in range(26): if cnt[i] % 2 == 1: x += 1 return x \u0026lt;= k ","permalink":"https://tzj2006.github.io/leetcode/2025-01-11/","summary":"\u003col start=\"1400\"\u003e\n\u003cli\u003eConstruct K Palindrome Strings\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-11"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/word-subsets/\nIntuition The brute method would take $O(N^2\\times (L+D))$ time, while $N$ is the length of the words, $L$ is the length of each word, and $D$ is the size of the dictionary (that is, 26 characters). However, $N \\le 10^4$, which means that we cannot use brute method. Then we found out that we do not need to compare all $N$ words, instead, we only need to compare the maximum of the occurence of each characters in words2. For example, if \u0026lsquo;a\u0026rsquo; appears 3 times in $words2[1]$, 2 times in $words2[2]$, 4 times in $words2[3]$, then \u0026lsquo;a\u0026rsquo; must appears at least 4 times in a $words1[i]$ to add one to the answer. Therefore, all we need is to count the occurence of each character in word1, and count the maximun occurence of each character in every word2.\nApproach Count the occurence of each character in word1, and count the maximun occurence of each character in every word2.\nComplexity Time complexity: $O(N \\times (L + D))$, while $N$ is the length of the words, $L$ is the length of each word, and $D$ is the size of the dictionary (that is, 26 characters).\nSpace complexity: $O(N \\ times D)$.\nCode class Solution: def wordSubsets(self, words1: List[str], words2: List[str]) -\u0026gt; List[str]: wordcnt1, wordcnt2 = [], [0] * 26 a = ord(\u0026#39;a\u0026#39;) for word in words1: cnt = [0] * 26 for ch in word: x = ord(ch) - a cnt[x] += 1 wordcnt1.append(cnt) for word in words2: cnt = [0] * 26 for ch in word: x = ord(ch) - a cnt[x] += 1 for j in range(26): wordcnt2[j] = max(wordcnt2[j], cnt[j]) ans = [] for i in range(len(words1)): flag = True for k in range(26): if wordcnt1[i][k] \u0026lt; wordcnt2[k]: flag = False break if flag: ans.append(words1[i]) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-10/","summary":"\u003col start=\"916\"\u003e\n\u003cli\u003eWord Subsets\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-10"},{"content":"Part Matplotlib If you want to make the word in matplotlib the a word that could be edited in Adobe illustration, please use the code below at the top of your code:\nmpl.rcParams[\u0026#34;pdf.fonttype\u0026#34;] = 42 mpl.rcParams[\u0026#34;ps.fonttype\u0026#34;] = 42 ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-09/","summary":"\u003ch2 id=\"part-matplotlib\"\u003ePart Matplotlib\u003c/h2\u003e\n\u003cp\u003eIf you want to make the word in matplotlib the a word that could be edited in Adobe illustration, please use the code below at the top of your code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003empl\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ercParams\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;pdf.fonttype\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003empl\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ercParams\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;ps.fonttype\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Bug Journal 2025-01-09"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/counting-words-with-a-given-prefix/description\nIntuition Do what the question ask!\nApproach The question yesterday can give us some insight of how to solve this question with minimal code.\nComplexity Time complexity: $O(N\\times L)$, N is the length of the words, L is the length of a single word.\nSpace complexity: $O(1)$, only some variables are stored.\nCode class Solution: def prefixCount(self, words: List[str], pref: str) -\u0026gt; int: ans = 0 for word in words: if word.startswith(pref): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-09/","summary":"\u003col start=\"2185\"\u003e\n\u003cli\u003eCounting Words With a Given Prefix\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-09"},{"content":"Part SSH To use ssh for UF server, eduVPN app must be downloaded.\nThe tutorial is in this link.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-08/","summary":"\u003ch2 id=\"part-ssh\"\u003ePart SSH\u003c/h2\u003e\n\u003cp\u003eTo use ssh for UF server, eduVPN app must be downloaded.\u003c/p\u003e\n\u003cp\u003eThe tutorial is in \u003ca href=\"https://docs.rc.ufl.edu/access/federated_login/?h=eduvpn#eduvpn-connection\"\u003ethis link\u003c/a\u003e.\u003c/p\u003e","title":"Bug Journal 2025-01-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-prefix-and-suffix-pairs-i/description/\nIntuition Do what the question ask!\nApproach Iterate two times and check the suffix and prefix of the string.\nComplexity Time complexity: $O(N^2\\times L)$, N is the length of the words, and L is the length of the string.\nSpace complexity: $O(1)$\nCode class Solution: def countPrefixSuffixPairs(self, words: List[str]) -\u0026gt; int: def checkpre(str1, str2): if len(str1) \u0026gt; len(str2): return False return str1 == str2[:len(str1)] def checksuf(str1, str2): if len(str1) \u0026gt; len(str2): return False return str1 == str2[len(str2) - len(str1):] ans = 0 for i in range(len(words)): for j in range(i+1, len(words)): if checkpre(words[i],words[j]) and checksuf(words[i], words[j]): ans += 1 return ans class Solution: def countPrefixSuffixPairs(self, words: List[str]) -\u0026gt; int: ans = 0 for i in range(len(words)): for j in range(i+1, len(words)): if words[j].startswith(words[i]) and words[j].endswith(words[i]): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-08/","summary":"\u003col start=\"3042\"\u003e\n\u003cli\u003eCount Prefix and Suffix Pairs I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/string-matching-in-an-array/description/\nIntuition Do what the question ask.\nApproach First sort the words by its length, then iterate all string that has a larger string length. If the current string is a substring of the new string, then put it into the answer list.\nActually you do not need to sort the array, but the sort would accelerate the process.\nComplexity Time complexity: $O(N^2\\times L)$, N is the length of the word list, L is the length of the word.\nSpace complexity: $O(N)$, because we need to store the answer.\nCode class Solution: def stringMatching(self, words: List[str]) -\u0026gt; List[str]: ans = [] words.sort(key = lambda x: len(x)) # This lambda is very import in python for i in range(len(words)): for j in range(i + 1, len(words)): if words[i] in words[j]: ans.append(words[i]) break return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-07/","summary":"\u003col start=\"1408\"\u003e\n\u003cli\u003eString Matching in an Array\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-07"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-number-of-operations-to-move-all-balls-to-each-box/description\nIntuition For each point, is it easy to find out that the answer of that point is the sum of the distance between i and all the 1s.\nThat is: $ans[i]=\\sum_{j=0}^{n-1} boxes[j] \\times abs(j-i)$.\nNow, imagine that there is a pointer moving left to right from 1 to n, calculating the result.\nWe can find that for $ans[i]$ and $ans[i+1]$, the difference would be the number of 1s from 1 to i minus the number of 1s from i+1 to n.\nThat is: $ans[i+1] - ans[i] = \\sum_{j=0}^{i} boxes[j] - \\sum_{j=i+1}^{n-1} boxes[j]$. Therefore, by calculating the number of 1s on the left hand side of i and the number of all 1s in the sequence, we can calculate all answers by $O(N)$.\nApproach First we need to calculate $ans[0]$ and the number of all 1s in the sequence by using the equation $ans[0]=\\sum_{j=1}^{n} boxes[j] \\times j$. Therefore, we need to iterate through the whole sequence. Next we need to calculate $ans[i+1] - ans[i] = \\sum_{j=0}^{i} boxes[j] - \\sum_{j=i+1}^{n-1} boxes[j]$ for every i from 1 to n-1. Because $ans[i+1] - ans[i] = \\sum_{j=1}^{i} boxes[j] - \\sum_{j=i+1}^{n} boxes[j] = 2 \\times \\sum_{j=0}^{i} boxes[j] - \\sum_{j=0}^{n-1} boxes[j]$. Therefore, all we have to. do is to count the 1s in our iteration to our answer, then apply the fomular above.\nComplexity Time complexity: $O(N)$, N is the size of the boxes.\nSpace complexity: $O(N)$, as we need to store our answer.\nCode class Solution: def minOperations(self, boxes: str) -\u0026gt; List[int]: now = 0 cnt1 = 0 for i in range(len(boxes)): if boxes[i] == \u0026#39;1\u0026#39;: now += i cnt1 += 1 ans = [now] * len(boxes) now_cnt1 = 0 for i in range(1, len(boxes)): if boxes[i-1] == \u0026#39;1\u0026#39;: now_cnt1 += 1 ans[i] = ans[i-1] + 2 * now_cnt1 - cnt1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-06/","summary":"\u003col start=\"1769\"\u003e\n\u003cli\u003eMinimum Number of Operations to Move All Balls to Each Box\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-06"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/shifting-letters-ii/description/\nIntuition We need to write a datastructure to achieve a multi-range addition and a one-time query. Therefore, we can use difference Array and prefix sum.\nApproach In a difference array, we only count the difference of the edges of the change zone. For example, if we add the range $[l,r]$ by $k$, we only pay attention to point l and point r: we add the difference array $diff[l]$ by $k$, and then add the difference array $diff[r + 1]$ by $-k$. Now, if we calculate the prefix sum $s$ in range $[l,r]$, we find that the effect of addition $k$ will be added only in range $[l,r]$ in $s$. Therefore, by using the difference array and the prefix sum, we can deal with the one change in $O(1)$. Now, since we want to change the character, we can first change it into ASCII code, then subtract by the code of \u0026lsquo;a\u0026rsquo;. Then we can use a module of 26 to acheive the effect of \u0026ldquo;character rotation\u0026rdquo;.\nComplexity Time complexity: $O(N + C)$, N is the length of the string, c is the number of changes.\nSpace complexity: $O(N)$, we need to store the difference array.\nCode class Solution: def shiftingLetters(self, s: str, shifts: List[List[int]]) -\u0026gt; str: dif = [0] * (len(s) + 1) for l,r,delta in shifts: if delta == 0: dif[l] -= 1 dif[r + 1] += 1 else: dif[l] += 1 dif[r + 1] -= 1 a = ord(\u0026#39;a\u0026#39;) cnt = 0 ans = \u0026#34;\u0026#34; for i in range(len(s)): cnt += dif[i] ans += chr((ord(s[i]) - a + cnt) % 26 + a) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-05/","summary":"\u003col start=\"2381\"\u003e\n\u003cli\u003eShifting Letters II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-05"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/unique-length-3-palindromic-subsequences/description/\nIntuition Because the question requires the count of unqiue subsequence. Therefore, the largest count is $26*26 = 676$. Therefore, all we have to do is to count how many unique characters are there between two same characters.\nApproach First we need to calculate the first and last occurance of a character. Then we need to iterate between l and r to count how many unique characters are there between l and r.\nComplexity Time complexity: $O(kN)$, k is the number of unique characters, here it means 26 different character. N is the length of the string.\nSpace complexity: $O(N)$\nCode class Solution: def countPalindromicSubsequence(self, s: str) -\u0026gt; int: st = [inf] * 26 en = [-1] * 26 a = ord(\u0026#39;a\u0026#39;) for i,ch in enumerate(s): nch = ord(ch) - a st[nch] = min(st[nch], i) en[nch] = max(en[nch], i) ans = 0 for x in range(26): if en[x] \u0026lt;= st[x]: continue # print(en[x], st[x], x) vis = set([]) for i in range(st[x] + 1, en[x]): vis.add(s[i]) ans += len(vis) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-04/","summary":"\u003col start=\"1930\"\u003e\n\u003cli\u003eUnique Length-3 Palindromic Subsequences\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-04"},{"content":"Part Ollama To run ollama serve on UF that stores models on large storage, please use:\nenv OLLAMA_MODELS=/orange/qsong1/zt81.duke/Models ollama serve To list all the ollama environment, please use:\nollama list Then check whether all the models pulled are in this list.\nNote that now when dealing with ollama input, multiple role input may cause potential error.\nPart Model Tuning What I did today:\nSet up llama for Sprax task.\nTest1: llama3.2 with reasoning. Result: Nearly everything outputs \u0026ldquo;Sensitive\u0026rdquo; (For both cells that has sensitive and resistent label).\n![image-20250103204922830](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20250103204922830.png)\nGuess: May because there are too many genes that mislead the result of the LLM.\nDecrease the number of genes considered Result: However, the model still outputs \u0026ldquo;Sensitive\u0026rdquo; for nearly every cells.\nTry not using reasoning Result: Very unstable. Sometimes the result is very good, but in most case, it is very bad.\nTry using llama3.3 70B Result: Not much better than guess.\n![image-20250103205231023](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20250103205231023.png)\nFuture work Train MLP Train an MLP using the label, and let LLM to distinguish the embedding.\nIn this case, we can tell whether the LLM is useless or the prompt is useless.\nCustomize tokenizer Change one cell to one token\nCheck Pathway Check whether the LLM is saying nonsense or saying things right.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-03/","summary":"run ollama on the server \u0026amp; model tuning","title":"Bug Journal 2025-01-03"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/number-of-ways-to-split-array/description/\nIntuition We need to calculate $sum(a[0] \\space to\\space a[i])$ and $sum(a[i+1]\\space to\\space a[n])$ according to the question. Since $sum(a[0]\\space to\\space a[i + 1]) = sum(a[0]\\space to\\space a[i]) + a[i+1]$ and $sum(a[i+1]\\space to\\space a[n]) = sum(a[0]\\space to\\space a[n]) - sum(a[0]\\space to\\space a[i])$. Therefore, we can new two variables. One $now$ that stores $sum(a[0]\\space to\\space a[i])$ and add $a[i+1]$ to it every iteration, one $summ$ that stores $sum(a[0]\\space to\\space a[n])$.\nApproach In this case, we only need to compare $now$ and $summ - now$. Then count all i that apply.\nComplexity Time complexity: $O(N)$, N is the length of the sequence.\nSpace complexity: $O(1)$, two new varables are stored.\nCode class Solution: def waysToSplitArray(self, nums: List[int]) -\u0026gt; int: now,summ, ans = 0, 0, sum(nums) for num in nums[:-1]: now += num if now \u0026gt;= summ - now: ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-03/","summary":"\u003col start=\"2270\"\u003e\n\u003cli\u003eNumber of Ways to Split Array\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-03"},{"content":"Part Hugo Part Upgrade The best method to upgrade Hugo is to use the exact same method you use when you install Hugo.\nFor example, when you use homebrew to install Hugo, the please use brew to upgrade Hugo.\nThe command of this is:\nbrew upgrade brew upgrade hugo Part config Always the best guides:\nhttps://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\nSometimes the config may have errors.\nFor example, if the baseUrl config does not work, add this to the config file:\nrelativeURLs: false canonifyURLs: true Moreover, we can change what we want to show on the main page, for example, the menu bar:\nmenu: main: - identifier: bugJournal name: bugJournal url: /bugJournal/ weight: 10 - identifier: leetcode name: leetcode url: /leetcode/ weight: 20 - identifier: posts name: posts \u0026amp; notes url: /posts/ weight: 30 To change which part of passage to show on the main page:\nparams: mainSections: - bugJournal - leetcode - posts ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-02/","summary":"update hugo to fix bugs","title":"Bug Journal 2025-01-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-vowel-strings-in-ranges/description/\nIntuition We need to cacluate the sum of a section, while the sum of the any section remains constant for every query. Therefore we can use prefix sum.\nApproach In one iteration, we can identify whether $words[i]$ is the a vowel string or not. In this case, we can apply a prefix sum to calculate the number of vowel strings between index 1 and index i. Therefore, when we want to calculate the number of vowel strings between index l and index r, we can just use $num(1\\space to\\space r) - num(1\\space to\\space l-1)$ as our result.\nTrick In python, $list[-1]$ means the final index of the list. Therefore, we can add a [0] at the end of our prefix sum list to avoid null index.\nComplexity Time complexity: $O(k\\times N + Q)$, while k is the number of vowels, N is the length of the words, Q is the length of the queries.\nSpace complexity: $O(N)$, because we stored a new list.\nCode class Solution: def vowelStrings(self, words: List[str], queries: List[List[int]]) -\u0026gt; List[int]: sumWords = [0] * (len(words) + 1) vowels = set([\u0026#39;a\u0026#39;,\u0026#39;e\u0026#39;,\u0026#39;i\u0026#39;,\u0026#39;o\u0026#39;,\u0026#39;u\u0026#39;]) for idx, word in enumerate(words): sumWords[idx] = sumWords[idx-1] if word[0] in vowels and word[-1] in vowels: sumWords[idx] += 1 ans = [] for x,y in queries: ans.append(sumWords[y] - sumWords[x-1]) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-02/","summary":"\u003col start=\"2559\"\u003e\n\u003cli\u003eCount Vowel Strings in Ranges\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-02"},{"content":"Part Hugo How to create a Hugo website This is a good tutorial.\nHowever, the themes may contain bugs. (The best way is to download another theme!)\nLanguage Code Error: Some theme use site.LanguageCode and others use site.Lang.LanguageCode Code Highlight Error: Change it to .post-content pre code { word-break: normal !important; white-space: pre !important; } Note 2025.01.03\nAll these error are caused by the version conflict of Hugo, go, and the theme. Update everything to the latest version solves all the problem.\nPart Conda Now I install conda under /orange/qsong1/zt81.duke/miniconda3\nTherefore, to use conda, please use:\ncd /orange/qsong1/zt81.duke/miniconda3 source miniconda3/bin/activate ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-01/","summary":"create hugo website \u0026amp; Install condo","title":"Bug Journal 2025-01-01"},{"content":"Part ChatGPT 4o ChatGPT 4o API demo from openai import OpenAI model_use = \u0026#34;gpt-4o-2024-08-06\u0026#34; client = OpenAI(api_key=\u0026#34;Your-API-key\u0026#34;) completion = client.beta.chat.completions.parse( model=model_use, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Extract the event information.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;}, ], ) event = completion.choices[0].message.parsed Note: I tried to use model \u0026ldquo;gpt-4o\u0026rdquo; but failed.\nHow to create ChatGPT API Key Log in to openai Use the search bar to search \u0026ldquo;API keys\u0026rdquo; Create a new secret key (Shown only once, invisible after closing the tab) Go to billing to add some credit to the account Part UniTox ChatGPT Read from fda.gov Read the label of the drug we are interested in from a .csv file.\nRead the .html file or the .pdf file on the page\nCreate a summary of the .html and .pdf files by ChatGPT\nUse the summary generated by ChatGPT to let ChatGPT decide whether the drug is toxic or not and how toxic the drug is.\nInitial Prompt: Provide a summary of all the parts of the drug label that discuss cardiotoxicity risks and cardiotoxic reactions for this drug. In your summary of each sentence, clearly state whether the drug itself was associated with or caused the cardiotoxicity risk. Output1 Toxidity Score Prompt: Given the above information about a drug, answer \u0026#39;was this drug associated with No Cardiotoxicity, Less Cardiotoxicity, or Most Cardiotoxicity?\u0026#39; Now, answer with just one word: No, Less or Most. Output1 (Summary) OUtput2 Toxidity Test Prompt: Given the above information about a drug, answer \u0026#39;was this drug associated with Cardiotoxicity?\u0026#39; Now, answer with just one word: Yes or No. Output1 Output3 \u0026lt;-\u0026gt; compare GT ![image-20241230175906492](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20241230175906492.png)\nPart Llama Part Ollama First open an ollama server on the server:\nml ollama # activate ollama ollama serve # open ollama server To use ollama in python: (demo)\npip install ollama from ollama import chat, Client, ChatResponse client = Client(host=\u0026#39;http://localhost:11434\u0026#39;) model_use = \u0026#34;llama3.2\u0026#34; completion = client.chat( model=model_use, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Extract the event information.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;}, ], ) completion[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] ","permalink":"https://tzj2006.github.io/posts/llm-study/","summary":"\u003ch2 id=\"part-chatgpt-4o\"\u003ePart ChatGPT 4o\u003c/h2\u003e\n\u003ch3 id=\"chatgpt-4o-api-demo\"\u003eChatGPT 4o API demo\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eopenai\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eOpenAI\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emodel_use\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;gpt-4o-2024-08-06\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eOpenAI\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eapi_key\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Your-API-key\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecompletion\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebeta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecompletions\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparse\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003emodel_use\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Extract the event information.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecompletion\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparsed\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: I tried to use model \u0026ldquo;gpt-4o\u0026rdquo; but failed.\u003c/p\u003e\n\u003ch3 id=\"how-to-create-chatgpt-api-key\"\u003eHow to create ChatGPT API Key\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eLog in to \u003ca href=\"https://platform.openai.com/docs/overview\"\u003eopenai\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eUse the search bar to search \u0026ldquo;API keys\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eCreate a new secret key (Shown only once, invisible after closing the tab)\u003c/li\u003e\n\u003cli\u003eGo to billing to add some credit to the account\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"part-unitox-chatgpt\"\u003ePart UniTox ChatGPT\u003c/h2\u003e\n\u003ch3 id=\"read-from-fdagov\"\u003eRead from fda.gov\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eRead the label of the drug we are interested in from a .csv file.\u003c/p\u003e","title":"LLM Study"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-score-after-splitting-a-string/description\nIntuition Do what the question asks.\nApproach Do what the question asks. Iterate from the back to the front to count how many 1s, iterate from the front to the back to count how many 2s. To speed up the process, we can first count how many 1s are there in the whole sequence, then use another iteration to count the remaining 1s in the sequence by doing a subtraction. Same to the question in Jan.03.2025, the number of 1s in index i + 1 to n = the number of 1s in the sequence - the number of 1s in index 1 to i.\nComplexity Time complexity: $O(N)$, N is the length of s.\nSpace complexity: $O(1)$, only a few new variables are stored.\nCode class Solution: def maxScore(self, s: str) -\u0026gt; int: num1 = 0 for ch in s: if ch == \u0026#39;1\u0026#39;: num1 += 1 now0, now1, ans = 0, 0, 0 for ch in s[:-1]: if ch == \u0026#39;0\u0026#39;: now0 += 1 else: now1 += 1 ans = max(ans, now0 + num1 - now1) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-01/","summary":"\u003col start=\"1422\"\u003e\n\u003cli\u003eMaximum Score After Splitting a String\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-01"},{"content":" 笔记本的 RAM 在关闭屏幕后还耗电吗 markdown 插入图片无法在网站上自动显示 Random 中加一个 checkbox desktop video 英文版 √ 孤波算法是什么 desktop video 多语言切换 √ 是什么成就了一个奢侈品？ Desktop Video 锁屏界面播放 M4 pro V.S. M3 Max ","permalink":"https://tzj2006.github.io/random/","summary":"\u003col\u003e\n\u003cli\u003e笔记本的 RAM 在关闭屏幕后还耗电吗\u003c/li\u003e\n\u003cli\u003emarkdown 插入图片无法在网站上自动显示\u003c/li\u003e\n\u003cli\u003eRandom 中加一个 checkbox\u003c/li\u003e\n\u003cli\u003edesktop video 英文版 √\u003c/li\u003e\n\u003cli\u003e孤波算法是什么\u003c/li\u003e\n\u003cli\u003edesktop video 多语言切换 √\u003c/li\u003e\n\u003cli\u003e是什么成就了一个奢侈品？\u003c/li\u003e\n\u003cli\u003eDesktop Video 锁屏界面播放\u003c/li\u003e\n\u003cli\u003eM4 pro V.S. M3 Max\u003c/li\u003e\n\u003c/ol\u003e","title":"Random Ideas"}]