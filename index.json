[{"content":"Linux 服务器下的指令 Docker 的安装和调试 Docker相当于一台虚拟机。安装之后就可以在这台虚拟机上跑代码了。\nDocker 的安装 Docker 的安装方式： 首先上 Dockerhub 挑选一个心仪的 docker, 下面以 nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 为例：\n然后运行以下代码：\ndocker run -it -rm\\ --name \u0026lt;your-instance-name\u0026gt; \\ --network host \\ nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 注：这里的 -it 指的是打开一个可交互界面，-rm 指的是用后即焚\n这时候就会自动下载 docker 并打开一个 bash 来用。\n现在你会发现这个虚拟机里面什么都没有，所以就需要 apt-get install\n另外，如果你的宿主机器的根目录比较小，想要挂载一个硬盘的话，就在 docker run 中间加上：\n-v /path/to/large/storage:/somepath \\ 这样就可以在 somepath 下挂载这个硬盘了。\n注：不能挂载在根目录下，必须挂载在一个文件夹下\n如果要用 GPU 语法和 CUDA_VISIBLE_DEVICES类似:\n--gpus all # 全部gpu --gpus 0,1,2,3 # 只用gpu 0,1,2,3 如果要加入系统变量, 比如 proxy\n-e HTTPS_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; 注意：image name (比如这里就是 nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04) 要放在最后一行\n这里在测试的时候建议加上 -rm,这样不会产生很多个休眠中的 docker 但是在要频繁使用的时候不建议使用 -rm, 而是就让 docker休眠就好。\n不知道我以前在这里写的啥 应该是说，反正休眠的 docker 可以通过 docker start 唤醒，如果不是必要就别删了呗\ndocker 的调试 常用的 docker 指令： # 启动 docker docker start \u0026lt;容器ID或名字\u0026gt; # 关闭 docker docker stop \u0026lt;容器ID或名字\u0026gt; # 重启 docker docker restart \u0026lt;容器ID或名字\u0026gt; # 删除容器 docker rm \u0026lt;容器ID或名字\u0026gt; # 进入容器 docker exec -it \u0026lt;容器ID或名字\u0026gt; bash # 查看正在运行的容器 docker ps # 查看所有容器（包括停止的） docker ps -a # 列出本地镜像 docker images # 删除镜像 docker rmi \u0026lt;镜像ID或名字\u0026gt; # 挂载目录 docker run -v /host/path:/container/path # 增加环境变量 docker run -e HTTP_PROXY=http://localhost:10086 代理的使用 这里使用的是 xray。\nxray 是这样运行的：\n./xray run -c config.json 运行之后你就可以看到哪个端口放开了，就可以在哪个端口上使用代理,比如 port: 10086。\n这时候如果你想使用代理就需要：\n输入几行代码\n这样你的下载就会走代理辣。\n非常重要 (大坑)\napt-get install 对代理的要求较高，没那么稳定的代理会很稳定的挂，报 Error 503 Service Unavailable. 这时候就直接换清华源就行了，别使用代理了，等之后下别的再用。\ndocker初始化 #!/bin/bash set -e echo \u0026#34;🔧 更新 apt 并安装基础工具...\u0026#34; apt update \u0026amp;\u0026amp; apt install -y \\ vim \\ git \\ curl \\ wget \\ ca-certificates \\ software-properties-common \\ build-essential \\ htop \\ unzip \\ tmux \\ sudo \\ lshw \\ libgl1-mesa-glx \\ libegl1-mesa-dev \\ libosmesa6-dev \\ patchelf \\ cmake \\ build-essential echo \u0026#34;安装 conda...\u0026#34; cd /work curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # 下载过了之后就可以删除或者注释掉 /work/miniconda3/bin/conda init source ~/.bashrc echo \u0026#34;修改 huggingface 下载路径\u0026#34; cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt;\u0026gt; ~/.bashrc export CACHE_ROOT=\u0026#34;/work/models\u0026#34; export HF_HOME=\u0026#34;$CACHE_ROOT\u0026#34; export HF_HUB_CACHE=\u0026#34;$CACHE_ROOT/hub\u0026#34; export HF_DATASETS_CACHE=\u0026#34;$CACHE_ROOT/datasets\u0026#34; export HF_ASSETS_CACHE=\u0026#34;$CACHE_ROOT/assets\u0026#34; export TRANSFORMERS_CACHE=\u0026#34;$CACHE_ROOT/transformers\u0026#34; export CUDA_VISIBLE_DEVICES=0,1,2,3 EOF 注意：不要在~/.bashrc 中添加 source ~/.bashrc，而是就运行一遍 source ~/.bashrc 就可以了 否则蹦服警告⚠️\ndocker 中关闭某个程序 这里以 python 为例\n这里的假设是：现在关闭了 SSH, 找不到原本正在运行的 terminal 了\n那我们就可以用这个方法寻找：\nps aux | grep python or\nps -eo pid,cmd | grep python 这样我们就可以定位到我们要找的进程 PID 了\n之后 kill \u0026lt;PID\u0026gt; 即可\n不行就 kill -9 \u0026lt;PID\u0026gt;\nX11 Forwarding over SSH Reverse Tunnel + Docker (macOS XQuartz) 在宿主机（macOS）通过 XQuartz 接收远端（经过 jumper）Docker 容器的 GUI 程序显示。\n关键步骤 1. macOS：开启 XQuartz 网络访问并放行本机连接\ndefaults write org.macosforge.xquartz.X11.plist nolisten_tcp 0 defaults write org.xquartz.X11 nolisten_tcp -bool false killall XQuartz 2\u0026gt;/dev/null; open -a XQuartz export DISPLAY=:0 xhost +localhost 2. 建立 SSH 反向隧道至目标宿主机\nssh -f -N -J jumper -R 6000:localhost:6000 \u0026lt;name\u0026gt;@\u0026lt;ip\u0026gt; 3. 远端宿主机：检查隧道监听并测试 GUI 应用\nss -tln | grep 6000 export DISPLAY=localhost:0 xclock \u0026amp; 4. Docker 容器中运行 GUI 应用\ndocker run -it --rm \\ --network host \\ -e DISPLAY=localhost:0 \\ \u0026lt;your_image\u0026gt; /bin/bash # 在容器中 xclock \u0026amp; 常见问题与坑点\n问题 原因 解决方法 “Authorization required…” XQuartz 安全设置导致拒绝 执行 xhost +localhost xhost 错误 $DISPLAY 为空 使用 XQuartz 自带终端或手动设置 export DISPLAY 反向隧道无监听 隧道可能没建好 重跑反向隧道建立命令 Docker 不显示 GUI 容器与宿主网络隔离 使用 --network host 或正确设置 DISPLAY 私钥权限过宽错误 SSH 要求私钥权限严格 chmod 600 ~/.ssh/jumper_key Python Python 的妙妙小方法 noqa 一个注释，用于忽略一些警告或者报错\n比如 # noqa: F401 就可以跳过 unimport error\ncprint 可以输出带有颜色的字符，记得 pip install cprint\nwandb wandb 是一个可以联网收集训练数据的网站，好看是好看(✧∀✧)，但是挺难用的（\n这里是tutorial\npytorch lightning 这个包真的非常好用，特别是当涉及到并行计算的时候，简直了\n这里是tutorial\ntorchvision 请看这篇 blog：Torchvision-使用说明\nhuggingface datasets package 请看这篇 blog：Huggingface-Dataset-使用说明\nhuggingface Transformer package 请看这篇 blog：Huggingface-Transformers-使用说明\nhuggingface PEFT hugging face PEFT LoRA 请看[这篇 blog：LoRA finetuning](https://tzj2006.github.io/bugjournal/2025-07-11#LoRA finetuning)\nargparse 这个的作用就是给 python 代码传参数的 写脚本的时候这个非常方便和有用\n首先呢，这个是写得非常详细的基础教程\n这里摘抄一些比较重要的部分\nimport argparse parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;-m\u0026#39;,\u0026#39;--model\u0026#39;, type=list, default=[\u0026#39;llama\u0026#39;],help=\u0026#34;which model to use\u0026#34;, required=True, metavar=\u0026#39;FILE\u0026#39;, nargs=\u0026#39;+\u0026#39;) cfg = parser.parse_args() 用法解释\n-m 是缩写 --model 是全称 type 是类型 default 是 默认值 help 是辅助信息(在 -help 的时候会显示) required 当这个值是 True 的时候就必须在命令中出现 metavar 是在帮助文档中显示的名称 nargs 是一种输入方式，可以输入很多个信息 使用方法：\npython main.py -m llama Gemma os 创建文件目录 # 只创建最后一层 os.mkdir(\u0026#39;your/path\u0026#39;, exist_ok=True) #创建所有中间目录 os.makedirs(\u0026#39;your/path\u0026#39;, exist_ok=True) 注意是 mkdir 和 makedirs\nConda \u0026amp; pip 安装Conda环境 conda --version conda create -n \u0026lt;yourname\u0026gt; python=\u0026lt;yourversion\u0026gt; -y conda activate \u0026lt;yourname\u0026gt; 自动激活与取消激活 base 环境 #修改默认配置 conda config --set auto_activate_base false\t# 默认不进入base环境 conda config --set auto_activate_base true\t# 默认进入base环境 conda 目录设置 conda config --remove envs_dirs /home/yourcondaenv # 移除不想要的路径 conda config --add envs_dirs /home/yourcondaenv # 添加新的 envs 目录 或者可以在 ~/.condarc 中修改这个参数\nenvs_dirs: - /path/to/your/env jupyter notebook 配置 首先安装 jupyter notebook\npip install jupyter 然后为坠落的内核命名\npython -m ipykernel install --user --name your-kernel-name --display-name \u0026#34;Custom Name (Python)\u0026#34; \u0026ndash;name：用于ipython识别 (建议用英文或下划线) \u0026ndash;display-name：Jupyter 显示在界面上的名字 (甚至可以有中文)\n如果要删除这个内核，则：\njupyter kernelspec uninstall your-kernel-name 理论上添加内核不需要重启 jupyter notebook server\npackage 配置 scipy package scipy.misc.derivative 弃用 在 1.16.0 + 版本中，scipy 提供了\nscipy.differentiate.derivative 作为替代\n使用方法：\nfrom scipy.differentiate import derivative scipy._lib._util._lazywhere 弃用 请更换成 np.where\nrpy2 package rpy2 package 找不到 R library 可能问题 1： 环境中根本就没有 R 解决方案： 安装 R 或者 module load R\n可能问题 2 (当使用 jupyternotebook + Module load R 的时候出现)： 由于一些原因，jupyter notebook 先于 Module load R 调用 因此jupyter notebook 中的 R 的指针指向了一个不能被 user 访问的 R 解决方案： 换 Code Server 即可解决\nVisual Studio Code VS code 连接不上但是 SSH 能连接上： 原因： 原因分析\n结论：\nVS code 远端服务器版本和本地版本不一致。 网络环境导致无法同步。 断点续传机制导致无法通过重启解决问题。 解决方案：\n在 VS Code 中打开命令面板 快捷键：(Ctrl/Cmd + Shift + P) , 输入 Remote-SSH: Kill VS Code Server on Host… ，选择对应主机，强制终止并清除旧实例 ssh 登录远程服务器，输入 rm -rf ~/.vscode-server 清除远程服务器缓存 Jupyter Notebook 取消 Warning import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) 如果想要取消 python 的 warning, 可以在运行的时候输入：\npython -W ignore Hugging face Hugging face 数据下载 方法1：Git LFS 但是GIT LFS有时候会丢失一部分数据，不知道为什么\n比如我现在正在下载libero dataset, 但是他的 Git LFS clone 就只会下载 Libero 10, Libero Spatial, Libero Object, 和 Libero Goal, 并不会下载 Libero 100\n我认为是仓库的设置有点问题，因为 Git LFS clone 下来的数据格式和hugging face上下下来的格式有点不同。\n但是 Git LFS真的是最简单轻松的下载方式了\n只需要点击这里然后跟着操作做就完事了。\n下载的速度也比较快，基本跑满了这个服务器的代理网络。\n听说Github不能断点续传，但是实测下来几乎没有断点，真的是最稳定的那个服务了。\n方法2： hugging_face CLI huggingface-cli 是 Hugging Face 官方提供的命令行工具，自带完善的下载功能。\n但是实际用起来体验很差，经常莫名其妙就 Internet Error\n无论是使用代理还是镜像体验都不是很好。\n使用方法：\n代理：\npip install -U huggingface_hub pip install -U hf_transfer # 先下载 huggingface-cli 本体和 hf_transfer 加速插件 # hf_transfer插件真的很快，特别是在境外的服务器速度真的很快 export HF_HUB_ENABLE_HF_TRANSFER=1 # 打开 hf_transfer huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 镜像：\npip install -U huggingface_hub # 还是先安装这个 huggingface-cli export HF_ENDPOINT=https://hf-mirror.com # 这里以 hf-mirror.com 为例 # 剩下的都一样的 huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 方法3：snapshot_download 同样是 hugging face 出品，同样的容易崩溃\n区别是这个可以在 python 中使用\n使用也很简单：\nfrom huggingface_hub import snapshot_download snapshot_download( # repo_type=\u0026#39;dataset\u0026#39;, # 这一条就看你是不是下数据的时候选择加还是不加了 repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False, # 加上这一条可以所见即所得 # 不会出现最后是个指针文件的情况 ) 方法4: hf-mirror 镜像站下载 hf-mirror镜像站 推出了 hfd, 一个 huggingface 专用下载工具，基于成熟工具 aria2，可以做到稳定高速下载不断线。\n这是 hf-mirror 网站给出的 tutorial, 方法清晰简单: 1. 下载hfd\nwget https://hf-mirror.com/hfd/hfd.sh chmod a+x hfd.sh 2. 设置环境变量\n# Linux export HF_ENDPOINT=https://hf-mirror.com or\n# Windows Powershell $env:HF_ENDPOINT = \u0026#34;https://hf-mirror.com\u0026#34; 3.1 下载模型\n./hfd.sh gpt2 3.2 下载数据集\n./hfd.sh wikitext --dataset 根据实测，速度也不赖\n就是他的这个 Copy 有点问题，要一行一行的 Copy 才可以正常运行\n或者直接从我这里copy也行\n方法5: 手动下载 打开 files \u0026amp; Versions, 点击下载即可\n环境变量配置 代理开启和关闭命令： 开启：\nexport http_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export https_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTP_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTPS_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; 关闭：\nunset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY 模型下载地址设置 命令行版本 export CACHE_ROOT=\u0026#34;/work/models\u0026#34; export HF_HOME=\u0026#34;$CACHE_ROOT\u0026#34; export HF_HUB_CACHE=\u0026#34;$CACHE_ROOT/hub\u0026#34; export HF_DATASETS_CACHE=\u0026#34;$CACHE_ROOT/datasets\u0026#34; export HF_ASSETS_CACHE=\u0026#34;$CACHE_ROOT/assets\u0026#34; export TRANSFORMERS_CACHE=\u0026#34;$CACHE_ROOT/transformers\u0026#34; python版本 import os os.environ[\u0026#34;CACHE_ROOT\u0026#34;] = \u0026#34;/work/models\u0026#34; os.environ[\u0026#34;HF_HOME\u0026#34;] = os.environ[\u0026#34;CACHE_ROOT\u0026#34;] os.environ[\u0026#34;HF_HUB_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;hub\u0026#34;) os.environ[\u0026#34;HF_DATASETS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;datasets\u0026#34;) os.environ[\u0026#34;HF_ASSETS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;assets\u0026#34;) os.environ[\u0026#34;TRANSFORMERS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;transformers\u0026#34;) GPU 使用设定 export CUDA_VISIBLE_DEVICES=0,1,2,3 Ollama 安装 在 Linux 上，安装非常简单，只需要一行指令：\ncurl -fsSL https://ollama.com/install.sh | sh 环境配置 OLLAMA 核心配置 export OLLAMA_GPU_LAYER=cuda export OLLAMA_HOST=0.0.0.0 export OLLAMA_KEEP_ALIVE=-1 export OLLAMA_MAX_LOADED_MODELS=3 export OLLAMA_MODELS=/work/models export OLLAMA_NUM_GPU=8 export OLLAMA_NUM_PARALLEL=4 export OLLAMA_SCHED_SPREAD=1 Hugo Hugo如何置顶一篇文章 添加 weight 参数 e.g.:\n--- title: \u0026#34;My Post\u0026#34; date: 2020-09-15T11:30:03+00:00 weight: 1 --- 这里 weight 越小置顶优先级越高\n除了文章的 weight, 还可以设置 tag weight 在同一个 tag 下排序： tags_weight: 10\nHugo 如何打开侧边目录 在页面 yaml 处添加：\n--- ShowToc: true TocOpen: true --- ShowToc 为 True 表示展示目录 TocOpen 为 True 代表展开所有\nGadgets 妙妙小道具 PDF压缩 注：需要先安装这个 package: apt-get install ghostscript\ngs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/default -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf input.pdf GPU占用可视化 nvitop 一个好用的 GPU 占用显示插件 (based on nvidia-smi, 需要 Nvidia 驱动)：\npip install nvitop nvitop ssh ssh 连接 没有 ssh 怎么办，先看(这篇文章)[https://blog.csdn.net/GitHub_miao/article/details/135050696]\n想要免密登录怎么办，再看(这篇文章)[https://zhuanlan.zhihu.com/p/350160634]\n一言以蔽之，在有 ssh 的情况下：\n# 示例代码：生成SSH密钥对, 名字可以改的 ssh-keygen -t rsa -b 4096 -f ~/.ssh/my_key # 上传公钥 注意是公钥 scp-copy-id user@host # 这一步也可以通过 # 复制 .pub 文件到 ./ssh 文件下 # 然后把这个文件用 cat xx.pub \u0026gt;\u0026gt; authorized_keys # 这样就可以啦 最后，记得在本地 VSCode 里修改 ssh config\n# 示例代码：为远程主机配置别名 Host my_server HostName remote_server User user IdentityFile ~/.ssh/my_key 软链接 有时候，我们会因为一些原因把一份文件复制得到处都是\n但是大部分时候，这种文件都非常的大\n那复制到很多个地方就会占用掉很多的磁盘空间\n在这种情况下呢，软连接就起作用了\n软连接可以理解为桌面快捷方式，其实就是一个指向真正文件的指针\n那要怎么做呢？\n很简单，只要：\nln -s 即可。\n具体用法：\nln -s /original_file /target_file 这样的话就可以把 original_file 链接到 target_file那里去\n这样就可以在 target_file 中访问 original_file 中的内容辣。\nGeneral Linux command 检查磁盘剩余空间 下面这条指令可以从大到小列举所有的文件的大小，包括那些隐藏的文件\ndu -sh .[!.]* * | sort -h e.g.: 这样就可以很好的看出这个文件夹下的那个文件比较大了\n注：\n.[!.]* 会列出所有隐藏文件夹/文件（以 . 开头的）。 * 会列出所有非隐藏文件夹/文件。 -s 只显示每个目录/文件的总大小，而不是逐层展开。 -h 以人类可读格式显示（KB/MB/GB）。 sort -h 按大小排序，方便看哪个文件夹最大。 而下面这条指令会输出当前目录下的文件大小(不会递归展示文件夹内部文件的大小)\ndu -h --max-depth=1 AI 技巧 GPT Prompts 文章阅读 Prompt 请仔细阅读这篇文章，并告诉我： 1. 这篇文章的动机是什么，要解决什么问题 2. 这篇文章大概讲了什么 3. 这篇文章的创新点是什么 4. 这篇文章解决了什么问题，之前的人为什么不能解决 5. 这篇文章还有什么问题没解决 6. 这篇文章有什么需要我注意的点 7. 这篇文章是如何做实验的，setting 是什么 8. 这篇文章的算力要求是多少，多少卡运行了多久，用了什么数据集，是不是可以公开获取的，模型代码呢，能不能公开获取 如果这篇文章提出了一个模型，那请告诉我 (如果没有提出模型请告诉我为什么这篇文章不需要提出新模型)： 1. 这个模型的输入是什么 2. 输出是什么 3. 输入和输出数据经过了什么处理 4. 这个模型是如何处理输入和输出数据的 Tensorboard 清除 tensorboard 端口占用： 这条适用于 tensorboard 端口占用无法打开新的 tensorboard 的时候\nkill $(ps -e | grep \u0026#39;tensorboard\u0026#39; | awk \u0026#39;{print $1}\u0026#39;) Mojoco Mujoco GPU 使用设置 MUJOCO_GL=egl # GPU render MUJOCO_GL=OSMesa # CPU render MUJOCO_EGL_DEVICE_ID=0 # Mujoco Device set Mujoco 多 GPU 配置 Mujoco 默认只会调用主 GPU, 也就是 GPU 0\n那如何让 Mujoco 调用其他的 GPU 呢\n很简单，只需重装一下驱动即可\n首先先 nvidia-smi一下看看 Nvidia-driver 是什么版本的， 这里以 CUDA 12.0; Nvidia-driver Version 525 为例\n直接\napt-get install nvidia-driver-525 如果遇到 Driver 配置问题，请见此处的解决方案\nTmux 会话（Session）管理 tmux new -s mysession tmux ls tmux a -t mysession tmux kill-session -t mysession 断开： Ctrl + b, d\n基本快捷键 前缀键：Ctrl + b\n快捷键 说明 Ctrl + b, c 新建 Ctrl + b, n 下一个 Ctrl + b, p 上一个 Ctrl + b, 数字 跳转 Ctrl + b, \u0026ldquo;,\u0026rdquo; 重命名 exit / Ctrl+d 关闭 分屏 操作 快捷键 左右分屏 Ctrl + b, % 上下分屏 Ctrl + b, \u0026quot; 移动 Ctrl + b, 方向键 调整大小 Ctrl + b, + Ctrl + 方向键 关闭 exit / Ctrl + d 最大化 Ctrl + b, z 显示编号 Ctrl + b, q 滚动 / 复制 操作 快捷键 进入 Ctrl + b, [ 选择 Space 复制 Enter 退出 q 会话切换 Ctrl + b, s\n常用命令速查 tmux new -s \u0026lt;name\u0026gt; tmux ls tmux a -t \u0026lt;name\u0026gt; tmux kill-session -t \u0026lt;name\u0026gt; Bug Fix 环境配置 bug 如果是 apt-get 导致的冲突，请先试试 apt --fix-broken install\nPackage 过期 bug scipy package scipy.misc.derivative 弃用 在 1.16.0 + 版本中，scipy 提供了\nscipy.differentiate.derivative 作为替代\n使用方法：\nfrom scipy.differentiate import derivative GPU 环境配置 bug nvidia-driver版本冲突 通常发生在重装 nvidia-driver 之后\n无法通过 apt --fix-broken install 解决\n这时候可以：\n方法 1：\nsudo dpkg --purge --force-all \\ nvidia-driver-525 \\ nvidia-dkms-525 \\ xserver-xorg-video-nvidia-525 \\ libnvidia-decode-525 \\ libnvidia-encode-525 sudo apt-get clean sudo apt-get update sudo dpkg --configure -a sudo apt-get -f install sudo apt-get autoremove --purge -y sudo apt-get install aptitude sudo aptitude purge \u0026#39;~i nvidia-*\u0026#39; sudo aptitude safe-upgrade 方法 2：\nsudo apt-mark unhold libnvidia-* nvidia-* # 先解除 hold sudo apt-get remove --purge \\ libnvidia-* nvidia-* \\ --allow-change-held-packages # 然后完成删除整条依赖链 sudo dpkg --configure -a sudo apt --fix-broken install sudo apt autoremove -y sudo apt clean # 最后让 apt 来收尾 方法3 (强行复写 apt package, 不建议在物理机上使用):\n# 先让 dpkg 走“强覆写”把损坏状态收尾 sudo apt-get -o DPkg::Options::=\u0026#34;--force-overwrite\u0026#34; \\ --fix-broken install # 如果还有半配置包，再跑一次 sudo dpkg --configure -a SSH bug 远程 X server 有时候可以用 X server 来把服务器上的窗口回传到本地：\n使用方法如下：\nssh -X name@ip 或者在 VS Code 中修改 config:\n在 config 中增加如下信息：\n# 相当于 ssh -X ForwardX11 yes # 使用受信任的 X11 转发（相当于 ssh -Y） ForwardX11Trusted yes # 如果 xauth 不在默认路径，可指定其位置（macOS 安装 XQuartz 后） # XAuthLocation /opt/X11/bin/xauth Hugging face bug 遇到 API 报错时的解决方案 详细说明：\n“requests.exceptions.MissingSchema: Invalid URL \u0026#39;/api/resolve-cache/models/openai/clip-vit-base-patch16/57c216476eefef5ab752ec549e440a49ae4ae5f3/config.json?%2Fopenai%2Fclip-vit-base-patch16%2Fresolve%2Fmain%2Fconfig.json=\u0026amp;etag=%229f7102db4ae77c02982bfec1c16a63039fbc78db%22\u0026#39;: No scheme supplied. Perhaps you meant https:///api/resolve-cache/models/openai/clip-vit-base-patch16/57c216476eefef5ab752ec549e440a49ae4ae5f3/config.json?%2Fopenai%2Fclip-vit-base-patch16%2Fresolve%2Fmain%2Fconfig.json=\u0026amp;etag=%229f7102db4ae77c02982bfec1c16a63039fbc78db%22?” 首先先看看 hugging face hub 有没有更新\npip install -U huggingface_hub transformers MySQL SQL 语法 SQL 初始化 更详细的内容，请见这个文档\nuse RUNOOB; 命令用于选择数据库 set names utf8; 命令用于设置使用的字符集。\nSELECT - 从数据库中提取数据 UPDATE - 更新数据库中的数据 DELETE - 从数据库中删除数据 INSERT INTO - 向数据库中插入新数据 CREATE DATABASE - 创建新数据库 ALTER DATABASE - 修改数据库 CREATE TABLE - 创建新表 ALTER TABLE - 变更（改变）数据库表 DROP TABLE - 删除表 CREATE INDEX - 创建索引（搜索键） DROP INDEX - 删除索引 e.g.\nSELECT column_name(s) FROM table_name WHERE condition ORDER BY column_name [ASC|DESC] MacOS 下的指令 Gadgets 小组件 硬盘 硬盘信息 brew install smartmontools # 安装 smartctl -a disk0 # 使用 效果： 软件占用信息 brew install macmon # 安装 macmon # 使用 效果： 彩蛋 这个 blog 的时间是 2011 年 11 月 11 日 11 点 11 分 11 秒，时区是 UTC + 11:11\n这个时间是故意设定的最近最长连续数字时间，一共刚好 16 个 1\n","permalink":"https://tzj2006.github.io/bugjournal/commanddictionary/","summary":"All the usefull commands","title":"Command Dictionary"},{"content":"日报 — 2026-02-17 在天河服务器上全面推进Error Recovery Benchmark项目（M5非冲量场景生成达454个、M6多策略评估调试、VLA服务器修复）、为Motion-based Self-Reflection Framework编写综合文档，并修复ccusage claude-opus-4-6零成本计费bug及对全部历史日期重新导出。\n今日任务 架构与策略 🔄 M5非冲量场景生成 — 创建benchmark_v4_nonimpulse.yaml配置，修复friction/pose_perturb/gripper_bias注入器的MuJoCo API兼容性bug。通过_augment_specs_with_alternatives()机制成功生成103个pose_perturb场景和100个friction场景，数据库从251扩展至454场景（impulse:121, natural:130, pose_perturb:103, friction:100）。gripper_bias场景尚未生成。 🔄 M6多策略评估框架（Random/BC-RNN/Pi0/Pi0.5） — 目标运行Random/BC-RNN/Pi0/Pi0.5四策略对比评估。更新collector.py和3_collect_data.py，支持VLA服务器策略、摄像头图像传递、load_policy函数VLA类型支持。Random策略已正常运行，BC-RNN因obs格式不兼容（object vs object-state键名差异，object向量维度37≠65）多次crash，仍在调试中。VLA双服务器（Pi0 port 5556, Pi0.5 port 5557）已就绪。 🔄 BC-RNN观测格式兼容性修复 — StateExtractor输出的state_info格式与robomimic期望的原始robosuite观测格式不兼容。经三轮迭代修复：手动构建观测向量→注入_raw_obs→使用step()返回的原始obs。最终解法是在collector的run_episode()循环中追踪env.step()返回的last_raw_obs并注入state_info供BC-RNN直接使用，但仍有object键映射问题待解决。 ✅ 修复ccusage claude-opus-4-6零成本计费bug — 在daily_summary.py中添加_FALLBACK_PRICING常量和_fix_zero_cost_models()函数，对ccusage无法从LiteLLM定价数据库获取价格的模型进行后处理补偿。初始实现错误使用mb.get(\u0026ldquo;model\u0026rdquo;)字段，修正为mb.get(\u0026ldquo;modelName\u0026rdquo;) or mb.get(\u0026ldquo;model\u0026rdquo;)后fix生效。 🔄 VLA模型验证与zhaoganlong checkpoints调研 — 探查已下载的VLA checkpoints（pi0_aloha, pi05_aloha等在zhaoganlong目录下）。发现Motion-based-Self-Reflection-Framework包含完整训练基础设施（norm_stats、评估代码，Stack D0 100%、Coffee D0 96%），但实际checkpoint权重文件已被清理删除，且训练任务不包含PickPlace。用户明确指出不能使用droid config，需要robosuite/mimicgen专用版本。 ✅ 为Motion-based Self-Reflection Framework编写综合README和教程 — 用中英混合语言撰写1491行的综合README.md，覆盖全部12个章节：项目概述、目录结构（含中文注释）、架构详解（MPM/MCM/Diffusion Policy三模块）、安装、数据准备教程（5步流程）、训练教程（4种训练方式）、推理教程（5种模式）、失败分析教程、依赖深度解析（7个依赖库）、配置参考、数据格式参考（37条运动指令码本）、FAQ。 实现与修复 ✅ RolloutGenerator注入器enabled标志支持 — 发现并修复RolloutGenerator加载所有注入器而不检查enabled标志的问题，使非冲力配置能正确禁用impulse注入器。 ✅ BC-RNN策略训练 — 使用robomimic训练BC-RNN 600 epochs，A800上约15分钟完成，checkpoint保存至bc_rnn_checkpoints/bc_rnn_pick_place/20260217112532/，已更新benchmark_v4.yaml中的模型路径。 ✅ Pi0.5 VLA服务器调试 — 修复三个串联bug：(1)IMAGE_KEY_MAP缺少pi05_libero条目导致observation/image键缺失；(2)DEFAULT_CONFIG_NAMES中pi05默认配置名错误（pi05_base→pi05_libero）；(3)pi05_base checkpoint缺少LIBERO norm_stats，通过复制franka norm_stats解决。修复后Pi0.5服务器在GPU3 port 5557正常运行。 ✅ 对全部历史日期（Feb 13-17）重新运行export修复成本数据 — 依次对2026-02-13至2026-02-17执行export命令，更新各天的token成本数据并同步至rclone远端。Feb 14成本从$3.14正确更新为$39.07，Feb 16从$5.67修正为$47.71。 ✅ SSH稳定连接方案实施 — 在服务器端实现tmux会话持久化、SSH keep-alive配置、claude-tmux一键脚本、proxy开关控制；生成本地SSH配置指导文档供本地Claude自动执行。核心设计是用tmux会话作为proxy隔离边界，保护其他共享账号用户不受影响。 ✅ 基线精度测试脚本 — 创建scripts/5_baseline_accuracy.py，运行Random+BC-RNN各20个rollout的基线测试。Random策略0%成功率（符合预期），BC-RNN因obs格式问题也显示0%（需修复后重测）。 ✅ 更新项目全景总结.md至v4.6 — 将Error Recovery Benchmark项目文档从v4.5.1升级到v4.6，涵盖12处编辑：版本号更新、里程碑表格（M5/M6/M9）、场景数据统计（271→454）、关键发现、zhaoganlong Pi0.5训练基础设施调研结果（新增§13）及更新日志。发现计划中150 natural scenes实际为130，通过子代理验证后修正。 ✅ 为Motion-based Self-Reflection Framework编写CLAUDE.md — 分析Motion-based Self-Reflection Framework代码库后，生成CLAUDE.md文件，涵盖项目架构（MPM/MCM/Diffusion Policy）、训练/推理命令、配置说明、检查点路径及关键依赖。 问题与解决方案 关键问题 1. 非冲力注入器（friction/pose_perturb/gripper_bias）使用mujoco.mj_name2id()，与robosuite封装不兼容 解决方案: 改用model.body_name2id()/.geom_name2id()方法，并添加_resolve_body_id辅助函数尝试name和name_main两种命名变体。\n关键洞察: robosuite的MuJoCo封装层将物体body名统一添加_main后缀，注入器需要兼容这一约定；同时robosuite的name2id方法在找不到时抛异常而非返回-1，需要try-except处理。\n2. friction/gripper_bias注入器依赖detector触发，但detector只提出impulse/pose_perturb类型的ErrorSpec，导致这两类注入器生成0个场景 解决方案: 在rollout_generator.py中添加_augment_specs_with_alternatives()方法：当detector触发时，为当前enabled但未被提出的注入器类型生成额外候选Spec，并标注augmented标记。修复后成功生成100个friction场景（stuck:55, tip_over:28, large_offset:17）。\n关键洞察: 注入器的触发不依赖于detector的感知逻辑，任何物理位置状态合适的时候都可以尝试任意注入器。detector负责\u0026rsquo;何时注入\u0026rsquo;，注入器类型应该独立枚举。\n3. BC-RNN策略接收的obs格式（StateExtractor输出）与robomimic期望的原始robosuite obs格式不兼容，经历多轮crash（int类型→object向量维度37≠65→_get_observations()不含拼接object键） 解决方案: 在collector的run_episode()循环中追踪env.step()返回的last_raw_obs，将其作为_raw_obs注入state_info，供BC-RNN直接使用。同时修改5_baseline_accuracy.py同步采用此方案。\n关键洞察: robosuite的observation拼接逻辑在step()内部完成，直接调用_get_observations()得到的是pre-concatenation的个别可观测量。策略模型应消费step()返回的obs。StateExtractor设计用于detector/validator的中间表示，不适合直接feed给策略。\n4. ccusage对claude-opus-4-6报告cost=0，原因是Claude Code日志中的模型名与LiteLLM数据库中的anthropic.claude-opus-4-6-v1不匹配 解决方案: 在daily_summary.py中添加_FALLBACK_PRICING字典和_fix_zero_cost_models()函数，仅对cost==0的条目补充定价，覆盖input/output/cache_creation/cache_read四类token计算。初始实现错误使用mb.get(\u0026ldquo;model\u0026rdquo;)字段，修正为mb.get(\u0026ldquo;modelName\u0026rdquo;) or mb.get(\u0026ldquo;model\u0026rdquo;)后fix生效。\n关键洞察: 第三方工具的定价数据库与实际日志中的模型名不一致是根本原因；修复应在调用方做后处理而非修改第三方包，且\u0026rsquo;仅修正零值\u0026rsquo;策略确保上游修复后自动退化为no-op，向前兼容。\n5. VLA评估需要robosuite/mimicgen版本的Pi0.5 config，但openpi内置config主要支持ALOHA和LIBERO 解决方案: 用户明确指出不能用droid config，需要robosuite专用版本；当前探查中未找到，等待确认是否需要finetune或使用其他适配方式。\n关键洞察: VLA模型与仿真环境强耦合（观察空间、动作空间不同），直接复用已有checkpoint需要确认接口兼容性。\n6. BC-RNN obs key映射不匹配：代码期望\u0026rsquo;object-state\u0026rsquo;但实际返回\u0026rsquo;object\u0026rsquo;，导致多策略评估无法运行 解决方案: 需在RobomimicPolicyAdapter的_to_robosuite_obs()方法中添加key重映射逻辑。\n关键洞察: BC-RNN和VLA使用不同的obs key命名约定，需要在adapter层统一处理而非修改核心代码。\n一般问题 7. SSH断连导致Claude Code会话丢失，无法通过VS Code Remote SSH恢复 解决方案: 三层防护：tmux保持会话（断连不丢）+ SSH keep-alive心跳（15s间隔，90s容忍）+ auto-ssh自动重连脚本；本地生成操作指南文档，让本地Claude自动执行配置。\n关键洞察: 分离服务器端（直接修改）和本地端（文档化），适应多设备协作场景。\n8. RolloutGenerator加载所有注入器，不支持enabled标志过滤 解决方案: 在rollout_generator.py中添加检查，跳过config中enabled=false的注入器。\n关键洞察: 配置驱动的注入器启用/禁用，避免为不同实验创建大量重复配置文件。\n9. Pi0.5服务器config_name=\u0026lsquo;pi05_base\u0026rsquo;不存在，正确配置名为\u0026rsquo;pi05_libero\u0026rsquo; 解决方案: 修改vla_server.py的DEFAULT_CONFIG_NAMES中pi05对应值，并同步更新IMAGE_KEY_MAP添加pi05_libero条目。\n关键洞察: Pi0.5的openpi配置命名规范与物理checkpoint路径命名不一致，pi05_base是checkpoint目录名，pi05_libero才是openpi训练配置名。\n10. 共享账号下proxy自动生效影响其他用户 解决方案: 注释掉.bashrc中自动source setproxy.sh，改为claude-tmux脚本内自动设proxy，保留proxy_on/proxy_off手动开关。\n关键洞察: 用tmux会话作为proxy作用域边界，将个人配置与共享账号隔离。\n11. pi05_base checkpoint缺少LIBERO norm_stats（AssetsNotFoundError） 解决方案: 将franka/norm_stats.json复制到libero对应路径，因为LIBERO任务使用Franka机械臂，norm_stats可以复用。\n关键洞察: pi05_base是通用基础模型，只有robot-specific norm_stats，没有task-dataset-specific的统计数据。但Franka机器人的状态归一化对所有Franka任务通用。\n12. VLA Server ADDRESS IN USE错误：端口5555已被占用 解决方案: 切换到端口5556（Pi0）和5557（Pi0.5），建立双服务器架构。\n关键洞察: 端口冲突揭示了双VLA服务器并行运行的可能性，意外推动了架构升级。\n13. 项目全景总结.md中场景数量数据与实际不符（计划写150 natural，实际是130） 解决方案: 先用子代理验证实际数据库文件数量，再执行编辑。\n关键洞察: 文档中的数字应以实际文件系统为准，而非以记忆或计划为准。\n14. 为第三方代码库编写综合文档时，单次agent探索因bash命令（du -sh）卡住 解决方案: 停止卡住的agent，直接用Read/Grep/Glob工具读取关键文件补充信息。\n关键洞察: 对于大型代码库文档任务，并行多agent+直接文件读取的混合策略优于单一agent全量探索。\n人类思路 vs AI 思路 战略层面 VLA模型选择：不应使用droid config 角色 思路 人类 用户明确指出这是robosuite环境，应使用robosuite/mimicgen版本的Pi0.5，而非droid或LIBERO的config。 AI AI在探查VLA模型时计划使用droid config，没有考虑robosuite环境的特殊性。 差异分析: 用户对模型-环境适配性有更深的认识，及时阻止了AI使用错误config导致的评估偏差。\nzhaoganlong Pi0.5训练基础设施调研 角色 思路 人类 发现zhaoganlong的PickPlace训练数据不可用（checkpoints已被删除）。 AI AI进一步调研发现了完整的训练基础设施（数据流水线、norm_stats、评估代码），并识别出PickPlace从未被训练过（跨任务泛化失败的根因）。 差异分析: 人类关注单一事实（checkpoints不在），AI挖掘了更深的系统性发现（PickPlace是训练盲区）。\nM5/M6实施计划设计 角色 思路 人类 用户提供了完整的多阶段实施计划，包含具体命令、文件路径、数据集来源，约束了使用预训练模型并跳过缺失的VLA模型。 AI AI基于计划逐步执行，发现了注入器API bug等实现层面的问题，并主动识别collector.py中VLA支持已存在从而精简计划。 差异分析: 用户负责高层架构规划和约束设定，AI负责底层实现调试，分工清晰有效。\n使用zhaoganlong已有的finetuned Pi0/Pi0.5 checkpoint 角色 思路 人类 用户主动提出到zhaoganlong目录下找robosuite训练的Pi0和Pi0.5 checkpoint直接使用，而不是继续用base model或重新训练。 AI AI试图寻找checkpoint但未主动提出这一替代路径，专注于调试现有base model的配置问题。 差异分析: 用户知道已有领域特定的finetuned模型存在，主动提供了更优资源路径。AI则陷入对base model配置问题的局部修复中，没有主动探索更好的起点。\nccusage定价修复策略 角色 思路 人类 在自己控制的脚本层添加回退定价，而非等待或fork第三方ccusage包。通过观察输出数字（$3.14 vs $39.07）来确认fix是否生效。 AI AI识别了问题根因（LiteLLM模型名格式差异），设计了\u0026rsquo;仅对cost==0补充\u0026rsquo;的安全策略，并在首轮fix无效后主动通过Python脚本打印modelBreakdown原始JSON来定位字段名问题。 差异分析: 人类确定了修复位置和关注宏观结果，AI设计具体安全性约束并主动排查中间状态。两者协作才发现了初始实现的字段名错误。\nMimicGen预训练BC-RNN checkpoint的存在性 角色 思路 人类 假设MimicGen官方会提供预训练checkpoint。 AI 通过探索官方仓库文档和本地文件系统，发现MimicGen只发布数据集不发布checkpoint，需要自行训练。 差异分析: AI主动验证假设而非接受前提，避免了基于错误预期的规划。\n实现层面 SSH配置方案拆分（服务器直接改+本地文档化） 角色 思路 人类 用户提出服务器端让AI直接改，本地端生成文档让本地Claude自动执行——这是一个跨设备协作的巧妙设计。 AI AI最初计划生成纯指导文档，没有考虑分拆为可自动执行的方式。 差异分析: 用户的分拆方案更具可操作性，利用了\u0026rsquo;在不同设备上有不同Claude实例\u0026rsquo;的架构特点。\nproxy开关设计 角色 思路 人类 用户主动提出需要proxy开关，且希望能按用户身份区分——最终确认按tmux会话边界隔离，而非真正的用户名区分（因为是共享账号）。 AI AI最初计划在claude-tmux脚本中自动设proxy，未考虑共享账号场景下影响其他用户的问题。 差异分析: 用户比AI更早意识到共享账号环境的约束，主动要求设计开关机制；AI在被提醒后才设计了tmux边界隔离方案。\nAI 局限性 重要局限 在探查VLA模型时计划使用droid config而非robosuite专用config，对模型-环境适配性的判断不准确，被用户纠正。 friction和gripper_bias注入器未能成功生成场景后，AI将其归为\u0026rsquo;可接受\u0026rsquo;而未深入分析检测器触发条件问题，存在回避困难问题的倾向。 在BC-RNN观测格式问题上花了三轮修复才找到根本原因。第一轮忽略了object向量的高维度；第二轮没有意识到robosuite的observation拼接在step()内部完成；第三轮才找到正确解法。 在调试过程中没有主动检查zhaoganlong目录中已有的finetuned checkpoints，需要用户主动提示才意识到这个更优的资源。 在规划阶段未能主动识别collector.py中VLA支持已存在，导致计划中包含了不必要的实现步骤，需要后续探索才能发现。 ccusage底层使用LiteLLM但本地缓存可能过时，AI无法直接访问第三方npm包的内部定价数据库结构，只能通过间接方式（打印原始JSON）来验证字段名。 一般局限 在第一次实现fallback定价时错误使用了mb.get(\u0026ldquo;model\u0026rdquo;)而非mb.get(\u0026ldquo;modelName\u0026rdquo;)，未事先验证ccusage输出的实际JSON字段结构，导致第一轮fix完全无效，需要人类发现后才重新排查。 未能主动考虑共享账号环境下proxy自动启用对其他用户的影响，需要用户提醒才意识到问题。 多次在计划模式（ExitPlanMode）下被用户拒绝，说明AI对用户需求的理解需要多轮迭代才能准确把握。 对Pi0.5 checkpoint的norm_stats问题采用了快速修复（复制franka stats到libero路径），而没有验证franka和LIBERO的状态空间是否完全相同，norm_stats是否可直接复用。 在编写项目全景总结.md时，最初接受了计划中的错误数字（150 natural scenes），需要人类要求验证后才主动核查实际数据。 多次尝试使用replace_all=false的Edit工具编辑包含重复字符串的文件，导致\u0026rsquo;Found 2 matches\u0026rsquo;错误，需要额外的上下文定位才能完成编辑。 今日收获 核心收获 robosuite的MuJoCo封装层将body名统一添加_main后缀（如Milk→Milk_main），且name2id方法抛异常而非返回-1；针对此类封装层，注入器代码需要同时尝试原名和加后缀名。 robosuite的observation pipeline：step()内部调用_get_observations()后按modality拼接各observable（object-state变成object键），直接调用_get_observations()无法得到已拼接的格式。策略模型应该消费step()返回的obs而不是_get_observations()的输出。 zhaoganlong的Pi0.5训练覆盖Stack/Coffee/StackThree/Threading/ThreePieceAssembly（96-100%成功率），但从未训练PickPlace，这是跨任务泛化失败（0%成功率）的根本原因。 当error injector依赖detector触发时，如果新的injector类型没有对应的detector，需要在architecture层面加入\u0026rsquo;机会型注入\u0026rsquo;机制：任何时候条件满足都可以尝试，不依赖特定detector感知。 VLA模型与仿真环境高度耦合，不同环境（ALOHA、LIBERO、robosuite）的观察/动作空间不同，不能直接复用checkpoint；需要寻找专门针对目标环境finetune的版本。 对第三方工具的集成bug应在调用方做防御性后处理：仅对cost==0的条目生效，当上游修复后自动退化为no-op，这是最小侵入且自我修复的修复策略。 Motion-based Self-Reflection Framework的闭环架构（MPM→MCM→Diffusion Policy）与Error Recovery Benchmark高度互补，前者的数据流水线（MimicGen→LeRoBot→Pi0.5 LoRA）可以复用于为PickPlace生成训练数据。 finetuned VLA模型权重（zhaoganlong的Pi0.5 Stack/Coffee/Threading评估结果显示98-100%成功率）因磁盘清理被删除，但训练pipeline和norm_stats完整保留，可以在30min内重新训练Stack D0/D1模型。 MimicGen官方只发布数据集（不发布预训练checkpoint），BC-RNN训练必须使用MimicGen生成的HDF5数据集从头训练。 _augment_specs_with_alternatives()可以从同一批impulse specs自动生成pose_perturb和friction场景，无需独立的数据收集pipeline。 实践收获 在实现前应先打印工具的原始输出结构来确认字段名，而不是依赖文档或推断，尤其对于第三方工具的JSON输出。 LiteLLM/ccusage的模型名格式与Claude API实际名称存在差异（claude-opus-4-6 vs anthropic.claude-opus-4-6-v1），在自有脚本层添加回退定价是比等待上游修复更实用的解决方案。 Pi0系列模型的openpi配置命名与checkpoint路径命名存在差异：pi05_base是HuggingFace checkpoint目录名，pi05_libero是openpi get_config()中注册的训练配置名。使用时需区分两者。 在多人共享账号的HPC环境中，个人配置修改需要通过会话边界（tmux）而非用户身份来隔离，避免影响他人。 设计\u0026rsquo;服务器端直接修改+本地端文档化\u0026rsquo;的分工模式，可以有效利用多Claude实例协同完成跨设备配置任务。 robomimic的BC-RNN在10个demo上训练600epoch约15分钟（A800），loss收敛至-23左右；这为今后的基线实验提供了时间参考。 会话摘要 ErrorRecoveryBenchmark 🔄 M5/M6里程碑实施：非冲力场景生成+BC-RNN训练+VLA评估框架 02:43:16.023 | claude_code 按详细计划推进M5（生成friction/pose_perturb/gripper_bias错误场景）和M6（多策略评估）。发现并修复了三个非冲力注入器的MuJoCo API兼容性bug（robosuite封装差异），成功生成103个pose_perturb场景（数据库达354场景）。修复RolloutGenerator的enabled标志支持，扩展collector.py和3_collect_data.py支持VLA策略评估。BC-RNN在A800上训练600epoch约15分钟完成，最终在确认VLA config选择时用户指出应使用robosuite/mimicgen专用版本而非droid config，VLA验证任务未完成。\n✅ M5场景生成：发现friction/gripper_bias注入器0场景问题并修复 04:07:38.756 | claude_code 发现非冲量场景配置虽然正确启用了friction/gripper_bias，但生成结果为0。通过深入分析rollout_generator.py确认根本原因：所有detector只提出impulse/pose_perturb类型的ErrorSpec，导致这两类注入器永远不被触发。在_try_inject_and_validate前添加_augment_specs_with_alternatives()方法，使enabled但未被提出的注入器也能获得候选机会。修复后成功生成100个friction场景（stuck:55, tip_over:28, large_offset:17），数据库总量从354增至454。\n🔄 BC-RNN策略与collector观测格式兼容性修复（三轮迭代） 04:07:38.756 | claude_code 多策略评估中BC-RNN策略因观测格式不兼容连续三次crash：(1)state_info包含int类型的step计数；(2)手动构建的object向量维度37远小于模型期望的65；(3)robosuite的_get_observations()不包含已拼接的object键。最终解法是在collector的run_episode()循环中追踪env.step()返回的last_raw_obs，将其作为_raw_obs注入state_info，供BC-RNN直接使用。同时修改5_baseline_accuracy.py同步采用此方案。\n🔄 规划M5+M6里程碑：非impulse场景生成与多策略对比评估 00:39:46.933 | claude_code 通过调研确认MimicGen官方不提供预训练BC-RNN checkpoint，必须自行训练。探索collector.py后发现VLA支持已完整实现，删除了计划中的冗余Phase 4。最终计划精简为5个阶段：M5非impulse场景生成、LIBERO数据集下载、BC-RNN训练、VLA双服务器评估、基准准确率脚本。用户两次拒绝了ExitPlanMode，会话未完成规划审批。\n🔍 已下载VLA模型探查与robosuite适配版本确认 04:19:57.872 | claude_code 探查已下载VLA checkpoints，发现主要是pi0_aloha和pi05_aloha等ALOHA系列模型（存于zhaoganlong/openpi_cache/）。用户明确指出不能使用droid config，需要robosuite/mimicgen适配版本的Pi0.5。在openpi框架config中未找到专用robosuite config，VLA验证任务暂停等待进一步确认。\n🔍 调研zhaoganlong的Pi0/Pi0.5 finetuned checkpoints可用性 04:07:38.756 | claude_code 用户要求利用zhaoganlong目录下已有的robosuite训练Pi0/Pi0.5 checkpoint。调查发现Motion-based-Self-Reflection-Framework包含完整的eval结果（Stack D0 100%, Coffee D0 96%, StackThree D0 94%等）、norm_stats和训练配置，但实际checkpoint权重文件已被清理删除。训练任务不包含PickPlace，只有Stack/Coffee/Threading/ThreePieceAssembly。当前对话因context耗尽在等待用户决策：重新训练vs继续用base model。\n✅ Pi0.5 VLA服务器三重配置错误修复：config_name/IMAGE_KEY_MAP/norm_stats 04:07:38.756 | claude_code Pi0.5服务器启动失败经历三轮调试：首先pi05_base配置名不在openpi注册表中（应为pi05_libero）；其次切换到pi05_libero后IMAGE_KEY_MAP缺少该条目导致observation/image键映射失败；最后pi05_base checkpoint缺少LIBERO norm_stats，通过复制franka norm_stats解决。修复后Pi0.5服务器在GPU3 port 5557正常运行，smoke test验证成功传输观测和接收动作。\n✅ 更新项目全景总结.md至v4.6（454场景、双VLA服务器、BC-RNN集成） 05:17:43.736 | claude_code 根据详细计划执行了12处文档更新，将Error Recovery Benchmark项目文档从v4.5.1升级至v4.6。关键更新包括：场景数据库271→454场景、M6状态从stub改为进行中、新增§13记录zhaoganlong Pi0.5训练基础设施调研（Stack 100%/Coffee 96%，但PickPlace从未训练）。在编辑过程中发现计划中的150 natural scenes实际为130，通过子代理验证后修正。\n🔄 SSH不稳定问题分析与方案设计：tmux+keep-alive+自动重连 04:10:05.872 | claude_code 用户反映通过proxy连接服务器的SSH不稳定，Claude Code会话断连后无法恢复。AI探查了系统工具（tmux可用，autossh/mosh不可用），与用户确认连接拓扑（本地→跳板机→服务器，主要用VS Code Remote SSH）。最终确定分两步走：服务器端直接修改配置+脚本，本地端生成文档供本地Claude自动执行。用户明确了需要按tmux会话边界隔离proxy，而非按用户名。\n✅ SSH稳定连接方案完整实施：服务器配置+proxy隔离+本地指南 04:35:51.391 | claude_code 按计划完成服务器端全部配置：.ssh/config添加keep-alive、.tmux.conf增强稳定性、创建~/bin/claude-tmux脚本、注释掉.bashrc中自动proxy并将~/bin加入PATH。核心设计是用tmux会话作为proxy隔离边界，其他用户登录不受影响。同时创建docs/local_ssh_setup_guide.md供本地Claude自动执行本地配置（SSH ControlMaster、auto-ssh脚本、VS Code设置）。\nMotionSelfReflection ✅ 为Motion-based Self-Reflection Framework编写1491行综合README和教程 05:12:25.761 | claude_code 与用户确认需求（中英混合、单文件、包含openpi/GraspVLA等所有组件）后，启动4个并行探索agent，同时直接读取关键文件。整合信息后撰写了1491行的综合README.md，覆盖12个主要章节（含37条运动指令码本、完整HDF5数据格式、5步数据准备流程）。一个agent因bash命令卡住被手动停止，通过直接文件读取弥补。\n✅ 为Motion-based Self-Reflection Framework分析代码库并创建CLAUDE.md 04:57:00.823 | claude_code 通过多个并行agent全面探索了Motion-based Self-Reflection Framework代码库（MPM/MCM/Diffusion Policy三模块架构）。成功创建了CLAUDE.md，涵盖项目概述、训练命令、推理流程、配置参数和检查点路径。同时将该项目的信息添加为Error Recovery Benchmark CLAUDE.md中的参考来源（Related Project章节）。\nGadgetSummarize 🔄 修复ccusage opus-4-6零成本bug并修正历史报告数据 20:12:16.423 | claude_code 在daily_summary.py中实现了三处修改（_FALLBACK_PRICING常量、_fix_zero_cost_models()函数、调用点），为ccusage无法识别的claude-opus-4-6补充回退定价。修复后本地日志已正确计算成本，但历史报告文件（2026-02-17.md/json）仍有零值数据。会话被用户中断，远程文件同步和MD更新未完成。\ngadget-summarize 🔄 探查ccusage无法从LiteLLM获取opus-4.6定价的根本原因 15:17:16.223 | claude_code 用户要求查找ccusage无法fetch LiteLLM定价的原因。AI启动了多个探索子代理分析ccusage集成架构、日志数据和LiteLLM定价数据库，确认根本原因是模型名不匹配（claude-opus-4-6 vs anthropic.claude-opus-4-6-v1），并制定了fallback定价的修复计划。由于用户拒绝了ExitPlanMode操作，该会话以计划完成但未实施结束。\n✅ 修复ccusage claude-opus-4-6零成本bug并对全部历史日期重新导出 16:31:06.227 | claude_code 用户提供了完整的修复计划，要求在daily_summary.py中添加fallback定价表来修复ccusage无法识别claude-opus-4-6的成本问题。AI实现了_FALLBACK_PRICING常量和_fix_zero_cost_models()函数，但初始实现错误使用了model字段（实际为modelName），导致第一轮fix无效。发现字段名错误后修正，对Feb 13-17五天数据重新export，成本数据恢复正常（Feb 16从$5.67修正为$47.71）。\n✅ 生成2026-02-17结构化日报（per-device汇总） 16:39:22.468 | claude_code 日报生成任务，分析当天的AI交互记录并以JSON格式输出结构化日报。该会话本身即为日报生成过程。\nToken 用量 总览 指标 数值 总 Token 64,566,918 输入 Token 77,305 输出 Token 38,791 Cache 创建 3,946,096 Cache 读取 60,504,726 Cache 命中率 93.9% 总费用 (USD) $43.4006 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-haiku-4-5-20251001 45,076 1,514 1,479,293 13,160,214 $3.2178 7.4% claude-opus-4-6 32,229 37,277 2,466,803 47,344,512 $40.1828 92.6% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-17/","summary":"在天河服务器上全面推进Error Recovery Benchmark项目（M5非冲量场景生成达454个、M6多策略评估调试、VLA服务器修复）、为Motion-based Self-Reflection Framework编写综合文档，并修复ccusage claude-opus-4-6零成本计费bug及对全部历史日期重新导出。","title":"Bug Journal 2026-02-17"},{"content":"日报 — 2026-02-16 在MIHD空间组学项目中完善了VLA研究报告、实现4种新融合策略并完成基准测试；在机器人错误恢复基准项目中完成了策略错误检测分类系统v4.3、VLA Policy Server集成修复、50次自然错误捕获rollout（场景总量达271个），同时扩展了日报工具的会话摘要功能\n今日任务 架构与策略 ✅ ENHANCEMENT_PLAN v2实施——4种新融合策略 — 实现ElementWiseSumFusion（STPath启发，零训练）、QFormer Register Tokens+Spatial Bias（可配置选项）、AdaLN+AdaLNAttentionFusion（section-aware）、SpatialAttentionBias模块，全部注册到FusionFactory和apply_fusion()；修复维度不匹配和CUDA OOM两个bug ✅ 策略错误检测分类系统完善（v4.3） — 实现完整的三级分类体系（3 Family→10 Category→25 Type），包含19个分类器；新增GraspWrongPoseClassifier、补全子包__init__.py导出、新增ErrorMetricsComputer类；发现并修复grasp_start_step=0被or运算符误判的关键bug；73/79个测试全部通过 ✅ VLA Policy Server集成修复 — 修复vla_server.py中错误的openpi API调用（改用create_trained_policy(train_config=\u0026hellip;)），添加\u0026ndash;config_name参数，修复图像键名映射，修复PolicyServerAdapter的obs预处理和action chunk缓冲，更新1c_generate_from_policy.py支持VLA和injection/natural_capture两种模式 ✅ VLA（Pi0）端到端pipeline验证 — 启动Pi0 VLA服务器（openpi05 conda环境，GPU0，端口5556），运行10次injection模式rollout，发现并修复_generate_from_single_rollout初始obs缺少图像的关键bug，最终生成3个error scene，验证全流程可运行 ✅ 运行50次VLA自然错误捕获rollout — 启动Pi0服务器，运行50次natural_capture模式rollout（~12.5分钟），共生成150个自然错误场景，项目场景总量从121增至271，超过M5的200目标；错误类型分布：joint_limit_approach 47%、grasp_wrong_pose 30% 🔄 v4.5升级方案规划 — 规划从人工错误注入转向VLA自然错误捕获的架构升级，涵盖7个步骤，包括DINOv2视觉错误检测器设计；用户在最终确认阶段中断了计划 ✅ VLA研究报告更新 — 在报告中新增§3.8「目标分布问题」（5种方案对比分析）、第五.五部分「全部21种融合方法排名」，并更新第七部分总结将Register Tokens+AdaLN升为首位 🔄 GCN hidden_dim消融实验基础设施 — 添加\u0026ndash;hidden_dim CLI参数、注入fusion_config、更新EvaluationJob/runner/pipeline_config，新增staig_hdim_{64,128,256,512}四个实验配置；启动151508上的消融实验，正在运行中 ✅ MIHD项目可视化结果分析与151672坍塌诊断 — 定位可视化结果文件，分析各方法聚类效果，发现151672 section上uni_staig_fusion严重坍塌（仅2个有效cluster，std=0.12），确认根本原因为GCN hidden_dim=64过小叠加spatial refinement放大 🔄 日报工具增加会话摘要章节 — 在Windows设备上扩展daily_summary.py，在LLM生成的报告JSON中增加conversation_summaries字段，为每个独立会话生成摘要。已完成计划设计（修改SUMMARY_PROMPT、Anthropic工具schema、generate_markdown、max_tokens），用户批准计划但实现工作被截断 ✅ 实现Gemini VLM错误检测器并集成到vlm_analyzer.py — 参考zhaoganlong的Gemini VLM实现，在vlm_analyzer.py中添加_call_gemini()方法，新增extract_error_frames.py和classify_error_vlm.py两个独立脚本，并在项目全景总结.md写入§12 VLM教程 ❌ 端到端VLA测试 — 尝试在GPU节点运行完整VLA pipeline，但被用户中断，未完成。VLA代码修复已就绪，实际运行验证尚未完成 🔄 M5/M6未完成目标分析与下一步规划 — 用户要求针对M5（非impulse注入器场景生成、多任务扩展）和M6（多策略对比评估）的未完成目标制定实施计划，AI正在通过子agent探索代码库 ✅ Claude Code CLI作为VLM错误分类器规划与实现 — 确认Claude Code CLI（claude -p）可通过Read工具分析图像帧进行错误分类，支持交互式和脚本式两种模式，并将使用方式写入tutorial.md和项目全景总结.md §12.2节 实现与修复 ✅ 151508 slide Benchmark测试 — 在151508 section上对element_wise_sum、adaln_attention、qformer_enhanced、plain qformer等新策略运行评估，收集ARI/NMI指标；同时完成scGPT+UNI2和PCA+UNI2两种编码器组合的基准测试 ✅ Error Recovery Benchmark文档修正 — 修正CLAUDE.md、项目全景总结.md和error_taxonomy.py中的事实性错误（检测器6→5、分类器19→20、错误类型25→24等），更新代码行数统计、场景数量（30→118）和评估运行次数，79个测试全部通过验证 ✅ VLA模型检查点定位与状态确认 — 搜索服务器上的VLA模型检查点，确认Pi0、Pi0.5和Phoenix模型已在zhaoganlong目录下完整下载（12GB），Flare模型未找到，BC-RNN仅有lift任务检查点 🔄 下载Pi0检查点和基准数据集 — 在tianhe服务器上启动并行下载任务：BC-RNN的lift成功但can/square/transport的URL返回404，MimicGen源数据集部分已有，Pi0大规模下载被用户暂缓 ✅ 批量可视化视频生成 — 使用5个GPU并行生成20个视频（10个baseline+10个randomized），涵盖3个demo和4种力量级别，零失败，输出到outputs/error_scenes/visualizations/目录 问题与解决方案 关键问题 1. Flow Matching用于embedding融合时不存在「真实目标分布」，导致FM框架与融合任务存在根本性张力 解决方案: 识别出5种目标分布定义方案：方案C（跨模态预测）和方案E（OT中间点）评分最高；方案E在线性路径下退化为mean fusion\n关键洞察: FM不应直接用于embedding融合，而是通过重新定义问题（如跨模态预测）来规避根本性张力\n2. vla_server.py使用错误的openpi API：手动加载norm_stats+错误的配置名称导致无法加载模型 解决方案: 通过检查openpi源码发现正确API为create_trained_policy(train_config=config, checkpoint_dir=\u0026hellip;)，config_name应为\u0026rsquo;pi0_libero'\n关键洞察: openpi的create_trained_policy自动处理norm_stats等，需要通过inspect.signature验证API而非靠文档猜测\n3. _generate_from_single_rollout初始obs中没有包含图像，导致VLA服务器收到\u0026rsquo;observation/image\u0026rsquo; KeyError 解决方案: 修改_get_current_obs()添加include_images参数；在_generate_from_single_rollout和capture_natural_errors中检测PolicyServerAdapter类型并传入include_images=True\n关键洞察: 初始obs和后续env.step()返回的obs路径不同，两处代码需同步修改\n4. VLA conda环境（openpi/openpi05）与mimicgen_env无法直接导入对方的包 解决方案: 设计TCP socket+pickle协议的跨进程策略服务器架构：VLA服务器在自己的conda环境中运行，通过pickle over TCP接收obs、返回动作块\n关键洞察: 跨conda环境通信的最简方案是Python stdlib的socket+pickle，协议用长度前缀帧（4字节大端）确保消息完整性\n5. 151672 section上uni+staig_fusion出现严重模型坍塌（std=0.12，仅2个有效cluster），怀疑hidden_dim=64过小 解决方案: 设计并实现hidden_dim消融实验框架，测试64/128/256/512维度\n关键洞察: STAIG默认hidden_dim=64与其他策略（256-512）存在显著差距，维度可能是坍塌原因；spatial majority vote refinement是二级放大因素\n6. grasp_start_step=0在Python中被or运算符误判为falsy，导致hold_duration计算错误，PrematureReleaseClassifier无法触发 解决方案: 将self._grasp_start_step or step改为self._grasp_start_step if self._grasp_start_step is not None else step\n关键洞察: Python的or运算符对0等「假值」短路，当step=0是合法初始值时必须显式检查None\n7. VLA模型输入键名不匹配：代码发送observation/image/agentview，但pi0_libero期望observation/image和observation/wrist_image 解决方案: 添加IMAGE_KEY_MAP字典，将摄像头名称映射到正确的openpi键名\n关键洞察: 通过阅读库的example函数（make_libero_example()）直接获取期望输入格式，比猜测更可靠\n8. SpatialAttentionBias预计算全局(H,4384,4384)偏置矩阵导致CUDA OOM 解决方案: 修改QFormerFusion.forward()将全局预计算改为按spot惰性计算——每次只计算当前spot的(H,ctx_len,ctx_len)子矩阵\n关键洞察: 稀疏空间图的attention bias不应预计算全局稠密矩阵，应利用已有的稀疏结构\n9. AI最初未能找到VLA检查点，错误报告\u0026rsquo;未下载任何检查点' 解决方案: 用户提示AI扩大搜索范围，AI在zhaoganlong目录下找到完整的Pi0检查点（12GB）\n关键洞察: AI的第一次搜索范围仅限于当前项目目录，未查看共享服务器上的其他用户目录；用户情景记忆弥补了AI跨会话记忆不足\n10. Pi0（pi0_libero checkpoint）在PickPlace任务上100%失败（success=False） 解决方案: 预期之内——pi0_libero是针对LIBERO任务训练的，domain mismatch。natural_capture模式价值在于收集真实失败模式数据\n关键洞察: 使用domain mismatch的预训练模型做自然错误捕获，反而能获得多样化的失败模式\n一般问题 11. 文档中记录的数字与实际代码不符，甚至error_taxonomy.py自身的docstring也写错了 解决方案: 通过直接读取注册表文件（init.py中的DETECTOR_REGISTRY、CLASSIFIER_REGISTRY）和枚举定义来获取精确数字，全面替换文档中的旧值\n关键洞察: 代码自注释（docstring）也可能是错的，验证时必须以实际注册表/枚举为准\n12. plain qformer benchmark与qformer_enhanced并行启动导致GPU OOM，后续独立进程因conda run输出缓冲运行超过100分钟无进度可见 解决方案: 终止该进程，转而从历史experiment_comparison.csv查找既有结果（2/14数据），获得plain qformer ARI=0.344\n关键洞察: conda run会缓冲所有输出到进程结束才释放，导致长时间任务无法监控进度；应改用\u0026ndash;no-capture-output或在General环境中直接执行\n13. Register Tokens+SpatialAttentionBias同时启用时QFormerBlock出现维度不匹配错误 解决方案: 在_build_context()中，当use_register_tokens=True时，对spatial_bias在register token位置补零，保持维度一致\n关键洞察: 模块组合时要考虑各模块对张量维度的影响，register tokens是架构级修改不只是权重\n14. conda run的stderr输出被缓冲，无法实时查看rollout进度 解决方案: 通过ss -tlnp检测端口监听状态、ps aux检测进程存活状态、nvidia-smi确认GPU使用情况\n关键洞察: 在无法直接看日志时，可通过网络端口、进程状态、GPU利用率三维度间接确认服务运行状态\n15. Robomimic BC-RNN预训练检查点的下载URL只有lift可用，can/square/transport返回403/404 解决方案: 尝试探索robomimic官方仓库的下载脚本，搜索替代来源，实际下载路径仍在确认中\n关键洞察: 官方文档给出的URL模式不一定对所有任务都适用，需要直接验证每个URL或使用官方下载脚本\n16. scGPT无法在General conda环境中运行，但测试需要scGPT embeddings 解决方案: 发现pipeline缓存已存在，创建符号链接指向run_benchmark.py期望的旧路径\n关键洞察: 缓存路径在两种工作流中不一致，symlink是最轻量的解决方案\n17. pipeline/runner.py在缺乏General conda环境时抛出ModuleNotFoundError: scanpy 解决方案: 改用conda run -n General执行pipeline脚本\n关键洞察: pipeline runner需要在General环境中运行，不能在系统Python环境中直接调用\n18. VLA服务器端口5555已被占用，导致首次启动失败 解决方案: 切换到端口5556，成功启动服务器\n关键洞察: 在集群环境中端口冲突很常见，需要动态检测可用端口\n19. configs/benchmark_v4.yaml中出现重复的policy_error配置块（来自两个会话的编辑操作） 解决方案: 定位两个policy_error条目，删除较旧的一个，用YAML解析验证确认\n关键洞察: 跨会话编辑同一文件时需在每次会话开始时重读文件状态，避免累积重复写入\n人类思路 vs AI 思路 战略层面 Flow Matching融合的可行性 角色 思路 人类 提出核心质疑：「目标分布如何定义？」——精准定位FM用于融合的根本矛盾 AI AI最初在研究报告中将方案A（均值投影作为目标）列为可行方案，没有主动指出循环定义问题 差异分析: 人类的一句追问揭示了AI设计中的概念缺陷，是AI倾向于给出「可行方案」而非「批判性评估」的典型案例\n能否在VLA rollout中自动注入错误 角色 思路 人类 用户提问：「能不能自动在VLA rollout的同时注入error？」将其视为新功能需求 AI AI发现generate_from_policy()已经实现了这个功能，问题只是让它端到端运行起来 差异分析: 人类将其视为新功能，AI识别出这已是现有架构的设计意图\nVLM检测器的选型决策 角色 思路 人类 用户主动想到使用Claude Code CLI作为视觉分类器（非常规用法），并记住zhaoganlong实现了Gemini编码器 AI AI最初误解为Claude API，需要用户纠正；AI负责搜索具体实现代码和整合方案 差异分析: 创意和跨项目知识迁移来自人类，AI负责技术实现\nVLA检查点的记忆与搜索策略 角色 思路 人类 用户记住了之前会话中曾下载过检查点，提示AI扩大搜索范围 AI AI最初仅在当前项目目录进行搜索，未利用历史会话记忆 差异分析: 人类的情景记忆弥补了AI跨会话记忆的不足，是找到检查点的关键\n错误分类系统设计的完整性 角色 思路 人类 人类提前规划了包含A-E五个大目标、25种错误类型、两层检测方案的完整体系，包含文献调研和技术路线选择 AI AI按计划实现，保持了插件式架构一致性，补充了测试套件，发现了grasp_start_step=0 bug 差异分析: 系统设计和分类学是人类的核心贡献，AI负责代码实现和细节调试\nSTAIG坍塌原因诊断 角色 思路 人类 用户直接指出151672的std=0.12异常，并假设hidden_dim=64是原因，要求做消融实验 AI AI接受该假设并设计实验框架，通过分析embedding NPZ文件确认坍塌，提出spatial refinement是第二个放大因素 差异分析: 性能诊断由人类主导，AI负责系统性验证和框架设计\nVLA API调用方式 角色 思路 人类 用户提供了详细的修复计划，明确指出vla_server.py使用错误的openpi API，给出了具体代码示例 AI AI按计划执行，但主动通过inspect.signature验证实际API签名，发现参数名为train_config而非config 差异分析: 人类提供正确方向，AI在执行时进行额外代码级验证，发现计划示例中的细节差异\nM6策略选型 角色 思路 人类 用户明确排除了训练BC-RNN的方案，只用预训练模型（Pi0.5、Mimicgen Checkpoints） AI AI提出训练BC-RNN作为首选方案，被用户否定 差异分析: 人类对资源约束和项目定位更清晰（benchmark不应自己训练模型），AI倾向于从技术完整性出发\n存储路径和项目文档更新规范 角色 思路 人类 人类明确提出工作流规则：生成计划时必须同步更新项目全景总结.md；checkpoint必须存储在HDD_POOL/tangzijia下 AI AI实现了技术功能，但未主动建立跨会话元数据管理规范 差异分析: 人类关注项目可持续性和团队协作规范，AI更关注技术实现本身\n融合策略优先级决策 角色 思路 人类 用户提出注重「验证程度\u0026amp;效果」和「训练难度」两个维度的双轴排名，要求Register Tokens作为QFormer配置选项而非独立策略 AI AI设计了Tier A-D四层分类体系，在实施范围和架构形式上询问用户确认 差异分析: 用户有明确的工程偏好（配置选项比新策略更轻量可维护），AI倾向于提问后再决策\n实现层面 测试结果解读 角色 思路 人类 用户主动要求查看单模态基线（scGPT-only, UNI2-only, PCA-only）来理解融合是否真的有帮助 AI AI在报告测试结果时没有主动提供基线对比，需要用户明确要求才去查找 差异分析: 比较分析需要基线，AI应该在汇报融合结果时主动查找和展示基线\nAI 局限性 重要局限 跨会话记忆缺失：AI无法自主记住前一次会话中执行过的下载任务，导致错误报告\u0026rsquo;未找到VLA检查点\u0026rsquo;，需要用户主动提示才重新搜索 AI在研究报告中将Flow Matching的方案A（均值投影作为目标分布）列为可行，没有主动指出循环定义问题。AI倾向于给出「可操作的方案」而对理论完备性缺乏批判性审视 AI实现的VLA端到端代码无法完全验证：vla_server.py的openpi API调用基于代码阅读推断，pi0_libero的state编码格式存在不确定性，未经过实际运行验证 在诊断151672的聚类问题时，AI最初误以为「只有1个cluster」是指n_clusters参数为1，而不是聚类后多数spot坍塌到同一类。用户提供图片才触发正确分析路径 初次搜索范围不足：搜索仅局限于当前项目目录，未主动扩展到共享服务器上的其他用户目录（如zhaoganlong的目录） 误解「Claude Code CLI」为「Claude API」：用户明确说要用Claude Code CLI作为VLM，AI将其理解为Claude Python SDK API调用，需要用户明确纠正 跨会话状态追踪不可靠：由于上下文压缩，AI在新会话开始时无法准确记忆上一会话修改了哪些文件，导致重复创建/修改操作 一般局限 对SpatialAttentionBias的内存消耗没有预先估算，直到CUDA OOM才修复设计。对大规模稀疏结构的内存模式缺乏直觉 无法主动监控后台长时间任务进度（conda run输出缓冲问题），需要反复tail文件或等待TaskOutput超时才能判断状态 在汇报基准测试结果时，没有主动查找和展示单模态基线，需要用户明确提醒才补充。缺乏「自发的完整性检查」意识 AI无法自主识别Flare是Phoenix框架的变体，将其报告为\u0026rsquo;NOT FOUND\u0026rsquo;，需要用户纠正 给出的Robomimic BC-RNN下载URL（can/square/transport）返回404，URL格式假设不正确 在发现「所有原始6个增强计划已被实现」时，AI是通过读取代码才发现的，无法主动追踪代码库外部变更 今日收获 核心收获 Flow Matching做embedding融合的根本性限制：FM需要明确的目标分布，但融合是「创造新表示」而非「逼近已知分布」。OT中间点（方案E）和跨模态预测（方案C）是仅有的两种有原则性定义的方案，方案E在直线传输假设下退化为mean fusion QFormer Enhanced（register tokens+spatial bias，50 epochs）比plain qformer（200 epochs）性能更高（ARI 0.401 vs 0.344），空间先验和模态标记的引入效益显著 诊断模型坍塌需要直接检查embedding的统计分布（std、方差），而不是仅看最终聚类标签分布。KMeans重新聚类是快速验证embedding质量的有效工具 跨conda环境的模型推理最简方案是TCP socket+pickle协议（Python stdlib），既无需额外依赖又能处理大型numpy数组，协议用长度前缀帧确保消息完整性 阅读库的example函数（如make_libero_example()）是了解期望输入格式的最直接方法。图像键名（observation/image vs observation/image/agentview）这类非直觉性差异只能通过读源码发现 VLA服务的obs预处理有两条路径：初始obs通过state_extractor.extract()获取（需显式include_images=True），后续obs通过env.step()返回raw robosuite obs（含agentview_image等键）。两处代码需同步修改 STAIG在151508上仍是最强策略（ARI 0.500），其他方法最佳结果（qformer_enhanced ARI 0.401）尚有20%差距，说明GNN的空间图结构建模能力是关键 scGPT+UNI2融合（ARI 0.028-0.113）远低于PCA+UNI2（ARI 0.181-0.193）。单模态基线：PCA-only 0.288 \u0026raquo; scGPT-only 0.115 \u0026raquo; UNI2-only 0.036。scGPT在DLPFC数据集上的zero-shot embedding质量不如PCA spatial majority vote refinement是一把双刃剑：能改善正常情况下的聚类边界，但会放大已经不平衡的聚类结果，将少数类彻底消除 Python的or短路运算符在用0作为合法初始值时是危险的陷阱，必须用is None检查代替or来区分「未初始化」和「第0步」 验证库API的最可靠方式是inspect.signature()而非阅读文档或示例代码。openpi的create_trained_policy参数名是train_config（非config），这种细节差异会导致运行时错误 Claude Code CLI可作为多模态VLM使用：通过claude -p 'prompt'非交互模式，配合图像帧文件路径，可以实现批量错误分类自动化，无需API key管理 Pi0（pi0_libero）在domain mismatch任务（PickPlace）上的失败模式分布：joint_limit_approach(47%) \u0026gt; grasp_wrong_pose(30%) \u0026gt; misalignment_pre_grasp(17%) \u0026gt; overshoot(5%)。使用domain mismatch模型做自然错误捕获反而能获得多样化失败模式 大型项目中「代码已完成」和「系统可运行」是两个不同的里程碑。VLA集成代码写完后仍需端到端测试（GPU+网络+正确模型路径+正确输入格式）才能真正投入使用 插件式分类器架构（BaseErrorClassifier ABC+CLASSIFIER_REGISTRY+PolicyErrorMonitor统一调度）使得添加新分类器只需实现两个方法（update/finalize）并注册，核心pipeline无需修改 adaln_attention（section-aware conditioning）在单section测试中表现不佳（ARI 0.159），其设计目标是多section联合训练场景，单section下无条件化优势 AI记忆文件的重要性：通过检查.claude/projects/*/memory/目录中的自动记忆文件，可以恢复跨会话的重要上下文信息，是解决AI跨会话记忆缺失的关键工作流 实践收获 项目文档（项目全景总结.md）应与代码同步更新，作为跨会话的项目状态唯一真值来源；代码自注释（docstring）和设计文档都可能随代码演化而过时，只有注册表和枚举定义才是事实来源 zhaoganlong的Gemini VLM实现位于/HDD_POOL/zhaoganlong/Motion-based-Self-Reflection-Framework/deps/video_analyzer/api_client.py，使用ChatAnywhere代理支持gemini-2.5-pro，可直接复用 element_wise_sum（零训练，STPath启发）在ARI上略优于concat（PCA+UNI2: 0.193 vs 0.181），是有价值的零成本基线升级 在GPU集群环境中通过ss -tlnp端口监听+ps aux进程状态+nvidia-smi GPU利用率三维度组合判断服务就绪状态，比依赖日志输出更可靠 会话摘要 MIHD空间组学 ✅ 优化ENHANCEMENT_PLAN并实现Batch 1+2新融合策略 04:37:59.162 | claude_code 用户要求基于研究报告优化增强计划并实现新融合策略。AI发现原6个计划已全部实现，设计了4批新融合策略。用户确认Register Tokens作为QFormer配置选项，实施范围为Batch 1+2。最终发现Batch 1+2代码（ElementWiseSumFusion、AdaLN、SpatialAttentionBias、QFormer register tokens）也已预先实现，pipeline_config.yaml已包含实验配置。\n🔄 Flow Matching融合可行性深度讨论与VLA研究报告更新规划 01:13:35.771 | claude_code 深度讨论了Flow Matching用于embedding融合的理论基础与局限性。用户的核心追问「目标分布如何定义」揭示了FM融合的根本性矛盾。讨论了5种方案，OT中间点（方案E）和跨模态预测（方案C）最有原则性。汇总了报告中所有21种embedding融合方法，规划了报告更新内容，但计划提交被用户拒绝。\n✅ 更新VLA研究报告：目标分布问题与21种融合方法排名 04:37:59.162 | claude_code 用户要求在研究报告末尾新增§3.8「FM目标分布问题」和「第五.五部分全部21种方法排名」，并将第七部分总结更新。AI完成3处文件编辑，包括5种目标分布方案（A-E）的对比分析，以及按验证程度和训练成本双维度排列的21方法综合排名表，并将Register Tokens+AdaLN升为首位推荐。\n✅ 151508 slide新融合策略benchmark测试与结果汇总 13:00:00.000 | claude_code 在151508 section上对所有新策略运行benchmark。QFormer Enhanced（register tokens+spatial bias）取得最佳成绩ARI=0.401，显著优于plain qformer（ARI=0.344）。element_wise_sum零训练ARI=0.199略优于concat，adaln_attention在单section下表现不佳（ARI=0.159）。STAIG仍以ARI=0.500领先。plain qformer测试因conda run输出缓冲问题耗时过长，最终从历史CSV获取结果。\n🔄 实现GCN hidden_dim消融实验支持并启动151508 ablation study 22:49:28.071 | claude_code 用户指出151672上STAIG出现模型坍塌，怀疑hidden_dim=64过小。AI实现了完整的消融框架：\u0026ndash;hidden_dim CLI参数、fusion_config注入、EvaluationJob hidden_dim字段、pipeline_config新增四个消融实验配置。单元测试全部通过。最后启动了151508上hidden_dim∈{64,128,256,512}的消融实验，正在运行中。\nMIHD空间组学融合 ✅ 基于VLA研究报告实现4种新融合策略（Batch 1+2）并完成基准测试 04:49:28.128 | claude_code 实现了ElementWiseSumFusion、QFormer Register Tokens、AdaLNAttentionFusion和SpatialAttentionBias四种融合增强。过程中修复了维度不匹配和CUDA OOM两个bug。在151508 section上完成了scGPT+UNI2和PCA+UNI2两种编码器组合的基准测试，element_wise_sum和adaln_attention均有表现，但最优结果因编码器组合不同而差异显著。\nMIHD空间转录组学 🔄 定位可视化结果并诊断151672 section的聚类坍塌问题 22:26:09.691 | claude_code 用户询问可视化结果位置，AI定位到outputs/benchmark_results/各方法的visualizations/目录。用户提供151672 section的图片，发现仅有2个cluster，AI通过分析embedding NPZ文件确认是GCN训练坍塌（std=0.12，95%的spot聚到同一区域）叠加spatial refinement放大导致。用户提出维度过小假设，AI确认STAIG的64维是显著瓶颈，设计了消融实验方案，但用户拒绝了ExitPlanMode。\ngadget日报工具 🔄 为日报输出增加每个AI对话会话的摘要章节 22:51:46.427 | claude_code 用户要求在日报的最终输出文件中增加一个部分来总结与AI的每段对话。AI读取了当前daily_summary.py状态，设计了在JSON报告中新增conversation_summaries字段的方案，包括修改SUMMARY_PROMPT、Anthropic工具schema、generate_markdown渲染和max_tokens扩容。用户批准了计划，但实现工作被截断，实际代码修改尚未完成。\nErrorRecoveryBenchmark ✅ 策略错误检测与分类系统v4.3完整实现（6阶段） 00:37:58.421 | claude_code 实现了完整的策略错误检测与分类系统：error_taxonomy.py（3 Family→10 Category→25 Type），core_policy_error.py，policy_error_monitor.py，19个分类器，vlm_analyzer.py，error_analysis.py，以及Collector集成和YAML配置。发现并修复了grasp_start_step=0被or运算符误判为falsy的关键bug。73个单元测试全部通过，随后更新了CLAUDE.md和项目全景总结.md到v4.3。\n🔄 VLA模型集成架构设计与数据集下载 04:45:08.678 | claude_code 用户明确了核心研究需求：向Pi0、Pi0.5、MimicGen、Phoenix、Flare等真实VLA策略注入错误并rollout，同时需要视觉+状态联合错误检测器。AI探索了服务器资源，用户选择Policy Server架构解决跨环境问题。并行启动了Pi0、BC-RNN和MimicGen源数据集下载，BC-RNN的lift成功但其他任务URL返回404，还在查找正确的下载路径。\n🔄 VLA Policy Server端到端运行修复 06:51:17.528 | claude_code 用户提供详细修复计划，要求让VLA rollout pipeline真正可运行。AI通过inspect.signature验证openpi API（发现参数名为train_config而非config），修复vla_server.py的模型加载方式，添加图像键名映射，修复policy_adapter.py的obs预处理，更新1c_generate_from_policy.py支持vla_server策略和injection/natural_capture两种模式，并更新Makefile、config和文档。所有文件语法检查通过，79个单元测试全部通过，但实际端到端VLA测试未完成（被用户中断）。\n✅ VLA策略集成+状态检测器实现（7步计划） 05:30:32.536 | claude_code 实现了完整的VLA策略服务器架构（vla_server.py，TCP+pickle协议，支持Pi0/Pi0.5/Phoenix），PolicyServerAdapter（跨环境策略适配），EnvWrapper相机观测支持，PolicyErrorDetector综合检测器，RolloutGenerator的capture_natural_errors()自然错误捕获模式。同时更新了benchmark_v4.yaml配置和文档。用户要求将工作流规则写入CLAUDE.md，AI同步更新。73个单元测试全部通过。\n🔄 VLA端到端测试：修复图像obs bug，完成injection验证和50次natural_capture rollout 16:23:03.741 | claude_code 实现了VLA（Pi0）端到端pipeline：启动Pi0服务器（openpi05，GPU0，端口5556），发现并修复_generate_from_single_rollout中初始obs缺少camera图像的关键bug。injection模式10次rollout生成3个场景验证成功；随后50次natural_capture rollout生成150个场景（共271个，超过M5的200目标），错误类型分布：joint_limit_approach 47%、grasp_wrong_pose 30%。最后规划M5/M6未完成目标的下一步方案（进行中）。\n✅ 实现Gemini VLM错误检测器和Claude Code CLI分类器，编写中文使用教程 07:14:56.791 | claude_code 用户要求利用zhaoganlong的Gemini实现作为错误检测器，并探讨使用Claude Code CLI作为VLM分析器的可行性。AI找到了Gemini VLM代码，在vlm_analyzer.py中添加了_call_gemini()方法，新建了extract_error_frames.py和classify_error_vlm.py两个脚本，在项目全景总结.md中写入了§12 VLM教程，并另外创建了tutorial.md中文使用手册（9章节）。用户纠正了AI对「Claude Code CLI」与「Claude API」的误解。\n🔄 VLA检查点定位、v4.5自然错误生成与视觉检测器升级规划 04:59:17.779 | claude_code 会话从确认VLA模型检查点下载状态开始，AI初次搜索未找到，经用户提示后在zhaoganlong目录发现完整的Pi0（12GB）等检查点。随后AI全面分析了v4.4已有基础设施，与用户共同规划了v4.5升级方案：将错误生成从人工注入改为VLA自然rollout捕获，并新增DINOv2预训练编码器+规则的混合视觉错误检测器。用户在计划执行的最终批准阶段中断，会话在待调整状态下结束。\n✅ 修正文档中的事实性错误并更新项目统计数字 04:35:19.032 | claude_code 用户执行/init触发CLAUDE.md改进，AI通过探索注册表文件核实了多处数字错误。AI用中文重写计划后实施，修正了CLAUDE.md、项目全景总结.md和error_taxonomy.py中的全部错误数字，更新了代码行数、场景数量（30→118）和评估运行次数，79个测试验证通过。随后用户询问VLA模型支持，AI探索了服务器上的Pi0/Phoenix资源，但最终计划被用户拒绝待后续处理。\n✅ 策略错误检测分类系统实现与Gap填补 02:19:08.022 | claude_code 用户要求实现策略错误检测分类系统的完整规划。AI探索后发现代码几乎全部已实现（79测试通过），识别出3个空白：缺少GraspWrongPoseClassifier、子包__init__.py为空、缺少ErrorMetricsComputer。AI逐一填补这些空白，最终达到79/79测试通过，20个分类器完整注册。\n🔄 批量可视化视频生成与端到端VLA测试尝试 16:17:30.808 | claude_code 用户先要求运行测试后再做可视化，AI运行79个单元测试（全通过），再运行batch_visualize.py生成20个视频（baseline+randomized，5个GPU并行，零失败）。随后用户要求端到端VLA测试，被用户中断，未完成。\n🔄 确认VLA模型checkpoints是否已下载 04:57:09.111 | claude_code 用户询问VLA checkpoint是否存在。AI检查了openpi和openpi05 conda环境的checkpoint目录，确认Pi0 LIBERO checkpoint存在（在zhaoganlong缓存），但Pi0.5 MimicGen Coffee和Phoenix的fine-tuned checkpoint未下载。同时发现vla_server.py的openpi API调用方式需验证，入口脚本需更新以支持vla_server策略类型。会话因API 403错误中断。\n🔄 搜索Pi0和MimicGen checkpoint下载方案并写入计划文件 00:37:58.420 | claude_code 用户要求下载Pi0和MimicGen的checkpoint。AI搜索了HuggingFace上的lerobot/pi0_base（4B参数）和NVlabs/mimicgen的数据集下载脚本。确认存储约束（HDD_POOL 1.5P可用，HOME仅50G），将三类下载任务写入计划文件，方案已获批但执行被用户打断。\n❌ 代码库CLAUDE.md初始化（/init命令，403错误失败） 04:34:54.791 | claude_code 用户触发/init命令请求AI分析代码库并生成CLAUDE.md文件，但遇到403 Forbidden错误（Request not allowed），任务未能完成。\nToken 用量 总览 指标 数值 总 Token 98,673,131 输入 Token 68,782 输出 Token 94,167 Cache 创建 5,322,063 Cache 读取 93,188,119 Cache 命中率 94.6% 总费用 (USD) $68.3728 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 11,875 93,007 3,284,031 79,207,395 $62.5134 91.4% claude-haiku-4-5-20251001 34,279 696 1,562,326 10,921,422 $3.0828 4.5% claude-sonnet-4-5-20250929 22,628 464 475,706 3,059,302 $2.7765 4.1% 各设备用量 设备 总 Token 输入 输出 费用 DCC 25,636,195 1,799 13,314 $18.2696 TzJsDesktop 1,110,085 29 82 $2.3905 tianhe 71,926,851 66,954 80,771 $47.7127 ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-16/","summary":"在MIHD空间组学项目中完善了VLA研究报告、实现4种新融合策略并完成基准测试；在机器人错误恢复基准项目中完成了策略错误检测分类系统v4.3、VLA Policy Server集成修复、50次自然错误捕获rollout（场景总量达271个），同时扩展了日报工具的会话摘要功能","title":"Bug Journal 2026-02-16"},{"content":"日报 — 2026-02-15 在 MacBook/TzJsDesktop 完成历史日报发布到 Hugo/GitHub Pages 及 daily_summary.py 多项核心增强（原子写入、LLM 结构化输出、多设备 rclone 同步、export 幂等跳过），在 DCC 集群推进 MIHD benchmark（Q-Former/scGPT 评估），在天河集群完成 Error Recovery Benchmark 的中性动作修复、批量可视化、策略驱动场景生成框架及多类型错误检测器扩展，并研究了 VLA 技术在空间组学中的应用方向\n今日任务 架构与策略 ✅ VLA 应用性研究 — 研究 VLA 技术（FAST 动作 token 化、Flow Matching、Register Tokens、AdaLN 等）在 MIHD 空间组学多模态融合中的应用可能性；获取用户 bugjournal 中的 VLA 笔记（2025-07/08 三篇）；委托子代理研究 Flow Matching 融合、Register Tokens + AdaLN、空间组学领域最新论文对比（因用户中断未完成深入报告） ✅ 修复可视化视频中力注入效果不可见的问题（neutral action） — 根因是 Phase 3 使用 demo actions 导致 OSC 控制器对抗注入力。修复方案：将 Phase 3 拆分为注入（neutral action + re-apply force）、沉降（neutral action）、demo 恢复三个子阶段，新增 \u0026ndash;settle_steps 参数（默认 20） ✅ 策略驱动错误场景生成框架（Policy-Based Error Injection） — 实现 PolicyAdapter 抽象层（RandomPolicyAdapter + RobomimicPolicyAdapter），扩展 RolloutGenerator.generate_from_policy()，更新 BCPolicy 使用真实 robomimic 推理，新建 scripts/1c_generate_from_policy.py CLI，更新配置和 Makefile，全部 41 个单元测试通过 🔄 策略驱动错误注入系统规划 — 设计并写入计划文件：给 RolloutGenerator 加 generate_from_policy() 方法，新增 PolicyAdapter 类支持 random/BC-RNN/VLA，新增 1d_generate_from_foundation_model.py 脚本 🔄 错误检测分类系统整体规划 — 规划策略 rollout 的错误自动检测分类系统：25 种错误类型、两层检测（规则+VLM）、与 collector 集成，计划文件已写入（中文大目标/中目标/小目标格式） ✅ 实现多设备 rclone 同步工作流 — 修改 _rclone_upload 增加 subdirectory 参数（export→logs/, merge→reports/）；新增 _rclone_download_logs 从远端下载 logs；merge 子命令支持 \u0026ndash;sync flag；_config_show 显示结构化远端路径；更新 CLAUDE.md、README 和 tutorial 文档 🔄 MIHD benchmark 持续运行（General 环境） — 从 175 个实验逐步推进到 240 个实验：完成 hipt/mlp/pca/pca_hipt_concat/pca_resnet50_concat/pca_uni2 系列等所有快速实验；Q-Former 单独运行，pca_uni2_qformer 完成 8/11 ✅ 错误检测器扩展：ProximityDetector 和 CollisionDetector 新增多类型 spec — 修改 ProximityDetector 新增 pose_perturb 候选，修改 CollisionDetector 新增 friction + pose_perturb 候选，使场景生成能覆盖 3 种以上错误类型 ✅ BC 策略加载接口实现 — 在 scripts/3_collect_data.py 中实现 BC 策略加载逻辑：从 config 的 evaluation.models 读取 checkpoint 路径，错误信息友好提示（null 路径 vs 未配置），复用现有 BCPolicy 类 ✅ 实现 export 智能跳过已 summarize 设备 — 路径计算提前，先读已有文件的 _merged_devices 列表；已 summarize 的设备跳过 API 调用复用已有 device_summary；device_summary 改为按设备名索引的 dict；新增 _source_device 字段标识来源 ✅ scGPT 特征提取与评估 — 在 scgpt_3 环境下提取 11 sections 的 scGPT gene embedding（约 10 分钟），随后在 General 环境运行 77 个 scGPT 评估实验（含 gene-only、concat、mean、attention、llava_mlp、staig_fusion），65 成功 12 失败（11 个因 Q-Former OOM，1 个 151676 NaN） 实现与修复 ✅ 实现 JSON 原子写入（_atomic_write） — 使用 tempfile + os.replace 实现原子写入函数 _atomic_write，替换了代码中 5 处 open(\u0026ldquo;w\u0026rdquo;) 直接写入，消除了写入中途崩溃导致 JSON 文件损坏的风险 ✅ 实现 LLM 强制结构化 JSON 输出 — 为 Anthropic API 实现 tool_use 模式强制 JSON 输出，为 OpenAI API 使用 response_format=json_object，确保 LLM 返回可解析的结构化数据 ✅ 轨迹保存 TODO 修复 — 修复 collector.py 中轨迹保存为存根的问题：_run_episode 返回 (EpisodeSummary, trajectory) 元组，_save_episode 将完整轨迹保存为 NPZ 文件（actions/rewards/eef_pos/gripper/物体位置/元数据） ✅ 将默认子命令行为改为 export（无 API 调用） — 无子命令时默认执行 export 而不调用 API，降低误触发 API 消费的风险 🔄 Q-Former benchmark 恢复与续跑 — pca_uni2_qformer pipeline 进程在完成 6/11 后崩溃，发现 GPU 已空闲后重启，从断点继续，已推进至 8/11；scgpt_uni2_qformer 因 GPU OOM 未能运行，等待 pca Q-Former 完成后重试 ✅ 批量生成 60 个可视化视频（30 baseline + 30 randomized） — 新建 scripts/batch_visualize.py，支持多 GPU 并行渲染。先生成 10 个场景的两组视频，后扩展为全部 30 个场景（覆盖 3 demos、3 种力大小），共生成 60 个 MP4（baseline 21MB + randomized 26MB） ✅ 将 test/ git submodule 转为普通文件并脱钩 — git rm \u0026ndash;cached test，删除 .gitmodules，克隆 TzJ2006/test 内容后移除 .git，将 benchmark/ 所有文件纳入 gadget 主仓库。更新 README.md、CLAUDE.md 中所有子模块引用，以及 report.py 和 benchmark_report.html 中的 GitHub 链接 ✅ 规模化场景生成（30→118 个） — 将 max_scenes_per_demo 从 3 提升到 10，对所有 10 条 pick_place demo 运行生成，额外运行 10 次策略 rollout，共生成 118 个场景 ✅ 全系统 smoke test 验证 — 在天河集群（8×A800 GPU）运行 41 个单元测试 + 4 阶段 smoke test + 策略生成测试，全部通过 ✅ 将历史日报发布到 Hugo bugJournal 并部署到 GitHub Pages — 读取 gadget/summarize/reports/ 下的 2026-02-12、02-13、02-14 三份日报，添加 Hugo front matter（title/date/keywords/summary/draft），写入 website/content/bugJournal/，运行 update.sh 构建 139 页并推送 public/ 目录到 GitHub Pages ✅ Export JSON 末尾加 _source_device 字段 — 在 cmd_export() 中，当不使用 \u0026ndash;summarize 时在 export_data 末尾追加 _source_device 字段，格式为 \u0026lsquo;device (platform, user@host) | summarized: false\u0026rsquo;，方便快速判断 log 是否需要先 summarize • 设备标识 + test 子模块合并（计划阶段） — 用户提出两件事：1) export 不使用 \u0026ndash;summarize 时在文件末尾加设备标识（最终通过 _source_device 字段在其他会话实现）；2) 将 test git submodule 迁移到当前 repo（状态不明，未确认完成） ✅ 生成多份结构化日报 — 分别为 2026-02-12（DCC MIHD 增强设计）、2026-02-13（7 Phase 增强实现 + 调度器）、2026-02-14（两阶段 Pipeline + CalendarPro）、2026-02-13（CalendarPro 重构）生成 JSON 格式结构化日报 ✅ 更新项目全景总结.md 到 v4.2.0 — 将文档从 v4.1.0 更新到 v4.2.0，更新里程碑状态（M5 进行中 30 场景，M7 GPU 已验证），新增 M5/M7 小目标打勾，更新数据统计（30 场景、60 视频、6 脚本），更新关键路径和下一步任务 ✅ 将 gadget 源码确认 push 到 GitHub — 检查 gadget repo git 状态，确认分支已是最新（nothing to commit），无需额外 push；website 目录不是独立 git repo，其部署通过 update.sh 脚本完成 问题与解决方案 关键问题 1. 可视化视频中力注入效果完全不可见（机械臂几乎不动） 解决方案: 将 Phase 3 拆分为三个子阶段：注入期使用 neutral action（零向量）+ 每步 re-apply 力；沉降期使用 neutral action 让物体自由运动；之后恢复 demo actions\n关键洞察: OSC 控制器（kp=150）在追踪 demo 轨迹时产生的关节力矩远大于注入的外力（15-20N），完全补偿了扰动。验证阶段（drop.py）使用 neutral actions 所以效果可见，可视化脚本需要同样的处理\n2. 多设备 export 上传同一目录时日志文件混在一起，merge 时多台报告相互覆盖 解决方案: 按用途分子目录：export 上传到 logs/，merge 输出到 reports/；通过 _rclone_download_logs 实现 \u0026ndash;sync 模式从远端拉取所有设备的 logs 合并\n关键洞察: 多设备协同的核心是按职责分离存储路径，而非让所有设备写同一平铺目录；此问题由用户主动提出，AI 在设计时未预见\n3. MuJoCo 每步自动清零 xfrc_applied，单次 apply 只作用一个物理步（0.002s） 解决方案: 在力注入窗口内每步都调用 injector.apply()，与验证阶段的实现保持一致\n关键洞察: MuJoCo 的 xfrc_applied 是一个每步都会被物理引擎清零的临时量，需要持续重新设置\n4. ProximityDetector 和 CollisionDetector 只生成 impulse 类型，消耗了 max_scenes 配额，导致场景类型单一 解决方案: ProximityDetector 新增 _generate_mixed_specs() 同时提议 impulse + pose_perturb；CollisionDetector 新增 friction + pose_perturb 候选。同时将 max_scenes_per_demo 从 3 提升到 10\n关键洞察: 场景多样性的瓶颈不在注入器（4 种都已实现），而在检测器提议的 spec 类型。Proximity 触发频率最高，优先消耗配额，导致其他类型检测器（GraspPrecon）没机会生成场景\n5. 每次 export \u0026ndash;summarize 都对已处理设备重复调用 API，浪费 token 解决方案: 读取已有输出文件中的 _merged_devices 列表，跳过已 summarize 的设备，复用已有 device_summary\n关键洞察: 幂等性设计：通过持久化的元数据判断已完成的工作，避免重复计算\n6. scGPT 和 pca Q-Former 并行运行时出现 CUDA OOM（GPU 32GB 几乎全被占用） 解决方案: scGPT Q-Former 的 11 个实验推迟到 pca Q-Former 完成后单独运行，避免显存冲突\n关键洞察: Q-Former 训练本身占用约 20GB+ GPU 显存，不能与其他 GPU 密集型任务并行；应按优先级串行调度\n7. Q-Former pipeline 进程在运行 6/11 sections 后崩溃，原因未明 解决方案: 等待 GPU 完全空闲后重启 pipeline，断点续跑机制（检查 embeddings.npz 是否存在）自动跳过已完成的 6 sections，从 151672 继续\n关键洞察: 断点续跑机制有效：重启后自动识别完成的 6 个 sections 并跳过，只安排剩余 5 个\n8. BCPolicy._ensure_loaded() 存在 bug：在 start_episode() 触发加载失败时 self._adapter 已被赋值但处于损坏状态 解决方案: 改为先将适配器创建到局部变量，调用 start_episode() 成功后再赋值给 self._adapter，失败时设置 _fallback=True\n关键洞察: 惰性加载模式中，必须确保对象的赋值与初始化的原子性；中间状态的对象会导致后续调用时出现难以追踪的错误\n9. 151676 的 STAIG fusion 系列出现 model collapse（embedding 所有列零方差、NaN），导致 mclust 失败 解决方案: 通过 fallback KMeans 部分处理，但 151676 STAIG 实验标记为永久失败（非代码 bug，而是模型训练不稳定）\n关键洞察: STAIG fusion 训练对特定 section 的数据分布敏感，存在 collapse 风险；应在训练后添加 embedding collapse 检测\n一般问题 10. JSON 文件写入中途崩溃会导致文件损坏，后续读取失败 解决方案: 实现 _atomic_write：先写入 tempfile，完成后用 os.replace 原子替换目标文件\n关键洞察: os.replace 在同一文件系统上是原子操作，是 Unix 下安全文件写入的标准范式\n11. LLM 返回的 JSON 不稳定，解析失败率高 解决方案: Anthropic 使用 tool_use 强制结构化输出；OpenAI 使用 response_format=json_object\n关键洞察: 约束模型输出格式应优先使用 API 层能力，而非依赖提示词中的格式要求，可靠性更高\n12. batch_visualize.py 中 MUJOCO_EGL_DEVICE_ID 与 CUDA_VISIBLE_DEVICES 不匹配导致崩溃 解决方案: 确保 MUJOCO_EGL_DEVICE_ID=gpu_id（物理 GPU 编号）而 CUDA_VISIBLE_DEVICES=str(gpu_id)，两者必须一致\n关键洞察: 当 CUDA_VISIBLE_DEVICES 限制可见 GPU 时，MuJoCo EGL 也需要相同的物理 GPU ID 才能找到正确的设备\n13. 第一次执行计划时用户在 ExitPlanMode 阶段中断，计划需要在新会话重新实施 解决方案: 用户将完整计划文本写好后在新会话粘贴，直接驱动 AI 执行\n关键洞察: 将计划写入文件后在新会话粘贴是跨会话保留上下文的有效方式，比依赖 ExitPlanMode 流程更健壮\n14. 日报 .md 文件缺少 Hugo front matter，无法直接用于 Hugo 站点 解决方案: 读取现有 bugJournal 条目的 front matter 格式作为模板，从日报的 blockquote 摘要行提取 summary 字段，生成标准 YAML front matter\n关键洞察: summary 字段直接复用日报 blockquote 第一行，保持一致性；date 使用 -05:00 时区与现有条目匹配\n15. VLA 研究的三个深入方向（Flow Matching、Register Tokens、空间组学论文对比）因用户中断未能完成 解决方案: 用户在 Task 工具调用时多次拒绝，研究被迫中止\n关键洞察: 子代理研究任务应与用户确认是否需要，避免在用户当前不需要时启动耗时研究\n16. batch_visualize.py 选 29/30 场景，选择逻辑有 bug 解决方案: 修复 select_scenes() 逻辑：当 n \u0026gt;= len(ALL_SCENES) 时直接返回全部场景，不走随机抽样路径\n关键洞察: 随机抽样去重逻辑在 n == len(ALL_SCENES) 时因随机性可能漏掉 1 个，边界情况需要显式处理\n17. BC 策略加载配置路径为 null 时报错不友好 解决方案: 分两种情况给出不同提示：config key 存在但为 null → 告知用户设置路径；key 不存在也不是文件路径 → 列出可用 model 名\n关键洞察: 错误信息的质量直接影响使用体验，特别是在多策略配置场景\n18. website 目录不是 git repo，用户要求 \u0026lsquo;push 到 github\u0026rsquo; 时无法直接 push 源文件 解决方案: 向用户解释 website 通过 update.sh 脚本将 public/ 目录推送到 GitHub Pages，源文件不需要独立版本管理\n关键洞察: Hugo 部署常用 public/ 子目录作为独立 git repo 推送到 GitHub Pages，源文件与部署产物分离是标准实践\n人类思路 vs AI 思路 战略层面 neutral action 修复方案的定位 角色 思路 人类 提供了精确的根因分析（OSC 控制器对抗注入力）和完整的修复方案（三阶段结构），说明验证阶段为什么没问题（使用 neutral action） AI 按照方案实现：拆分 Phase 3 为三个子阶段，新增 \u0026ndash;settle_steps 参数，测试通过 差异分析: 根因分析完全由人类完成，包括 drop.py neutral action 与可视化脚本 demo action 的对比；AI 专注于实现和验证\n可视化视频无位移问题的根因识别 角色 思路 人类 观看视频后直接指出「机械臂根本就没有位移」，否定了 AI 基于日志数值分析的「117mm 位移」结论 AI 依赖日志中的 body position 数值变化（eef_pos 从 A 到 B）判断修复成功，没有考虑控制器可能补偿外力 差异分析: 人类通过直接观察结果（视频）发现问题，AI 通过中间指标（日志数值）误判成功。关键洞察是人类的「看视频」触发了 AI 的深层分析，发现了控制器竞争问题\nbenchmark 调度策略的调整 角色 思路 人类 观察到 Q-Former 每个 section 需要 ~87 分钟，主动要求停止当前 pipeline，将 Q-Former 拆分出来单独跑，先完成快速实验 AI AI 最初将 Q-Former 包含在 core_multimodal 实验组中同步运行，导致快速实验（concat/mean/attention 等）被 Q-Former 阻塞 差异分析: 人类从整体效率角度判断需要拆分慢速实验；AI 倾向于按计划顺序执行，缺乏对耗时差异的主动感知与调度优化\n多设备 rclone 同步架构设计 角色 思路 人类 主动识别出多设备共用同一目录会导致文件混淆的问题，提出按设备分目录或按用途分子目录的需求 AI 在用户提出问题后制定具体实施方案（logs/ 和 reports/ 子目录分离、_rclone_download_logs 函数、\u0026ndash;sync flag） 差异分析: 系统性架构问题（多设备协同数据隔离）由人类发现并定义，AI 负责落地实现；AI 在初始设计时未考虑多设备写冲突场景\n策略驱动场景生成的架构设计 角色 思路 人类 提供了完整的架构方案：PolicyAdapter 继承层次、RolloutGenerator 扩展方式、与现有管线的集成点、8 个文件的修改清单 AI 实现了所有 8 个修改点，发现并修复了 BCPolicy 的惰性加载 bug，补充了边界情况处理 差异分析: 架构决策（适配器模式、现有管线复用、不修改 detectors/validators）由人类提前规划；AI 在实现中发现了设计文档未覆盖的 bug\n策略注入的动机澄清 角色 思路 人类 明确提出「我现在想 inject error to policy 是为了能够生成更多样化、更贴近真实使用情况的 error」，补充了目标的核心动机 AI 将策略注入理解为技术扩展（支持更多策略类型），侧重架构设计 差异分析: 人类关注的是多样性和真实性（为什么做），AI 关注的是技术实现（怎么做）。人类的补充让计划更有针对性\nVLA 注入的可行性判断 角色 思路 人类 直接问「为什么不能 inject 到一个 VLA 里面？」，质疑 AI 把 VLA 集成放到「未来」的定位 AI 初始设计中将 VLA/Pi0 集成定位为扩展项，没有解释清楚当前架构在 action 层面已经是策略无关的 差异分析: 人类的追问促使 AI 重新分析，发现当前架构其实已经可以支持任意策略（action 来源替换即可）\nexport 智能跳过已处理设备的需求提出 角色 思路 人类 提出 export 阶段应能识别哪些设备已被 summarize，避免重复 API 调用 AI 实现了基于 _merged_devices 元数据的幂等检查，将 device_summary 重构为按设备名索引的 dict 差异分析: 幂等性优化需求由人类驱动，AI 选择了合适的实现方式（持久化元数据 + dict 索引）\n实现层面 VLA 研究方向的选择 角色 思路 人类 选择同时深入三个方向（Flow Matching 融合、Register Tokens + AdaLN、空间组学论文对比），随后拒绝了 AI 发起的子代理研究任务 AI 在初步报告后主动询问用户意向，获得确认后立即并行启动三个子代理研究 差异分析: 人类在选择研究方向后又中断了执行，说明可能只想看初步报告而非深入分析；AI 没有预判这一点，过于激进地启动了资源密集的研究任务\ntest/ 子模块脱钩方案 角色 思路 人类 在修复 test 链接的基础上，补充要求「与原本 github repo 脱钩」，指出需要清除所有指向 TzJ2006/test 的引用 AI 完成了 .gitmodules/git index 的脱钩，AI 主动搜索出 report.py 和 benchmark_report.html 中两处隐藏的 GitHub 链接并一并修复 差异分析: AI 主动的全局搜索发现了人类计划未提及的两个文件引用，体现了防御性检查的价值\n部署流程的主导方式 角色 思路 人类 主动在第二次会话中提供完整计划文本，绕过了第一次被中断的 ExitPlanMode 流程，直接驱动实施 AI 按照人类提供的计划逐步执行：读文件、生成 front matter、写入目标目录、运行 deploy 脚本 差异分析: 人类通过提前写好计划并在新会话粘贴来解决跨会话状态丢失问题；AI 执行层面较顺畅，但关键决策（格式、路径）由计划文本预先锁定\ngit push 请求的处理 角色 思路 人类 要求「把所有应该 push 到 github 的都 push」，意图是确保代码已保存 AI 检查两个 repo 状态（gadget: clean，website: 非 git repo），发现无需 push，向用户解释当前状态 差异分析: AI 优先检查状态再行动，避免无意义操作；但在解释 website 不是 git repo 时可以更主动地提供后续建议（如是否需要为源文件建立版本管理）\nAI 局限性 重要局限 通过日志数值（body position 变化 117mm）误判可视化修复成功，没有考虑机器人控制器会在物理子步补偿外力，直到人类看视频才发现 在 benchmark 调度中未主动识别不同 fusion 策略的耗时差异，需要人类观察到 Q-Former 阻塞问题后才调整策略 初始设计 rclone 同步时未考虑多设备写同一目录的冲突问题，需要用户主动指出后才提出按子目录分离的方案 在可视化视频问题上，AI 无法通过看视频直接诊断根因，需要人类提供详细的根因分析文档（包括与验证阶段的对比）才能定位问题 初始将 VLA/Pi0 集成定位为「扩展项」，没有主动分析当前架构是否已支持策略无关的 action 注入，需要人类追问才澄清 Q-Former pipeline 崩溃原因未能诊断（日志中未明显报错），只能被动等待 GPU 空闲后重启，缺乏主动的进程健康监控机制 export 阶段的幂等性（跳过已处理设备）未在初始设计中考虑，依赖用户需求驱动才实现，缺乏主动的资源利用优化意识 BCPolicy 惰性加载 bug：AI 实现的第一版存在对象中间状态问题（start_episode 失败后 _adapter 已赋值但损坏），需要测试才暴露 一般局限 启动 VLA 深入研究子代理时未先确认用户是否需要完整报告，导致用户多次拒绝工具调用后研究被中断，浪费了交互回合 第一次 ExitPlanMode 被用户中断后，AI 没有预见到跨会话状态丢失的问题，未主动将计划写入文件供下次使用 在调用 ExitPlanMode 时没有预见用户会中断，且在用户明确说「不要运行」时仍然尝试执行计划 生成日报时作为被调用的工具角色，只处理传入的对话记录，无法主动获取当天其他未被传入的对话（如 DCC 上午的会话） test 子模块迁移任务（会话中提出）最终完成状态不明，AI 未能在会话结束时明确告知用户该任务未完成 今日收获 核心收获 MuJoCo 的 xfrc_applied 每步自动清零；可视化力注入效果必须在 neutral action（非 demo action）模式下才能看到，因为 OSC 控制器在追踪轨迹时会产生补偿力矩 VLA 和空间组学领域正在独立趋同到相同的架构创新（Diffusion Transformer、模态 tokens、对比对齐），Flow Matching 和 Register Tokens + AdaLN 是最值得引入 MIHD 的技术方向 对任意策略注入错误，只需替换 action 来源（demo 数组→policy.predict()），检测→注入→验证管线完全复用，无需修改 benchmark 调度应按耗时分层：秒级（concat/mean/attention）、分钟级（staig_fusion/llava_mlp）、小时级（Q-Former）应分别调度，避免慢速实验阻塞快速实验 多设备协同写入远端存储时，按用途分子目录（logs/ 和 reports/）比所有设备写同一平铺目录更安全，是分布式日志收集的标准实践 策略适配器模式（PolicyAdapter）是解决多策略兼容问题的标准方案：统一接口 + 惰性加载 + 优雅降级，比在各处硬编码策略类型更易扩展 错误场景生成的多样性瓶颈在检测器的 spec 提议，而非注入器。触发频率最高的检测器（Proximity）会消耗 max_scenes 配额，解决方案是让它同时提议多种类型 中间指标（日志数值）可能与最终结果（视频效果）不一致，特别是当物理系统有补偿机制时。需要同时检查因果链而不只看数字 将计划写入 .md 文件后在新会话粘贴，是跨会话保留上下文的有效方式，比依赖 ExitPlanMode 流程或工具调用链更健壮 断点续跑机制需要在调度器和底层两层都做：底层 \u0026ndash;skip_cached 检查 npz 文件；调度器层提前检测可以避免不必要的进程启动开销 幂等性设计：通过持久化元数据（_merged_devices 列表）判断已完成的工作，避免重复调用昂贵操作（LLM API），是 pipeline 设计的重要原则 惰性加载模式中必须保证对象赋值与初始化的原子性：先赋值到局部变量、验证成功后才写入 self 属性，避免损坏状态 STAIG fusion 对特定 section 数据分布敏感，存在 model collapse（所有嵌入列零方差）风险；应在融合后检测 embedding collapse 并早退 实践收获 os.replace 原子写入（tempfile + os.replace）是 Unix 下安全文件写入的标准范式，应作为所有持久化输出的默认实现 约束 LLM 输出格式应优先使用 API 层能力（tool_use / response_format），而非依赖提示词中的格式要求，可靠性更高 Hugo 部署常用 public/ 子目录作为独立 git repo 推送到 GitHub Pages，源文件 repo 与部署 repo 分离是标准实践；部署通过脚本完成，源文件不需要单独版本管理 MuJoCo EGL 渲染要求 MUJOCO_EGL_DEVICE_ID 与 CUDA_VISIBLE_DEVICES 指定的物理 GPU 编号严格一致，多进程 GPU 分配需要显式管理 robomimic v0.3.1 已安装在 mimicgen_env，BCPolicy 加载 API 为 FileUtils.policy_from_checkpoint()。pretrained checkpoints 可从 robomimic 官方 model zoo 下载 会话摘要 MIHD（空间组学 Benchmark） 🔄 MIHD benchmark 续跑：Q-Former + scGPT 并行评估，完成 240 个实验 00:01:07.460 | claude_code 从 175 个完成实验出发，AI 将 Q-Former 从 core_multimodal 实验组拆分独立运行，并行启动 scGPT 特征提取（11 sections，约 10 分钟完成）和 scGPT 融合评估（77 个实验，65 成功）。Q-Former 在完成 6/11 sections 后进程崩溃，GPU 重新空闲后重启续跑至 8/11。总计完成 240 个实验，scgpt_uni2_qformer 因 OOM 推迟。\nMIHD（VLA 研究） 🔄 研究 VLA 技术在 MIHD 空间组学中的应用可能性 22:08:57.348 | claude_code 用户请求研究 VLA 方法是否可用于 MIHD，AI 抓取用户 bugjournal 中的 VLA 笔记（FAST、Flow Matching、扩散模型等），结合 MIHD 架构生成初步研究报告，识别 Flow Matching 融合和 Register Tokens + AdaLN 为最有价值的迁移方向。用户选择深入三个方向，但随后多次拒绝子代理研究工具调用，研究被中断。\nGadget（日报发布工具） ✅ 实施日报发布计划并部署到 GitHub Pages 05:16:45.089 | claude_code 用户在新会话中提供完整计划文本，要求直接实施。AI 读取三份日报（2026-02-12/13/14），为每份生成 Hugo front matter，写入 website/content/bugJournal/，运行 update.sh 成功构建并部署 139 页到 GitHub Pages。随后确认 gadget repo 已是最新，website 不是独立 git repo，部署已通过脚本完成。\n🔄 探索日报格式与 bugJournal 格式，制定发布计划 05:05:36.473 | claude_code 用户请求将 gadget/summarize/reports 下的 .md 日报发布到 Hugo 网站的 bugJournal 板块并部署。AI 探索了两个目录的文件结构和格式，制定了添加 front matter 并运行 update.sh 的计划，但在 ExitPlanMode 阶段被用户中断。\nGadget（日报工具） ✅ 汇总 2026-02-13 DCC+Mac 多设备对话，生成结构化日报 00:05:25.538 | claude_code 用户以日报分析师身份调用 AI，传入来自 Mac 的对话记录（gadget 日报工具初版开发、GitHub 仓库初始化、ccusage 集成等），AI 生成 JSON 格式结构化日报，涵盖 6 个任务、4 个问题解决方案、2 个人机差异分析。\n✅ 汇总 2026-02-12 DCC MIHD 增强设计对话，生成结构化日报 05:03:37.389 | claude_code 用户传入 DCC 集群上的 MIHD 增强设计对话记录（Q-Former 调研、归一化分析、QueST 集成设计），要求生成 2026-02-12 结构化日报。AI 提取了任务状态、问题与解决方案，并分析了人机差异（用户主导计划中断与文件保存的流程控制）。\n✅ 汇总 2026-02-13 DCC MIHD benchmark 和 7 Phase 增强实现，生成结构化日报 05:00:32.130 | claude_code 用户传入 DCC 集群上的 MIHD benchmark 执行（run_all_benchmarks.py 调度器、断点续跑修复）和 7 Phase 增强实现（归一化/Q-Former/LLaVA MLP/Niche 查询/批次校正），AI 生成 JSON 日报，涵盖 8 个任务和详细问题分析。\nCalendarPro ✅ 汇总 2026-02-13 CalendarPro 代码重构和定时自检功能对话，生成结构化日报 02:10:33.706 | claude_code 用户传入 Windows 桌面上 CalendarPro 项目的代码重构（26 个测试通过）和定时自检/循环日程功能实现（18 个新测试通过）的对话记录，AI 生成 JSON 格式日报，分析了 8 个问题解决方案和 3 个人机差异。\nGadget 日报工具 ✅ 实现多设备 rclone 同步工作流（按子目录分离） 04:18:30.364 | claude_code 用户在新会话提供完整计划文本，要求直接实施多设备 rclone 同步改进。AI 完成 6 项修改：默认行为改为 export、_rclone_upload 增加 subdirectory 参数、新增 _rclone_download_logs、merge 支持 \u0026ndash;sync flag、_config_show 显示结构化路径、更新三份文档。此改动解决了多设备写同一目录导致文件混淆的根本问题。\n✅ 实现 export 智能跳过已 summarize 设备 02:02:44.426 | claude_code 用户要求实现 export 幂等性优化：先读取已有文件的 _merged_devices 列表，已处理的设备跳过 API 调用并复用已有 device_summary。AI 将 device_summary 重构为按设备名索引的 dict，新增 _source_device 字段。期间用户询问 rclone 无配置时的行为，AI 解释了不执行逻辑。\n🔄 实现 JSON 原子写入 + LLM 强制结构化输出 03:46:30.063 | claude_code 用户要求修复 JSON 文件损坏风险和 LLM 输出不稳定问题。AI 实现了 _atomic_write 函数，替换了 5 处直接写入；为 Anthropic 实现 tool_use 强制 JSON 输出，为 OpenAI 使用 response_format=json_object。用户询问 gdrive:summarize 路径格式后提出改为通用格式，AI 完成修改。随后用户提出多设备 rclone 写冲突问题，AI 制定改进方案后被中断。\n🔄 设备标识字段 + test 子模块迁移（计划阶段） 00:08:53.716 | claude_code 用户提出两个需求：1) export 不使用 \u0026ndash;summarize 时在文件末尾加设备标识；2) 将 test git submodule 迁移到当前 repo。AI 探索了代码库结构，制定了详细计划，但在 ExitPlanMode 阶段被用户中断。需求 1 最终在后续会话中通过 _source_device 字段实现，需求 2（test 子模块迁移）状态未明确确认。\nGadget（Export 优化） ✅ Export JSON 加设备标识 + 将 test/ 子模块转为普通文件 00:28:42.707 | claude_code 用户提供完整实施计划，要求执行两个任务：1）在 export 不使用 \u0026ndash;summarize 时追加 _source_device 字段；2）将 test/ git submodule 转为普通文件并清除所有 TzJ2006/test 的 GitHub 引用。AI 顺利完成两项任务，并主动发现 report.py 和 benchmark_report.html 中未在计划中提及的两处链接并修复。用户追加要求脱钩原 GitHub repo，AI 全面清理了相关引用。\nError Recovery Benchmark 🔄 可视化视频修复：力 re-apply → 发现真正根因（控制器补偿） 00:05:51.186 | claude_code 实施可视化修复计划（每步 re-apply force），三个测试全部 exit code 0。但用户观看视频后发现机械臂仍无位移。AI 深入分析后发现真正根因：Phase 3 使用 demo actions，OSC 控制器（kp=150）在每个物理子步追踪轨迹产生校正力矩，远大于注入的 15-20N 外力，完全补偿了其效果。新修复计划写入文件：在注入窗口改用 neutral actions。\n✅ 修复可视化视频中力注入效果不可见（neutral action） 03:58:45.066 | claude_code 用户提供根因分析文档：OSC 控制器在追踪 demo 轨迹时会完全补偿注入的外力，而验证阶段使用 neutral action 所以效果可见。AI 按方案将 Phase 3 拆分为三个子阶段（注入/沉降/demo 恢复），新增 \u0026ndash;settle_steps 参数，运行三个验证测试均通过，体位移在 40N 力下约 90mm。\n✅ 实现策略驱动错误场景生成框架（PolicyAdapter + generate_from_policy） 22:32:03.029 | claude_code 用户提供完整架构方案，要求实现支持任意策略（demo/BC-RNN/VLA/random）的错误场景生成框架。AI 实现了 8 个修改点：新建 policy_adapter.py、扩展 rollout_generator.py、更新 collector.py/database.py/init.py，新建 CLI 脚本和配置。发现并修复了 BCPolicy 惰性加载 bug，全部 41 个单元测试通过。\n🔄 策略错误检测分类系统整体设计与规划 23:20:48.619 | claude_code 用户提出构建策略错误检测分类系统的需求：对已有错误场景运行策略，自动检测并分类策略犯的错误。AI 并行探索了代码库、进行了文献调研（RoboFail 等 7 篇论文）、设计了 25 种错误分类体系（3 Family × 10 Category）。用户确认错误注入已实现，澄清需要构建「反向系统」（检测策略错误而非注入）。计划文件以中文大目标/中目标/小目标格式写入。\n✅ BC 策略加载、轨迹保存修复 + 多类型场景生成扩展 05:21:04.778 | claude_code 实施三项改进：(1) collector.py 轨迹保存从存根改为实际 NPZ 保存（5 个轨迹文件验证成功）；(2) 3_collect_data.py 实现 BC 策略从 config 加载；(3) ProximityDetector/CollisionDetector 新增多类型 spec 提议，max_scenes_per_demo 3→10。41/41 单元测试通过。用户随后要求向 foundation model（MimicGen/Pi0）注入错误，AI 分析后发现当前架构只需替换 action 来源即可支持。\n🔄 规划并探索下一步：规模化多类型场景生成 23:16:07.000 | claude_code 用户要求了解下一步应该做什么。AI 探索了项目全景总结和代码库，分析了当前 30 个场景全为 impulse/tip_over 的根因（Proximity 检测器消耗了全部配额），提出了 P1.3+P2.3 的综合方案（修改 proximity.py 和 collision.py 以生成多样化 specs，调整 benchmark_v4.yaml 参数），并制定了详细实施计划（用户中断了 ExitPlanMode）。\n✅ 批量生成 60 个可视化视频（baseline + randomized） 04:32:53.393 | claude_code 用户要求批量生成两组视频：30 个使用原始参数，30 个参数随机增加 0-100%。AI 新建 scripts/batch_visualize.py，修复 EGL GPU ID 匹配问题，生成全部 30 个场景共 60 个视频（21MB+26MB）。修复了选场景逻辑的边界 bug（选 29 而非 30）。\n✅ v4.1.0 smoke test 验证 + 规模化场景生成（118 个） 23:19:02.619 | claude_code 用户要求实施 smoke test 计划。在天河集群 8×A800 GPU 节点：41/41 单元测试通过，4 阶段 smoke test 全通过，new policy-based generation 验证通过（新 scene source_type:policy）。随后对所有 10 条 demo 运行全量生成（max_scenes_per_demo=10），生成 118 个场景；运行 10 次 policy rollout 额外生成 1 个场景。\n✅ 更新项目全景总结.md 到 v4.2.0 05:08:28.895 | claude_code 用户要求根据最近完成的工作更新项目进度文档。AI 阅读了项目全景总结.md 和对话历史，对 10 个部分进行了系统性更新（版本号、里程碑状态、数据统计、关键路径、下一步任务等），消除了所有过时的「无 GPU」和「3 个场景」描述。\nToken 用量 总览 指标 数值 总 Token 50,386,100 输入 Token 55,833 输出 Token 27,443 Cache 创建 3,196,161 Cache 读取 47,106,663 Cache 命中率 93.6% 总费用 (USD) $33.6135 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 29,094 26,621 1,637,740 29,025,555 $25.5596 76.0% claude-haiku-4-5-20251001 23,790 444 848,634 5,667,553 $1.6536 4.9% claude-sonnet-4-5-20250929 2,949 378 709,787 12,413,555 $6.4003 19.0% 各设备用量 设备 总 Token 输入 输出 费用 DCC 16,688,953 36,225 10,457 $14.1690 MacBook 871,193 191 130 $1.0724 tianhe 32,825,954 19,417 16,856 $18.3722 ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-15/","summary":"在 MacBook/TzJsDesktop 完成历史日报发布到 Hugo/GitHub Pages 及 daily_summary.py 多项核心增强（原子写入、LLM 结构化输出、多设备 rclone 同步、export 幂等跳过），在 DCC 集群推进 MIHD benchmark（Q-Former/scGPT 评估），在天河集群完成 Error Recovery Benchmark 的中性动作修复、批量可视化、策略驱动场景生成框架及多类型错误检测器扩展，并研究了 VLA 技术在空间组学中的应用方向","title":"Bug Journal 2026-02-15"},{"content":"日报 — 2026-02-14 在DCC高性能集群上推进MIHD多模态空间转录组学benchmark（完成两阶段Pipeline架构实现）、修复ErrorRecoveryBenchmark中力注入机制和pre_grasp检测器的多个根本性bug、为CalendarPro Discord Bot新增批量删除功能并修复嵌套Claude环境问题，以及为gadget/summarize工具完整实现配置文件、rclone云盘同步和Claude CLI后端支持。\n今日任务 架构与策略 ✅ 修复pre_grasp检测器不触发 — 发现并修复三个额外的bug：1) _check_will_close_soon()用零动作模拟导致夹爪永不闭合，改为使用真实demo动作前瞻；2) 全局近距离门控(0.05m)阻断了pre_grasp检测窗口，改为per-detector跳过；3) 全局冷却计时器阻断了pre_grasp触发窗口，改为per-detector独立冷却；4) target body名称错误，改为使用EEF。 ✅ 两阶段Pipeline架构设计与实现 — 基于用户提供的详细计划，完整实现了MIHD两阶段Pipeline：新建pipeline/模块（8个文件：cache_manager, data_preparer, extraction_planner, gene_extractor, vision_extractor, evaluation_planner, runner, init）；新建3个入口脚本（run_pipeline.py, phase1_extract.py, phase2_evaluate.py）；新建pipeline_config.yaml。通过section 151508的端到端测试验证所有核心组合（concat/mean/attention/staig_fusion）均正常工作。 ✅ 批量删除日历事件功能 — 扩展DeleteData模型（新增date_from/date_until/batch字段）、更新LLM prompt、让search_events支持自定义时间范围、重写_handle_delete支持批量操作，并新增BatchDeleteApprovalView Discord确认按钮组件。 ✅ gadget/summarize配置文件+rclone云盘同步+机器标识 — 新增~/.config/summarize/config.json配置文件支持（device_name/logs_dir/reports_dir/rclone_remote/rclone_path），实现_load_config/_resolve_output_dir/_get_device_name/_rclone_upload/_find_rclone函数，添加config \u0026ndash;show/\u0026ndash;init子命令，支持headless server无sudo安装rclone，更新tutorial.md和README.md。 ✅ 修复get_task_phase()始终返回unknown — 将get_task_phase()从调用永远返回False的stub方法_check_phase_condition()改为使用已有完整实现的get_task_completion_stages()，任务阶段现在可以正确返回reach/grasp/lift/transport/place/pre_reach。 ✅ 调试力注入机制 - 定位力施加到错误body的bug — 通过调试日志发现_get_eef_body_name()的possible_names列表缺少\u0026rsquo;gripper0_eef\u0026rsquo;，导致fallback到geom搜索，第一个匹配到包含\u0026rsquo;ee\u0026rsquo;的geom是robot0_screen_collision，属于bin2（垃圾箱）。在possible_names列表开头添加\u0026rsquo;gripper0_eef\u0026rsquo;、\u0026lsquo;robot0_right_hand\u0026rsquo;、\u0026lsquo;gripper0_gripper_base\u0026rsquo;修复该问题。 ✅ 修复Google Calendar 403 insufficientPermissions — 将get_user_timezone()从使用需要更高权限的calendarList().get() API改为settings().get() API（CalendarPro方案），或直接使用config默认时区（gadget/summarize场景），消除每次用户消息时出现的403 WARNING日志。 ✅ 添加Claude CLI后端并设为默认 — 新增summarize_with_claude_cli函数，使用claude \u0026ndash;print \u0026ndash;model sonnet生成总结，无需API key，并设为默认后端。 ✅ 实现持续力注入机制 — 修改collect_rollout_stats()支持duration_steps，在验证rollout期间循环重施加力；修改rollout_generator.py传递error_spec和injector参数。 ✅ 批量删除LLM失败时的fallback修复 — 嵌套Claude Code环境导致LLM超时、intent.data为空，通过_raw_input传递原始输入，用_extract_delete_query正则提取关键词、日期范围和batch标志。 ✅ export阶段检测并合并已有log — 当同一天同一设备已有export log时，以(source, project, timestamp)去重后合并新旧conversations，避免重复运行丢失数据。 🔄 调试30N力注入后机械臂仍无明显运动 — 即使将力从3N增加到30N，视频中机械臂仍无明显扰动。用户提出尝试infinite力来诊断问题，会话在计划阶段被用户中断。 实现与修复 🔄 Discord回复详情增强 — 用户反馈Bot回复缺少具体内容（如安排了什么任务在什么时间），已改进删除操作回复格式包含日期时间，但schedule等其他intent的回复详情问题尚未完全解决。 ✅ 修复ccusage Opus计费为$0 — 发现ccusage \u0026ndash;offline模式下Opus 4.6定价为0，去掉\u0026ndash;offline参数改为在线获取最新定价，修复后Opus费用从$0正确显示为$19.11。 ✅ 修复Windows上npx/claude找不到问题 — ccusage调用加shell=True，claude CLI调用移除CLAUDECODE环境变量以绕过嵌套检测。 ✅ 可视化Phase 3改为继续执行demo actions — 修改2_visualize_scene.py，Phase 3从trigger_step继续执行demo actions，demo用完后fallback到neutral action，添加\u0026ndash;force_override参数覆盖力大小，实现duration_steps后自动清除xfrc_applied。 🔄 MIHD Benchmark进度监控与调度修复 — 在DCC上持续监控多模态benchmark进程，处理basic_contrastive超时问题（从3600s改为7200s），删除staig_fusion_e2e实验组，修复配置文件run_all_benchmarks.py/benchmark_config.yaml/summarize_benchmark.py，并重启调度器继续运行剩余176个实验。 ✅ 空消息Discord崩溃修复 — BatchDeleteApprovalView通过channel.send发送确认后返回空字符串，主循环尝试发送空消息导致Discord API报错，添加if response检查解决。 ✅ 代码质量修复（3项） — 1) dual_verify.py补充4个新intent的中文默认回复；2) executor.py裸except改为except (ValueError, TypeError)；3) periodic_checker.py将Scheduler实例移到循环外复用。 ✅ 修复ccusage使用原始JSON+更详细展示 — fetch_ccusage改为直接保存ccusage原始输出，generate_markdown增加cache命中率、模型费用占比、per-device设备明细等展示。 ✅ 清理database.py中的死代码 — 删除从未被调用的add_rejected_candidate()、get_rejected_candidates()、analyze_rejected()方法，以及DatabaseMeta中的total_rejected和rejection_stats字段，更新所有相关引用。 🔄 RTX 5000 Ada GPU加速benchmark — 利用新的NVIDIA RTX 5000 Ada (32GB) GPU，通过新pipeline架构重新运行所有benchmark。Phase 1提取：完成PCA/MLP gene embedding全部11 sections，HIPT/ResNet50/UNI2/UNI/STAIG gene feat缓存，共约127/190实验完成。Q-Former因每section需5.7小时被减至50 epochs，STAIG fusion因NaN梯度崩溃。 ✅ gadgets工具集清理和文档更新 — 用户删除了不再使用的gadgets（Video, audio, git, image, papers, png2text.py, text），对应更新了README.md和CLAUDE.md，移除过时工具列表，更新test/模块为新的benchmark/包结构（python -m benchmark.cli接口）。 ✅ 切换Claude CLI模型为Sonnet — 将summarize_with_claude_cli中的claude \u0026ndash;print改为claude \u0026ndash;print \u0026ndash;model sonnet，降低成本和等待时间。 ✅ 添加requirements.txt — 创建summarize/requirements.txt，列出anthropic和openai可选依赖。 问题与解决方案 关键问题 1. 30N力注入后机械臂仍无可见移动 解决方案: 通过调试日志追踪到力被施加到bin2（垃圾箱）而非gripper0_eef（末端执行器），修复_get_eef_body_name()的possible_names列表\n关键洞察: _get_eef_body_name()的possible_names缺少robosuite实际使用的body名称，fallback逻辑匹配到包含\u0026rsquo;ee\u0026rsquo;子串的无关body；调试力注入问题时应第一步验证力的施加目标\n2. Claude CLI在嵌套Claude Code会话内报错拒绝运行，返回空内容 解决方案: 在subprocess调用时从环境变量中移除CLAUDECODE和CLAUDE_CODE_ENTRY，绕过嵌套检测\n关键洞察: Claude Code通过环境变量检测嵌套会话，子进程清除这些变量即可运行；这是Claude CLI的已知设计机制\n3. _check_phase_condition()是未实现的stub，永远返回False，导致所有依赖task_phase的检测器失效 解决方案: 直接复用已有的get_task_completion_stages()实现，用阶段优先级判断逻辑替换stub调用\n关键洞察: 代码中已有完整实现但被stub绕过，这是典型的\u0026rsquo;实现遗漏但接口存在\u0026rsquo;的bug；代码审查时要区分\u0026rsquo;已实现的功能\u0026rsquo;和\u0026rsquo;stub占位符\u0026rsquo;\n4. Google Calendar API 403 insufficientPermissions错误，calendarList API需要额外scope 解决方案: CalendarPro改为使用settings().get()（只需calendar.events scope）；gadget/summarize直接使用config默认时区避免API调用\n关键洞察: Google Calendar API不同端点权限要求不同；对于可接受近似值的场景，静默降级优于使用权限不足的API\n5. 批量删除LLM超时导致intent.data为空，批量删除请求被拒绝 解决方案: 通过_raw_input将原始用户输入传递给handler，用正则从中提取搜索关键词、日期范围和batch标志\n关键洞察: 关键功能的执行路径不能完全依赖LLM结构化数据，需要独立的信息提取fallback；关键词正则+日期识别是轻量级备选方案\n6. ccusage \u0026ndash;offline模式下Opus 4.6费用计为$0，总费用严重低估 解决方案: 去掉\u0026ndash;offline参数，在线获取最新模型定价，Opus费用从$0正确显示为$19.11\n关键洞察: 离线定价表滞后于新模型发布，对于高价模型影响极大；在线模式稍慢但数据准确性优先\n7. MuJoCo每步自动清除xfrc_applied，导致力只持续1步 解决方案: 在验证rollout循环中持续重新施加力，duration_steps步后清除\n关键洞察: MuJoCo的xfrc_applied是每步重置的，需要在仿真循环中主动维持\n8. _check_will_close_soon()使用零动作(neutral actions)模拟前瞻，夹爪永远不会闭合 解决方案: 通过state_info[\u0026lsquo;future_demo_actions\u0026rsquo;]传入真实的未来demo动作，使前瞻检查基于真实轨迹\n关键洞察: 模拟前瞻必须使用与原始录制相同的动作序列，否则轨迹发散\n9. 全局近距离门控(max_eef_object_distance=0.05m)阻断了pre_grasp场景的保存，因为pre_grasp在0.05-0.1m范围内触发 解决方案: 让有内置距离检查的检测器（pre_grasp、grasp_precond）绕过全局门控\n关键洞察: 全局过滤规则不应覆盖检测器自身的距离阈值逻辑\n10. LLM语义路由在嵌套环境下超时，将「安排睡眠和吃饭」误分类为update_energy 解决方案: 通过unset CLAUDECODE让LLM正常工作，从根源上解决分类问题\n关键洞察: 语义路由（SR）的准确性在模糊输入上不如LLM，LLM双重验证对模糊指令非常重要\n11. 全局冷却计时器让一个检测器触发后，其他检测器在同一时间窗口内也被阻断 解决方案: 从单一全局cooldown_counter改为per-detector独立冷却字典\n关键洞察: 不同类型检测器的触发时机完全独立，全局冷却会造成不必要的互斥\n12. STAIG fusion训练中tau参数衰减至0导致loss=NaN，实验失败 解决方案: Pipeline自动跳过失败实验继续执行，该问题为已知STAIG稳定性问题\n关键洞察: 对比学习中温度参数（tau）需要设置下界，避免分母趋近于0导致数值不稳定\n一般问题 13. pre_grasp检测器将任务配置的物体名称(\u0026ldquo;Milk\u0026rdquo;)直接作为MuJoCo body名称，导致Body not found错误 解决方案: 改为使用{\u0026rsquo;eef\u0026rsquo;: True}作为target，与proximity检测器保持一致\n关键洞察: 任务级别的物体名称与MuJoCo模型中的body名称不同，需要通过EEF间接操作\n14. Q-Former每个section需要约5.7小时（200 epochs），11个section总计63小时，严重阻塞其他实验 解决方案: 将Q-Former epochs从200减至50（加速4倍），通过pipeline_config.yaml的extra_config.qformer.epochs参数传入\n关键洞察: 在大规模benchmark中，训练型fusion需要预先评估每个epoch的计算成本，并为验证性实验设置合理的epoch上限\n15. 批量删除确认View发送后返回空字符串，主循环尝试向Discord发送空消息报错 解决方案: 在主循环发送响应前添加if response检查\n关键洞察: 使用channel.send直接发送交互式View的handler应返回空字符串，调用方需要处理这种情况\n16. Windows上subprocess.run找不到npx（.cmd文件） 解决方案: 添加shell=True让Windows cmd解析.cmd后缀，bash环境已有完整路径不受影响\n关键洞察: Windows npm全局命令是.cmd文件，bash的subprocess需要shell=True才能找到\n17. LLM返回内容包含JSON前后的解释文字，导致json.loads失败 解决方案: 重写_parse_json_response为三步尝试：直接解析→提取```json代码块→提取首尾花括号\n关键洞察: 健壮的JSON提取应假设LLM总会在响应中加额外文字，需要多重fallback策略\n18. basic_contrastive fusion每个section耗时约77分钟（550 epochs），超过旧的1小时超时限制导致实验失败 解决方案: 将超时时间从3600s修改为7200s（2小时），重启调度器继续运行\n关键洞察: 训练型fusion（basic_contrastive/staig_fusion）本质上是端到端训练，耗时远超简单特征拼接；超时设置应根据fusion类型动态调整\n19. staig_fusion Phase 2中mclust聚类因rpy2未安装（ImportError）而失败 解决方案: 在runner.py的try/except中同时捕获ImportError和ValueError，触发KMeans fallback\n关键洞察: 跨环境部署时，可选依赖（如rpy2）应在异常处理中明确捕获ImportError，而不仅是ValueError\n20. ExperimentLogger调用了不存在的save_comparison_csv()方法，导致pipeline在Phase 2结束时崩溃 解决方案: 移除对save_comparison_csv()的外部调用（ExperimentLogger在log_experiment内部自动调用_update_comparison_csv()）\n关键洞察: 接口调用前应检查方法实际名称，内部方法（前缀_）通常由类自身管理，不应从外部调用\n21. SSH命令中cd无法持久化 解决方案: 使用绝对路径运行脚本，或编写wrapper shell脚本\n关键洞察: SSH单行命令中cd后的状态不会跨\u0026amp;\u0026amp;传递，需要在同一子shell中执行或用绝对路径\n人类思路 vs AI 思路 战略层面 LLM嵌套问题的发现与解决 角色 思路 人类 人类从Discord的用户视角出发，发现Bot回复异常（\u0026ldquo;Energy level updated\u0026quot;不匹配「安排睡眠和吃饭」的请求），并提示AI排查。 AI AI通过分析日志发现LLM超时警告和SR误分类，追溯到CLAUDECODE环境变量阻止嵌套Claude会话，提出unset CLAUDECODE的解决方案。 差异分析: 人类从功能结果层面发现问题，AI从技术实现层面定位根因；两者形成互补的问题发现链条。\nrclone和headless server上传方案 角色 思路 人类 用户提出headless server的特殊需求，要求调研CLI上传方案，并提出在文件中加机器标识。 AI AI调研了rclone/gdrive/onedrive-cli/brig等方案，推荐rclone并设计了完整的配置文件和机器标识方案。 差异分析: 用户识别出实际使用场景的痛点；AI负责技术方案的系统化设计。\nPreGrasp检测器的触发逻辑设计 角色 思路 人类 用户提出：夹爪是跳变的（0→1），只需检测\u0026rsquo;N步后是否从0变为1\u0026rsquo;即可，不需要复杂的\u0026rsquo;正在接近\u0026rsquo;判断。 AI AI设计了复杂的多条件检测（连续2帧距离递减、task_phase判断、接近计数器等）。 差异分析: 用户对任务特性的理解更准确——夹爪动作确实是离散跳变而非连续变化。用户的简化思路更鲁棒，AI的过度工程化反而引入了false negative。\npre_grasp检测器不触发的根本原因分析 角色 思路 人类 人类提供了完整的计划：识别出get_task_phase()调用stub、database死代码两个根本原因，并给出具体修复方案。 AI AI执行了计划，并在运行验证时发现了计划未涵盖的3个额外bug（前瞻用零动作、全局门控距离、全局冷却计时器）。 差异分析: 人类做了离线的静态代码分析，找到了主要根因；AI在动态验证中发现了更深层的运行时bug，这些在静态分析中不易发现。\n两阶段Pipeline架构的核心洞察 角色 思路 人类 用户主动指出：应该先为每张切片提取embedding并缓存，然后对同样的embedding做不同的fusion，而不是每次换fusion都重新跑编码器。这是整个架构重写的驱动思想。 AI AI原本在旧框架内做增量优化（添加缓存层、优化patch提取并行性），没有主动提出完全分离两个阶段的设计。 差异分析: 用户从工作流层面识别了根本性的效率问题（编码与融合耦合），AI倾向于在现有框架内优化。用户提出了完全重写pipeline的决策，AI负责执行实现。\n批量删除功能的实现策略 角色 思路 人类 人类识别出根本原因：1) delete流程仅支持单条删除；2) 用户需要按日期范围批量删除。人类直接给出了涉及5个文件的完整修改方案（模型扩展+prompt+搜索范围+handler重写+确认UI）。 AI AI先探索代码库、验证各处细节，然后按照人类提供的方案实现，过程中发现了人类方案未涉及的额外问题（如空消息崩溃）并主动修复。 差异分析: 人类具备更好的系统架构全局视角，能在理解问题后直接给出跨多文件的完整方案；AI更擅长发现实现过程中的细节问题，但在初始设计上依赖人类指导。\n诊断机械臂无响应的原因 角色 思路 人类 用户直觉认为是力的作用时间太短或需要增大力的量级。 AI AI最初认为3N力对受控机械臂太弱，提议增大力量，通过调试日志才发现力施加到错误body。 差异分析: 用户的直觉方向（作用时间、力量）也是合理的猜测，但实际根本原因是body选择错误这一代码bug。两者都没直接想到要检查force施加的目标body。\nccusage计费问题定位 角色 思路 人类 用户通过手动执行npx ccusage daily \u0026ndash;json发现Opus费用显示正常，对比脚本输出后直接指向\u0026ndash;offline参数。 AI AI先系统性排查代码逻辑，然后对比有无\u0026ndash;offline的命令行输出差异，最终确认定价表缺失。 差异分析: 用户凭经验快速猜中根本原因；AI通过对比实验验证假设——两者配合效率最高。\n30N力仍无效的调试策略 角色 思路 人类 人类提出用infinite力作为极端测试用例，快速区分\u0026rsquo;力注入机制本身失效\u0026rsquo;和\u0026rsquo;力被控制器补偿\u0026rsquo;两种根因。 AI AI在被中断前倾向于制定更复杂的调试计划（分析xfrc_applied是否生效、duration_steps影响等）。 差异分析: 人类的调试策略更激进但更高效——先用极端用例验证机制是否工作，再做量化分析。\nrejection log缺失的分析 角色 思路 人类 用户注意到total_rejected=0异常，直接追问\u0026rsquo;如果根本没有写这个部分，请把它删掉\u0026rsquo;，说明用户怀疑是dead code。 AI AI解释rejection log的工作机制，后来发现_log_rejection方法从未被调用（通过数据库API记录但代码路径不同）。 差异分析: 用户的死代码嗅觉是正确的——rejection logging存在实现碎片化问题。\n实现层面 Discord回复详情的改进方向 角色 思路 人类 人类明确指出回复缺乏具体信息（如「安排了什么工作在什么时间」），要求AI提供更丰富的回复内容。 AI AI在改进删除回复格式后，判断schedule等intent已通过reply_message提供足够信息，但实际上未完全解决用户需求。 差异分析: 人类从用户体验角度提出了AI未主动考虑的细节需求；AI的修复停留在问题的一部分而没有全面解决。\nQ-Former epochs的处理方式 角色 思路 人类 用户选择减少Q-Former epochs（而非跳过或保持原样），接受精度损失换取4x速度提升。 AI AI识别出Q-Former的速度问题后给出三个选项（跳过/减少epochs/保留），等待用户决策，然后找到了通过pipeline_config.yaml extra_config传参的方法而非修改模型代码。 差异分析: 资源/精度权衡决策由用户做出；AI找到了非侵入式的实现方式（配置文件参数化）。\nstaig_fusion_e2e的取舍 角色 思路 人类 用户直接决定删除staig_fusion_e2e实验组（\u0026ldquo;delete all of this part\u0026rdquo;），并主动要求跳过已测试的实验以节省时间。 AI AI按指令删除了相关代码和已有结果文件，同步更新了3个配置文件保持一致性。 差异分析: 实验取舍决策由用户做出，AI负责代码层面的一致性维护。\nAI 局限性 重要局限 AI在调试SSH命令时重复犯了同一个错误（忘记在SSH命令中加cd），同一错误重复20次以上，缺乏自我纠正能力。 AI没有第一时间检查force施加的目标body是否正确，而是把问题归因于力的大小，导致绕了一圈才找到根本原因。 AI在最初的vision encoding分析中，未能主动识别\u0026rsquo;编码与融合耦合\u0026rsquo;这一根本性架构问题，只是分析了patch提取并行化、GPU推理频繁清缓存等局部瓶颈，直到用户明确提出两阶段分离思路后才按此实现。 AI在处理LLM失败场景时的fallback设计不完整：实现了批量删除的fallback，但对于schedule等其他intent在LLM失败时的体验问题没有系统性解决。 AI在验证代码修改时难以进行真正的端到端测试（因为在嵌套Claude Code环境中），只能依赖单元测试，导致某些运行时问题（如空消息崩溃）未能提前发现。 多次生成的日报JSON出现解析失败（raw_response），说明LLM对自己的JSON输出格式没有严格约束，仍需要额外的解析健壮性处理。 AI对PreGrasp检测器的初始设计过于复杂（多条件、多状态机），用户一句话就指出了更简单的核心逻辑。 AI没有主动检查get_task_phase()是否是stub实现，而是等到专门的代码探索任务才发现。 静态代码分析未发现动态运行时bug：原始计划只识别了2个根本原因，但实际执行时发现了4个额外的运行时bug（前瞻逻辑、门控逻辑、冷却逻辑、body名称），说明AI对代码的端到端运行行为理解不足。 对力注入机制效果的预测失误：预计30N会产生≥3cm的可见位移，但实际视频中机械臂仍无明显运动，说明对OSC控制器实时补偿能力的估计存在偏差。 一般局限 AI在runner.py中只捕获了ValueError而遗漏了ImportError，导致mclust在无rpy2环境中的fallback失效。这类跨环境依赖问题需要更系统性地处理。 AI在实现批量删除时未能预见「返回空字符串导致Discord发送空消息崩溃」的问题，需要在测试发现后才修复。 AI对Discord回复内容的改进仅覆盖了删除操作，未主动检查和改进其他所有intent（schedule/update/query等）的回复详情，需要用户明确指出才会处理。 对Claude CLI嵌套会话的检测机制理解不足，仅清除CLAUDECODE变量后才成功，未能事先预见需要同时清除CLAUDE_CODE_ENTRY。 在Windows + Git Bash环境下对subprocess找不到.cmd文件的问题需要用户实际运行后才发现，未能提前识别。 在不需要proxy的场景（本地文件读取、MuJoCo渲染）仍习惯性添加source setproxy.sh，缺乏对网络需求的判断。 AI在调用ExperimentLogger时假设存在save_comparison_csv()方法，未先验证接口，导致端到端测试最后一步崩溃。 Grep工具在大文件（daily_summary.py约40K行）上多次超时，需要降级到bash grep或Read工具。 今日收获 核心收获 两阶段Embedding缓存架构：将encoder提取（Phase 1，按encoder×section去重）与fusion评估（Phase 2，从缓存加载）完全分离，可将UNI2提取次数从77次降至11次，数据加载从3000次降至11次，在大规模benchmark中效率提升显著。这是大规模多模态实验设计的关键模式。 CLAUDECODE环境变量是Claude Code的嵌套会话保护机制，在需要从应用程序内部调用Claude CLI时必须通过env -u CLAUDECODE或unset CLAUDECODE清除，否则CLI会返回空内容。 rclone是跨平台云盘CLI同步的最佳方案，支持70+云盘，headless server可通过token copy方式认证，无需sudo安装。 多台设备协作的工具需要从设计之初考虑设备标识（文件名、log内容）和数据同步（云盘或rclone）策略，而非事后添加。 MuJoCo的xfrc_applied在每次mj_step后自动清零，要实现持续力必须在仿真循环中每步重新设置。 调试力注入问题时应第一步验证力的施加目标（body_id和body_name），不应假设目标正确。 per-detector独立冷却和独立距离门控是多检测器pipeline的正确设计模式——全局冷却/全局门控会产生不可预料的跨检测器干扰。 Google Calendar API各端点有不同的OAuth scope要求：calendar.events scope只支持事件CRUD，不支持calendarList读取，但足够支持settings API；设计时应使用最小权限原则并选择对应的API端点。 在LLM + SR双重分类架构中，SR fallback路径的信息提取能力很关键：SR只能判断intent类型，无法提取结构化参数；需要为fallback路径设计独立的参数提取逻辑（如正则、规则），而不是依赖LLM的结构化输出。 工具脚本的环境变量隔离很重要：子进程继承父进程的环境变量可能导致意外行为（如Claude CLI的嵌套检测）；关键子进程调用应显式清理不需要的环境变量。 关键功能不应完全依赖LLM的结构化输出，需要独立的正则fallback路径（如批量删除的搜索词提取）。 robosuite Sawyer机器人的EEF body名称是\u0026rsquo;gripper0_eef\u0026rsquo;（body_id=21），不是常见的\u0026rsquo;gripper_link\u0026rsquo;或\u0026rsquo;hand\u0026rsquo;等名称，fallback逻辑容易匹配到错误body。 代码审查时要区分\u0026rsquo;已实现的功能\u0026rsquo;和\u0026rsquo;stub占位符\u0026rsquo;，get_task_completion_stages()和get_task_phase()同时存在但后者是stub，这类情况需要主动验证。 模拟前瞻必须使用真实的demo动作序列，而非零动作或中性动作——否则OSC控制器在不同动作下的行为完全不同。 OSC控制器的实时补偿能力极强，30N的外力在1步（20ms）内可能完全被消除，需要持续施力或大幅增加力才能产生可见效果。 训练型fusion（basic_contrastive/staig_fusion/Q-Former）在benchmark中的耗时差异极大（简单concat ~5s vs Q-Former ~5.7h/section），设计benchmark时需要为不同类型的fusion设置不同的超时策略，并预先评估总耗时。 日历应用的批量操作需要三要素：搜索关键词+时间范围+明确的批量意图确认；在用户自然语言中，「取消所有…」「都删掉」等词语是批量意图的可靠信号，可以通过关键词匹配实现轻量级识别。 对于机器人控制任务，离散跳变的控制信号（夹爪0→1）比连续信号更难用通用逻辑检测，应针对具体任务特性设计检测器。 调试\u0026rsquo;功能不工作\u0026rsquo;时应先用极端测试用例（如infinite力）验证机制本身是否工作，再做量化分析——这比逐步增加参数更高效。 对比学习中温度参数tau需要设置下界（如tau_min=0.01），否则tau衰减至接近0时会导致loss=NaN，使整个实验失效。这是STAIG类方法的已知数值稳定性问题。 实践收获 通过pipeline_config.yaml的extra_config传递模型超参数（如qformer.epochs），可以在不修改模型代码的情况下实现benchmark级别的配置覆盖，这是更好的关注点分离设计。 Discord Bot中使用discord.ui.View发送交互式组件时，handler应返回空字符串；调用方（主消息循环）需要用if response判断是否需要额外发送文本响应，否则会触发Discord空消息API错误。 ccusage的\u0026ndash;offline模式对新模型可能有计费盲区，生产环境应默认在线获取最新定价。 LLM生成JSON的健壮解析需要多重fallback：直接解析→提取代码块→提取花括号范围。 会话摘要 MIHD-Benchmark ✅ 当前Benchmark逻辑说明与两阶段重写决策 20:36:10.896 | claude_code 用户提出了核心问题：应该先为每张切片提取embedding并缓存，再对同样的embedding做不同fusion。详细分析了现有流程中pca/uni2 embedding被重复计算77次的低效问题，与用户讨论了重构范围（仅加缓存层vs完全重写），用户选择完全重写pipeline。\n✅ Vision Encoding耗时分析与Pipeline重写方案设计 00:51:22.137 | claude_code 分析了vision encoding的主要耗时瓶颈（Patch串行提取+GPU批量推理中频繁调用empty_cache），并向用户说明了当前benchmark中编码器与fusion耦合导致的重复计算问题。用户决定完全重写pipeline，由此引出了两阶段架构的设计与实现。\n🔄 RTX 5000 Ada GPU加速+两阶段Pipeline架构实现与测试 21:52:14.896 | claude_code 在新RTX 5000 Ada (32GB) GPU上，通过新实现的两阶段pipeline运行所有benchmark。Phase 1快速完成了所有11 sections的gene/vision embedding提取（PCA/MLP/HIPT/ResNet50/UNI/UNI2），Phase 2达到127/190实验完成。Q-Former每section需5.7小时，经用户确认后减至50 epochs。端到端测试验证了concat/mean/attention/staig_fusion等核心组合的正确性，修复了ExperimentLogger方法名错误和mclust ImportError fallback缺失两个bug。\n🔄 Benchmark进度监控与basic_contrastive超时修复 00:07:35.904 | claude_code 监控DCC上的benchmark进度（31/207完成），发现basic_contrastive fusion因超时（3600s限制）失败。将超时时间修改为7200s，但意识到修改对已在运行的进程无效。持续等待basic_contrastive完成（实测每section约77分钟），然后重启了调度器继续运行剩余实验。\nGadgets ✅ gadgets工具集清理：删除过时工具并更新文档 00:58:36.268 | claude_code 用户删除了不再使用的gadgets（Video/audio/git/image/papers/png2text.py/text），通过/init命令重新生成了CLAUDE.md，然后手动更新了README.md，移除了所有过时工具的描述，保留并更新了summarize/和test/（新benchmark/包结构）的说明。\nCalendarPro 🔄 实现批量删除日历事件功能（5文件修改） 20:08:24.708 | claude_code AI按照用户提供的完整方案，修改了5个文件以支持批量删除：扩展DeleteData模型、更新LLM prompt、search_events支持时间范围、重写_handle_delete含批量逻辑、新增BatchDeleteApprovalView确认按钮、添加语义路由样本。启动app测试时发现CLAUDECODE嵌套问题，通过unset解决。\n✅ Google Calendar 403权限错误根因分析与修复计划 00:47:55.337 | claude_code 用户发现CalendarPro每次用户消息都出现403 insufficientPermissions警告。AI分析发现get_user_timezone()调用了calendarList API但OAuth scope只有calendar.events。AI提出了两套修复方案（settings API或直接用config时区），用户选择settings API方案。\n✅ LLM超时fallback：从原始输入提取删除参数 20:08:24.708 | claude_code 测试中发现嵌套Claude Code环境导致LLM超时，intent.data为空，批量删除请求被拒绝。AI在intent.data中添加_raw_input字段，并实现_extract_delete_query静态方法用正则从原始输入提取关键词、日期范围和batch标志。通过unset CLAUDECODE根本上解决了LLM嵌套问题。\n🔄 实现403修复并发现其他代码质量问题 00:50:22.155 | claude_code AI实施了get_user_timezone()从calendarList改为settings API的修复。随后探索发现3处代码质量问题（新intent缺少默认回复、裸except、Scheduler实例化重复），并制定了修复计划，但用户拒绝了该计划。\n🔄 修复批量删除运行时bug：空消息崩溃和Discord回复详情 20:08:24.708 | claude_code 测试中发现两个问题：1) BatchDeleteApprovalView发送后返回空字符串导致Discord拒绝空消息，通过if response检查修复；2) 用户反馈回复缺少具体信息（任务名称、时间），改进了删除操作和批量删除成功消息的格式。\n✅ 代码质量修复（dual_verify、executor、periodic_checker） 01:42:23.757 | claude_code AI实施了3项代码质量修复：补充新intent默认中文回复、裸except改为具体异常类型、Scheduler实例复用优化。所有82项核心测试通过，4项异步测试因缺少pytest-asyncio插件而跳过（已知问题）。\ngadget/summarize ✅ 配置文件+rclone云盘同步+机器标识全量实现 22:14:06.675 | claude_code 根据用户提出的headless server无GUI上传需求和机器标识需求，AI调研rclone后设计并实现了完整方案：新增_load_config/_resolve_output_dir/_get_device_name/_rclone_upload/_find_rclone函数，支持device_name和rclone_path配置，export文件名改用device_name，添加config \u0026ndash;show/\u0026ndash;init子命令，更新文档添加无sudo安装说明。\n✅ 添加Claude CLI后端支持，设为默认API，更新文档和requirements.txt 21:29:22.461 | claude_code 用户请求将summarize工具的AI后端从Anthropic API改为Claude Code CLI。AI新增summarize_with_claude_cli函数并更新\u0026ndash;api参数默认值为claude_cli。同时更新了tutorial.md（中文说明三种后端和安装方式）和README.md，创建requirements.txt。\n✅ ccusage计费修复+Claude CLI模型切换+export合并逻辑 23:24:58.955 | claude_code 三个独立改进：1) 用户发现\u0026ndash;offline导致Opus $19.11计费丢失，去掉参数修复；2) 用户指定将claude CLI模型从默认改为sonnet；3) 用户提出export时检测已有log并合并conversations，以(source, project, timestamp)去重、新数据优先。\n✅ 为summarize工具添加配置文件与rclone云盘同步（CalendarPro视角） 21:55:40.143 | claude_code 用户要求为daily_summary.py添加配置文件支持和rclone多设备同步。AI实现了~/.config/summarize/config.json配置文件、_load_config/_rclone_upload等工具函数、config \u0026ndash;show/\u0026ndash;init子命令，并更新了tutorial.md和README.md。\n✅ 修复Windows npx/claude找不到+ccusage移到AI总结后运行 23:26:16.140 | claude_code 实际运行脚本后发现两个问题：npx在bash subprocess下找不到（需要shell=True），claude CLI在Claude Code会话内被嵌套检测拒绝（需清除CLAUDECODE环境变量）。同时将ccusage调用移到AI总结之后，确保总结本身的token消耗也被统计。\n✅ 完善README和tutorial，优化rclone上传、ccusage原始JSON、JSON解析健壮性 22:51:43.654 | claude_code 用户要求完善文档并修复三个问题：rclone统一上传到同一目录（去掉sub_dir）、ccusage直接使用原始JSON格式（不做格式转换）、_parse_json_response改为三步fallback解析。文档重写后将配置/云盘/机器标识各章节整合进自然工作流叙述中。\n❌ 尝试调用daily_summary.py生成日报失败（Claude CLI空内容） 22:31:02.666 | claude_code 用户尝试运行daily_summary.py \u0026ndash;date 2026-02-14，Claude CLI因嵌套Claude Code环境返回空内容，JSON解析失败并报错。AI开始排查subprocess调用逻辑，但对话在定位到问题后中断。\nErrorRecoveryBenchmark ✅ 修复pre_grasp检测器不触发：实现get_task_phase()并清理死代码 22:56:34.002 | claude_code 执行人类制定的修复计划：重写get_task_phase()使用get_task_completion_stages()，删除database.py中的三个死方法和DatabaseMeta中的两个未使用字段。所有41个单元测试通过。但运行验证时发现pre_grasp仍未触发，AI主动调查并发现了计划未涵盖的4个额外bug，逐一修复后pre_grasp最终在demo_1的step115和demo_2的step124触发。\n✅ 调试30N力注入无效果并修复EEF body定位bug 21:27:34.381 | claude_code 用户发现视频中机械臂没有任何扰动。AI通过添加调试日志发现力被错误地施加到bin2（垃圾箱），根本原因是_get_eef_body_name()的possible_names列表缺少\u0026rsquo;gripper0_eef\u0026rsquo;，fallback匹配到包含\u0026rsquo;ee\u0026rsquo;子串的robot0_screen_collision。修复后30N力正确施加到gripper0_eef(body_id=21)，inf力测试显示机械臂有显著移动。同时添加了\u0026ndash;force_override和\u0026ndash;duration_override参数。\n🔄 调查post-injection机械臂静止不动的原因及可视化改进 00:38:54.892 | claude_code 用户发现可视化视频中post-injection阶段机械臂完全静止。AI确认这是设计如此（neutral action=保持EEF位姿），用户选择改为继续执行demo actions。随后实现了\u0026ndash;force_override参数、duration_steps自动清零xfrc_applied、将force_norm_range从[3,15]提升到[15,45]N。但生成30N视频后用户反馈机械臂仍无明显运动，用户提议用infinite力诊断，会话在计划制定阶段被中断。\n🔄 实现持续力注入机制和PreGrasp检测器 22:05:14.381 | claude_code 按用户需求实现持续力（0.5-1秒）和抓取前检测器。修改collect_rollout_stats()支持循环重施加力，新建pre_grasp.py按用户建议的跳变检测逻辑实现，注册并配置。测试发现PreGrasp检测器未触发，调试脚本分析夹爪轨迹后发现demo_0夹爪物理状态始终≥0.98，demo_1有7个候选步。最终发现根本原因是get_task_phase()调用stub方法永远返回\u0026rsquo;unknown\u0026rsquo;，导致task_phase检查失败。\n❌ 设计修复PreGrasp不触发和清理死代码的plan 22:49:50.381 | claude_code 定位到两个根本原因：1)get_task_phase()的_check_phase_condition()是STUB永远返回False；2)total_rejected始终为0可能是dead code。用户中止了plan的执行，工作日结束。\n✅ 实现Phase 3可视化继续执行demo actions并在an49上测试 00:45:39.241 | claude_code 用户要求将可视化脚本Phase 3从neutral action改为继续执行demo actions。AI修改了2_visualize_scene.py并通过SSH在an49上运行测试，发现Phase 3日志显示'533 remaining demo actions\u0026rsquo;，EGL渲染成功生成视频。测试显示场景为tip_over类型，impulse施加在EEF上。\n✅ SSH命令格式调试及集群GPU节点访问方式确认 01:35:54.892 | claude_code 尝试通过SSH到GPU节点(an53/an49)运行可视化脚本，反复因遗漏cd命令失败超过20次。用户最终解释需要cd到项目目录并激活conda环境。最后用户告知当前已通过跳板直连GPU节点，无需SSH，直接检测nvidia-smi确认有5个A800 GPU，在当前节点成功生成视频。更新了CLAUDE.md记录正确的GPU访问策略。\nToken 用量 总览 指标 数值 总 Token 118,217,669 输入 Token 37,508 输出 Token 110,857 Cache 创建 5,794,068 Cache 读取 112,275,236 Cache 命中率 95.1% 总费用 (USD) $81.7633 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 19,430 108,575 3,877,057 98,860,665 $76.4735 93.5% claude-haiku-4-5-20251001 9,712 1,788 1,412,273 12,219,193 $3.0059 3.7% claude-sonnet-4-5-20250929 8,366 494 504,738 1,195,378 $2.2839 2.8% 各设备用量 设备 总 Token 输入 输出 费用 DCC 20,264,782 9,756 35,824 $14.5508 MacBook 51,452 3 2 $0.1314 TzJsDesktop 38,391,199 5,128 29,861 $28.0100 tianhe 59,510,236 22,621 45,170 $39.0710 ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-14/","summary":"在DCC高性能集群上推进MIHD多模态空间转录组学benchmark（完成两阶段Pipeline架构实现）、修复ErrorRecoveryBenchmark中力注入机制和pre_grasp检测器的多个根本性bug、为CalendarPro Discord Bot新增批量删除功能并修复嵌套Claude环境问题，以及为gadget/summarize工具完整实现配置文件、rclone云盘同步和Claude CLI后端支持。","title":"Bug Journal 2026-02-14"},{"content":"日报 — 2026-02-13 今日工作横跨四个项目：MIHD多模态基准测试框架完成7个Phase增强与286个实验自动化调度；CalendarPro完成代码整理、P0稳定性修复、P1智能功能（学习采集、能量个性化、随机想法系统）、循环日程与定时自检；gadget日报工具重构为多设备两阶段架构并推送GitHub；error_recovery_benchmark修复demo回放环境配置不匹配根本bug、cvel角速度读反误触发、移植3个新注入器并成功生成30个有效错误场景。\n今日任务 架构与策略 ✅ 修复demo回放环境配置不匹配bug — 修复1_generate_scenes.py和2_visualize_scene.py中create_env()从HDF5元数据加载env_args（含controller_configs），而非创建裸PickPlace环境，恢复proximity detector合理阈值。修复后成功生成30个有效错误场景（触发步骤52-121，EEF在5cm内）。 ✅ 修复rollout_generator注册表集成并完成smoke test — 修复rollout_generator.py硬编码注入器为基于注册表动态加载，修复detectors与injectors参数命名不匹配，修复friction/gripper_bias注入的clear()调用缺失，修复2个运行时bug（bool字符串转换、MetricsWithCI格式化），在an49上4步smoke test全部通过。 ✅ MIHD Enhancement 7个Phase全量实现 — 实现Config Foundation、Normalization Integration、UNI2+scGPT实验配置、Q-Former+LLaVA MLP融合策略、Niche查询、批次校正+Joint模式共7个Phase。所有语法检查通过，22个__all__条目导入正常，批次校正测试3/3通过。 ✅ MIHD Benchmarking 自动化调度器实现 — 创建scripts/run_all_benchmarks.py（可恢复的实验调度器）、更新benchmark_config.yaml（A-F六类实验组共286个实验）、新增p0_full_matrix和scgpt_full_matrix实验组、创建scripts/summarize_benchmark.py（结果汇总与可视化）。dry_run验证总实验数286，环境过滤正确。 ✅ MIHD benchmark 数据修复 — 从Dataset.zip提取151510/151672/151674的缺失空间坐标文件(tissue_positions_list.csv)和图像文件；修复151674频域滤波空patch边界检查；为151676添加STAIG模型坍缩时的KMeans fallback。 ✅ CalendarPro 代码库整理 — 按PLANNING.md分析执行三阶段整理：删除10个临时/废弃文件及目录，提取time_utils.py和provider_selector.py共享模块，统一关键词常量和餐食时间定义，更新8处重复的AI提供商选择逻辑。最终26/26核心测试通过。 ✅ CalendarPro 定时自检与循环日程自动安排 — 创建src/scheduling/模块（RecurringTaskStore、PeriodicChecker），修改models.py、config.py、intent_routes.py、prompts.py和discord_bot.py，编写18个测试用例全部通过。 ✅ 修复MuJoCo cvel速度分量读反bug — env_wrapper.py中cvel的线速度和角速度索引互换（:3改为3:，3:改为:3），修正注释，清除旧场景数据并重新生成。 ✅ ErrorRecoveryBenchmark代码库整理 — 删除fingerprint.py中~190行死代码（ReplaySystem类、compute_drift_metric函数），修复proximity.py重复detect()方法，从mimicgen_workspace移植FrictionInjector、PosePerturbInjector、GripperBiasInjector三个注入器，删除12个冗余.md文档，为11+核心模块添加中文docstring，版本升至4.1.0。 🔄 286个基准实验后台批量运行 — 全部286个实验在HPC后台运行，平均每个实验约217秒，截至会话结束时进度12/207，ETA约12小时。 ✅ AI日报工具两阶段架构实现 — 重构daily_summary.py为export/merge两阶段，支持多设备工作流，添加ccusage token统计，Hugo部署集成，支持三种数据源解析（Claude Code JSONL、ChatGPT导出、通用JSON）。 ✅ 创建CalendarPro docs/PLANNING.md项目规划文档 — 对CalendarPro整个代码库进行深度探索，创建330+行中文规划文档，涵盖项目定位、核心理念、当前模块成熟度（8个模块星级评分）、大/中/小三层目标、架构决策理由、P0-P3行动路线图、5个维度的已知差距。 ✅ CalendarPro P0稳定性修复 — 实现JSON解析容错（平衡括号计数算法）、三个AI提供商超时处理（asyncio.wait_for+httpx.Timeout）、消息队列done_callback防止task静默死亡、服务层错误处理加固，创建56个意图分类边界测试用例。 ✅ Random Thoughts收集与自动整理功能 — 新建src/thoughts/模块（ThoughtStore JSONL存储、IdleDetector空闲检测、ThoughtOrganizer后台整理器）；更新IntentType、Settings、semantic routes、discord_bot集成；25个测试全通过。 ✅ 修复SR/LLM意图分类mismatch问题 — dual_verify.py在SR和LLM分类结果不一致时改为优先使用LLM结果，修复了\u0026rsquo;晚点做leetcode\u0026rsquo;被误判为manage_recurring的问题。 ✅ 修复初始化震动误触发+添加EEF近距离门控 — 将instability detector的min_step从10调整到100，在rollout_generator.py中插入EEF近距离门控逻辑，经多轮参数调整后生成有效场景。 ✅ 创建ErrorRecoveryBenchmark项目全景总结文档 — 整理15+个计划文档，汇总成395行中文全景文档，涵盖项目愿景、架构、目标层次分解、诚实进度评估（场景数3/200=1.5%）、下一步行动计划。 🔄 MIHD增强计划文档化 — 读取ENHANCEMENT_PLAN.md，探索代码库，制定7个阶段的详细实现计划（归一化、Q-Former、LLaVA MLP、niche查询、批次校正、配置整合），但用户未批准ExitPlanMode。 🔄 CalendarPro P0/P1功能规划 — 深入探索代码库后制定了详细的P0和P1实现计划，但实现被中断（用户拒绝了ExitPlanMode的操作）。 ❌ 初始化静置+EEF近距离门控计划（未批准） — 用户提出场景与human demo不一致、物体震动来自初始化、注入需要机械臂5cm以内。已写好修复计划但用户未批准执行。 ✅ CalendarPro P1智能功能实现 — 实现学习数据自动采集（backfill_energy_change）、能量投影个性化（历史权重混合昼夜节律）、智能排程整合学习数据、会话上下文结构化（带意图流追踪，限制5轮历史）。 🔄 执行smoke test（S7.1） — 在an49 GPU节点上逐步执行smoke test pipeline：Step1（场景生成）和Step2（拒绝日志分析）通过，Step3因旧场景指纹不匹配失败，已清除旧数据准备重新运行。 实现与修复 ✅ 修复CalendarPro多项系统Bug — 修复日历操作错误信息不反馈给用户（CalendarServiceError替代静默失败）、循环事件更新应只修改单个实例（剥离recurrence字段）、Windows启动脚本Conda环境检测问题（新建start.ps1+stop.ps1）。 ✅ 实现LargeOffsetValidator（S5.1）和StuckValidator（S5.2） — 创建large_offset.py（xy平面位移\u0026gt;=0.10m验证通过）和stuck.py（\u0026gt;=80%帧速度\u0026lt;0.002m/step验证通过），注册到VALIDATOR_REGISTRY，编写16个单元测试，全部41个测试通过。 ✅ 实现可视化视频帧标注 — 在scripts/2_visualize_scene.py中添加帧标注功能（顶部色带区分三阶段、左下角检测信息框、右下角帧计数器），修复hardcode ImpulseInjector的遗留bug，修复帧丢失问题（从HDF5加载demo actions全程渲染）。 ✅ GPU利用率追踪修复 — 重写ResourceMonitor.start_monitoring()，添加后台守护线程每秒采样GPU/CPU峰值；修复了之前峰值永远为0的问题。 ✅ 实验日志时间戳自动更新 — 在_append_to_global_log方法中添加_update_header_timestamp()调用，实现experiments_log.md文件头部时间戳的自动维护。 ✅ gadgets GitHub仓库初始化 — 初始化git repo，添加test/为submodule，写.gitignore排除敏感文件，推送到TzJ2006/gadget.git，为所有gadget工具写了详细README。 ✅ 重写ErrorRecoveryBenchmark CLAUDE.md — 将615行充满重复和冗余的CLAUDE.md精简为101行，删除与README重复的内容，保留服务器约束、命令、架构说明和常见陷阱。 ✅ CalendarPro README更新教程 — 更新README.md，新增Discord命令速查表、扩展能量/学习系统文档（包括三步自动采集流程）、FAQ新增AI超时/容错/学习系统说明。 问题与解决方案 关键问题 1. MuJoCo cvel布局是[angular(3), linear(3)]，但代码按[linear(3), angular(3)]读取，导致面包的角速度0.445 rad/s被当成线速度→误触发instability detector。 解决方案: 交换cvel的:3和3:索引赋值，修正注释。修复后触发步骤从step 11推迟到step 15，误报消失。\n关键洞察: MuJoCo cvel格式是反直觉的（angular在前），仅靠变量名无法发现，需查阅MuJoCo官方文档确认布局顺序。\n2. demo回放环境与录制环境配置不匹配：1_generate_scenes.py使用裸PickPlace环境（无controller_configs），而demo用PickPlace_D0+OSC_POSE kp=150录制，导致相同action产生完全不同轨迹，机械臂全程未靠近物体。 解决方案: 修改create_env()从HDF5 dataset的data.attrs[\u0026rsquo;env_args\u0026rsquo;]读取完整配置（含controller_configs、robots等），并将PickPlace_D0等MimicGen变体名转换为base环境名，恢复被错误放宽的proximity阈值。\n关键洞察: 这是整个项目上一轮迭代中只有instability detector能生成scene、而proximity detector无效的根本原因——因为arm从未靠近过物体。症状修复（放宽阈值）掩盖了根本原因。\n3. conda run在subshell中无法正常激活conda环境，导致subprocess输出文件为空（0字节），进程虽在运行但无法确认状态。 解决方案: 使用绝对Python路径/hpc/group/yizhanglab/zt81/condaenv/General/bin/python直接调用，绕过conda run的环境激活问题。\n关键洞察: 在HPC环境中，conda run的subshell环境激活不可靠，应直接使用conda环境的绝对Python路径以保证输出缓冲正常工作。\n4. 调度器中子进程使用相对路径scripts/run_benchmark.py，导致子进程工作目录不正确：58个实验显示成功但仅15个npz文件实际保存。 解决方案: 使用Path(file).parent.parent构建绝对路径，在subprocess.run中设置cwd=project_root，将output_dir转为绝对路径。\n关键洞察: 在调度器启动子进程时，所有路径（脚本路径、输出目录）必须使用绝对路径，工作目录必须显式设置，否则文件写入会静默失败。\n5. tokens/目录含真实API token，差点被提交到git。 解决方案: 在commit前检查文件大小（37 bytes），确认是token后加入.gitignore并用git rm \u0026ndash;cached移除。\n关键洞察: 始终在首次commit前检查敏感文件；token文件要第一时间加入.gitignore。\n6. rollout_generator.py硬编码只使用ImpulseInjector，3个新注入器从未被调用。 解决方案: 将初始化改为从INJECTOR_REGISTRY动态加载所有4个注入器，同时将VALIDATOR_REGISTRY中的4个验证器全部注册。\n关键洞察: 代码通过了所有单元测试但实际功能有缺失——注册表设计本身是正确的，但调用方没有使用注册表。\n7. EEF-object距离始终\u0026gt;=0.22m，机械臂从未接触物体。 解决方案: 深层调查发现根本原因是demo回放环境配置不匹配（已在上述条目中合并）。\n关键洞察: 当物体在整个demo回放中完全不动时，是环境配置不匹配导致机械臂动作错误，而非距离计算bug。\n8. 151510/151672/151674三个section缺少tissue_positions_list.csv，导致空间坐标加载失败。 解决方案: 发现文件存在于Dataset.zip中但未解压，用unzip直接提取到正确路径。\n关键洞察: 数据文件不是真的缺失，只是未解压。需要先检查数据源压缩包。\n9. 多设备（Windows/Linux/Mac）需要各自导出AI对话日志，再集中汇总生成日报，而原始设计是单机一体式。 解决方案: 重新设计为两阶段架构：export阶段（各设备本地运行，无需API）+ merge阶段（汇总机器调用AI API生成报告），中间产物为可移植JSON文件。\n关键洞察: 将解析（依赖本地文件系统）与总结（依赖API）拆分，使得跨设备工作流成为可能；中间格式设计为自描述JSON包含device字段，便于merge时区分来源。\n10. SR意图路由器基于关键词嵌入匹配，用户说\u0026rsquo;晚点做leetcode就好\u0026rsquo;时，\u0026rsquo;leetcode\u0026rsquo;命中了manage_recurring的utterances，导致意图被错误分类。 解决方案: SR和LLM分类结果不一致时，改为优先使用LLM的结果。LLM理解完整对话上下文，在语义歧义时判断更准确。\n关键洞察: 语义路由器（SR）擅长快速分类明确意图，但在语义模糊或上下文依赖的场景下不如LLM准确。双重验证系统mismatch时应信任上下文更丰富的LLM。\n11. friction/gripper_bias注入修改（geom_friction/xfrc_applied）在set_sim_state_flat()后持续生效，污染下一个候选的初始状态。 解决方案: 在_try_inject_and_validate的所有3个退出路径（验证失败、稳定性失败、成功）都调用injector.clear(env, spec)。\n关键洞察: set_sim_state_flat()只恢复位置/速度状态，不恢复物理参数修改——这是一个微妙的MuJoCo状态管理问题。\n12. detectors生成的ErrorSpec参数名与injectors期望的参数名不匹配：instability检测器发送friction_scale类型和scale参数，而注入器注册表键名是friction且期望friction_scale参数。 解决方案: 修改instability.py将error_type改为\u0026rsquo;friction\u0026rsquo;，将参数键从\u0026rsquo;scale\u0026rsquo;改为\u0026rsquo;friction_scale\u0026rsquo;；修改grasp_precond.py的pose_perturb和gripper_bias参数命名；修复base_detector.py中的family_map映射。\n关键洞察: 接口契约没有通过测试强制执行，导致两端独立开发时出现了命名漂移。\n13. 可视化视频三阶段画面几乎一样，看不到面包倾倒过程。 解决方案: 去掉collect_rollout_stats调用（它内部step了20步但不渲染），改为从HDF5加载demo actions全程回放并捕获每一帧。\n关键洞察: validate逻辑吃掉了关键帧，可视化不需要重新验证，应该把所有step都渲染出来。\n14. ExitPlanMode被用户说\u0026rsquo;please continue\u0026rsquo;拒绝，AI误解为等待，停止工作。 解决方案: 用户两次拒绝ExitPlanMode并说\u0026rsquo;please continue\u0026rsquo;，实际意思是隐式批准计划并要求立即执行。\n关键洞察: 用户说\u0026rsquo;continue\u0026rsquo;时应该直接开始实施，而不是停止等待重新指令。\n15. 151676 section STAIG模型坍缩导致mclust聚类失败。 解决方案: 在mclust调用外加try/except，失败时自动fallback到KMeans。\n关键洞察: 对外部R包的调用要做好错误处理和fallback策略。\n16. CalendarPro中AI提供商选择逻辑在8个位置重复，关键词常量在2处重复，时间解析函数在3处重复，餐食时间在3处重复。 解决方案: 提取共享模块：src/core/time_utils.py（统一时间解析）、src/ai/provider_selector.py（统一提供商选择），models.py中提升关键词为模块级常量，energy_projection从EssentialScheduler.DEFAULT_MEALS动态获取餐食时间。\n关键洞察: 代码重复的根本原因是这些逻辑跨层分布（models/scheduler/energy），提取时需要分析依赖方向，避免循环导入。\n17. 多个JSON块时贪婪正则{[\\s\\S]*}会从第一个{匹配到最后一个}，导致解析出无效JSON。 解决方案: 改用平衡括号计数算法逐字符找到最外层完整JSON对象，修复后所有56个解析测试通过。\n关键洞察: 正则无法可靠匹配嵌套括号结构，平衡计数是正确解法。\n一般问题 18. update_event在更新循环事件时包含recurrence字段，会影响整个系列。 解决方案: 检测recurringEventId字段，如是循环实例则剥离recurrence相关字段，用instance ID更新，让Google Calendar只创建该日期的exception。\n关键洞察: Google Calendar API的循环事件有两种ID：父事件ID和实例ID（格式parentId_YYYYMMDDTHHMMSSZ）。用实例ID+剥离recurrence字段更新，API会自动处理为exception。\n19. Google Calendar API返回403 insufficientPermissions等错误时，calendar_service.py只打印到控制台，用户在Discord只看到操作失败，无法判断原因。 解决方案: 将所有print()/返回None模式改为raise CalendarServiceError，上层handler捕获后将错误简述呈现给用户。\n关键洞察: 错误处理应该贯穿所有层级直到用户界面，静默失败对用户调试毫无帮助。\n20. smoke test第4步崩溃：TypeError: unsupported format character，原因是episodes.jsonl中success字段被保存为字符串\u0026rsquo;False\u0026rsquo;而非布尔值False。 解决方案: 在4_analyze_results.py中增加bool转换：success=bool(data[\u0026lsquo;success\u0026rsquo;]) if isinstance(data[\u0026lsquo;success\u0026rsquo;], bool) else data[\u0026lsquo;success\u0026rsquo;] == \u0026lsquo;True\u0026rsquo;。\n关键洞察: JSON序列化/反序列化时Python布尔值在某些情况下被存为字符串，需在加载时显式转换。\n21. run_evaluation()内部使用了getattr(args, \u0026lsquo;pca_n_components\u0026rsquo;, 50)，但args对象在该函数作用域内不可用。 解决方案: 改为使用extra_config.get(\u0026lsquo;pca_n_components\u0026rsquo;, 50)从配置字典中读取参数。\n关键洞察: 函数重构时需注意参数对象的作用域边界，避免依赖外部全局args对象。\n22. 代码引用了不存在的from utils.clustering_utils import cluster_embeddings，但该函数实际定义在run_benchmark.py自身中。 解决方案: 移除错误的import语句，直接使用本地定义的cluster_embeddings函数。\n关键洞察: 在大型代码生成任务中，AI容易混淆函数来源，将本地函数误认为外部模块。\n23. ResourceMonitor GPU峰值始终为0.0。 解决方案: 原来update_peak_stats()从未被调用；改为在start_monitoring()中启动后台守护线程每秒定期采样。\n关键洞察: 监控类需要主动轮询，而不是依赖单次快照。\n24. start.bat在Windows cmd环境下无法激活Conda环境（CONDA_PREFIX不存在，conda activate需要conda init）。 解决方案: 新建start.ps1，PowerShell原生支持conda activate。start.bat改为自动调用PowerShell版脚本。通过PID文件记录进程ID，stop.ps1可精确停止服务。\n关键洞察: Windows的cmd和PowerShell对conda的支持有本质差异，conda推荐的shell是PowerShell。\n25. MetricsWithCI对象不支持f\u0026rsquo;{metric:.3f}\u0026lsquo;格式化，导致results分析脚本崩溃。 解决方案: 在MetricsWithCI类中添加__format__方法，委托给.value属性的格式化。\n关键洞察: Python的format protocol需要__format__才能支持格式说明符，__str__不够。\n26. spatial_coords变量在fusion代码块内部定义，但在代码块外部被引用，导致NameError。 解决方案: 在引用点使用try/except重新加载坐标数据。\n关键洞察: 局部变量的作用域边界需在实现时明确规划，跨块引用应通过重新获取或提升作用域解决。\n27. test/目录已有独立git repo，直接git init会产生嵌套repo问题。 解决方案: 将test/添加为git submodule，指向已有的TzJ2006/test.git。\n关键洞察: 独立维护的子项目应该用submodule而非直接包含。\n28. pytest中对PeriodicChecker内部懒加载导入（Scheduler在方法内部import）的Mock路径不正确，导致essentials调用测试失败。 解决方案: 调整Mock patch路径，改为在periodic_checker模块内部patch Scheduler，使测试能够正确拦截懒加载的import。\n关键洞察: 懒加载import（在函数体内import）需要patch目标模块内的引用路径，而非原始模块路径。\n29. SSH运行脚本时报\u0026rsquo;No such file or directory\u0026rsquo;，AI反复尝试同一命令。 解决方案: 使用绝对路径指定脚本，或在SSH命令中先cd到项目目录。已固化为CLAUDE.md规则。\n关键洞察: SSH默认cwd是home目录而非项目目录，需要显式cd或使用绝对路径。\n30. Windows设备上的\u0026rsquo;nul\u0026rsquo;文件无法被Python shutil或rm -f删除，报Access Denied。 解决方案: 识别nul是Windows系统设备名，无法真正删除；改为将其添加到.gitignore中，避免意外跟踪。\n关键洞察: Windows保留设备名（nul, con, prn等）不是普通文件，操作系统层面拒绝删除，需要换思路处理而非强行删除。\n31. ruff检查发现新模块中import顺序未排序、field未使用、行过长等lint问题。 解决方案: 调整import顺序（按字母排序）、移除未使用的field import、将过长行拆分为局部变量赋值后再拼接。\n关键洞察: 新模块需在开发时同步运行lint检查，避免积累style问题。\n32. 在Windows下无法直接用conda activate激活环境，which python指向全局而非conda环境。 解决方案: 使用完整绝对路径调用calendarpro环境的Python：C:/Users/tongt/miniconda3/envs/calendarpro/python.exe。\n关键洞察: 在bash子进程中conda activate不工作，但直接用绝对路径调用env内的可执行文件是更可靠的方式。\n人类思路 vs AI 思路 战略层面 多设备日报工作流架构设计 角色 思路 人类 人类明确指出需要两阶段架构：各设备export→汇总机器merge，这是对实际工作场景（多台电脑）的准确把握。AI最初设计的是单机一体式工具，未考虑多设备场景。 AI AI实现了完整的两阶段CLI结构，并补充了Hugo部署集成、ccusage token统计等细节。 差异分析: 架构方向完全由人类驱动；AI负责技术实现和扩展功能。用户从实际使用场景出发，提出了AI未预见的跨设备需求。\ndemo回放bug的根因分析 角色 思路 人类 人类通过观察到\u0026rsquo;上一轮只有instability detector能生成scene\u0026rsquo;这一现象，结合对controller_configs差异的了解，精准定位到环境配置不匹配是根本原因，并提供了完整的3层根因分析。 AI AI上一轮在不知根因的情况下，只是放宽proximity阈值来\u0026rsquo;凑\u0026rsquo;出scene，是症状修复而非根因修复。 差异分析: 人类有更强的系统性思维，能从\u0026rsquo;为什么某个检测器从不触发\u0026rsquo;的现象反推到环境配置层面的根本问题；AI倾向于在当前层面调整参数直到测试通过。\nEEF-object距离异常的根因定位 角色 思路 人类 人类怀疑距离计算本身有问题，追问\u0026rsquo;距离是如何判断的\u0026rsquo;，引导AI深入调查实际距离计算链路。人类基于领域常识（抓取任务必须接触物体）发现了问题的不合理性。 AI AI最初接受了0.22m的数据并尝试调整阈值来适应，没有质疑物理合理性。 差异分析: 人类基于领域常识发现了问题的不合理性，AI则接受了数据并在错误假设下调整参数。\n286个实验的调度策略与实验矩阵设计 角色 思路 人类 人类预先设计了完整的A-F六组实验矩阵，明确了gene encoder×vision encoder×fusion的组合逻辑，并规划了可恢复调度器的需求。 AI AI负责将人类的矩阵设计转化为代码实现，并实现断点续传逻辑。 差异分析: 实验设计的战略层面完全由人类规划；AI专注于工程实现层面，但对实验设计背后的科学动机缺乏主动理解。\nEnhancement计划的Phase划分 角色 思路 人类 人类将复杂的多模态增强工作拆分为7个Phase，包含Config Foundation到Joint模式的完整路径。 AI AI逐Phase实现，但实现过程中出现3个需要人类纠正的错误（作用域问题、错误import）。 差异分析: 高层架构规划由人类完成，AI在实现细节层面出现了系统性错误，说明AI对跨文件代码上下文的维护能力仍有局限。\ncvel bug发现与诊断 角色 思路 人类 用户通过观察异常现象（机械臂远离面包时就触发）+具体数值（0.445 m/s超过0.3阈值）+查阅MuJoCo文档，定位到cvel布局问题，并提供了精准的影响范围分析。 AI AI按照用户的精准plan执行修复，读文件确认代码位置，完成实际修改和验证。 差异分析: Bug的根因诊断和影响范围分析完全由人类完成，AI起到执行器的作用。人类对MuJoCo内部布局的了解是AI所不具备的先验知识。\nconda环境问题的根因定位 角色 思路 人类 人类通过观察ps aux确认进程在运行，但输出文件为空，提供了现象描述。 AI AI识别出根因是conda run的subshell环境激活问题，并提出使用绝对Python路径的解决方案。 差异分析: 人类提供现象，AI完成根因分析和方案设计，此处AI贡献较强。\nCalendarPro代码库整理的优先级判断 角色 思路 人类 用户提供了预先分析好的PLANNING.md，其中明确列出了哪些文件需要删除、哪些需要合并、哪些逻辑需要复用，以及具体的行号。 AI AI负责执行：读取源文件验证内容、创建新的共享模块、替换各处调用点，并进行验证（import检查、pytest、ruff）。 差异分析: 架构分析和优先级判断完全来自人类（体现在PLANNING.md中），AI的价值在于精确执行和并行处理多文件修改。\nCalendarPro功能架构设计与实现计划 角色 思路 人类 用户提前制定了完整的架构计划：明确指定了3个新建文件的接口定义（含方法签名、字段列表）和5个修改文件的具体改动位置（含行号和代码片段），以及核心逻辑（每日首次/每次检查的区别、mark_scheduled防重复、时间已过任务跳过逻辑）。 AI AI完全按照用户计划执行，先读取所有相关文件了解现有代码结构，再逐步实现各模块，最后运行lint和测试验证。 差异分析: 架构设计和接口规范完全来自用户；AI的贡献在于代码实现细节。AI没有提出任何新的架构思路，完全是执行角色。\n发现真实Bug的途径 角色 思路 人类 用户直接从运行日志中发现了意图分类错误，通过实际使用而非代码审查发现问题。 AI AI通过阅读错误日志条目、分析代码逻辑找到bug根因，并同时修复了多个相关问题。 差异分析: 人类通过实际运行场景发现功能性bug；AI在理解bug后能系统性地发现和修复相关的隐藏问题。\nRandom Thoughts功能设计 角色 思路 人类 用户提出核心需求概念：快速捕获+空闲时段自动整理+层级结构输出，并指定利用已有的EssentialScheduler时间窗口作为空闲检测数据源。 AI AI设计了完整的模块架构（ThoughtStore/IdleDetector/ThoughtOrganizer分离关注点），实现了JSONL追加存储、平衡括号算法、异步后台任务等技术细节。 差异分析: 产品需求和数据源复用策略来自人类；模块化架构设计和具体实现由AI完成。\n代码清理范围决策（ErrorRecoveryBenchmark） 角色 思路 人类 用户主动提出整理代码库，并通过回答两个问题明确了范围：移植部分mimicgen错误类型+删除冗余文档。 AI AI探索了代码库后提出了5个问题让用户选择，通过AskUserQuestion工具收集决策。 差异分析: 用户给出了清晰的战略方向（移植而非整合），AI负责技术分解。\nPLANNING.md文档深度（CalendarPro） 角色 思路 人类 用户希望文档面向自己，梳理思路和追踪进度，要求按大/中/小目标三层结构，包含设计理由。 AI AI深度探索代码库（48+文件+OpenClaw生态）后，补充了基于实际代码的模块成熟度评级、真实差距分析、P0-P3优先级路线图，比用户预期更系统化。 差异分析: 人类提供了文档框架和目的，AI通过代码实证填充了具体内容，两者协作产出了比单独任一方更完整的文档。\nAI 局限性 重要局限 AI未能主动预见subprocess工作目录问题——即使生成了相对路径代码，也未提前警告在调度器场景下相对路径会导致静默失败（文件写入到错误目录但进程不报错）。 在大规模代码生成（7个Phase，跨多文件）时，AI混淆了函数定义位置，将本地函数误当作外部模块导入（cluster_embeddings），需要人类纠正。 ExitPlanMode被用户反复拒绝（多次），说明计划文件内容不够完整，用户期望计划文件能完全自包含以便下次继续。AI需要在第一次就把所有探索结果、精确行号、完整代码和用户决策全部写入计划文件。 接受了0.22m EEF-object距离这一在pick-and-place场景下明显不合理的数据，没有基于物理常识发出质疑，而是在错误假设下调整参数。 上一轮迭代中AI通过放宽proximity detector阈值（0.15→0.30, 0.5→0.2）来使测试通过，而未能识别出环境配置不匹配这一根本原因，导致生成了表面看似工作实则物理上不正确的场景。 面对MuJoCo cvel布局这种反直觉的API，代码注释本身也写错了，说明AI生成注释时可能照搬了错误的假设而没有验证MuJoCo文档。 AI在两次ExitPlanMode被拒绝后无法正确理解用户意图（\u0026lsquo;please continue\u0026rsquo;实际是隐式批准），导致两次停止工作等待用户重新指令。 AI没有主动识别conda run在HPC subshell中的环境激活局限性，需要人类先观察到现象（输出文件为空）才触发根因分析。 会话因使用限制中断，说明AI在长上下文、大规模代码生成任务中存在会话长度约束，无法在单次会话中完成超大型实现计划。 AI设计单机工具时未主动考虑多设备/多平台使用场景，需要用户明确提出后才能调整架构。对于工具的实际部署环境缺乏主动预见。 调参过程中（5cm→15cm→25cm→30cm）缺乏系统性：每次失败后仅凭经验增大阈值，而没有先获取完整的统计分布（如EEF距离的percentile分布）再做决策。 修复cvel bug后重新生成场景，AI未能主动评估场景质量（trigger_step=15在659步demo中是否合理），需要人类观看视频后指出问题。 在两次ExitPlanMode操作均被用户拒绝后，AI没有追问用户为什么拒绝，直接在下次对话中重新开始。AI无法从被拒绝的隐式信号中推断用户真正的意图。 一般局限 AI在函数重构时未追踪参数对象的作用域边界，导致getattr(args, \u0026hellip;)在args不可用的作用域中被使用，属于代码审查阶段应捕获的基础错误。 SSH命令中反复声称添加了cd但实际command string中没有包含，缺乏自我验证机制，连续失败10+次才被用户纠正。 AI在创建PLANNING.md时，通过后台Agent探索OpenClaw生态，但在Agent完成前已写出文档，导致技能系统成熟度评级不准确，需要在Agent返回结果后回头修正。多线程探索和主线写作的信息同步存在延迟。 在git submodule checkout后，进入test/子目录操作，导致后续git status命令在错误的目录执行。AI应该始终在主仓库根目录执行git命令。 在Windows环境下处理特殊文件（nul设备名、全角字符路径）时，AI先尝试了标准的shell方式并失败，才改用Python os模块。对平台特殊性的预判不足。 AI在处理懒加载import的Mock时出错，生成的测试patch路径不正确，需要通过运行测试后报错才能发现并修复，说明对Python import机制的边界情况（函数体内lazy import）缺乏主动预判。 AI无法直接查看mp4视频文件，只能通过元数据（JSON summary）描述场景内容，限制了对实际渲染效果的验证。 今日收获 核心收获 在HPC环境中启动子进程时，必须：(1)使用conda环境的绝对Python路径而非conda run；(2)所有脚本路径和输出路径使用绝对路径；(3)显式设置subprocess的cwd。任何一项缺失都可能导致静默失败（进程运行但文件不落盘）。 MuJoCo data.cvel的布局是[angular(3), linear(3)]（角速度在前），与直觉相反。使用时必须用linvel=cvel[3:6]，angvel=cvel[0:3]。应通过mujoco.mj_objectVelocity或文档确认布局，不能假设。 robosuite环境必须从HDF5 dataset的env_args元数据重建，才能保证demo回放的物理一致性——controller_configs（kp、damping、ramp_ratio等）直接影响action→force的映射，不同配置会导致完全不同的轨迹。MimicGen的任务变体名（如PickPlace_D0）需要转换为robosuite base环境名，但controller_configs等其他配置应保留。 计划文件要做到完全自包含（self-contained）：包括精确行号、关键代码片段、所有设计决策和用户偏好，这样下次resume时无需重新探索代码库。 多设备工具的设计核心是\u0026rsquo;分离关注点\u0026rsquo;：解析（依赖本地文件系统）和总结（依赖API）应拆分为独立阶段，中间状态用自描述JSON传递，而不是要求所有操作在同一机器上完成。 双重验证（SR+LLM）的mismatch处理策略：SR快速分类→LLM验证→不一致时信任LLM。SR的弱点是关键词嵌入在语义模糊时会错误匹配，而LLM的完整上下文理解能纠正这类错误。 MuJoCo的set_sim_state_flat()只恢复qpos/qvel等动力学状态，不恢复geom_friction和xfrc_applied等物理参数修改——因此在注入器clear()之前不能依赖状态恢复来撤销副作用。 demo回放时必须从HDF5的env_args和controller_configs恢复原始环境配置，否则相同的action sequence在不同控制器下会产生完全不同的轨迹，导致机械臂无法到达物体。 当某个检测器从不触发时，应首先验证物理前提条件是否满足（如EEF是否真的靠近物体），而非调整检测器参数。症状修复（放宽阈值）掩盖了根本原因。 大规模实验调度中，dry_run验证是必要步骤，可以在实际运行前发现实验矩阵配置错误；结合npz文件数与成功记录数的交叉验证可发现路径问题。 多模态融合实验（Q-Former、LLaVA MLP）的规范实现需要在三个归一化节点（post-encoder、pre-fusion、post-fusion）分别控制，这是提升实验可比性的关键设计决策。 数据缺失问题先检查压缩包：数据文件\u0026rsquo;缺失\u0026rsquo;时，先检查同目录是否有.zip/.tar存档，很可能文件在其中只是未解压。 代码重复整理时的依赖分析：提取共享模块时必须分析跨模块的导入方向，避免循环导入。正确方向：models→scheduler，而不是反向。energy_projection从essential_scheduler导入时需要用staticmethod延迟导入。 注册表模式（INJECTOR_REGISTRY/VALIDATOR_REGISTRY）是扩展性很好的设计，但必须确保调用方实际使用注册表而不是硬编码实例化——单元测试需要覆盖注册表查找路径。 接口契约（detector生成的ErrorSpec参数 vs injector期望的参数）需要集成测试或运行时验证来保证一致性，纯粹的代码审查很容易遗漏命名漂移。 错误注入参数调整应该基于数据分布（如EEF-object距离的p25/p50/p75），而不是经验猜测。debug script打印完整分布比逐步调整阈值更有效率。 可视化流程不应重复执行validation rollout，因为validate内部会step仿真但不渲染帧，导致关键效果帧丢失。应直接使用scene.validated_by中已保存的验证结果。 error scene benchmark的场景质量需要两个物理门控条件：（1）初始化稳定期（等~5s/100步让震动消散），（2）EEF近距离门控（机械臂与目标物体≤5cm时才触发注入）。 AI生成大规模跨文件代码后，应系统性地验证：语法检查、模块导入、单元测试、变量作用域。单元测试覆盖度不足以替代代码审查。 Python asyncio后台循环模式（参考ThoughtOrganizer）：使用asyncio.create_task+_running标志+notify_callback回调，是一种可复用的后台服务模式，适合定时检查、定时同步等场景。 嵌套括号JSON提取应用平衡计数算法而非贪婪正则，正则{[\\s\\S]*}在多JSON块时会跨越多个对象边界产生无效结果。 实践收获 Google Calendar API循环事件处理：singleEvents=True展开为实例；实例ID格式为parentId_YYYYMMDDTHHMMSSZ；用实例ID+剥离recurrence/recurringEventId字段更新，API自动创建exception只影响该实例。 错误处理应该是完整链路：底层raise（携带详细信息）→中层传播→上层捕获并向用户展示。任何一层静默失败（print+return None）都会导致用户看到无信息的失败。 ccusage CLI工具可以通过npx ccusage@latest daily \u0026ndash;since YYYYMMDD \u0026ndash;until YYYYMMDD \u0026ndash;json \u0026ndash;offline获取精确的token用量和费用JSON，非常适合集成到日报脚本中。 CLAUDE.md应记录\u0026rsquo;共享模块的使用规范\u0026rsquo;，特别是新创建的provider_selector和time_utils，否则后续开发者（包括未来的AI）可能再次写重复代码。 后台监控线程模式：需要追踪峰值资源使用时，用守护线程定期采样比单次快照可靠得多。 Windows系统的隐性陷阱：nul/con/prn等是保留设备名，无法创建或删除；全角/半角字符混用会导致路径解析失败；conda activate在bash子进程中无效，需要直接使用Python可执行文件的绝对路径；cmd不原生支持conda，应使用PowerShell。 JSONL存储+dataclass是轻量级持久化的有效组合：无需数据库，支持追加写入，读取时逐行解析，适合小规模用户数据（循环任务、碎片想法等）。 SSH远程执行脚本时，working directory是远端home dir，应始终使用绝对路径并通过PYTHONPATH设置模块解析路径。 semantic_router Route的utterances多样性直接影响意图分类准确率，为MANAGE_RECURRING提供了30个样例（中英文混合、多种表达方式），是提升路由精度的关键实践。 会话摘要 MIHD Benchmarking 🔄 实现286个基准实验自动化调度器并修复conda环境和工作目录问题 19:04:03.641 | claude_code 人类提供了完整的A-F六组实验矩阵设计（共286个），AI实现了可恢复调度器、实验配置和汇总可视化脚本。运行过程中发现两个关键bug：conda run导致输出文件为空（改用绝对Python路径解决）、子进程相对路径导致npz文件未正确保存（改用绝对路径+显式cwd解决）。修复后实验在后台稳定运行，进度12/207，ETA约12小时。\n✅ 为实验日志文件添加自动更新的头部时间戳 22:36:47.766 | claude_code 人类询问日志写入机制，AI说明了三个实时日志文件的工作方式。人类随后提出在experiments_log.md头部维护\u0026rsquo;最后更新：\u0026lsquo;时间戳的需求，AI通过在_append_to_global_log中添加_update_header_timestamp()方法（读取→正则替换→写回）完成实现。\nMIHD Enhancement ✅ 实现MIHD多模态增强全部7个Phase（归一化、Q-Former、Niche查询、批次校正等） 17:39:21.063 | claude_code 人类提交了Phase 1-7的完整实现计划，AI逐一完成Config Foundation、三点归一化、Q-Former+LLaVA MLP融合、Niche查询工具、对抗批次校正和Joint模式。所有模块语法和导入验证通过，批次校正单元测试3/3通过。过程中AI出现3处需人类纠正的错误：args作用域混淆、spatial_coords作用域越界、错误的外部import。\n❌ MIHD Enhancement计划实现尝试（因使用限制中断） 06:41:43.063 | claude_code 人类提交了与会话2相同的MIHD Enhancement实现计划，但会话因达到使用上限而中断，无任何代码产出。该会话时间早于会话2，推测是第一次尝试失败后人类重新发起了会话2。\nMIHD ✅ MIHD benchmark数据修复与代码增强：修复4个失败section、GPU监控、mclust fallback 18:14:32.089 | claude_code 用户提交完整MIHD项目计划文档，AI开始按P0优先级实施。发现151510/151672/151674三个section的tissue_positions_list.csv存在于Dataset.zip但未解压，提取后修复了坐标加载问题；修复了151674频域滤波空patch的边界检查；为151676添加了mclust→KMeans fallback；修复了GPU监控峰值始终为0的后台线程问题；扩展了benchmark_config.yaml实验矩阵；更新了CLAUDE.md文档。\n🔄 MIHD Enhancement Plan详细实现计划制定（7阶段归一化/Q-Former/Niche/BatchCorrection） 06:10:33.492 | claude_code 用户提供ENHANCEMENT_PLAN.md（归一化不一致、Q-Former/LLaVA MLP融合、niche查询、批次校正、配置整合6个方向），AI深度探索代码库，制定了7个阶段的详细实现计划并写入计划文件。用户要求计划文件完全自包含，AI三次ExitPlanMode均被拒绝，最终写入了包含精确行号和完整代码的版本。用户选择了Phase 1→7顺序实现、Q-Former逐spot循环版本、同时实现联合多section分析模式。\nGadgets ✅ AI日报工具重构为多设备两阶段架构+ccusage集成+GitHub仓库初始化 22:17:00.538 | claude_code 用户提交详细的两阶段架构计划，AI重构daily_summary.py为export/merge子命令模式，添加设备信息采集、可移植JSON log格式、Hugo部署集成。随后添加ccusage token统计功能（通过npx ccusage \u0026ndash;json采集，存入token_usage字段，在Markdown日报中渲染表格）。最后初始化GitHub仓库，将test/添加为submodule，排除了tokens/敏感文件，推送到TzJ2006/gadget.git。为所有gadget工具写了README。\nalgorithms-gadgets 🔄 实现AI对话日报工具并扩展为多设备两阶段架构 22:08:03.355 | claude_code 用户要求实现详细计划中的daily_summary.py工具，用于分析AI对话记录并生成日报。AI实现了三种数据源解析（Claude Code JSONL、ChatGPT导出、通用JSON）、AI总结调用和双格式输出。中途用户提出多设备需求，AI询问了传输方式（手动拷贝）和是否需要中间API调用（是），设计了两阶段架构，但最终实现被用户拒绝ExitPlanMode而中断。\n✅ 初始化代码库分析并创建CLAUDE.md 20:57:21.853 | claude_code 用户通过/init命令请求分析algorithms/gadgets代码库并创建CLAUDE.md。AI探索了代码库结构，发现这是一个包含性能测试、媒体处理、OCR、论文下载、AI日报等独立工具的Python工具集。创建了涵盖运行命令、架构说明和依赖项的CLAUDE.md文件，但遇到了git初始化问题（工作目录不是git仓库）。\nCalendarPro ✅ 执行代码库整理：删除冗余文件、提取共享模块、统一重复逻辑 18:23:24.871 | claude_code 用户提供了预先分析好的PLANNING.md，要求AI执行三阶段代码整理。AI删除了10个临时/废弃文件，创建了time_utils.py和provider_selector.py两个共享模块，将8处重复的AI提供商选择逻辑统一到provider_selector，将3处重复的时间解析统一到time_utils，提升了关键词常量到模块级，整合了餐食时间来源，更新了测试文件，修改了.gitignore和pyproject.toml。最终26/26核心测试通过，新文件通过ruff lint。随后更新了CLAUDE.md反映新架构，并开始规划P0/P1功能实现但被中断。\n✅ 实现定时自检与循环日程自动安排：新增scheduling模块并集成到Discord Bot 22:18:34.787 | claude_code 用户提供了完整的架构计划，要求实现定时后台检查功能，自动安排餐食/睡眠等基本活动，并支持用户自定义循环任务（如每日LeetCode）。AI依次创建了RecurringTaskStore（JSONL持久化）、PeriodicChecker（asyncio后台循环）和scheduling模块入口，同步修改了models.py（新增IntentType）、config.py（新增配置项）、intent_routes.py（新增语义路由）、prompts.py（新增解析prompt）和discord_bot.py（集成初始化、启停、意图处理）。最终编写18个测试用例全部通过，lint检查无报错。\n✅ 修复日志中发现的多个系统Bug 18:08:34.455 | claude_code 用户从运行日志中发现意图分类错误（\u0026lsquo;晚点做leetcode\u0026rsquo;被误判为manage_recurring）和Windows服务停止问题。AI系统性地修复了四个相关问题：1）dual_verify.py mismatch时改用LLM intent；2）calendar_service.py用CalendarServiceError替代静默失败；3）start.bat改用PowerShell+新增stop.ps1；4）用户追问后修复循环事件只改单实例的逻辑。所有修改打包为一个commit（8917042）。\n✅ 创建PLANNING.md项目规划文档 18:08:34.455 | claude_code 用户要求为CalendarPro创建中文项目规划文档。AI通过并行启动多个后台Agent深度探索代码库（48+源文件）和OpenClaw生态，同时直接读取关键模块（models、scheduler、energy_projection、semantic router等），最终创建330+行文档，包含模块成熟度评级、三层目标体系、6个架构决策理由和P0-P3行动路线图。Agent返回后根据准确信息（4内置+6工作区技能）修正了技能系统成熟度评级。\n✅ P0稳定性修复+P1智能功能实现（JSON容错、超时、队列、学习、思想整理） 19:04:15.684 | claude_code 按照预先制定的计划，系统性实现了9项任务：JSON解析容错（平衡括号算法）、三个AI提供商超时处理、消息队列done_callback、服务层错误加固、56个意图分类测试。P1部分实现了学习数据自动回填、能量投影个性化（历史权重混合昼夜节律）、智能排程整合学习数据、会话上下文结构化。全套测试82/85通过（3个为预存在的浏览器测试问题）。\n✅ Random Thoughts碎片想法收集与空闲期自动整理功能 20:43:47.000 | claude_code 实现完整的Random Thoughts系统：src/thoughts/模块含JSONL存储（ThoughtStore）、基于EssentialScheduler时间窗口的空闲检测（IdleDetector）、异步后台整理器（ThoughtOrganizer）；更新IntentType增加CAPTURE_THOUGHT/QUERY_THOUGHTS；添加双语语义路由；discord_bot集成捕获、查询、通知三个处理方法；25个测试全通过，lint清洁。\n✅ 生成项目CLAUDE.md（/init命令） 17:56:31.853 | claude_code 对代码库进行深度探索，发现CLAUDE.md缺少多个重要系统的文档（语义路由双重验证、授权系统、消息队列、优化确认UUID-TTL机制、会话历史、语言检测）。重写CLAUDE.md，添加这些系统的架构说明，同时删去冗余的环境变量表格和可以直接从代码发现的文件结构列表。\nerror-recovery-benchmark 🔄 修复MuJoCo cvel速度分量读反bug+重新生成场景 22:37:16.748 | claude_code 在env_wrapper.py中修复cvel布局读取错误（angular/linear互换），清除旧场景数据，在an49上重新生成场景（由之前在step 11误触发改为step 15真实触发）并生成可视化视频。用户随后指出仍有三个问题（场景与demo不一致、初始化震动、机械臂距离太远就注入），AI准备了修复计划但用户未批准执行。\n✅ tianhe节点登录（本地命令，无实质交互） 17:30:59.823 | claude_code 设备tianhe执行了/login本地命令，登录成功。无AI实质性交互内容。\nErrorRecoveryBenchmark ✅ 实现S5.3：修复注册表集成并在an49 GPU节点完成smoke test 20:00:00.000 | claude_code 用户要求在GPU节点跑rollout。AI发现rollout_generator.py仍硬编码单个注入器和验证器，detectors与injectors之间存在参数命名不匹配（friction_scale vs friction，scale vs friction_scale等），且friction/gripper_bias注入没有clear()调用导致状态污染。AI修复了所有问题，创建了run_on_gpu.sh脚本，在an49上运行smoke test时发现并修复了2个运行时bug（bool字符串转换、MetricsWithCI格式化），最终4步全部通过。\n✅ 修复demo回放环境配置不匹配并重新生成30个错误场景 23:49:10.823 | claude_code 人类精准定位了3层根因：demo录制用PickPlace_D0+OSC_POSE控制器，而生成脚本使用裸PickPlace无controller_configs，导致arm全程未靠近物体。修复了create_env()从HDF5 env_args加载完整配置，恢复了被错误放宽的proximity阈值，修复了rollout_generator默认值。重新生成后得到30个有效场景（触发步骤52-121，EEF在5cm内），并生成两个可视化视频。\n🔄 实现初始化静置+EEF近距离门控，发现demo回放环境不匹配的根本问题 23:00:21.081 | claude_code 按照预定计划修改benchmark_v4.yaml（min_step和max_eef_object_distance参数）以及rollout_generator.py（添加近距离门控逻辑）。在调试过程中发现EEF距物体始终0.22m的异常，逐步调大阈值后虽然生成了1个场景（step 121 vs原来的step 15），但深层根因调查揭示了demo回放环境配置不匹配（PickPlace vs PickPlace_D0+controller_configs）的根本问题，并更新了计划文档。\n🔄 修复帧丢失：从HDF5加载demo actions全程渲染三阶段因果链 21:58:20.627 | claude_code AI分析出关键帧丢失根因（collect_rollout_stats内部step但不渲染），提出从HDF5加载demo actions回放至trigger_step再全程渲染的方案。实现后验证成功（132帧），但用户进一步追问detector触发的因果逻辑，发现面包没动但detector已触发，引出cvel读反bug的诊断，最终确认是linvel/angvel读反导致所有instability scene都是误触发，讨论至cvel bug修复方案但尚未实施。\n✅ 代码库整理：死代码删除、mimicgen注入器移植、中文注释添加 18:30:00.000 | claude_code 用户要求整理代码库。AI发现3类问题：死代码（fingerprint.py约190行）、重复代码（proximity.py的duplicate detect()）、未整合资源（mimicgen有17种错误类型）。用户选择移植部分mimicgen错误类型并删除冗余文档。AI删除了12个过时.md、移植了3个新注入器（friction/pose_perturb/gripper_bias）、建立了注册表、为11+模块添加了详细中文注释，最终41个单元测试全部通过。\n🔄 更新项目状态文档、实现S5.1/S5.2验证器、执行S7.1 smoke test 19:12:32.829 | claude_code 首先更新了项目全景总结.md中的过时数据（注入器1→4，验证器2→4，代码量等）以反映v4.1.0实际状态。随后实现了LargeOffsetValidator（xy偏移\u0026gt;=0.10m时验证通过）和StuckValidator（80%帧速度\u0026lt;0.002m/step时验证通过），并编写了16个单元测试，所有41个测试全部通过。最后在an49 GPU节点上逐步执行smoke test：Step 1（场景生成）和Step 2（拒绝日志分析）通过，Step 3（数据采集）因旧场景指纹不匹配失败，清除旧数据后smoke test仍在进行中。\n✅ 为错误恢复基准项目创建中文全景总结文档（395行） 18:05:16.582 | claude_code 用户要求整理15+个计划文档为一个全景文档。AI通过并行探索agent读取所有.md文件和统计代码量后，创建了395行的项目全景总结.md，涵盖10个章节：项目定义、动机、架构、目标层次（大/中/小）、诚实进度对比（场景数3/200=1.5%）、优先级行动计划（P1-P3）、风险、版本演化和快速参考。\n🔄 代码库整理：删除死代码、移植注入器、添加注释 18:30:39.228 | claude_code 按照人类设计的5阶段计划执行代码库整理。删除了fingerprint.py中~190行死代码和proximity.py中重复的detect()方法（含bug修复），删除12个冗余.md文档，从mimicgen_workspace移植3个新注入器（friction、pose_perturb、gripper_bias）并更新注册表，framework版本升至4.1.0。Phase 4（添加注释）和Phase 5（移动文件）在截断前未完成。\n🔄 实现可视化视频帧标注（三阶段色带+信息框+帧计数器） 21:38:31.627 | claude_code 用户要求在可视化视频上添加明显的标注区分normal/injection/post-error三阶段，并修复hardcode ImpulseInjector的bug。AI实现了build_annotation_texts和annotate_frame函数，重构了render loop，并通过an49 SSH运行验证生成了132帧视频。随后用户发现三阶段画面几乎一样，AI诊断出collect_rollout_stats吞帧问题。\n🔄 规划视频可视化标注增强方案 21:30:00.000 | claude_code 用户查看了生成的mp4视频后要求增加明显的三阶段标注（正常/注入错误/注入后）和文字说明\u0026rsquo;检测到\u0026lt;情况\u0026gt;，触发\u0026rsquo;。AI读取了2_visualize_scene.py和场景JSON元数据，设计了基于cv2.putText和半透明矩形的标注方案，计划在ExitPlanMode时被用户拒绝，最终session结束时仍处于计划阶段。\n🔄 重写CLAUDE.md并分析项目全景状态 17:31:49.623 | claude_code 通过/init命令分析代码库后将615行CLAUDE.md精简为101行，消除重复内容和冗余的文件树列表。随后用户要求整理所有计划文档汇总为中文全景总结，AI探索了15+个.md文件后设计了项目全景总结.md的9章节结构，但在进入plan mode请求用户批准时被中断。\nToken 用量 总览 指标 数值 总 Token 101,386,135 输入 Token 65,053 输出 Token 68,378 Cache 创建 5,977,600 Cache 读取 95,275,104 Cache 命中率 94.1% 总费用 (USD) $72.2753 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 9,992 66,699 4,042,071 79,408,980 $66.6849 92.3% claude-haiku-4-5-20251001 54,659 1,036 1,538,261 13,263,291 $3.3090 4.6% claude-sonnet-4-5-20250929 402 643 397,268 2,602,833 $2.2815 3.2% 各设备用量 设备 总 Token 输入 输出 费用 DCC 41,018,007 38,866 26,730 $31.2547 MacBook 8,020,979 1,356 3,374 $4.7980 TzJsDesktop 52,347,149 24,831 38,274 $36.2226 ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-13/","summary":"今日工作横跨四个项目：MIHD多模态基准测试框架完成7个Phase增强与286个实验自动化调度；CalendarPro完成代码整理、P0稳定性修复、P1智能功能（学习采集、能量个性化、随机想法系统）、循环日程与定时自检；gadget日报工具重构为多设备两阶段架构并推送GitHub；error_recovery_benchmark修复demo回放环境配置不匹配根本bug、cvel角速度读反误触发、移植3个新注入器并成功生成30个有效错误场景。","title":"Bug Journal 2026-02-13"},{"content":"日报 — 2026-02-12 在DCC集群上为MIHD空间转录组多模态融合框架制定并开始实施大规模增强计划，涵盖嵌入归一化、Q-Former/LLaVA MLP融合、QueST风格的niche查询与批次效应校正、以及全局超参数配置化。\n今日任务 架构与策略 ✅ MIHD增强计划制定与文档化 — 基于6个研究想法，设计了包含7个实施阶段的详细计划（Big Aim→Middle Aim→Small Aim→Tiny Aim层级），并写入docs/ENHANCEMENT_PLAN.md ✅ 创建utils/normalization.py — 实现了L2归一化、StandardScaler归一化、dispatcher函数以及PyTorch等价函数，支持三点归一化（编码器后、融合前、融合后） 🔄 Q-Former + LLaVA MLP融合实现 — 计划创建models/QFormerFusion.py，实现BLIP-2风格Q-Former和LLaVA 2层MLP连接器两种融合策略 🔄 扩展config.yaml — 开始向config.yaml添加encoder/fusion超参数配置节，实施Phase 1 Config Foundation 🔄 QueST niche查询与批次校正实现 — 计划创建utils/niche_utils.py、models/NicheEncoder.py、models/BatchCorrection.py，将QueST的niche查询和对抗式批次校正迁移到MIHD 实现与修复 ✅ 更新CLAUDE.md — 将CLAUDE.md从579行精简至188行（减少67%），消除冗余，补充缺失的GCN编码器、basic_contrastive融合、utils模块说明、测试运行说明等，并添加ASCII管道流程图 问题与解决方案 关键问题 1. MIHD当前嵌入归一化不一致：UNI2/scGPT/PCA已L2归一化，但HIPT/ResNet50/MLP/GCN未归一化，且融合前后均无统一归一化 解决方案: 设计三点归一化方案（post-encoder、pre-fusion、post-fusion），通过utils/normalization.py统一管理，L2归一化对已归一化向量幂等所以安全\n关键洞察: 归一化不一致是下游聚类质量下降的根本原因之一；L2归一化的幂等性使其可无副作用地应用于所有编码器输出\n2. 现有config.yaml和config_manager.py缺乏对大量超参数的支持，导致代码中存在大量硬编码值 解决方案: 设计完整的配置化方案，将normalization、Q-Former、LLaVA MLP、批次校正、niche查询的所有超参数纳入config.yaml和对应dataclass\n关键洞察: 配置化是超参数搜索的前提；必须先建立config foundation才能进行其他所有改进\n3. LLaVA名称歧义：用户说\u0026rsquo;Q-Former in LLaVA\u0026rsquo;，但实际上LLaVA使用的是简单2层MLP，Q-Former来自BLIP-2 解决方案: AI澄清了两种架构的区别，并向用户确认是否两者都实现；用户选择两者都实现\n关键洞察: 在实现前澄清架构名称歧义，避免了错误实现；LLaVA MLP比Q-Former更轻量适合作为基线对比\n一般问题 4. 原CLAUDE.md存在大量冗余（smart runner命令出现两次、squidpy警告出现3次、组件列表重复），且缺少关键信息 解决方案: 重写CLAUDE.md，精简67%同时补充缺失内容（GCN编码器、测试运行指令、utils模块说明等）\n关键洞察: 文档冗余会降低AI助手的工作效率；精简到关键信息同时保留架构概述最有价值\n人类思路 vs AI 思路 战略层面 6个研究想法的提出 角色 思路 人类 人类从实际研究需求出发，提出了嵌入归一化、Q-Former融合、QueST应用、批次效应校正、全局超参数配置化共6个互相关联的改进方向，并指定了QueST代码库路径 AI AI分析了依赖关系，确定了实施顺序（Config→Norm→UNI2实验→Q-Former→Niche→批次校正→完整配置），并发现了技术细节（如LLaVA实际使用MLP而非Q-Former） 差异分析: 人类提供高层次研究方向和直觉，AI负责将其转化为具体可执行的技术任务层级；AI主动澄清了\u0026rsquo;LLaVA中的Q-Former\u0026rsquo;这一名称误用\nQueST功能的使用方式 角色 思路 人类 人类想了解QueST如何处理疾病数据和批次效应，并希望在MIHD中实现类似功能 AI AI探索了QueST代码库（BatchDiscriminator、GINLayers、TLS查询逻辑），并设计了将这些功能移植到MIHD的具体方案 差异分析: 人类关注\u0026rsquo;能否做到\u0026rsquo;，AI负责\u0026rsquo;如何做到\u0026rsquo;的技术细节；AI发现QueST使用对抗训练方式进行批次校正\n实现层面 实施顺序决策 角色 思路 人类 人类在AI开始执行后主动打断，要求先写计划文件再执行 AI AI初始倾向于直接开始实施（创建了task列表并开始读取文件），未先产出可审查的计划文档 差异分析: 人类的工作流偏好是先文档化计划，这是一个AI未能预判的工作习惯；被打断后AI立即调整策略\nAI 局限性 一般局限 AI在未问清用户工作流偏好的情况下，直接开始创建任务列表和读取文件，而未先输出可供审查的计划文档，导致被用户打断 AI将\u0026rsquo;LLaVA中的Q-Former\u0026rsquo;作为需要主动澄清的歧义，而非直接按字面意思实现，这是正确的；但需要依赖用户提问环节而非自主判断最合理方案 今日收获 核心收获 空间转录组多模态融合中，嵌入归一化不一致是常见但容易忽视的问题；L2归一化的幂等性使其成为安全的默认选择，可统一应用于所有编码器输出而不损害已归一化的向量 Q-Former（BLIP-2）vs LLaVA MLP连接器是两种不同的跨模态对齐机制：Q-Former使用可学习查询向量+交叉注意力，LLaVA使用更简单的2层MLP投影；两者都值得作为融合策略尝试 QueST使用对抗式批次判别器（BatchDiscriminator + GINLayers）进行批次效应校正，通过最大化批次不可区分性来学习批次无关表示；这种方法可以作为独立的后处理模块插入到任意融合管道中 大型代码改进项目中，配置化必须先于功能实现；建立统一的配置体系（config.yaml + dataclass）是后续所有超参数搜索和实验对比的基础 实践收获 在AI辅助开发工作流中，\u0026lsquo;先写计划文档再执行\u0026rsquo;是一个重要的检查点，允许人类在大量代码生成前审查和调整方向，特别适合多阶段、多文件的大型重构任务 会话摘要 🔄 为MIHD生成CLAUDE.md并规划6大增强方向 17:12:36.312 | claude_code 用户通过/init命令触发CLAUDE.md生成，AI探索了整个MIHD代码库和QueST代码库，并对Q-Former架构进行了研究。用户随后提出6个互相关联的改进想法（嵌入归一化、UNI2+scGPT实验、Q-Former融合、QueST niche查询、批次效应校正、全局超参数配置化），AI通过澄清问题确认了实现范围（两种融合策略均实现、QueST功能移植到MIHD、完整管道配置化）。最终AI生成了详细的7阶段实施计划并写入计划文件，但用户在AI尝试退出计划模式时拒绝，会话以计划审查中状态结束。\n🔄 开始实施MIHD增强计划：config基础层和归一化工具 17:34:50.362 | claude_code AI开始按照计划实施：读取了config.yaml、config_manager.py、Fusion.py、run_benchmark.py等关键文件，读取了QueST的layers.py和model.py以了解BatchDiscriminator和GINLayers实现。成功创建了utils/normalization.py（包含L2归一化、StandardScaler归一化、dispatcher函数），并开始修改config.yaml添加超参数节，但在编辑config.yaml时被用户打断。整体实施进度约完成Phase 1-2的30%。\nToken 用量 总览 指标 数值 总 Token 5,459,892 输入 Token 45,436 输出 Token 1,231 Cache 创建 543,037 Cache 读取 4,870,188 Cache 命中率 90.0% 总费用 (USD) $4.1489 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 66 1,150 311,998 3,370,495 $3.6643 88.3% claude-haiku-4-5-20251001 45,370 81 231,039 1,499,693 $0.4845 11.7% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-12/","summary":"在DCC集群上为MIHD空间转录组多模态融合框架制定并开始实施大规模增强计划，涵盖嵌入归一化、Q-Former/LLaVA MLP融合、QueST风格的niche查询与批次效应校正、以及全局超参数配置化。","title":"Bug Journal 2026-02-12"},{"content":"日报 — 2026-02-09 在MIHD项目中为聚类可视化添加ARI/NMI指标叠加，并运行所有DLPFC切片的RM-Ideal基准测试，完成多种embedding方法的系统性评估。\n今日任务 实现与修复 🔄 运行所有DLPFC切片的RM-Ideal评估 — 对所有有embedding结果的切片（151508/151509有多种方法，151669-151675只有uni_staig_fusion）并行运行evaluate_rm_ideal.py，使用\u0026ndash;niche_label all计算所有层级的Spearman/P@K/SameLabel指标并生成可视化。5个切片已完成，151508/151509仍在运行中（因为有多个embedding方法需评估）。 🔄 运行pca+uni+staig_fusion配置的全部DLPFC切片基准测试 — 用户要求重新运行之前表现最好的配置（pca+uni+staig_fusion+STAIG对齐）在所有11个DLPFC切片上，不覆盖已有结果。151508/151509已缓存跳过，151510因缺少spatial coordinates失败，正在处理其余切片。 ✅ 为聚类可视化添加ARI/NMI/Silhouette指标叠加 — 将utils/visualization.py从scanpy的save=方式重构为显式fig/axes布局，添加metrics参数，在图底部叠加ARI/NMI/Silhouette文本框。同时修改run_benchmark.py中的调用传递metrics dict，以及evaluate_rm_ideal.py的visualize()方法添加Spearman/P@K/SameLabel指标叠加。发现run_benchmark.py内部还有一个本地copy的create_clustering_visualization函数也需同步更新。 问题与解决方案 一般问题 1. utils/visualization.py的修改未生效，因为run_benchmark.py内部有一个同名函数的本地copy覆盖了import 解决方案: 找到并同步更新run_benchmark.py:1644处的本地copy函数\n关键洞察: 代码重复（本地copy vs 模块import）导致修改无效，需要检查调用链\n2. 用户指出应参考pca_uni_staig_fusion文件夹的已有结果，而不是重新运行性能差的配置 解决方案: 检查experiments_log和summary CSV，确认pca+uni+staig_fusion是最佳配置，切换到该配置并使用\u0026ndash;skip_cached\n关键洞察: 先查看已有结果再决定运行策略，避免重复计算\n3. 三个实验并行运行时GPU内存不足，PCA+UNI2+STAIG进程被OOM Killed 解决方案: 停止并行任务，改为顺序运行（用\u0026amp;\u0026amp;链接）\n关键洞察: GPU内存不够同时加载多个STAIG模型，必须串行运行\n4. staig_fusion在151510/151672等切片上报错：requires spatial coordinates 解决方案: 这些切片数据缺少spatial CSV文件，属于数据问题，暂时跳过\n关键洞察: 部分DLPFC切片的spatial坐标文件缺失，staig_fusion无法处理\n人类思路 vs AI 思路 实现层面 选择运行配置 角色 思路 人类 用户知道pca_uni_staig_fusion文件夹里有好结果，直接指向具体目录 AI AI倾向于重新运行最新生成的uni_staig_fusion（没有pca基因编码器）配置 差异分析: AI忽略了已存在的高质量结果，用户的历史知识更准确地指向了正确配置\n并行vs串行执行策略 角色 思路 人类 用户未明确指定，但期望任务顺利完成 AI AI首先尝试并行运行三个实验，导致OOM崩溃，然后改为串行 差异分析: AI高估了GPU内存容量，需要用户环境约束来纠正策略\nAI 局限性 一般局限 在修改visualization函数后未检查是否存在同名本地copy，导致修改未生效，需要第二轮调试才发现问题 未能记住之前会话中pca_uni_staig_fusion是最佳配置，需要用户提醒才重新查看 首次计划运行时（ExitPlanMode）被用户拒绝，说明AI在实现前花费了过多时间在规划阶段 今日收获 实践收获 scanpy的sc.pl.spatial(save=)方式会自动保存到figures/目录且不返回figure对象，需要改用ax=参数+手动fig.savefig()才能添加自定义注释 DLPFC数据集中151510/151672/151674/151676等切片缺少spatial坐标，staig_fusion无法处理；需要在pipeline中做更好的数据验证 会话摘要 🔄 实现可视化指标叠加并运行全DLPFC基准测试 21:50:31.276 | claude_code 用户提供了完整的实现计划，AI依次修改了utils/visualization.py（重构为显式fig/axes，添加metrics文本框）、scripts/run_benchmark.py（传递metrics dict，同时发现并修复了本地copy函数）、scripts/evaluate_rm_ideal.py（添加Spearman/P@K叠加）。验证运行151508后图片正确显示metrics。随后用户要求运行全部DLPFC切片，AI先尝试并行运行三种配置但OOM崩溃，后改串行；用户纠正应使用pca_uni_staig_fusion配置（已有高质量结果），最终使用\u0026ndash;skip_cached避免覆盖，完成7/11切片的可视化。最后并行启动了所有可用切片的RM-Ideal评估，5个切片已完成，2个仍在运行。\n🔄 为聚类可视化添加NMI/ARI指标叠加（规划阶段） 21:46:17.845 | claude_code 用户要求在聚类可视化图上添加NMI/ARI分数，以及在RM-Ideal可视化上添加p值等基准结果。AI探索了visualization.py和evaluate_rm_ideal.py的实现，制定了四步修改计划：重构可视化函数使用显式fig/axes布局，在各调用处传递metrics参数。用户最终拒绝了ExitPlanMode执行请求，等待下一个会话实现。\nToken 用量 总览 指标 数值 总 Token 10,207,919 输入 Token 276 输出 Token 8,008 Cache 创建 510,804 Cache 读取 9,688,831 Cache 命中率 95.0% 总费用 (USD) $7.8716 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 248 7,991 453,924 9,483,634 $7.7799 98.8% claude-haiku-4-5-20251001 28 17 56,880 205,197 $0.0917 1.2% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-09/","summary":"在MIHD项目中为聚类可视化添加ARI/NMI指标叠加，并运行所有DLPFC切片的RM-Ideal基准测试，完成多种embedding方法的系统性评估。","title":"Bug Journal 2026-02-09"},{"content":"日报 — 2026-02-08 在HPC服务器上为QueST和MIHD项目实现了RM-Ideal（Region Matching Ideal）评分功能，从零开始移植WWL图核算法，并集成到两个独立的代码库中\n今日任务 架构与策略 ✅ QueST RM-Ideal算法实现（utils.py） — 在QueST的src/utils.py中新增4个函数：wwl_node_features()、_wasserstein_distance()、rm_score()和compute_rm_ideal_scores()，实现了完整的WWL图核+最优传输的RM-Ideal评分计算 ✅ MIHD RM-Ideal核心模块（rm_ideal.py） — 创建utils/rm_ideal.py，移植QueST的WWL核算法，适配MIHD框架（无squidpy/torch_geometric依赖），并创建RMIdealEvaluator类支持k-hop子图预计算和.npy缓存 ✅ MIHD CLI评估脚本（evaluate_rm_ideal.py） — 创建scripts/evaluate_rm_ideal.py，包含NicheQueryEvaluator类，支持按标签/空间/索引定义niche、计算Spearman相关、Precision@K、SameLabel@K指标，以及scanpy可视化和CSV汇总 ✅ QueST QueSTTrainer集成（trainer.py） — 在QueSTTrainer类中新增compute_rm_scores()方法，遍历所有查询niche和参考样本，将RM-Ideal分数存储到adata.obs，命名格式与Tutorial 2一致 实现与修复 ✅ MIHD空间工具扩展（spatial_utils.py） — 在MIHD的utils/spatial_utils.py中新增build_grid_connectivity()（squidpy-free空间邻接图构建）和bfs_k_hop()（纯networkx BFS k-hop子图提取）两个函数 ✅ 实际运行MIHD RM-Ideal评估（151508/Layer_3） — 在DLPFC section 151508的Layer_3 niche上运行评估，使用uni_staig_fusion embeddings，获得Spearman r=0.424, SameLabel@50=1.0的结果 ✅ 修复SameLabel@K为0的bug — 发现并修复evaluate_rm_ideal.py中同标签检索率错误为0的bug：同标签计算应在全量valid spots上进行，而非排除niche spots后的子集 问题与解决方案 关键问题 1. SameLabel@K结果全部为0，明显不合理 解决方案: 发现bug：niche定义为\u0026rsquo;Layer_3\u0026rsquo;时，所有Layer_3 spots都被排除在eval_mask之外，导致无Layer_3可检索。修复为同标签率在全量spots上计算\n关键洞察: 当niche定义覆盖某类型的所有spots时，\u0026lsquo;排除niche spots\u0026rsquo;的逻辑会使该类型完全消失，指标语义需要区分：Precision@K（检索质量vs RM-Ideal）应排除自身，SameLabel@K（类型检索能力）不应排除\n2. MIHD不依赖squidpy和torch_geometric，但QueST的RM-Ideal实现依赖这两个库 解决方案: 用sklearn NearestNeighbors重写空间邻接图构建（build_grid_connectivity），用纯networkx BFS替代PyG的k_hop_subgraph\n关键洞察: 跨项目代码移植时需要识别并替换不可用的依赖，保持算法等价性的同时适配目标框架的依赖约束\n一般问题 3. POT（Python Optimal Transport）库在任何conda环境中均未安装，且quest conda环境不存在 解决方案: 实现POT优先、scipy.optimize.linprog兜底的双路径最优传输求解器；使用General/AI环境运行\n关键洞察: 在HPC环境中不能假设特定Python包已安装，需要为关键依赖提供fallback路径\n4. QueST项目文件路径错误：初始任务指向/hpc/group/yizhanglab/zt81/MIHD/src/utils.py，但该路径不存在 解决方案: 通过glob搜索找到实际路径为/hpc/group/yizhanglab/zt81/QueST/src/utils.py\n关键洞察: 任务文档中的路径可能与实际项目结构不符，需要先验证文件是否存在再实施\n5. utils.py中导入了umap等重型库，直接import无法在没有激活相应conda环境的shell中运行 解决方案: 将核心算法函数内联到独立Python -c测试脚本中进行单元测试，或用source activate General激活环境\n关键洞察: 在HPC环境测试时需要注意conda环境激活，内联测试是快速验证算法正确性的有效方法\n人类思路 vs AI 思路 战略层面 SameLabel@K为0的问题发现 角色 思路 人类 用户立即发现SameLabel@50=0.00的结果不合理，追问\u0026rsquo;But why the SameLabel@50 has the value of 0?\u0026rsquo; AI AI完成了代码实现并输出了结果，但未主动检查SameLabel@K指标的合理性 差异分析: 用户对结果有直觉判断能力（Layer_3 spots应该能被检索到），发现了AI实现中的逻辑错误；AI需要人类提示才发现bug\nRM-Ideal集成范围的扩展 角色 思路 人类 用户主动提出将RM-Ideal不仅集成到QueST，还要移植到MIHD项目中，并要求同时支持within-section和cross-section两种查询模式 AI AI最初只考虑完成给定的QueST集成计划，未主动提出扩展到MIHD 差异分析: 用户具有跨项目整合的战略视野，主动推动评估框架的复用；AI倾向于完成当前任务范围内的工作\n实现层面 可视化展示选择 角色 思路 人类 用户在看到文字结果后主动要求\u0026rsquo;can you do the visualization of the spots you picked?' AI AI实现了完整的3-panel可视化功能，但在输出文字结果后等待用户指示才运行可视化 差异分析: 用户更倾向于空间数据的直觉图形展示；AI的默认输出是文字指标摘要\nAI 局限性 重要局限 实现了完整的指标计算框架但未主动验证结果的合理性，SameLabel@K=0这种明显异常值需要人类提示才被检测到 一般局限 在多个会话中重复尝试实现相同的RM-Ideal计划（三个会话均有Implement the following plan的记录），表明AI的实现结果没有被持久化或验证，导致重复工作 多次尝试调用不存在的conda环境\u0026rsquo;quest\u0026rsquo;，直到运行失败才发现，说明AI没有在执行前验证环境是否存在的习惯 conda run命令超时（30秒未返回），AI处理超时的策略是直接内联代码测试，跳过了环境验证步骤，这虽然有效但绕过了原始测试目标 今日收获 核心收获 WWL（Wasserstein Weisfeiler-Lehman）图核算法：通过K轮迭代邻居标签聚合+哈希构建节点特征向量，两图共享哈希表保证一致编码，再用最优传输计算Earth Mover\u0026rsquo;s Distance，最终RM=1-W。K+1维Hamming归一化确保距离在[0,1] 评估指标设计原则：Precision@K（检索与ground truth的排名一致性）应排除查询niche自身；SameLabel@K（类型检索能力）不应排除，否则当niche覆盖某类型全部spots时指标会失效 DLPFC 151508 Section上，uni_staig_fusion embeddings的Layer_3 niche查询结果：Spearman r=0.424（统计显著），SameLabel@50=1.0。说明embedding在类型级别检索准确，但无法精确复现graph-kernel级别的niche结构 实践收获 HPC多conda环境管理实践：conda env list查看可用环境，source activate [env]激活，不能假设特定环境存在。QueST在General环境下运行，quest环境不存在 会话摘要 QueST ✅ RM-Ideal算法实现、单元测试验证（8/8通过） 04:42:56.919 | claude_code 实现了QueST RM-Ideal评分系统的完整5个函数（wwl_node_features、_wasserstein_distance、rm_score、compute_rm_ideal_scores、QueSTTrainer.compute_rm_scores）。发现QueST repo实际位于/hpc/group/yizhanglab/zt81/QueST/而非MIHD目录。通过内联测试（绕过重型conda依赖）完成了8个单元测试，全部通过，包括自相似=1.0、完全不同=0.0等关键边界情况。\n🔄 QueST RM-Ideal实现方式调研及MIHD集成方案探讨 05:16:22.879 | claude_code 用户询问QueST如何使用RM-Ideal以及patch间相似度计算方式。AI探索QueST代码库发现RM-Ideal在src/utils.py已实现，查询机制基于GIN编码器+余弦相似度。随后讨论了如何将两者集成到MIHD评估框架，用户确认需要both within-section和cross-section支持，选择QueST风格的Niche query模式。会话在生成实施计划时被用户中断（未批准ExitPlanMode）。\n❌ QueST RM-Ideal二次实施尝试（与第一个会话重复） 03:13:27.888 | claude_code 收到与04:42会话相同的实施计划。探索发现QueST repo位于正确路径，读取utils.py和trainer.py，理解了现有架构。开始实施流程（写入plan文件、AskUserQuestion确认scope）时被用户中断，未实际执行代码修改。这是同一任务的另一次启动，最终在04:42会话中完成。\nMIHD ✅ MIHD RM-Ideal全套实现：spatial_utils扩展、rm_ideal模块、CLI评估脚本 05:36:39.779 | claude_code 按计划实现了MIHD的RM-Ideal评估框架：扩展spatial_utils.py添加build_grid_connectivity和bfs_k_hop，创建utils/rm_ideal.py（含RMIdealEvaluator类，支持预计算子图缓存），创建scripts/evaluate_rm_ideal.py（含NicheQueryEvaluator、Spearman/Precision@K/SameLabel@K指标、scanpy可视化）。在151508/Layer_3运行后发现SameLabel@K=0的bug，经用户指出后诊断并修复。最终结果：Spearman r=0.424，SameLabel@50=1.0。\n🔍 RM-Ideal与Niche Query集成MIHD的需求分析与规划 02:23:10.030 | claude_code 用户提出将QueST的RM-Ideal和niche查询能力集成到MIHD以评估多种embedding方法。讨论了graphcompass库（QueST论文中引用的外部库），确定了集成路径为src/utils.py。澄清了scope（utils.py+trainer.py）和OT求解器策略（POT优先+scipy兜底）。规划讨论被用户中断，计划写入plan文件后进入下一会话实施。\nToken 用量 总览 指标 数值 总 Token 6,943,631 输入 Token 3,635 输出 Token 3,815 Cache 创建 529,772 Cache 读取 6,406,409 Cache 命中率 92.4% 总费用 (USD) $4.5642 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 3,485 3,400 292,673 4,233,345 $4.0483 88.7% claude-haiku-4-5-20251001 150 415 237,099 2,173,064 $0.5159 11.3% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-08/","summary":"在HPC服务器上为QueST和MIHD项目实现了RM-Ideal（Region Matching Ideal）评分功能，从零开始移植WWL图核算法，并集成到两个独立的代码库中","title":"Bug Journal 2026-02-08"},{"content":"日报 — 2026-02-07 在DCC服务器上同时推进两个生信项目：为QueST项目创建CLAUDE.md并调研RM-Ideal评分指标，以及修复MIHD基准框架中STAIG严格对齐模式的四个关键差异，最终将151673切片ARI从~0.09提升至0.54。\n今日任务 架构与策略 ✅ MIHD STAIG严格对齐模式修复（4个差异） — 系统对比MIHD与原始STAIG实现，发现并修复4个关键差异：(1)UNI encoder跳过ImageNet归一化，(2)patch尺寸改为256×256，(3)训练后不做StandardScaler，(4)drop_feature使用CPU随机数生成器；ARI从0.09提升至0.54 ✅ MIHD mclust聚类0%ARI bug修复 — 诊断并修复mclust聚类报错\u0026rsquo;Error in svd: a dimension is zero\u0026rsquo;，根因为numpy2ri.activate()在循环中反复调用未deactivate导致转换栈损坏，以及embedding可能存在零方差列 ❌ RM-Ideal评分指标调研 — 用户询问QueST方法中RM-Ideal评分的工作原理，AI在代码库、arXiv PDF、HTML版本及网络搜索中均未找到该术语；发现论文实际使用的是RM（Region Matching）分数，基于Wasserstein Weisfeiler-Lehman图核 实现与修复 ✅ 图像patch提取性能优化 — 修复_extract_patches_from_image中np.array(pil_img)在循环内重复调用导致的严重性能问题（每个spot转换一次整张大图），将其移至循环外，并用numpy切片替代PIL crop，改为预分配输出tensor ✅ QueST项目CLAUDE.md创建 — 分析QueST（空间转录组niche查询）代码库结构，包括model.py、trainer.py、layers.py、utils.py，生成完整的CLAUDE.md指导文档 🔄 224×224 patch尺寸消融实验 — 用户要求对比patch_size=224与256的性能差异，基准测试因缓存问题未能正确运行，session结束时仍未获得结果 ✅ 更新MIHD教程文档 — 将STAIG严格对齐4项修复内容写入THOROUGH_TUTORIAL.md，说明每个修复的原理和效果 问题与解决方案 关键问题 1. STAIG严格模式的ARI远低于原始STAIG（~0.09 vs 0.45+） 解决方案: 通过逐行对比发现4个差异并逐一修复：(1)跳过ImageNet归一化，(2)256×256 patch尺寸+dynamic_img_size，(3)去除StandardScaler，(4)drop_feature强制使用CPU随机数。修复后ARI=0.54\n关键洞察: ImageNet归一化是最大的差异来源——原始STAIG只做ToTensor()不做Normalize，MIHD多了这步导致UNI embedding分布完全不同；StandardScaler对mclust聚类也有显著影响\n2. mclust报错\u0026rsquo;Error in svd: a dimension is zero\u0026rsquo;，在处理12个DLPFC切片的循环中，某些切片失败 解决方案: 根因是numpy2ri.activate()在每次调用cluster_with_mclust()时执行但从不deactivate，导致rpy2转换栈在多次调用后损坏。修复：用try/finally包裹activate/deactivate，并在传入R前过滤零方差列和NaN/Inf值\n关键洞察: 原始STAIG只调用一次mclust，因此这个bug不会触发；MIHD在循环中调用12次才暴露了潜在问题\n3. RM-Ideal评分在QueST论文和代码中均未找到 解决方案: 未解决；AI在arXiv PDF、HTML、代码库和网络搜索中均未找到该术语；论文中实际的评估指标是RM（Region Matching）分数（基于WWL图核）和Best Niche Match Accuracy\n关键洞察: 用户提到的\u0026rsquo;RM-Ideal\u0026rsquo;可能来自论文的某个补充材料或较新版本，arXiv HTML版本的访问受限影响了调研\n一般问题 4. patch提取极慢（每个spot重复将整张大图转换为numpy数组） 解决方案: 将np.array(pil_img)移至循环外只做一次，用numpy数组切片替代PIL crop，预分配输出tensor而非list.append\n关键洞察: 对于~3500个spot的样本，移除循环内的冗余大图转换可带来10-50倍加速\n人类思路 vs AI 思路 战略层面 STAIG与MIHD差异的发现方式 角色 思路 人类 人类（用户）观察到ARI结果与预期不符，提出\u0026rsquo;应该已经全部都一样了吧\u0026rsquo;的疑问，驱动了调查 AI AI通过系统性逐行对比两个代码库，运行多个并行探索agent，梳理出4个具体技术差异 差异分析: 人类的直觉判断（\u0026lsquo;应该一样\u0026rsquo;）与实际结果不符，触发了AI的系统性调查；最终发现的根因（ImageNet归一化、CPU随机数等）需要深入代码对比才能找到\nmclust bug的诊断 角色 思路 人类 用户直接提供了完整的错误traceback AI AI读取了clustering.py代码，识别出numpy2ri.activate()未配对deactivate的模式，并与原始STAIG单次调用的上下文对比，定位根因 差异分析: 人类提供了精确的错误信息和traceback，AI利用代码上下文对比（STAIG只调用一次 vs MIHD循环调用12次）找到了根因\nRM-Ideal评分的定位 角色 思路 人类 用户提供了arXiv论文链接，期望AI能从中找到RM-Ideal的定义 AI AI尝试了多种访问方式（abstract、PDF、HTML v1/v2），以及网络搜索，均未找到该术语；发现代码库中存在\u0026rsquo;RM score\u0026rsquo;（不含\u0026rsquo;ideal\u0026rsquo;），并向用户报告未找到的结论 差异分析: 人类可能记忆中混淆了术语名称，或该术语来自论文的不可访问部分；AI诚实地报告未找到而不是编造答案\nAI 局限性 重要局限 无法访问arXiv论文的HTML v2版本和部分页面，导致无法完全调研RM-Ideal评分的定义（可能在论文某些补充材料中） 一般局限 在STAIG严格对齐模式调查中，AI最初通过多个并行探索agent收集信息，但需要多轮交互才能汇总所有差异；对STAIG图像预处理链（UNI→PCA→pseudo-labels→edge weights的完整链路）需要多文件才能理解 在224×224消融实验中，AI未预见到\u0026ndash;staig_alignment参数期望YAML文件路径而非inline JSON字符串，导致实验设置失败 今日收获 核心收获 原始STAIG的UNI encoder不做ImageNet归一化（只用ToTensor），这是与大多数视觉模型benchmark不同的关键细节；将MIHD对齐这一点是ARI提升最大的单一修复 rpy2的numpy2ri.activate()在多次调用时会累积状态，必须在每次使用后配对调用deactivate()；这个问题在原始STAIG中不会触发（单次调用），但在批量benchmark循环中会导致隐蔽的维度错误 CPU vs GPU随机数生成器即使种子相同也会产生不同序列；原始STAIG中drop_feature的随机mask是在CPU生成的，复现时必须保持一致才能对齐结果 实践收获 图像patch提取的性能关键在于避免在循环内将整张高分辨率组织图像（~20000×20000）重复转换为numpy数组；移至循环外可带来10-50倍加速 会话摘要 QueST 🔄 QueST代码库分析、CLAUDE.md创建与RM-Ideal评分调研 20:12:02.765 | claude_code AI分析了QueST（空间转录组niche查询）项目的完整代码结构，包括model.py（三损失训练）、trainer.py、layers.py、utils.py，创建了CLAUDE.md指导文档。随后用户询问RM-Ideal评分机制，AI在代码库、arXiv论文（PDF/HTML）和网络搜索中均未找到该术语，发现论文实际使用RM分数（WWL图核）和Best Niche Match Accuracy作为评估指标。\nMIHD ✅ STAIG严格对齐模式与原始STAIG四项关键差异修复 20:04:02.000 | claude_code 用户发现MIHD的STAIG严格模式ARI远低于原始STAIG。AI通过系统性对比两个代码库，发现并修复了4个关键差异：UNI encoder跳过ImageNet归一化（最大差异）、patch尺寸改为256×256（加dynamic_img_size）、训练后不做StandardScaler、drop_feature使用CPU随机数。修复后在151673切片上ARI从0.09提升至0.5435，远超原始STAIG的0.45基准。\n✅ 修复mclust\u0026rsquo;维度为零\u0026rsquo;报错与图像patch提取性能优化 02:44:18.061 | claude_code 用户提供了mclust聚类的完整错误traceback。AI诊断出两个问题：numpy2ri.activate()在循环中反复调用未配对deactivate，以及patch提取中将整张大图在每个spot循环内反复转换为numpy数组。AI修复了clustering.py中的activate/deactivate配对问题并添加了零方差列检测，同时重构了_extract_patches_from_image将大图转换移至循环外。\n🔍 RM-Ideal Score集成调研与graphcompass API探索 22:13:40.661 | claude_code 用户询问如何将RM-Ideal Score集成进QueST项目，但实际指向的是MIHD项目。AI探索了MIHD代码库结构和graphcompass库的RM Score API，在代码库中发现了\u0026rsquo;region matching (RM) score\u0026rsquo;的文档说明（基于WWL图核）。会话主要停留在调研阶段，未产生具体代码修改。\nToken 用量 总览 指标 数值 总 Token 15,893,955 输入 Token 804 输出 Token 13,371 Cache 创建 1,199,406 Cache 读取 14,680,374 Cache 命中率 92.4% 总费用 (USD) $11.4595 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 513 12,841 784,015 10,614,015 $10.5307 91.9% claude-haiku-4-5-20251001 291 530 415,391 4,066,359 $0.9288 8.1% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-07/","summary":"在DCC服务器上同时推进两个生信项目：为QueST项目创建CLAUDE.md并调研RM-Ideal评分指标，以及修复MIHD基准框架中STAIG严格对齐模式的四个关键差异，最终将151673切片ARI从~0.09提升至0.54。","title":"Bug Journal 2026-02-07"},{"content":"今天开始测试 recovery benchmark\n和Gemini讨论了一下，Gemini建议我从issac环境开始，先做一个demo出来\n所以就开始配置issac环境\n第一个碰倒到的bug居然是网络问题\n问题如下，这个问题的报错是 No route to host, 这意味着无法连接到代理服务器\n用人话说就是计算节点是没法联网的\n要回到登录节点才行\n然后回退就work了（（\n今天做的第二件事就是配置Claude Code + GLM的配置\n为什么要用GLM呢？因为GLM一个月￥20, 但是用量可以到达100M tokens\n据说比Github Copilot 更加聪明\n所以我想试试\n具体的链接在这里\n但是呢，在加入VS code的时候遇到了方法一失败的问题，这时候可以使用方法二，即可解决。\n这个有点NB，这样在所有的服务器上都可以用claude code帮我解决问题了\n这几天试试看看好不好用\n","permalink":"https://tzj2006.github.io/bugjournal/2026-01-09/","summary":"\u003cp\u003e今天开始测试 recovery benchmark\u003c/p\u003e\n\u003cp\u003e和Gemini讨论了一下，Gemini建议我从issac环境开始，先做一个demo出来\u003c/p\u003e\n\u003cp\u003e所以就开始配置issac环境\u003c/p\u003e\n\u003cp\u003e第一个碰倒到的bug居然是网络问题\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1767989278653\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2026-01-09/1767989278653.png\"\u003e\u003c/p\u003e\n\u003cp\u003e问题如下，这个问题的报错是 \u003ccode\u003eNo route to host\u003c/code\u003e, 这意味着无法连接到代理服务器\u003c/p\u003e\n\u003cp\u003e用人话说就是计算节点是没法联网的\u003c/p\u003e\n\u003cp\u003e要回到登录节点才行\u003c/p\u003e\n\u003cp\u003e然后回退就work了（（\u003c/p\u003e\n\u003cp\u003e今天做的第二件事就是配置Claude Code + GLM的配置\u003c/p\u003e\n\u003cp\u003e为什么要用GLM呢？因为GLM一个月￥20, 但是用量可以到达100M tokens\u003c/p\u003e\n\u003cp\u003e据说比Github Copilot 更加聪明\u003c/p\u003e\n\u003cp\u003e所以我想试试\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://docs.bigmodel.cn/cn/coding-plan/tool/claude\"\u003e具体的链接在这里\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e但是呢，在加入VS code的时候遇到了方法一失败的问题，这时候可以使用方法二，即可解决。\u003c/p\u003e\n\u003cp\u003e这个有点NB，这样在所有的服务器上都可以用claude code帮我解决问题了\u003c/p\u003e\n\u003cp\u003e这几天试试看看好不好用\u003c/p\u003e","title":"Bug Journal 2026-01-09"},{"content":"Today\u0026rsquo;s problem 3432. Count Partitions with Even Sum Difference\nIntuition In this question, we need to partition the array into two parts. And the difference between these two parts are even.\nNow, both parts must have the same module to 2. That is, they are both even or both odd. So, the sum of the array needs to be even.\nNow, if the sum of the array is even, then every partition must have the same module to 2. Otherwise, the sum of the array would be odd.\nTherefore, the answer would be 0 when the sum of the array is odd, and $n-1$ when the sum of the array is even.\nApproach Return 0 when the sum of the array is odd, and $n-1$ when the sum of the array is even.\nComplexity Time complexity: $O(n)$, n is the length of the array.\nSpace complexity: $O(n)$, n is the length of the array.\nCode class Solution: def countPartitions(self, nums: List[int]) -\u0026gt; int: if sum(nums) % 2 == 1: return 0 return len(nums) - 1 ","permalink":"https://tzj2006.github.io/leetcode/2025-12-05/","summary":"\u003col start=\"3432\"\u003e\n\u003cli\u003eCount Partitions with Even Sum Difference\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-12-05"},{"content":"Question Burst Balloons\nIntuition While the ballons burst, it will not affect the score of another ballon, and the length of the list will always decrease. This means that we can use dynamic programming to solve this problem.\nApproach Consider a case, when $dp[i][j]$ means the maximum score you can get by bursting all the balloons between i and j. In this case, our answer would be $dp[0][n-1]$, if n is the length of the list.\nNow, let\u0026rsquo;s figure out how to get $dp[i][j]$. As we mentioned in part Intuition, the length of the balloon is always decreasing, which means that we might be able to get $dp[i][j]$ by first counting a shorter interval, and the increase the interval to $0 \\to n-1$.\nNow, we have our solution:\nFirst, we iterate the length of the interval.\nNext, we can break the interval $i \\to j$ into two sub intervals $i \\to k-1$ and $k+1 \\to j$.\nNow we can compute the score of interval $i \\to j$ by adding up sub intervals $i \\to k-1$, $k + 1 \\to j$, and burst the balloon k.\nBecause the balloons in sub interval $i \\to k-1$ and $k + 1 \\to j$ are already gone, so the score addition would be $nums[k] \\times nums[i-1] \\times nums[j+1]$.\nThe only thing we now need to consider is the edge of these subintervals. Knowing that the balloon we are now bursting can be the edge of the interval $i \\to j$, meaning that we need to iterate $[i, j]$ inclusively.\nFinally, you will get your result in $dp[0][n-1]$.\nComplexity Time complexity: $O(N^3)$. N is the length of the list. Space complexity: $O(N^2)$. N is the length of the list. Code class Solution: def maxCoins(self, nums: List[int]) -\u0026gt; int: n = len(nums) nums = nums + [1] dp = [[0 for _ in range(n+1)] for _ in range(n+1)] for i in range(n): dp[i][i] = nums[i] * nums[i-1] * nums[i+1] for i in range(1, n): for l in range(n): if i + l == n: break r = i + l for k in range(l, r+1): dp[l][r] = max(dp[l][r], dp[l][k-1] + dp[k+1][r] + nums[k] * nums[l-1] * nums[r + 1]) # print(dp) return dp[0][n-1] ","permalink":"https://tzj2006.github.io/leetcode/p312_burst_balloons/","summary":"Interval Dynamic Programming with clear explaination and how you can get to the solution","title":"LeetCode Question P.312 Burst Balloons"},{"content":"FAST: Efficient Action Tokenization for Vision-Language-Action Models RSS 2025 By Physical Intelligence\n当前方法局限：都是离散数据，没有办法获取高频机器人控制数据的时间相关性\n因此使用一种名字叫做DCT的压缩算法，可以把连续信号转移到频率域(一种离散域)\n(话虽如此，但是真的需要用到这么高的频率吗？)\n问题 ：传统的朴素分箱方案在处理高频连续动作时，由于动作变化小，会导致许多连续时间步的动作被离散化为相同或高度相似的标记。这意味着每个新标记带来的“边际信息”（即在已知之前标记的情况下，新标记所包含的额外信息）非常低。 朴素分箱方案独立地处理每个时间步和每个维度，完全忽略了连续动作序列中固有的时间相关性（例如，机器手臂的移动通常是平滑且连续的）。这使得模型难以学习到动作的动态模式。 Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets UW + Toyota\n读前问题：\n这个模型有什么优势？ 这个模型是如何实现“视频无标签学习\u0026quot;的？ 什么叫\u0026quot;naturally facilitate video learning\u0026quot; 觉得有意思的点：\nImitation learning do not explicitly capture temporal dynamics that are naturally present in demonstration trajectories or videos (Page 1, Introduction) 一个 Diffusion Model： UWM 使用一个单一的 Diffusion Transformer 模型来处理所有的输入和输出。它不是为每种任务（策略、正向动力学、逆向动力学、视频预测）训练一个单独的模型。 Tokenization 和输入： 就像您说的，无论是当前的观测（图片）、动作、还是未来的观测，它们都会被“tokennize”成模型能够理解的表示形式（例如，图片通过编码器转换成图像特征，动作也被编码）。这些 token 以及动作和未来观测的独立扩散时间步长 t_a 和 t_o' 会一起输入到 Diffusion Transformer 中。 灵活的输出： 通过巧妙地控制这些扩散时间步长 t_a 和 t_o'，UWM 可以在推理时灵活地得到您想要的结果。例如： 如果想得到 策略（Policy） ，就将未来观测的时间步长 t_o' 设置为最大值 T（表示未来观测完全被噪声掩盖，模型只关注动作），然后模型会输出动作。 如果想得到 正向动力学（Forward Dynamics） ，就将动作时间步长 t_a 设置为 0（表示给定明确的动作输入），然后模型会输出未来观测。 如果想得到 逆向动力学（Inverse Dynamics） ，就将未来观测时间步长 t_o' 设置为 0（表示给定明确的未来观测输入），然后模型会输出动作。 如果想得到 视频预测（Video Prediction） ，就将动作时间步长 t_a 设置为最大值 T（表示动作完全被噪声掩盖，模型只关注视频），然后模型会输出未来观测。 模态独立的扩散时间步长 (Modality-Specific Diffusion Timesteps)： 这是 UWM 最核心的设计之一。传统的多模态扩散模型可能使用一个共享的时间步长来对所有模态进行加噪和去噪。然而，UWM 引入了两个独立的扩散时间步长：t_a 用于动作 (actions)，t_o' 用于未来观测 (future observations)。 原因： 这种分离使得模型能够独立控制每种模态的噪声水平。通过在推理时灵活地设置这些时间步长（例如，将某个时间步长设为 T 表示完全加噪以“掩蔽”该模态，设为 0 表示完全去噪以“条件化”该模态），UWM 可以实现前文提到的策略、正向动力学、逆向动力学和视频预测等多种推理模式。 效果： 它使得一个单一的模型能够充当多种角色，极大地提高了模型的通用性和灵活性，同时增强了模型对模态间因果关系的理解。 统一的 Transformer 架构 (Unified Transformer Architecture)： UWM 采用单一的 Transformer 骨干网络来处理所有模态的输入（当前观测、动作、未来观测）并预测其噪声。Transformer 以编码后的图像特征、动作 token 以及独立的扩散时间步长作为输入。 原因： 这种统一的架构促进了不同模态之间的特征共享。在 Transformer 内部，动作和图像信息可以相互交流和融合，从而使模型能够学习到更丰富、更全面的表征，并捕获它们之间的潜在因果关系。 效果： 避免了为每个任务设计独立模型的复杂性，简化了模型结构，并可能提高学习效率。 寄存器令牌 (Register Tokens)： UWM 在 Transformer 的输入序列中加入了随机初始化的“寄存器令牌”。 原因： 经验性研究发现，这些冗余令牌有助于提高模型性能，尤其是在多模态数据上。研究人员推测，动作和潜在图像块是不同的模态，寄存器可以作为一个中间媒介，存储来自任一模态的信息，并在后续 Transformer 层中被检索，从而促进模态间的信息交换和更好的多模态特征共享。 效果： 尽管这些令牌最终会被丢弃，但它们在中间层起到了重要的信息桥梁作用，提升了模型的表现力。 Adaptive Layer Normalization (AdaLN) 条件化： UWM 使用 AdaLN [33] 来将扩散时间步长（以及图像特征）条件化到 Transformer 的每个块中。 原因： AdaLN 是一种有效的条件化机制，它通过调整层归一化 (Layer Normalization) 的参数（缩放和偏移）来注入条件信息。 效果： 这种方式允许模型以一种非侵入性且高效的方式将时间步长信息融入到 Transformer 的处理流程中，有助于模型更好地理解噪声水平和生成过程。 潜在扩散范式 (Latent Diffusion Paradigm)： 对于图像扩散，UWM 采用了潜在扩散范式。它使用一个冻结的预训练 VAE (Variational AutoEncoder)（例如来自 Stable Diffusion XL [34]）将原始高分辨率图像压缩到低维的潜在空间中进行加噪和去噪，最后再解码回全尺寸图像。 原因： 直接在高分辨率像素空间进行扩散通常计算成本高昂且效率低下。潜在扩散可以在更小的潜在空间中操作，显著减少计算量。 效果： 提高了图像处理的效率，同时保持了高质量的图像生成能力。 这些特殊设计共同使得 UWM 能够有效地应对多模态机器人学习的挑战，实现了一个能够从大规模异构数据中学习并灵活执行多种推理任务的强大模型。\nVILA: On Pre-training for Visual Language Models https://www.alphaxiv.org/overview/2312.07533v4\nby Nvidia \u0026amp; MIT\n更新大型语言模型 (LLM) 是必要的 ：研究发现，在预训练过程中解冻并更新 LLM 对于获得良好的上下文学习（ICL）能力至关重要。仅仅进行提示调整（prompt tuning）虽然在零样本（0-shot）准确率上表现尚可，但在上下文学习能力方面却不足。使用一个简单的线性投影层而不是 Transformer 块作为投影器，可以促使 LLM 更好地学习处理视觉输入，从而带来更好的泛化能力。 交错式视觉语言语料库有助于预训练 ：预训练时，交错式的图像-文本数据集（如 MMC4）比单纯的图像-文本对（如 COYO）更有益。交错式数据有助于保持 LLM 的文本处理能力，并提供了更准确的梯度更新。与仅使用图像-文本对相比，使用交错式数据进行预训练能显著提高视觉语言任务的准确性，并减少文本能力退化。 通过联合监督微调（SFT）恢复 LLM 性能退化 ：尽管交错式数据有助于保持文本能力，但仍存在一定的准确率下降。通过在 SFT 阶段混入文本指令数据，可以同时恢复文本处理能力的退化，并提高视觉语言任务的准确性。这表明，文本指令数据有助于提升模型的指令遵循能力。 扩大 VLM 预训练的规模 ：模型在以下几个方面进行了扩展以形成最终模型： 更高的图像分辨率 ：将图像分辨率从 224×224224×224 提高到 336×336336×336，从而能够包含更多视觉细节，这对于需要精细细节的任务（如 TextVQA）非常有帮助。 更大的 LLM ：将 LLM 主干模型从 Llama-2 7B 扩展到 Llama-2 13B，以进一步提高性能。 预训练数据 ：同时使用交错式图像-文本数据和图像-文本对进行预训练，以提高数据多样性，尽管预训练语料库的总规模为 50M 图像，小于十亿规模的数据集，但仍取得了显著的性能提升。 SFT 数据 ：包含了 LLaVA-1.5 中更高质量、更多样化的 SFT 数据，进一步提升了下游评估指标。 ","permalink":"https://tzj2006.github.io/bugjournal/2025-08-29/","summary":"\u003ch3 id=\"fast-efficient-action-tokenization-for-vision-language-action-models\"\u003eFAST: Efficient Action Tokenization for Vision-Language-Action Models\u003c/h3\u003e\n\u003cp\u003eRSS 2025 By Physical Intelligence\u003c/p\u003e\n\u003cp\u003e当前方法局限：都是离散数据，没有办法获取高频机器人控制数据的时间相关性\u003c/p\u003e\n\u003cp\u003e因此使用一种名字叫做DCT的压缩算法，可以把连续信号转移到频率域(一种离散域)\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1756516611535\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-08-29/1756516611535.png\"\u003e\u003c/p\u003e\n\u003cp\u003e(话虽如此，但是真的需要用到这么高的频率吗？)\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1756516932637\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-08-29/1756516932637.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e ：传统的朴素分箱方案在处理高频连续动作时，由于动作变化小，会导致许多连续时间步的动作被离散化为相同或高度相似的标记。这意味着每个新标记带来的“边际信息”（即在已知之前标记的情况下，新标记所包含的额外信息）非常低。\u003c/li\u003e\n\u003cli\u003e朴素分箱方案独立地处理每个时间步和每个维度，完全忽略了连续动作序列中固有的时间相关性（例如，机器手臂的移动通常是平滑且连续的）。这使得模型难以学习到动作的动态模式。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"unified-world-models-coupling-video-and-action-diffusion-for-pretraining-on-large-robotic-datasets\"\u003eUnified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets\u003c/h3\u003e\n\u003cp\u003eUW + Toyota\u003c/p\u003e\n\u003cp\u003e读前问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e这个模型有什么优势？\u003c/li\u003e\n\u003cli\u003e这个模型是如何实现“视频无标签学习\u0026quot;的？\u003c/li\u003e\n\u003cli\u003e什么叫\u0026quot;naturally facilitate video learning\u0026quot;\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e觉得有意思的点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eImitation learning do not explicitly capture temporal dynamics that are naturally present in demonstration trajectories or videos (Page 1, Introduction)\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e一个 Diffusion Model：\u003c/strong\u003e UWM 使用一个单一的 Diffusion Transformer 模型来处理所有的输入和输出。它不是为每种任务（策略、正向动力学、逆向动力学、视频预测）训练一个单独的模型。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTokenization 和输入：\u003c/strong\u003e 就像您说的，无论是当前的观测（图片）、动作、还是未来的观测，它们都会被“tokennize”成模型能够理解的表示形式（例如，图片通过编码器转换成图像特征，动作也被编码）。这些 token 以及动作和未来观测的独立扩散时间步长 \u003ccode\u003et_a\u003c/code\u003e 和 \u003ccode\u003et_o'\u003c/code\u003e 会一起输入到 Diffusion Transformer 中。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e灵活的输出：\u003c/strong\u003e 通过巧妙地控制这些扩散时间步长 \u003ccode\u003et_a\u003c/code\u003e 和 \u003ccode\u003et_o'\u003c/code\u003e，UWM 可以在推理时灵活地得到您想要的结果。例如：\n\u003cul\u003e\n\u003cli\u003e如果想得到 \u003cstrong\u003e策略（Policy）\u003c/strong\u003e ，就将未来观测的时间步长 \u003ccode\u003et_o'\u003c/code\u003e 设置为最大值 T（表示未来观测完全被噪声掩盖，模型只关注动作），然后模型会输出动作。\u003c/li\u003e\n\u003cli\u003e如果想得到 \u003cstrong\u003e正向动力学（Forward Dynamics）\u003c/strong\u003e ，就将动作时间步长 \u003ccode\u003et_a\u003c/code\u003e 设置为 0（表示给定明确的动作输入），然后模型会输出未来观测。\u003c/li\u003e\n\u003cli\u003e如果想得到 \u003cstrong\u003e逆向动力学（Inverse Dynamics）\u003c/strong\u003e ，就将未来观测时间步长 \u003ccode\u003et_o'\u003c/code\u003e 设置为 0（表示给定明确的未来观测输入），然后模型会输出动作。\u003c/li\u003e\n\u003cli\u003e如果想得到 \u003cstrong\u003e视频预测（Video Prediction）\u003c/strong\u003e ，就将动作时间步长 \u003ccode\u003et_a\u003c/code\u003e 设置为最大值 T（表示动作完全被噪声掩盖，模型只关注视频），然后模型会输出未来观测。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e模态独立的扩散时间步长 (Modality-Specific Diffusion Timesteps)：\u003c/strong\u003e 这是 UWM 最核心的设计之一。传统的多模态扩散模型可能使用一个共享的时间步长来对所有模态进行加噪和去噪。然而，UWM 引入了两个独立的扩散时间步长：\u003ccode\u003et_a\u003c/code\u003e 用于动作 (actions)，\u003ccode\u003et_o'\u003c/code\u003e 用于未来观测 (future observations)。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 这种分离使得模型能够独立控制每种模态的噪声水平。通过在推理时灵活地设置这些时间步长（例如，将某个时间步长设为 T 表示完全加噪以“掩蔽”该模态，设为 0 表示完全去噪以“条件化”该模态），UWM 可以实现前文提到的策略、正向动力学、逆向动力学和视频预测等多种推理模式。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 它使得一个单一的模型能够充当多种角色，极大地提高了模型的通用性和灵活性，同时增强了模型对模态间因果关系的理解。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e统一的 Transformer 架构 (Unified Transformer Architecture)：\u003c/strong\u003e UWM 采用单一的 Transformer 骨干网络来处理所有模态的输入（当前观测、动作、未来观测）并预测其噪声。Transformer 以编码后的图像特征、动作 token 以及独立的扩散时间步长作为输入。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 这种统一的架构促进了不同模态之间的特征共享。在 Transformer 内部，动作和图像信息可以相互交流和融合，从而使模型能够学习到更丰富、更全面的表征，并捕获它们之间的潜在因果关系。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 避免了为每个任务设计独立模型的复杂性，简化了模型结构，并可能提高学习效率。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e寄存器令牌 (Register Tokens)：\u003c/strong\u003e UWM 在 Transformer 的输入序列中加入了随机初始化的“寄存器令牌”。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 经验性研究发现，这些冗余令牌有助于提高模型性能，尤其是在多模态数据上。研究人员推测，动作和潜在图像块是不同的模态，寄存器可以作为一个中间媒介，存储来自任一模态的信息，并在后续 Transformer 层中被检索，从而促进模态间的信息交换和更好的多模态特征共享。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 尽管这些令牌最终会被丢弃，但它们在中间层起到了重要的信息桥梁作用，提升了模型的表现力。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdaptive Layer Normalization (AdaLN) 条件化：\u003c/strong\u003e UWM 使用 AdaLN [33] 来将扩散时间步长（以及图像特征）条件化到 Transformer 的每个块中。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e AdaLN 是一种有效的条件化机制，它通过调整层归一化 (Layer Normalization) 的参数（缩放和偏移）来注入条件信息。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 这种方式允许模型以一种非侵入性且高效的方式将时间步长信息融入到 Transformer 的处理流程中，有助于模型更好地理解噪声水平和生成过程。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e潜在扩散范式 (Latent Diffusion Paradigm)：\u003c/strong\u003e 对于图像扩散，UWM 采用了潜在扩散范式。它使用一个冻结的预训练 VAE (Variational AutoEncoder)（例如来自 Stable Diffusion XL [34]）将原始高分辨率图像压缩到低维的潜在空间中进行加噪和去噪，最后再解码回全尺寸图像。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 直接在高分辨率像素空间进行扩散通常计算成本高昂且效率低下。潜在扩散可以在更小的潜在空间中操作，显著减少计算量。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 提高了图像处理的效率，同时保持了高质量的图像生成能力。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这些特殊设计共同使得 UWM 能够有效地应对多模态机器人学习的挑战，实现了一个能够从大规模异构数据中学习并灵活执行多种推理任务的强大模型。\u003c/p\u003e","title":"Bug Journal 2025-08-29"},{"content":"总结 目前在 Benchmark 1: SQA3D上 GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models 效果最好。达到了 Exact Maching 62.4% 的准确率。 在Benchmark 2: ScanQA上 Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (AAAI 2024) 效果最好。达到了 Exact Maching 31.29% 的准确率。\n动机 想看看VLA / 空间推理VLM 发展得怎么样，SOTA是什么\n调研方式 对于这个Task,我打算从CVPR 2025入手，看看最新的VLA都是如何实现的，又是如何比较的\nPlan 1: 首先，我会先寻找一下CVPR 2025中和VLA有关的任务，并且看看他们的表现 Paper 1: DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering 链接 From 中大 HCP\n结果： 结果一: 这张图用的是 这个benchmark (SQA3D)\n结果二：\n这张图用的是 这个benchmark (ScanQA)\nPaper 2: LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning (CVPR 2024)\n链接\nFrom Fudan, Tencent, and National University of Singapore\n结果 这张图用的是 这个benchmark (ScanQA) 如果横向对比这两个模型，其实有些数据还是这个模型高一些\nPaper 3: Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (AAAI 2024)\n链接\nFrom PKU\n结果 这张图用的是 这个benchmark (ScanQA) 和 这个benchmark (SQA3D)\n如果我们比较这一篇和第一篇CVPR2025 我们会发现，实际上这篇效果更好\nPaper 4: SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding (ECCV 2024)\n链接\nFrom BIGAI, Beijing\n结果 可以看到效果没那么理想\nPaper 5: Scene-LLM: Extending Language Model for 3D Visual Reasoning (WACV 2025) 链接\nFrom Brown University \u0026amp; Meta\n结果 可以看到这个模型在SQA3D的表现比其他模型更好，达到了54.2%\nPaper 6: Unifying 3D Vision-Language Understanding via Promptable Queries (ECCV 2024)\nFrom BIGAI, Beijing\n链接\n结果 把这个模型放在这里的原因是：这个模型的 MENTOR 和 CIDEr matrix 的表现都比之前的模型好\n注：这个模型和 Paper 4 是同一个组做的\nPaper 7: Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers (NIPS 2024)\nFrom 浙大, 上海AI lab, and 字节\n链接\n结果 这个模型比较了之前的SOTA, 结果SQA3D提高了0.4%的准确率。。。\nPaper 8: GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models (arxiv preprint March 2025)\n链接\nFrom HKU \u0026amp; 上海AI lab\n结果： 这个模型在SQA3D上做到了62.4%的准确率\nPaper 9: Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding (CVPR 2025)\n链接\nFrom The Chinese University of Hong Kong\n结果 这个模型在ScanQA的CIDEr上做到了SOTA\nPaper 10 (To be continued): 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding (arxiv preprint July 2025)\nFrom Shanghai University of Engineering Science \u0026amp; PKU\n链接\n结果： Report了非常奇怪的结果，需要进一步细看\nBenchmark 1: SQA3D SQA3D\n上图是一个询问的例子\n这个图解释了为什么有些方法会用 \u0026ldquo;Which\u0026rdquo; \u0026ldquo;How\u0026rdquo; 这些来分类。 总结来说，以这几个词开头的问句较多，且方向不同\n最后统计的是准确率(按照文章中的说法，居然是做一个706维度的分类？) 需要和Ground Truth完全一致\nBenchmark 2: ScanQA ScanQA\n上图是一个询问的例子\n同样是准确率 需要和Ground Truth完全一致\n和上面不同，这里也加入了其他metric去算答案和Ground Truth的相似性\n","permalink":"https://tzj2006.github.io/bugjournal/2025-08-25/","summary":"SOTA VLA","title":"Bug Journal 2025-08-25"},{"content":"Hi, 你好，我的读者。\n好久不见了，有没有想我呀(╹▽╹)\n上次见面的时候还是7月16号，已经隔了快一个月了呢\n这个月其实也发生了许多事情，但最后我发现，原来，有时候就是遵从自己的内心也是煎熬的。\n我现在开始相信本我、自我和超我了，本我就是最原本的“内心\u0026quot;，而超我则是最原本的”幻想\u0026quot;\n他们两个就像是两个截然不同的 loss function 一样，不断对抗着，希望我朝着某一个方向前进\n而这个 loss function 的正则项，\n","permalink":"https://tzj2006.github.io/bugjournal/2025-08-08/","summary":"\u003cp\u003eHi, 你好，我的读者。\u003c/p\u003e\n\u003cp\u003e好久不见了，有没有想我呀(\u003cem\u003e╹▽╹\u003c/em\u003e)\u003c/p\u003e\n\u003cp\u003e上次见面的时候还是7月16号，已经隔了快一个月了呢\u003c/p\u003e\n\u003cp\u003e这个月其实也发生了许多事情，但最后我发现，原来，有时候就是遵从自己的内心也是煎熬的。\u003c/p\u003e\n\u003cp\u003e我现在开始相信本我、自我和超我了，本我就是最原本的“内心\u0026quot;，而超我则是最原本的”幻想\u0026quot;\u003c/p\u003e\n\u003cp\u003e他们两个就像是两个截然不同的 loss function 一样，不断对抗着，希望我朝着某一个方向前进\u003c/p\u003e\n\u003cp\u003e而这个 loss function 的正则项，\u003c/p\u003e","title":"Bug Journal 2025-08-08"},{"content":"现在遇到了如图所示的这个问题：\n“现阶段的VLA本质上就是数据量不足，就像你说VLA让他去开车，根本不可能，神经网络没有见过这样的数据就是不理解，现阶段还是要搞数据工程，逼近scaling law”\n“确实，只要数据够多就没有out of distribution了！”\n而对于 lifelong task 来说，这种情况更是一个问题：\n“为什么不直接加数据训练？而是要通过 lifelong 持续的学习”\n记录一下我现在的想法\n首先是持续学习\n这个有什么好处呢，好处就是，这样的模型可以不用在最开始的时候就学会所有东西\n而是可以等到之前的东西学完之后然后再继续学下一个 task\n什么意思呢，就是说，我之前学习到了一个 distribution 下的所有的内容\n我让机械臂学会了这些知识\n但是呢，我现在遇到了一些 OOD (Out of distribution) 的 task\n这种情况怎么办呢？ 现在的 model 就直接束手无策，束手就擒了\n但是 lifelong learning 的 task 就可以在这种情况下，在不遗忘之前学习的 task 的情况下学习到 OOD 的 task\n这样这种模型的拓展能力就会更好\n可是。。。\n如前文所说，只要数据够多就没有out of distribution了\n而在 lifelong setting 的情境下，本来就要有数据训练，那为什么不直接 finetune 原本的模型呢\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-16/","summary":"\u003cp\u003e现在遇到了如图所示的这个问题：\u003c/p\u003e\n\u003cp\u003e“现阶段的VLA本质上就是数据量不足，就像你说VLA让他去开车，根本不可能，神经网络没有见过这样的数据就是不理解，现阶段还是要搞数据工程，逼近scaling law”\u003c/p\u003e\n\u003cp\u003e“确实，只要数据够多就没有out of distribution了！”\u003c/p\u003e\n\u003cp\u003e而对于 lifelong task 来说，这种情况更是一个问题：\u003c/p\u003e\n\u003cp\u003e“为什么不直接加数据训练？而是要通过 lifelong 持续的学习”\u003c/p\u003e\n\u003cp\u003e记录一下我现在的想法\u003c/p\u003e\n\u003cp\u003e首先是持续学习\u003c/p\u003e\n\u003cp\u003e这个有什么好处呢，好处就是，这样的模型可以不用在最开始的时候就学会所有东西\u003c/p\u003e\n\u003cp\u003e而是可以等到之前的东西学完之后然后再继续学下一个 task\u003c/p\u003e\n\u003cp\u003e什么意思呢，就是说，我之前学习到了一个 distribution 下的所有的内容\u003c/p\u003e\n\u003cp\u003e我让机械臂学会了这些知识\u003c/p\u003e\n\u003cp\u003e但是呢，我现在遇到了一些 OOD (Out of distribution) 的 task\u003c/p\u003e\n\u003cp\u003e这种情况怎么办呢？ 现在的 model 就直接束手无策，束手就擒了\u003c/p\u003e\n\u003cp\u003e但是 lifelong learning 的 task 就可以在这种情况下，在不遗忘之前学习的 task 的情况下学习到 OOD 的 task\u003c/p\u003e\n\u003cp\u003e这样这种模型的拓展能力就会更好\u003c/p\u003e\n\u003cp\u003e可是。。。\u003c/p\u003e\n\u003cp\u003e如前文所说，只要数据够多就没有out of distribution了\u003c/p\u003e\n\u003cp\u003e而在 lifelong setting 的情境下，本来就要有数据训练，那为什么不直接 finetune 原本的模型呢\u003c/p\u003e","title":"Bug Journal 2025-07-16"},{"content":"LoRA finetuning 之前一直听说 LoRA 的大名，今天来看看 LoRA 到底在做什么\n正好我看到了 huggingface 上有 LoRA tutorial 所以就过来研究了一下🧐\n注：代码顺序与 tutorial中略有出入\n省流：想看 LoRA specific 的同学请直接从[这里开始看](#Step 5: 训练前准备)\nStep1: 导入需要用到的 package import transformers import accelerate import torch import peft print(f\u0026#34;transformers version: {transformers.__version__}\u0026#34;) print(f\u0026#34;accelerate version: {accelerate.__version__}\u0026#34;) print(f\u0026#34;torch version: {torch.__version__}\u0026#34;) print(f\u0026#34;peft version: {peft.__version__}\u0026#34;) 需要导入的 package 大概就这么几个，首先是 transformers, accelerate, torch 和 peft\n今天这个 LoRA 主要就是用的 peft package.\n至于 accelerate, 就是用来加速 peft 的\nStep2: 数据集加载 # 接下来加载数据集 # 这里我们使用的是 Hugging Face 的 datasets 库来加载 Food101 数据集 from datasets import load_dataset # dataset = load_dataset(\u0026#34;food101\u0026#34;, split=\u0026#34;train[:5000]\u0026#34;) # 只加载前5000个样本以加快速度 dataset = load_dataset(\u0026#34;food101\u0026#34;, split=\u0026#34;train+validation\u0026#34;) # 加载整个数据集 from datasets import Image as DatasetsImage dataset = dataset.cast_column(\u0026#34;image\u0026#34;, DatasetsImage(decode=False)) # 然后把标签拿出来 # 这里 huggingface 要求一个 labeltoid \u0026amp; idtolabel # 所以这里要弄两个 map 互相 map labels = dataset.features[\u0026#34;label\u0026#34;].names label2id, id2label = dict(), dict() for i, label in enumerate(labels): label2id[label] = i id2label[i] = label 这里demo中用的是Food101数据集 (老演员了[doge]),\n但是这个作为 classification 的数据集其实不太好（因为食物的种类非常非常多\n但是作为LoRA刚刚好，因为LoRA就是专门针对这个事情的\n加载这个部分倒是没什么好说的\n最后这个 .cast_column 是后来发现这个数据集存的是二进制文件，所以不能 decode (by ChatGPT)\nStep3: 加载预训练模型 这个很重要！！, 不要到时候跑起来了发现没有导入预训练模型\n注：如果没有加载预训练模型，不会有任何 Warning or Error，什么都可以正常跑，但是跑到最后才会发现出问题\n# 接下来我们需要加载一个预训练的模型和图像处理器 # 这里我们使用的是 Hugging Face 的 transformers 库来加载一个 ViT 模型 # 以及一个图像处理器（image processor）来处理输入的图像 from transformers import AutoImageProcessor model_checkpoint = \u0026#34;google/vit-base-patch16-224-in21k\u0026#34; image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, use_fast=True) # 接下来我们加载一个预训练的模型 # 这里我们使用的是 Hugging Face 的 transformers 库来加载一个 ViT 模型 from transformers import AutoModelForImageClassification, TrainingArguments, Trainer model = AutoModelForImageClassification.from_pretrained( model_checkpoint, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True, # provide this in case you\u0026#39;re planning to fine-tune an already fine-tuned checkpoint ) 这里 tutorial 中输入的是 google 的 VIT 模型\n唯一要注意的是这里的 use_fast=True,\n加上之后据说图片的导入能快10x to 30x\nStep4: 加载 Dataset # 导入一些常用的图像处理函数 from torchvision.transforms import ( CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, Resize, ToTensor, ) # 接下来我们做一些预处理 # 首先是normalize normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std) # 然后是train和validation的图像变换 train_transforms = Compose( [ RandomResizedCrop(image_processor.size[\u0026#34;height\u0026#34;]), RandomHorizontalFlip(), ToTensor(), normalize, ] ) # validation也是一样的处理 val_transforms = Compose( [ Resize(image_processor.size[\u0026#34;height\u0026#34;]), CenterCrop(image_processor.size[\u0026#34;height\u0026#34;]), ToTensor(), normalize, ] ) # 然后写一个函数 process dataset def preprocess_train(example_batch): \u0026#34;\u0026#34;\u0026#34;Apply train_transforms across a batch.\u0026#34;\u0026#34;\u0026#34; example_batch[\u0026#34;pixel_values\u0026#34;] = [ train_transforms(Image.open(io.BytesIO(image[\u0026#34;bytes\u0026#34;])).convert(\u0026#34;RGB\u0026#34;)) for image in example_batch[\u0026#34;image\u0026#34;] ] return example_batch # validation也是一样的处理 def preprocess_val(example_batch): \u0026#34;\u0026#34;\u0026#34;Apply val_transforms across a batch.\u0026#34;\u0026#34;\u0026#34; example_batch[\u0026#34;pixel_values\u0026#34;] = [ val_transforms(Image.open(io.BytesIO(image[\u0026#34;bytes\u0026#34;])).convert(\u0026#34;RGB\u0026#34;)) for image in example_batch[\u0026#34;image\u0026#34;] ] return example_batch # 接下来我们对数据集进行预处理, 拆分成训练集和验证集 splits = dataset.train_test_split(test_size=0.1) train_ds = splits[\u0026#34;train\u0026#34;] val_ds = splits[\u0026#34;test\u0026#34;] train_ds = train_ds.cast_column(\u0026#34;image\u0026#34;, DatasetsImage(decode=False)) val_ds = val_ds.cast_column(\u0026#34;image\u0026#34;, DatasetsImage(decode=False)) # 然后transform一下 train_ds.set_transform(preprocess_train) val_ds.set_transform(preprocess_val) 这里和其他的模型也没有区别，都是一样的处理方式\n虽然这里看起来很长，但是其实都是 到处copy-paste 🤦‍♂️\nStep 5: 训练前准备 # 这个函数的作用是打印模型的可训练参数和总参数数量 # 这个函数是可以复用的，在这里标记一下 def print_trainable_parameters(model): trainable_params = 0 all_param = 0 for _, param in model.named_parameters(): all_param += param.numel() if param.requires_grad: trainable_params += param.numel() print( f\u0026#34;trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\u0026#34; ) # 加载config from transformers import TrainingArguments, Trainer model_name = model_checkpoint.split(\u0026#34;/\u0026#34;)[-1] batch_size = 128 args = TrainingArguments( f\u0026#34;{model_name}-finetuned-lora-food101\u0026#34;, remove_unused_columns=False, eval_strategy=\u0026#34;epoch\u0026#34;, # 注：这里在 transformers 4.46+ 版本后从 evluation_strategy 改为 eval_strategy save_strategy=\u0026#34;epoch\u0026#34;, learning_rate=5e-3, per_device_train_batch_size=batch_size, gradient_accumulation_steps=4, per_device_eval_batch_size=batch_size, fp16=True, num_train_epochs=5, logging_steps=10, load_best_model_at_end=True, metric_for_best_model=\u0026#34;accuracy\u0026#34;, push_to_hub=True, label_names=[\u0026#34;labels\u0026#34;], ) # 接下来我们设置LoRA训练参数 from peft import LoraConfig, get_peft_model config = LoraConfig( r=16, lora_alpha=16, target_modules=[\u0026#34;query\u0026#34;, \u0026#34;value\u0026#34;], # 这里的 target_modules 是指我们要对哪些模块进行 LoRA 微调 # 也就是说，我们甚至不是对整个模型的矩阵进行分解 # 而是对模型中的某些特定模块进行分解 # 这里我们选择了 query 和 value 模块 lora_dropout=0.1, bias=\u0026#34;none\u0026#34;, modules_to_save=[\u0026#34;classifier\u0026#34;], # 这里的 modules_to_save 是指我们要保存哪些模块的参数 # 也就是说，我们只保存分类器的参数 # 这样的话，需要训练的参数就会进一步减少 ) import numpy as np import evaluate metric = evaluate.load(\u0026#34;accuracy\u0026#34;) # 定义计算指标的函数 def compute_metrics(eval_pred): \u0026#34;\u0026#34;\u0026#34;Computes accuracy on a batch of predictions\u0026#34;\u0026#34;\u0026#34; predictions = np.argmax(eval_pred.predictions, axis=1) return metric.compute(predictions=predictions, references=eval_pred.label_ids) # 这个函数是huggingface transformers 中和 pytorch dataloader __getitem__ 方法对应的函数 # 它的作用是将数据集中的每个样本转换为模型可以接受的格式 # 这里我们将图像转换为 pixel_values，并将标签转换为 label def collate_fn(examples): pixel_values = torch.stack([example[\u0026#34;pixel_values\u0026#34;] for example in examples]) labels = torch.tensor([example[\u0026#34;label\u0026#34;] for example in examples]) return {\u0026#34;pixel_values\u0026#34;: pixel_values, \u0026#34;labels\u0026#34;: labels} 训练前准备包括打印参数的辅助函数，以及所有的 hyper_parameters\n还有 evaluation matrix 和 collate_fn 函数\n在这里，这些都是另外定义的，而不是像 pytorch 那样，定义在 class 里的\nStep 6: 定义模型 from transformers import AutoModelForImageClassification, TrainingArguments, Trainer # 首先定义原本的pretrain model model = AutoModelForImageClassification.from_pretrained( model_checkpoint, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True, # provide this in case you\u0026#39;re planning to fine-tune an already fine-tuned checkpoint ) # 打印模型的可训练参数和总参数数量 print_trainable_parameters(model) lora_model = get_peft_model(model, config) print_trainable_parameters(lora_model) 这一步就是把前面的config导入一下，\nTransformer is all you need\nTransformer库中的 AutoModel 会帮你完成一切的\nStep 7: 模型训练 # ok, 现在我们创建一个 Trainer 对象，然后就可以开始训练了 trainer = Trainer( lora_model, args, train_dataset=train_ds, eval_dataset=val_ds, tokenizer=image_processor, compute_metrics=compute_metrics, data_collator=collate_fn, ) # 但是先不着急，我们先看看大模型 zero-shot 的效果如何 print(\u0026#34;Evaluating the base model without LoRA...\u0026#34;) zero_shot_results = trainer.evaluate(val_ds) print(f\u0026#34;Zero-shot accuracy: \u0026#34;, zero_shot_results) with open(\u0026#34;results.txt\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(f\u0026#34;Zero-shot accuracy: {zero_shot_results}\u0026#34;) print(\u0026#34;Training the model with LoRA...\u0026#34;) train_results = trainer.train() print(\u0026#34;Evaluating the LoRA model...\u0026#34;) LoRA_results = trainer.evaluate(val_ds) print(f\u0026#34;LoRA accuracy: \u0026#34;, LoRA_results) with open(\u0026#34;results.txt\u0026#34;, \u0026#34;a\u0026#34;) as f: f.write(f\u0026#34;LoRA accuracy: {LoRA_results}\u0026#34;) 最后就是训练的部分了\n我们只需把 model, datasets, criteria_matrix 和 tokenizer 输入进去就 ok 了\n剩下的 .fit() 和 .evaluate() 会帮我们完成\n最后我们就可以输出结果啦♪\nStep 8: Results origianl model trainable params: 85876325 || all params: 85876325 || trainable%: 100.00 LoRA trainable params: 667493 || all params: 86543818 || trainable%: 0.77 可以看到这里 LoRA 只会训练 0.77% 的参数\nZero-shot accuracy: {\u0026#39;eval_loss\u0026#39;: 4.615139007568359, \u0026#39;eval_model_preparation_time\u0026#39;: 0.0094, \u0026#39;eval_accuracy\u0026#39;: 0.01287128712871287, \u0026#39;eval_runtime\u0026#39;: 68.1035, \u0026#39;eval_samples_per_second\u0026#39;: 148.304, \u0026#39;eval_steps_per_second\u0026#39;: 1.16} LoRA accuracy: {\u0026#39;eval_loss\u0026#39;: 0.4589368402957916, \u0026#39;eval_model_preparation_time\u0026#39;: 0.0094, \u0026#39;eval_accuracy\u0026#39;: 0.8781188118811881, \u0026#39;eval_runtime\u0026#39;: 60.1694, \u0026#39;eval_samples_per_second\u0026#39;: 167.859, \u0026#39;eval_steps_per_second\u0026#39;: 1.313, \u0026#39;epoch\u0026#39;: 5.0} 但是 LoRA 的效果还是挺好的，准确率从 1.3% -\u0026gt; 87.8%, 提升还是非常明显的\n我这里一共训练了一个小时左右。\npytorch lightning 版本 然后，我看 openpi_pytorch 在用 pytorch lightning, 说是懒人最爱的 AI package, 我就想试试\n省流 总结，pytorch lightning trainer 爆杀了 huggingface trainer\n那么，代价是什么呢\n你需要多写一个 module class 和 dataset class\n甚至 20 行就能搞定\nStep 9: pytorch lightning dataset class 首先，在刚才的代码的基础上，我们先定义一个 dataset:\n# 这里定义一下 Dataset class Food101DataModule(pl.LightningDataModule): def __init__(self, train_ds, val_ds, collate_fn, batch_size): super().__init__() self.train_ds = train_ds self.val_ds = val_ds # collate_fn 相当于__get_item__ self.collate_fn = collate_fn self.batch_size = batch_size def train_dataloader(self): return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn, num_workers=64, pin_memory=True, prefetch_factor=4) def val_dataloader(self): return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn, num_workers=64, pin_memory=True, prefetch_factor=4) data_module = Food101DataModule( train_ds=train_ds, val_ds=val_ds, collate_fn=collate_fn, batch_size=args.per_device_train_batch_size, ) 和之前一样，对吧♪\n只不过现在要多写一个 class 而已，其他的都没有变哦♪\nStep 10: pytorch lightning model 然后，我们只需要再写一个 pytorch lightning model 就可以了\n# 定义一下 PyTorch Lightning 模型 # 注：self.log的部分是 wandb 的内容，用于导出log的 class Food101Module(pl.LightningModule): def __init__(self, model, learning_rate, metric, collate_fn): super().__init__() self.model = model self.lr = learning_rate self.metric = metric self.collate_fn = collate_fn # PyTorch Lightning Module 的 forward 方法 def forward(self, pixel_values): # Huggingface 的模型输出的时候会返回一个字典，包含 logits 和其他信息 # 这里我们只需要 logits return self.model(pixel_values).logits def training_step(self, batch, batch_idx): pixel_values = batch[\u0026#34;pixel_values\u0026#34;] labels = batch[\u0026#34;labels\u0026#34;] logits = self(pixel_values) # 这一句话等价于 logits=self.forward(pixel_values) loss = self.model.compute_loss(labels, logits) if hasattr(self.model, \u0026#34;compute_loss\u0026#34;) else F.cross_entropy(logits, labels) # self.log(\u0026#34;train_loss\u0026#34;, loss, on_step=True, on_epoch=True) return loss def validation_step(self, batch, batch_idx): pixel_values = batch[\u0026#34;pixel_values\u0026#34;] labels = batch[\u0026#34;labels\u0026#34;] logits = self(pixel_values) # also compute and log validation loss loss = F.cross_entropy(logits, labels) # self.log(\u0026#34;val_loss\u0026#34;, loss, on_epoch=True) preds = torch.argmax(logits, dim=1) acc = self.metric.compute(predictions=preds, references=labels)[\u0026#34;accuracy\u0026#34;] # self.log(\u0026#34;val_acc\u0026#34;, acc, on_epoch=True) def configure_optimizers(self): return torch.optim.AdamW(self.parameters(), lr=self.lr) lightning_module = Food101Module( model=lora_model, learning_rate=args.learning_rate, metric=metric, collate_fn=collate_fn, ) 也和之前一样，不过是要定义一下 forward 和 optimizer 而已\n我认为唯一要注意的点就是：huggingface dataset 会返回的不止是 logits, 还有别的信息\n所以 forward return 的时候只需返回 .logits 即可\nStep 11: train and validation pl_trainer.fit(lightning_module, datamodule=data_module) res = pl_trainer.validate(lightning_module, datamodule=data_module) 没有什么好说的，就是 .fit 和 .validate\nStep 12: 结果 对此，我只能说:\n遥遥领先\n只用了 200 秒钟就完成了1个 epoch + validation\n差不多是hugging face的4.5倍速度。\n只不过呢，像 wandb这些都要自己手动加，\n而 huggingface 就会自己 push 到所有检测到的 loggers\n总结 测试了一下 pytorch lightning 和 huggingface 的 LoRA 我发现 pytorch lightning 在 CPU IO 瓶颈的情况下的表现显著比 huggingface trainer 要好\n$$ \\text{huggingface train 1 epoch} \\approx 12 min $$\n$$ \\text{pytorch lightning train 1 epoch} \\approx 2 min $$\n$$ \\text{pytorch lightning validation} \\approx 8-10 s $$\n$$ \\text{huggingface validation} \\approx 60 s $$\n最后 validation 的结果都是差不多的,但是 pytorch lightning 就是比 huggingface tutorial 要快好几倍\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-11/","summary":"LoRA tutorial","title":"Bug Journal 2025-07-11"},{"content":"Huggingface Dataset 使用说明 load_dataset 函数 详情请见：Hugging Face 页面\nSplit参数 Hugging Face 的 load_dataset(..., split=...) 参数非常强大，下面是详细说明：\nsplit=\u0026quot;train\u0026quot;\n加载整个训练集。\nsplit=[\u0026quot;train\u0026quot;,\u0026quot;test\u0026quot;]\n返回一个元组 (train_ds, test_ds)，分别为训练集和测试集。\nsplit=\u0026quot;train+test\u0026quot;\n将训练集和测试集合并为一个数据集。\n切片（slice）支持\n你可以对拆分集按行号或百分比进行切片：\n行号切片 ：\nsplit=\u0026quot;train[10:20]\u0026quot; → 加载训练集第 10 至 19 条记录（10 包含，20 不包含）。\n百分比切片 ：\nsplit=\u0026quot;train[:10%]\u0026quot; → 前 10% 数据\nsplit=\u0026quot;train[-20%:]\u0026quot; → 最后 20% 数据\nsplit=\u0026quot;train[10%:20%]\u0026quot; → 从第 10% 开始到第 20% 结束\n组合切片 ：\nsplit=\u0026quot;train[:10%]+train[-80%:]\u0026quot; → 取前 10% 和最后 80% 数据拼接\n高级用法：交叉验证切片\nsplits = [ f\u0026#34;train[{k}%:{k+10}%]\u0026#34; for k in range(0,100,10) ] datasets = load_dataset(\u0026#39;bookcorpus\u0026#39;, split=splits) 生成 10 个 10% 验证切片，方便交叉验证\n更加高级的用法：ReadInstruction API\n更结构化方式定义切片，用于程序化构建：\nsplits = [ f\u0026#34;train[{k}%:{k+10}%]\u0026#34; for k in range(0,100,10) ] datasets = load_dataset(\u0026#39;bookcorpus\u0026#39;, split=splits) 类似作用，但写法更灵活。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-09/","summary":"\u003ch2 id=\"huggingface-dataset-使用说明\"\u003eHuggingface Dataset 使用说明\u003c/h2\u003e\n\u003ch3 id=\"load_dataset-函数\"\u003eload_dataset 函数\u003c/h3\u003e\n\u003cp\u003e详情请见：\u003ca href=\"https://huggingface.co/docs/datasets/v4.0.0/loading#slice-splits\"\u003eHugging Face 页面\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"split参数\"\u003eSplit参数\u003c/h4\u003e\n\u003cp\u003eHugging Face 的 \u003ccode\u003eload_dataset(..., split=...)\u003c/code\u003e 参数非常强大，下面是详细说明：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ccode\u003esplit=\u0026quot;train\u0026quot;\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e加载整个训练集。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ccode\u003esplit=[\u0026quot;train\u0026quot;,\u0026quot;test\u0026quot;]\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e返回一个元组\u003c/strong\u003e \u003ccode\u003e(train_ds, test_ds)\u003c/code\u003e，分别为训练集和测试集。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ccode\u003esplit=\u0026quot;train+test\u0026quot;\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e将训练集和测试集合并为一个数据集。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e切片（slice）支持\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e你可以对拆分集按行号或百分比进行切片：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e行号切片\u003c/strong\u003e ：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[10:20]\u0026quot;\u003c/code\u003e → 加载训练集第 10 至 19 条记录（10 包含，20 不包含）。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e百分比切片\u003c/strong\u003e ：\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[:10%]\u0026quot;\u003c/code\u003e → 前 10% 数据\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[-20%:]\u0026quot;\u003c/code\u003e → 最后 20% 数据\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[10%:20%]\u0026quot;\u003c/code\u003e → 从第 10% 开始到第 20% 结束\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e组合切片\u003c/strong\u003e ：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[:10%]+train[-80%:]\u0026quot;\u003c/code\u003e → 取前 10% 和最后 80% 数据拼接\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003e高级用法：交叉验证切片\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esplits\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;train[{k}%:{k+10}%]\u0026#34;\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e100\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edatasets\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eload_dataset\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;bookcorpus\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003esplit\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003esplits\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e生成 10 个 10% 验证切片，方便交叉验证\u003c/strong\u003e\u003c/p\u003e","title":"Bug Journal 2025-07-09"},{"content":"Huggingface Transformers 使用说明 详情请见：Hugging Face 页面\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-10/","summary":"\u003ch2 id=\"huggingface-transformers-使用说明\"\u003eHuggingface Transformers 使用说明\u003c/h2\u003e\n\u003cp\u003e详情请见：\u003ca href=\"https://huggingface.co/docs/transformers\"\u003eHugging Face 页面\u003c/a\u003e\u003c/p\u003e","title":"Bug Journal 2025-07-10"},{"content":"Torchvision 使用说明 详情请见：Torchvision 页面\ntorchvision.transforms Compose 这个函数的作用就是把几个 transforms 连到一起，有点像 nn.Sequential 那样\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-08/","summary":"\u003ch2 id=\"torchvision-使用说明\"\u003eTorchvision 使用说明\u003c/h2\u003e\n\u003cp\u003e详情请见：\u003ca href=\"https://docs.pytorch.org/vision/main/auto_examples/index.html\"\u003eTorchvision 页面\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"torchvisiontransforms\"\u003etorchvision.transforms\u003c/h3\u003e\n\u003ch4 id=\"compose\"\u003eCompose\u003c/h4\u003e\n\u003cp\u003e这个函数的作用就是把几个 transforms 连到一起，有点像 nn.Sequential 那样\u003c/p\u003e","title":"Bug Journal 2025-07-08"},{"content":"我现在的思路是这样的： 之前我们发现了 Libero 作为 benchmark 的一些可能的不合理的点\n数据太少 任务简单 没有长程任务 以及 DMPEL 作为 lifelong task 的一些可能的不合理的点\nlifelone task 的定义可能太过狭隘 (只在训练过的 task 上能够表现好) 方法可能聚焦于如何把 LIBERO 数据集的榜刷高，但是场景变化一点点就无法解决问题 所以我现在想做的是： 用一些 数字/分数 来证明我的猜想是成立的 比如：\n在场景变换的时候 DMPEL 成功率的下降 当任务变复杂的时候 DMPEL 成功率的下降 现在我已经做的是：\n对于开柜子的任务，无论是柜子移动，旋转，还是更换语义相同但是词语不同的句子，DMPEL 的成功率都会骤降 (100% -\u0026gt; 5%) 但是，开柜子的 task 可能过难了， 所以我现在准备测试 pick and place 的 task 当物品移动，旋转，以及更换语义相同但是词语不同的句子的时候 DMPEL 的成功率会下降多少\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-07/","summary":"\u003cp\u003e我现在的思路是这样的：\n之前我们发现了 Libero 作为 benchmark 的一些可能的不合理的点\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e数据太少\u003c/li\u003e\n\u003cli\u003e任务简单\u003c/li\u003e\n\u003cli\u003e没有长程任务\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e以及 DMPEL 作为 lifelong task 的一些可能的不合理的点\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003elifelone task 的定义可能太过狭隘 (只在训练过的 task 上能够表现好)\u003c/li\u003e\n\u003cli\u003e方法可能聚焦于如何把 LIBERO 数据集的榜刷高，但是场景变化一点点就无法解决问题\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e所以我现在想做的是：\n用一些 数字/分数 来证明我的猜想是成立的\n比如：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e在场景变换的时候 DMPEL 成功率的下降\u003c/li\u003e\n\u003cli\u003e当任务变复杂的时候 DMPEL 成功率的下降\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e现在我已经做的是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e对于开柜子的任务，无论是柜子移动，旋转，还是更换语义相同但是词语不同的句子，DMPEL 的成功率都会骤降 (100% -\u0026gt; 5%)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e但是，开柜子的 task 可能过难了，\n所以我现在准备测试 pick and place 的 task\n当物品移动，旋转，以及更换语义相同但是词语不同的句子的时候 DMPEL 的成功率会下降多少\u003c/p\u003e","title":"Bug Journal 2025-07-07"},{"content":"\n这是现在的成果，总之是让环境里的物品移动了位置了\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-30/","summary":"\u003cp\u003e\u003cimg alt=\"1751276858157\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-30/1751276858157.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是现在的成果，总之是让环境里的物品移动了位置了\u003c/p\u003e","title":"Bug Journal 2025-06-30"},{"content":"DMPEL \u0026amp; Libero 环境的总结和疑惑 在 DMPEL 模型 和 Libero 模拟环境的设定下，他们是这样设定他们的训练和验证集的：\n训练 train：\n首先，他们会在一个 90 条数据的大数据集上先做预训练，得到他们的 backbone 模型 然后对于4个 各 10 条数据的小数据集，他们会在这 4 个数据集上分别做训练，作为 lifelong learning training\n验证 validation：\n之后他们会对他们的训练做验证，验证方式如下： 对于第 x 个 lifelong learning task, 他们会计算训练这个 task 的时候的 10 个 epoch 的平均成功率，作为他们的成功指标 1 （希望模型学得越快越好） 他们还会计算前 x 个 task 的平均成功率，作为他们的成功指标 2 （防止模型遗忘）\n但是在这里我就产生了一个疑问🤔： 我认为这种设置并不非常合理\n他们没有对预训练的那 90 个 task 去做遗忘测试 他们的 task 的定义似乎有点太窄了，比如：都是对于“打开盒子的第一个盖子”， 盒子放在环境的左上角和左下角就算是两个 task,感觉泛化能力也不太行 在预训练的时候那 90 个 task 在设定的时候会比 lifelong task 复杂 (训练的时候一般有 3 个步骤， 但是 lifelong task 只有 1 个，比如“把碗放进抽屉并合上” VS \u0026ldquo;打开抽屉\u0026rdquo;)， 但是他们的模型在 Zero-shot lifelong task 的时候表现也很差，很少有成功的例子 在可视化结果的时候，我发现其实模型并没有理解要做什么，而是顺着之前的 task 做。 比如，上一个 task 做的是“抓起一个碗”，下一个是“打开柜子”，在一开始，机械臂会一直想去抓碗而不是开柜子 感觉 Libero 这个数据集的数据量也不是很大，难度也不是很高，(1-2 epoch 就可以让成功率 \u0026gt; 90%) 导致了像上面那样的 overfit 的情况 测试 于是我做了更多的测试：\n现在这是一个 Libero_long 的测试，任务难度会比之前更高\n目前这个任务是：“打开炉子并且把烧水壶放在炉子上”\n您的浏览器不支持 video 标签。 但是我们看到了：\n您的浏览器不支持 video 标签。 直接拿锅的机械臂\n您的浏览器不支持 video 标签。 没开炉子的机械臂\n您的浏览器不支持 video 标签。 摔倒了之后不知所措的机械臂\n之后我们再来看看所谓成功的 task\n您的浏览器不支持 video 标签。 放上去就是成功（\n最后，是真正成功的 task\n您的浏览器不支持 video 标签。 接下来该做的方向：\n首先，我认为 Libero 的设定不太合理\n所以我们不去刷 Libero 的榜\n我猜测已有模型的泛化能力不行，benchmark设置的不合理\n所以我要通过实验依据作证清楚，然后提出更合理的设置和新的评估benchmark\n最后在此基础上研究我的新方法\n那现在有这些方向可以走：\n如果我把任务分解和lifelong结合起来，分解primitive，直接跳出现在这些paper， 那就是很好的paper\n总结：\n这是我现在要做的事情：\n首先先研究 benchmark 设置，看看怎么样的设置能够把上面说的方向结合进来\n然后再看看现有的 method 在这些 setting 下表现如何\n最后看看我的方法比起这些方案有多少进步\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-26/","summary":"DMPEL \u0026amp; Libero","title":"Bug Journal 2025-06-26"},{"content":"Libero dataset 的可视化 发现了一个讲 Libero 讲得很详细的 blog 强烈推荐阅读这篇 blog！\n一共有 9 讲，9 讲的链接全部在这一讲中写了，直接点感兴趣的讲看就 ok 了\n但是，纸上得来终觉浅，绝知此事要躬行 有一些部分可能因为环境的不同不能适应所有情况，比如 headless server (没有显示输出的 server) 所以下面我会根据我的测试写针对 headless server 的 Libero dataset 可视化\n我的 Libero 测试报告 首先，第一个要注意的点就是：.libero 文件夹在 主目录下 (我认为此处说 根目录 下更准确) 也就是在 ~/ 下, 而不是 clone libero 下来的主目录。\n剩下的就很丝滑，所有的测试都平平无奇的通过了\n直到\u0026hellip;\n真正开始运行 Robosuit Demo 的时候，我遇到了困难：\n在运行完这段代码之后 python 就会直接崩溃：\nenv = suite.make( env_name=\u0026#34;Lift\u0026#34;, # try with other tasks like \u0026#34;Stack\u0026#34; and \u0026#34;Door\u0026#34; robots=\u0026#34;Panda\u0026#34;, # try with other robots like \u0026#34;Sawyer\u0026#34; and \u0026#34;Jaco\u0026#34; has_renderer=True, has_offscreen_renderer=False, use_camera_obs=False, ) 这是因为我正在用的服务器上并没有可以 render 的设备， 没法打开一个新窗口，所以就会直接崩溃\n因此这里要改成这样：\n# %% import numpy as np import robosuite as suite # MuJoCo gets imported *inside* robosuite # Camera frames are returned in the observation dict because we pass use_camera_obs=True env = suite.make( env_name=\u0026#34;Lift\u0026#34;, robots=\u0026#34;Panda\u0026#34;, has_renderer=False, # no on-screen window has_offscreen_renderer=True, # build a single off-screen context use_camera_obs=True, # include images in obs dict camera_names=(\u0026#34;frontview\u0026#34;,), # at least one camera is required when use_camera_obs=True camera_heights=480, camera_widths=640, ) obs = env.reset() # robosuite allocates context here # The observation dict now has an RGB image rgb = obs[\u0026#34;frontview_image\u0026#34;] # shape (480, 640, 3), dtype uint8 print(\u0026#34;Initial RGB image shape:\u0026#34;, rgb.shape) 下一步就是模拟机械臂操作，一共随机 1000 步：\n# %% frames = [] for _ in range(1000): a = 0.1 * np.random.randn(env.robots[0].dof) # small random torques obs, reward, done, info = env.step(a) frames.append(obs[\u0026#34;frontview_image\u0026#34;]) frames = [np.rot90(frame, k=2) for frame in frames] print(f\u0026#34;Collected {len(frames)} frames\u0026#34;) 但是从这里采集出来的图片是上下颠倒的\n这是因为 Mujoco 和 numpy 坐标系的原点位置不一样\n所以我们还得对这个 frame 做一个 180° 的颠倒\n现在，我们就可以输出我们的结果了：\n比如这是第一帧：\nimport matplotlib.pyplot as plt plt.imshow(frames[0]) plt.axis(\u0026#34;off\u0026#34;) 那至于我们要如何把这些帧变成一个视频呢？\n我们可以通过 imageio 这个 package 来实现\nimport numpy as np import imageio # \u0026#39;frames\u0026#39; is your list of (480, 640, 3) uint8 RGB arrays collected earlier # e.g., frames = [obs[\u0026#34;frontview_image\u0026#34;] for _ in range(1000)] # Create an MP4 writer at 120 fps writer = imageio.get_writer(\u0026#34;simulation.mp4\u0026#34;, fps=120) # wraps ffmpeg :contentReference[oaicite:6]{index=6} # Append each frame for frame in frames: writer.append_data(frame) # send numpy array to ffmpeg :contentReference[oaicite:7]{index=7} # Finalize and close the file writer.close() # flushes buffers and writes trailer :contentReference[oaicite:8]{index=8} 您的浏览器不支持 video 标签。 最终，我们通过 moviepy 来实现 jupyter notebook 中播放视频的效果：\nfrom moviepy.editor import VideoFileClip # 载入前面生成的 simulation.mp4 clip = VideoFileClip(\u0026#34;simulation.mp4\u0026#34;) print(\u0026#34;原始视频时长：\u0026#34;, clip.duration, \u0026#34;秒；分辨率：\u0026#34;, clip.size) clip.ipython_display(width=640) 在 DMPEL 中的可视化 那么至此我们可以发现：其实想做一个可视化非常的简单\n只需要找到创建 env 的地方，每一次出现 env.step 的时候我们就记录一下就可以了\n比如上面是\nobs, reward, done, info = env.step(a) frames.append(obs[\u0026#34;frontview_image\u0026#34;]) 那我们就也学着这里这样做就行了\n不过呢，有一个小区别：一个是这里是一个并行的环境\n更具体地说：为了保证 CPU 和 GPU 的利用率，DMPEL 设置了 20 个环境并行运行\n那么我们就要根据环境的不同设置不同的输出路径\n周游这个区别而已。\n另外呢，因为我想看一下这个 模型到底是在什么时候成功，又是在什么时候失败\n所以对于所有成功和失败的 task,我会分开两个文件夹存储\n好的，现在我们就完成了 DMPEL 的可视化\n现在，我们获得了一些结果：\n您的浏览器不支持 video 标签。 ❌ 完全打不开柜子的1号机械臂\n您的浏览器不支持 video 标签。 ✅ 犹犹豫豫打开正确柜子的2号机械臂\n您的浏览器不支持 video 标签。 ❌ 打开错误柜子的3号机械臂\n您的浏览器不支持 video 标签。 ✅ 干脆利落打开正确柜子的4号机械臂\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-25/","summary":"\u003ch3 id=\"libero-dataset-的可视化\"\u003eLibero dataset 的可视化\u003c/h3\u003e\n\u003ch4 id=\"发现了一个讲-libero-讲得很详细的-blog\"\u003e发现了一个讲 Libero 讲得很详细的 blog\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://blog.csdn.net/weixin_53610475/article/details/136421802\"\u003e强烈推荐阅读这篇 blog！\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e一共有 9 讲，9 讲的链接全部在这一讲中写了，直接点感兴趣的讲看就 ok 了\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e但是，纸上得来终觉浅，绝知此事要躬行\n有一些部分可能因为环境的不同不能适应所有情况，比如 headless server (没有显示输出的 server)\n所以下面我会根据我的测试写针对 headless server 的 Libero dataset 可视化\u003c/p\u003e\n\u003ch4 id=\"我的-libero-测试报告\"\u003e我的 Libero 测试报告\u003c/h4\u003e\n\u003cp\u003e首先，第一个要注意的点就是：.libero 文件夹在 \u003cstrong\u003e主目录下\u003c/strong\u003e (我认为此处说 \u003cstrong\u003e根目录\u003c/strong\u003e 下更准确)\n也就是在 \u003ccode\u003e~/\u003c/code\u003e 下, 而不是 clone libero 下来的主目录。\u003c/p\u003e\n\u003cp\u003e剩下的就很丝滑，所有的测试都平平无奇的通过了\u003c/p\u003e\n\u003cp\u003e直到\u0026hellip;\u003c/p\u003e\n\u003cp\u003e真正开始运行 Robosuit Demo 的时候，我遇到了困难：\u003c/p\u003e\n\u003cp\u003e在运行完这段代码之后 python 就会直接崩溃：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eenv\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esuite\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emake\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eenv_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Lift\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e# try with other tasks like \u0026#34;Stack\u0026#34; and \u0026#34;Door\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003erobots\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Panda\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# try with other robots like \u0026#34;Sawyer\u0026#34; and \u0026#34;Jaco\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ehas_renderer\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ehas_offscreen_renderer\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003euse_camera_obs\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这是因为我正在用的服务器上并没有可以 render 的设备，\n没法打开一个新窗口，所以就会直接崩溃\u003c/p\u003e","title":"Bug Journal 2025-06-25"},{"content":"这两天做了啥： Basically, 做了这些事情：\n发了一个issue Hi DMPEL team, thanks for releasing the code! I’ve managed to run the full lifelong-learning pipeline on the libero_goal benchmark, but the success rates I obtain are lower than those reported in the paper. I’d like to confirm whether I’m missing any important tricks or recommended hyper-parameter settings.\nReproduction details\nCommand: bash exp_scripts/lifelong_scripts/dmpel.sh:\ntorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth GPU: 1 x A100 CUDA / Driver: CUDA12.0, Driver Version 525.147.05 python: 3.8.13 package: same as requirements.txt seed: 100\nMy Results\nMethod Paper’s success rate (LIBERO-Goal, 10 tasks) Re-run success rate DMPEL ≈ 28 % (Figure 5, “Lifelong” column) \u0026lt; 5 % ISCIL ≈ 23 % (same figure) \u0026lt; 5 % Additional findings\nPre-trained checkpoint sanity-check The provided multitask checkpoint matches the paper on its pre-training tasks (good success rate, most of them over 85%). Questions\nIs there a specific environment configuration (e.g., in MuJoCo or Robosuite) that significantly impacts performance? Do specific seeds, task-order curriculum strategies, or “hidden” hyperparameters (beyond what’s documented) affect performance in a meaningful way? Could you advise on recommended approaches or tools for visualizing results (e.g., exporting the robot move to .mp4 files) to diagnose where the pipeline may be underperforming? 测试了一下 DMPEL 的结果 首先，要运行他们的pipeline, 有几个地方的 code 是一定要改的：\n一个是 libero/libero/main.py 69 行 for i inrange(n_manip_tasks): 这个 for 循环里面\n要把 try 中的 get_dataset 函数的 demos parameter 删除或者注释掉\n之后在下面 90 行左右有个 manip_datasets.append(task_i_dataset) 也要移动到 try 里面去\n不然的话这个 try except 根本就没有作用，该报错还是报错\n同样，在 libero/libero/dataset.py 这个文件中的 get_dataset 函数的 demos 参数也要删除/注释掉\n因为 robomimic.utils.dataset.SequenceDataset 里面根本就没有 demos 参数。\n这样的话，这份代码就可以正常跑起来了\nP.S. 运行用的代码是这一条：\nbash exp_scripts/lifelong_scripts/dmpel.sh: i.e. torchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth 我发现，诶，怎么结果这么差？\n然后我思考了一下🤔\n认为可能是 pretrain checkpoint 有问题\n所以我就去试了一下 pretrain checkpoint\n验证方式也非常简单，就是直接拿 pretrain checkpoint 去做 Libero_90, 也就是 pretrain checkpoint 训练的时候用的数据集\n这要怎么做呢？\n很简单，只要稍微改变一下运行的命令就可以了：\ntorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_90 \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/90 \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth 没毛病吧，一下就从 Libero_goal 变回 Libero 90 了\n结果非常的 Amazing 啊，发现这个算出来还挺正常的，绝大部分的成功率都在 80% 以上\n注：其实很多点都重合在一起了，能看到的都是 outliers 🤦‍♂️\n那就说明问题一定是出在 lifelong learning 这一步了\n所以我就测试了一下 其他的 lifelong learning methods.\n结果发现，除了 ER 有一点起色之外，其他 lifelong learning methods 都是 0% 成功率\n这就表明一定是某处的设定有问题了，但是我找了一天也没发现到底是哪里有问题\n所以我就写了上面那个 issue 给 DMPEL 的团队\n最后发现竟然是 train.epoch 被设置成了 0\n所以最后的结果就是 Pretrain model 在做 zero-shot!\n在此之前，我其实做了一个实验：如果我执意让 Pretrain model 做 Zero-shot, 确实得到了一个非常相近的结果\n但是我没有怀疑到 epoch = 0 上面来，因为在测试的时候有一个点干扰了我的判断：\n在 Pretrain model Zero shot test 中, GPU 是基本没有占用的\n但是在之前的 test 中, 哪怕 train.epoch = 0 也仍然是有一个训练的 epoch 的\n只不过那个训练出来的结果并没有被拿出来 validate\n或者说就算拿出来 validate 了效果也不算好\n因此这个点干扰了我的判断。\n最终，我得到了一个和论文中相近的结果：\n在这个lifelong learning训练过程中, 显存占用不超过 20GB, 所以只要是 3090 + 级别的显卡都能运行\n最终，这两天我终于成功跑通了 DMPEL 的代码\n下一步我准备可视化他们的代码，看看哪里做得好，哪里做得不好，所以我得去研究一下 Libero 数据集了。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-24/","summary":"\u003ch3 id=\"这两天做了啥\"\u003e这两天做了啥：\u003c/h3\u003e\n\u003cp\u003eBasically, 做了这些事情：\u003c/p\u003e\n\u003ch4 id=\"发了一个issue\"\u003e发了一个issue\u003c/h4\u003e\n\u003cp\u003eHi DMPEL team, thanks for releasing the code! I’ve managed to run the full lifelong-learning pipeline on the \u003ccode\u003elibero_goal\u003c/code\u003e benchmark, but the success rates I obtain are lower than those reported in the paper. I’d like to confirm whether I’m missing any important tricks or recommended hyper-parameter settings.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eReproduction details\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCommand:\nbash exp_scripts/lifelong_scripts/dmpel.sh:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        policy=bc_foundation_dmpel_policy lifelong=dmpel \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eGPU: 1 x A100\nCUDA / Driver: CUDA12.0, Driver Version 525.147.05\npython: 3.8.13\npackage: same as requirements.txt\nseed: 100\u003c/p\u003e","title":"Bug Journal 2025-06-24"},{"content":"WoW, 6 月 20 了呢，过得好快(✧∀✧)\n上次看到的时候还是 5 月初、5 月底，现在一下就 6 月 20 号了。\n算起来今天是来实验室的一个月了，该总结一下了。\n这个月看了很多和机器人、具身智能有关的文章，这是我目前对这个领域的总结：\n目标： 目标和做视频生成的其实差不多，都是根据一个条件(环境)生成下一部分东西(机器人 move) 这里说和视频生成差不多其实是因为视频生成的维度比文字生成要大得多，更加接近机器人 move 的维度 维度这个问题还是很恐怖的，因为机器人是在一个连续的三维空间中的，所以理论上来说有无限的自由度。 虽然说目前 FP8 的精度其实已经非常非常够用了，但你想，机器人的自由度非常高，至少有 6 - 7 DoF. 那如果每一个 DoF 都是 1000 个度量，那么一共就至少会有 $10^{18}$ 种不同的情况。 这就会让传统的 SoftMax 很难招架住了，因为这种情况得要一个 $10^{18}$ head 的 MLP 才行 所以这时候预测的值就不再是一个 softmax了，而是像 VAE 那样的一个 Distribution,只有这种 Distribution 才不会出问题 所以从目前我的角度来看，其实做机器人和做 LLM 完全就是一回事，唯二的区别是: LLM 很容易获得大量知识，许多 LLM 都能获得互联网级别的数据，而且现在的多模态技术使得几乎互联网上的所有数据都能被喂到 LLM 中。反观机器人，目前机器人在显示场景中的数据仍然非常依赖人工采集，因此数据不可能有非常多。当然，对于做学术的我来说这个不是问题，因为在学术界想要获得互联网级别的数据，和能够运行这么多数据的计算资源也是极为困难的。因此对于我们来说，其实做小数据大成果也许是一个更好的选择。 第二个区别其实就是把最后一层的 softmax 给他去掉，然后让两个 Distribution 做 loss. 学到的知识： 这是一个很大的一个板块了，但是我尽量不重不漏地把我学的全部写在这里。\n首先，目前有两个主流的机器人训练方法\n行为模仿 Behavior Cloning (下面简称 BC) 强化学习 Reinforcement Learning (下面简称 RL) 当然，也出现了这两个的有机结合，比如：先 BC 再 RL, 先 RL 再 BC, 动态分配 RL 和 BC 的比例。。。\n总之，一般来说，RL 更加难训练，BC 更加容易训练。RL 的效果上限更高，但是也更容易过拟合。\n另外，目前也有两个主流的机器人训练模型\n自回归 (Auto-regression) Diffusion policy (and/or Flow matching) 自回归理解起来比较容易，其实只要理解一个式子就行：\n$$ X_{t_i} = F(X_{t_{i-1}}, X_{t_{i-2}}, \\dots) = \\alpha_{t_{i-1}}X_{t_{i-1}} + \\alpha_{t_{i-2}}X_{t_{i-2}} + \\dots $$\n对吧，这就是自回归的意思：对于自己做回归，让所有前面的状态影响到后面的状态\n突然想起以前人们对 人生 和 DP 的总结：\n人生就像 DP, 不一定要走局部最优解，但要走全局最优解 但是我认为人生更像自回归，因为你并不能从其他状态转移过来，你只能从你之前的时刻回归到你现在的时刻。\n至于 Diffusion Policy, 他的思路也很简单：\n图片 from weijian.sun\n去马赛克。\n意思是：既然我们可以通过给一段数据不断加噪音得到一个完全噪音，那我们也可以通过一个噪音反过来还原一段数据。\n这个理念其实在很早就已经出现过了，不管是 GAN 还是 VAE 都用到了这个 Idea.\nThat is. 如果我们有一个函数从一个随机数映射到一个域中，这个域中的每一个数都是我想要的\n那每一次我就只需要随机生成一个数，经过这个函数之后就能得到我想要的东西了，有点点石成金的意思了。\n那 Diffusion 的关键是什么。其实还是数据。\n用 Diffusion 我很轻松就可以用一段数据弄出 1000 个数据，并且这 1000 个数据都是比较优质的数据：\n假设这段数据和噪声之间的差是$\\Delta$, 那每一次我只需要在这段数据上加上$\\frac{\\Delta}{1000}$不就创造了一个新的数据出来吗？\n那我现在要学的东西是：我让一个模型去预测这个$\\frac{\\Delta}{1000}$, 不就完事了吗。\n那为什么这里要设定 1000 步这么多呢，这是因为：模型在去噪的能力上其实并不是无敌的，而是有一个极限的\n这个极限在这里是马尔科夫的小方差近似。\n什么意思呢\n首先我们先回顾一下 Diffusion 在做什么\nDiffusion 的本质是在求解一个函数到另一个函数的映射\n在这里，所有的数值都是抽象的，是连续的，所以必须由一个函数来表示。\n但是计算机中的模型是离散的，所以我们要通过随机采样来近似。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-20/","summary":"Jornal","title":"Bug Journal 2025-06-20"},{"content":"今天真的是 journal 了\n因为其实没有看文章，只是在解决一些工程上的问题。\n首先是数据下载，这个下了好久才下下来\n而且下的时候还出了很多 bug, 但总之已经搞定了\n下一步就是要去把代码跑通，不过这个估计得明天才能弄了，毕竟这个数据还有 3 个小时才下完\n这个代码我看了一下他给的 pipeline,我发现：\n首先要跑一个 pretrain model 这个 model 不是 baseline,就是要 pretrain 之后让模型有个初步认知\n之后才能加 MOE 进去跑 continue learning\n我看到 hugging face 上面有他的跑好的 check point,到时候下下来看一下\n到时候有机会的话自己也跑一跑他的 pretrain 的那部分，积累一点经验。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-19/","summary":"Journal","title":"Bug Journal 2025-06-19"},{"content":"今天折腾了一天的数据下载，踩了一些坑，这里记录一下。\n这是一篇不错的笔记\n首先是从Hugging Face下载数据，目前看起来最稳定的最简单方式就是Git LFS clone.\n所以我们先来介绍一下Git LFS\n方法1：Git LFS 但是GIT LFS有时候会丢失一部分数据，不知道为什么\n比如我现在正在下载libero dataset, 但是他的 Git LFS clone 就只会下载 Libero 10, Libero Spatial, Libero Object, 和 Libero Goal, 并不会下载 Libero 100\n我认为是仓库的设置有点问题，因为 Git LFS clone 下来的数据格式和hugging face上下下来的格式有点不同。\n但是 Git LFS真的是最简单轻松的下载方式了\n只需要点击这里然后跟着操作做就完事了。\n下载的速度也比较快，基本跑满了这个服务器的代理网络。\n听说Github不能断点续传，但是实测下来几乎没有断点，真的是最稳定的那个服务了。\n方法2： hugging_face CLI huggingface-cli 是 Hugging Face 官方提供的命令行工具，自带完善的下载功能。\n但是实际用起来体验很差，经常莫名其妙就 Internet Error\n无论是使用代理还是镜像体验都不是很好。\n使用方法：\n代理：\npip install -U huggingface_hub pip install -U hf_transfer # 先下载 huggingface-cli 本体和 hf_transfer 加速插件 # hf_transfer插件真的很快，特别是在境外的服务器速度真的很快 export HF_HUB_ENABLE_HF_TRANSFER=1 # 打开 hf_transfer huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 镜像：\npip install -U huggingface_hub # 还是先安装这个 huggingface-cli export HF_ENDPOINT=https://hf-mirror.com # 这里以 hf-mirror.com 为例 # 剩下的都一样的 huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 方法3：snapshot_download 同样是 hugging face 出品，同样的容易崩溃\n区别是这个可以在 python 中使用\n使用也很简单：\nfrom huggingface_hub import snapshot_download snapshot_download( # repo_type=\u0026#39;dataset\u0026#39;, # 这一条就看你是不是下数据的时候选择加还是不加了 repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False, # 加上这一条可以所见即所得 # 不会出现最后是个指针文件的情况 ) 方法4: hf-mirror 镜像站下载 hf-mirror镜像站 推出了 hfd, 一个 huggingface 专用下载工具，基于成熟工具 aria2，可以做到稳定高速下载不断线。\n这是 hf-mirror 网站给出的 tutorial, 方法清晰简单: 1. 下载hfd\nwget https://hf-mirror.com/hfd/hfd.sh chmod a+x hfd.sh 2. 设置环境变量\n# Linux export HF_ENDPOINT=https://hf-mirror.com or\n# Windows Powershell $env:HF_ENDPOINT = \u0026#34;https://hf-mirror.com\u0026#34; 3.1 下载模型\n./hfd.sh gpt2 3.2 下载数据集\n./hfd.sh wikitext --dataset 根据实测，速度也不赖\n就是他的这个 Copy 有点问题，要一行一行的 Copy 才可以正常运行\n或者直接从我这里copy也行\n方法5: 手动下载 当然，最后的最后还可以再网站上手动下载。\n但是对于这个分了1629个断点的数据集来说，手动下载太痛苦了。\n所以此处暂且按下不表。\nAppendix 代理开启和关闭命令：\n开启：\nexport http_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export https_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTP_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTPS_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; 关闭：\nunset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY ","permalink":"https://tzj2006.github.io/bugjournal/2025-06-18/","summary":"Data Download","title":"Bug Journal 2025-06-18"},{"content":"对于这些任务，以下是所有任务都必须面对的问题：“灾难性遗忘” 简单来说，就是如何让模型学得又快又好？\n在这里，快是指：如何只用很少的样例 / 数据就可以让模型学到某一个任务\n好是指：如何让模型不会在学习新任务的时候忘掉旧任务\n那为什么会忘掉旧任务呢？\n其实是因为 SGD 太强了，毕竟在初始化的时候模型是一个随机化的值 而经过训练，模型就能“学会”这些动作。\n所以在学习新的任务的时候不刻意保留原本的值， 就会如同从随机模型学到一个新的动作一样，直接“遗忘掉”过去学到的知识， 变成只会新动作的机器人了\n而下面的大部分办法都是为了解决这个问题而存在的。\nLOTUs: Continual lmitation Learning for Robot Manipulation Through Unsupervised Skil Discovery 目标： 灾难性遗忘：神经网络在新的数据分布上训练时，往往会覆盖先前学习的知识，导致早期任务的性能下降。\n样本效率：真实世界的机器人学习受到数据收集高成本的限制，这使得需要大量数据的方法变得不切实际。\n任务复杂性：基于视觉的操作任务涉及复杂的感知和控制挑战，使得单片学习方法尤其困难。\n知识转移：在任务之间高效地转移知识（无论是向前还是向后）对于有效的持续学习至关重要，但难以实现。\n方法： 构建一个技能库，这个技能库是这个模型能够学习的新技能的上限 然后每次在获得一个新的数据的时候，模型会自动判断应该是更新旧的技能还是学习新的技能 最后通过模仿学习更新这个技能 那如何识别这个技能是旧的技能还是新的技能呢？\n答案是判断这个任务和之前的某个任务是否有 Sematic 上的相似\n这里是设定了一个阈值 $\\tau$, 如果这个新的数据的任务在经过一个 DinoV2 之后 和某一个技能的聚类的相似度超过 $\\tau$, 那就更新这个技能\n问题：如果多个任务相似性都超过了这个阈值，那么就更新一个相似度最高的任务\n结果 M2distill: Multi-modal distillation for lifelong imitation learning 目标 想要解决这些问题：\n潜在表征漂移：随着模型按顺序学习新任务，它们内部对先前学习任务的特征表征会逐渐变形。这意味着机器人对熟悉物体、空间关系和任务上下文的理解会随时间扭曲。\n动作分布不一致性：随着策略参数的更新以适应新技能，机器人对先前学习任务产生适当动作的能力会下降。\n方法 蒸馏\n啥是蒸馏?_?\n其实就是最小化 embedding 的差\n和谁蒸馏呢？\n其实就是和上一步做蒸馏\n啥意思呢。\n就是说：\n我保存了上一个任务训出来的参数，然后也保存了这个任务的参数 我希望在学习到这个新任务的基础上，这个参数的变化越小越好。\n结果 比上一篇Lutos好点\nFew-Shot Vision-Language Action-Incremental Policy Learning 目标 在少数据的情况下不遗忘之前学过的旧数据\n方法 引入了两个特殊的机制来解决这个问题：\n任务特定提示（TSP）：可学习的提示向量，与多模态输入数据交互，以从有限的演示中提取任务特定信息 持续演化策略（CES）：一种构建和利用任务关系图以在任务之间传递知识并减轻灾难性遗忘的机制 什么是任务特定提示呢？\n任务特定提示就是： $$ Z_p = \\text{MVTransformer}([Z_v, Z_l, P]) $$\n其中，$Z_v$表示视觉 tokens，$Z_l$表示语言 tokens，$P$表示任务 prompt。\n就是增加训练了这么一个 encoder 就可以提高 15%-17% 的成功率\n那什么是持续演化策略呢？\n持续演化策略就是： 首先，训练一个 base network, 这个 base network 在训练的时候会有 multi-task 这样的话，这个模型就可以有一个还不错的通用性能 然后，建立一个方法库，对于每一个新任务放入一个新的方法库中 对于每次训练的时候可以把之前学过的旧任务的权重加权平均，融入新策略中 这个权重是根据旧任务和新任务之间的相似性来的。 注：这里的任务库中的任务非常大 包含了一个完整的 action head.\n结果 Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning 问题背景与动机 当前的终身学习方法主要分为三类，每类都存在显著局限性：\n重放方法存储并重训练旧数据，表现良好但需要过多的内存和计算资源。对于涉及高维视觉数据的机器人操作任务，这种方法成本高得令人望而却步。\n正则化方法，如弹性权重整合（EWC），通过限制参数更新来保留旧知识，但在长任务序列中难以平衡可塑性-稳定性。\n架构方法，包括参数高效微调（PEFT）技术，创建任务特定模块，但面临两个关键问题：\n在测试时依赖预言机任务标识符，这在实际部署中不切实际 任务之间的知识隔离，阻碍了有效的前向迁移 DMPEL通过将PEFT的效率与动态专家选择和知识共享机制相结合，解决了这些局限性。\n方法 动态的专家模型: 这个保证了在只需要微调的基础上新学习到知识，同时不会忘记旧知识(因为之前学习到的参数没有变) 一个 novel 经验回放的方法: 文中的经验回放不是把之前的场景回放一遍，只回放了\u0026quot;如何选取专家\u0026quot;的权重。这样既做到了\u0026quot;回放\u0026quot;的效果，算力开销也不算大 上下文感知专家路由器：一个轻量级神经网络，它以多模态上下文（视觉、语言、本体感受）作为输入，并生成专家组合的系数，从而无需预言机任务标识符。 其他细节：\nEncoder 用的是 CLIP ViT-B/16 amazing\n对于每个新任务，DMPEL：\n使用正交初始化方式初始化一个新的低秩专家，以最大程度地减少干扰 在当前任务上训练该专家，同时冻结基础策略 任务完成后，将训练好的专家添加到专家库中 更新专家路由器，将新专家纳入动态组合中\n结果 在 FWT 上稍逊于 M2 Distill, 但是 AUC 远高于 M2 Distill, 差距 \u0026gt; 10%\nSPECl: Skill Prompts based Hierarchical Continual lmitation Learning for Robot Manipulation 目标 希望机器人能够持续学习并且适应新任务\n方法 SPECI 框架对机器人学习领域做出了几项重要贡献：\n新型分层 CIL 框架：SPECI 将多模态感知和融合、上下文感知技能推断和低级动作执行统一到一个有凝聚力的流程中。\n动态技能获取：可扩展的技能代码本支持隐式持续技能获取，无需手动技能抽象，从而促进有效的技能级别知识转移。\n增强的知识转移：模式近似通过使用特定任务和任务共享知识丰富策略来增强任务级别的知识转移。\n同样的可扩展的技能代码本：对于每个新任务，都会分配一个新的技能向量子集，而现有的技能向量则被冻结。这种方法允许持续扩展技能库。\n同样的注意力驱动的技能选择：一种机制根据技能向量与当前状态的相似性，选择前 C 个相关的技能向量，并通过加权求和将它们组合起来，从而形成合成的潜在技能。\n同样的时间 Transformer 架构：此组件旨在推断随时间的逻辑技能，从而捕获技能执行过程中的时间依赖性。\n区别是，这里不再用 LoRA,而是使用 $W = W_0 + ΔU * V^T$ 去微调\n结果 效果都比 DMPEL 差，特别是 LIBERO-GOAL 和 LIBERO-LONG\n图中其他模型效果和 DMPEL 中有些偏差\n推测是 DMPEL中的 ER 效果更好\n可惜 DMPEL 并没有测试没有 ER 的效果。\nIncremental Learning of Retrievable Skills for Efficient Continual Task Adaptation 目标 and 动机 IsCiL 的动机源于认识到终身代理，特别是家用机器人，必须在以下环境中运行：\n完整的专家演示昂贵且难以获取 任务频繁变化，没有明确的边界 隐私问题需要能够“遗忘”特定行为 不同任务之间的通用技能应被利用以提高效率 解决了三个主要挑战：\n收集全面专家演示要很高成本 在非平稳环境中进行鲁棒适应的需求 隐私问题 方法 当在新任务演示中遇到一个新技能时，做以下事情：\n原型初始化：系统在新技能数据处理时识别出最常检索到的现有技能原型。 适配器初始化：新技能的适配器参数通过复制最相似的现有技能适配器进行初始化。 适配器更新：使用模仿损失在新技能的演示上对初始化的适配器进行微调。 系统更新：新的技能原型及其适配器映射被添加到系统中。 结果 You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations 目标 双臂协作机器人训练\n该框架解决了当前双臂机器人学习方法中的根本局限。传统方法要么依赖预定义的动作分类法，限制了通用性；要么需要大量耗时且易受噪声影响的远程操作数据收集。YOTO通过从人类演示中提取丰富的时空信息，并将这个单一的教学实例快速增殖为适合现代模仿学习算法的多样化训练数据，从而弥补了这一差距。\n方法 手部运动提取与注入 提取模块通过几个复杂的处理步骤，将原始人类视频转换为机器人可执行的动作。\n3D手部轨迹处理：YOTO并未依赖可能不稳定的直接3D手部网格重建，而是采用了两阶段方法。系统首先使用WiLoR检测手部边界框并估计3D手部形状，然后将这些3D点投影到2D图像平面上，再通过立体匹配将其提升回3D。这种投影-提升策略显著提高了轨迹的稳定性和一致性。\n基于关键帧的动作表示：系统并未建模可能包含噪声和冗余的连续轨迹，而是提取对应于重要运动事件的稀疏关键帧——例如抓取器状态变化、速度极值或轨迹拐点。这使得动作空间维度从可能数百个连续帧减少到一组可管理的离散关键姿态。\n运动掩码生成：一个关键创新是运动掩码的推导，它编码了协调策略。这些二值掩码指示在每个关键帧时，每只手臂是应该移动还是保持，有效地捕获了双手任务的时间协调模式。这解决了双手操作中的一个核心挑战——学习手臂何时应该协作以及何时应该独立操作。\n演示增殖策略 增殖模块通过两种互补的方法解决了单次学习中的基本数据稀缺问题。\n自动回放验证：提取的关键帧具有高度可解释性和可编辑性，允许系统地修改物体姿态和属性。通过调整关键帧参数（例如物体的6自由度姿态或用相似类别的物体替换）并在真实机器人上执行这些修改，系统能够快速生成多样化、经过验证的演示。这种方法比传统的远程操作显著更快，同时保持了高数据质量。\n几何变换增强：系统对被操作物体的3D点云执行受控的几何变换——在机器人可达工作空间内的旋转和平移。这些变换自动生成关键帧动作的相应更新，创建了理论上无限的合成变体，而无需额外的机器人执行。\n这些策略的结合使得从单个真人演示中生成数百个多样化的训练示例成为可能，有效地弥补了单次示教与现代深度学习方法所需数据之间的差距。\n双手扩散策略 BiDP（双手扩散策略）代表了扩散模型在双手操作任务中的专门适应。\n以物体为中心的观察：BiDP并非处理完整的场景点云或RGB图像，而是专门关注任务相关物体的3D点云。这种设计选择减少了视觉噪声，加速了训练收敛，并通过移除不相关的环境变化提高了泛化能力。\n关键姿态预测：该策略预测离散的关键姿态而非连续动作序列，显著降低了扩散空间的维度并简化了学习问题。这种方法与提取阶段基于关键帧的动作表示自然契合。\n运动遮罩整合：运动遮罩在统一动作空间表示方面起着至关重要的作用。对于异步任务，遮罩能够将双手动作重组为时间有序的单臂动作序列，有效消除冗余的“保持”状态。对于同步任务，两只手臂始终保持活动。这种统一的表示允许单一的策略架构处理两种协调模式。\n底层扩散模型使用基于PointNet的修改编码器，具有SIM(3)等变性，用于处理点云观测，使其对尺度和位置变化具有鲁棒性。FiLM层提供条件机制，而卷积U-Net处理动作预测。\n结果 比较性能：BiDP在所有任务中取得了76.8%的平均成功率，显著优于包括ACT (5.7%)、标准扩散策略 (15.8%)、3D扩散策略 (19.4%) 和 EquiBot (23.4%) 在内的强大基线。这种性能差距证明了双手操作专门设计选择的有效性。\n消融研究：系统的消融实验验证了每个组件的贡献。从完整场景到仅对象观测的转变将成功率从24.1%提高到48.1%。稀疏关键帧表示进一步提高到51.9%。运动遮罩机制带来了额外的增益，而几何数据增殖显示出最显著的影响，将性能从61.1%提升至77.8%。\n泛化能力：对新颖、未见过对象的域外测试显示了BiDP卓越的泛化能力。尽管基线方法在分布外设置中表现出显著的性能下降，但BiDP保持了相对强大的性能（平均成功率为35.0%，而次优基线为8.8%），展示了强大的迁移能力。\nIn Praise of Stubbornness: The Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLM 目标 LLM是怎么出现“灾难性遗忘”的\n方法 和 结果 首先，检测大模型中是否出现了“不一致行为”\n什么是不一致行为呢\n就是说：大模型中有些行为是“新”的，有些是“收悉”的，而有些是“不一致”的。\n什么意思呢？就是说，在训练的时候我们可以从大模型的中间状态来判断：\n这个完全没学过，这个曾经学过，还是这个新学的和之前学的不一样。\n分析表明，结合激活值和梯度特征始终优于单独使用任何一种特征集。基于梯度的特征在微调模型中表现出更强的判别性，这可能是因为过拟合为熟悉信息与不熟悉信息创建了清晰的梯度信号。对于预训练模型，激活值和梯度都做出了显著贡献，这表明内部表示和学习动态对于区分信息类型都很重要。\n这篇文章指出：只要没有矛盾，LLM 就是稳定的\n否则，LLM 就会很不稳定\nJoint Flashback Adaptation for Forgetting-Resistant Instruction Tuning 目标 如何使用指令微调让LLM避免“灾难性遗忘”\n方法 核心创新在于在训练新任务期间引入有限数量的“闪回”——来自先前学习任务的提示。该框架通过几个关键机制运行：\n新任务学习：模型在新的任务数据上进行标准的监督微调，使用交叉熵损失（$L_{SFT}$），保持任务无关操作，无需显式任务识别。\n闪回整合：一小部分来自旧任务的提示作为“闪回”，无需其对应的标签即可使用。这些提示可以从验证集中采样、手动制作或由LLM合成。\n散度损失预防：一个关键组件通过散度损失（$L_{DIV}$）来防止遗忘，该损失衡量当前模型输出与参考模型在闪回提示上的输出之间的偏差。该方法采用双向KL散度来惩罚生成令牌分布的差异：\n$L_{DIV} = \\sum [KL(P_{ref} || P_{current}) + KL(P_{current} || P_{ref})]$\n组合优化：最终目标结合了两种损失：$L = L_{SFT} + \\alpha * L_{DIV}$，其中 $\\alpha$ 平衡了新任务学习与旧任务保留。PCGrad（投影冲突梯度）在优化过程中解决了竞争目标之间的冲突。\n为了解决闪回数据稀疏性并促进知识共享，JFA通过潜在任务表示整合了联合任务学习：\n潜在任务表示：系统维护潜在任务作为元组 ($e_j$, $\\Delta \\theta_j$)，其中 $e_j$ 表示任务编码，$\\Delta \\theta_j$ 包含通过LoRA实现的相应权重增量，以提高参数效率。\n动态知识检索：对于每个输入，系统使用预训练编码器（例如 RoBERTa）对其进行编码，并根据与冻结的正交潜在任务键的余弦相似度检索 k 最近邻潜在任务。\n参数重参数化：模型不是直接更新参数，而是通过添加检索到的潜在任务增量的加权平均值来动态重参数化有效参数：$\\theta\u0026rsquo;$ = $\\theta_{base}$ + $\\Delta \\theta_i$。\n联合优化：系统同时优化基础模型参数和潜在任务权重增量，从而实现跨不同输入和任务的共享知识的持续改进。\n结果 新任务性能：在SNI上，JFA在两种模型的所有指标上都取得了最高分。对于Llama3.1-8B，JFA得分43.72 (BLEU) 和51.45 (ROUGE-1)，显著优于SFT (41.50, 50.18) 和其他基线。\n旧任务保留：JFA在旧任务上始终提供顶级的性能。对于Llama3.1-8B，它在GSM-8K (83.09%) 和SVAMP (84.40%) 上取得了最高准确率，甚至超过了那些能够访问大量旧任务数据的方法。\n灾难性遗忘示例：简单的SFT在旧任务上表现出显著的性能下降（例如，Llama3.1-8B在GSM-8K上的准确率从82.79%下降到8.95%），清楚地表明了灾难性遗忘的严重性，并凸显了JFA的有效性。\nLibero: Benchmarking knowledge transfer for lifelong robot learning Please visit this file\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-17/","summary":"VLA 中的持续学习","title":"Bug Journal 2025-06-17"},{"content":"持续学习中避免灾难性遗忘：具身智能领域的研究进展综述 引言与问题背景 持续学习（Continual Learning，也称终身学习）指模型在数据分布和学习目标不断变化的情境下，能够连续学习新任务且不忘记已有知识的能力arxiv.orgar5iv.org。传统深度学习假设训练数据 i.i.d.且一次性可用，这在现实具身智能（如机器人、自主系统）中往往不成立ar5iv.orgar5iv.org。其中最大挑战之一是灾难性遗忘（catastrophic forgetting） ，即模型在顺序学习多个任务时，新知识的获取会导致旧知识被快速、大幅遗忘en.wikipedia.org。这一现象最早由 McCloskey 和 Cohen 于1989年在联结主义神经网络中发现arxiv.org，体现了所谓 稳定-可塑性权衡 ：模型既要对新信息足够可塑（plasticity），又要对既有知识保持稳定（stability）en.wikipedia.org。与生物神经系统相比，人工神经网络在顺序学习时更容易“灾难性”遗忘过去经验，而人类和动物通常表现出渐进、有选择的遗忘cell.comcell.com。如何在保持模型泛化能力的同时避免旧知识被破坏，成为持续学习研究的核心问题cell.com。为此，大量研究围绕不同技术路线展开，包括利用附加数据回放、正则化约束、动态模块化架构和外部记忆等方法来缓解遗忘arxiv.org。本文将按时间脉络梳理持续学习避免遗忘的关键进展，重点聚焦具身智能场景的应用，并比较不同方法在这些场景中的优劣差异。\n早期探索：灾难性遗忘的提出与初步对策 灾难性遗忘现象的提出（1980s–1990s）: 神经网络中的灾难性遗忘问题由 McCloskey 和 Cohen (1989)en.wikipedia.org以及 Ratcliff (1990) 首次严谨描述en.wikipedia.org。他们发现序列训练两个任务时，后学任务会显著干扰先前任务的记忆en.wikipedia.orgen.wikipedia.org。这一问题可视为Grossberg提出的稳定-可塑性两难的极端表现en.wikipedia.org。此后，研究者逐步意识到，若要让人工智能具备人类般持续学习的能力，必须解决神经网络的遗忘灾难。1990年代中期，一些学者开始探索初步对策。例如，Robins (1995) 提出了重演/伪重演 (rehearsal/pseudorehearsal) 方法，即在学习新任务时将旧任务样本或由模型生成的“伪样本”一并训练，以巩固旧知识ar5iv.org。这种方法模拟了生物大脑在睡眠中重放记忆的过程，缓解了遗忘问题，被视为后续生成式回放方法的雏形ar5iv.orgar5iv.org。与此同时，Grossberg 等人在稳定-塑性理论指导下发展**自适应共振理论 (ART)**网络，通过网络结构与记忆单元的设计减少旧记忆被覆盖的风险。这一时期的工作揭示了灾难性遗忘的严重性，并奠定了若干基本思想：通过保留过去经验（真实或模拟）或限制参数剧烈更新来保护已有知识cell.com。然而，由于当时神经网络规模和应用场景有限，这些早期方法未形成统一框架，但为后续深度学习时代的持续学习研究提供了宝贵思路。\n深度学习时代兴起前的持续学习理念: 在2000年前后，机器学习领域也出现了一些“终身学习”思想，例如 Thrun 和 Mitchell 等人在机器人领域讨论让机器人不断积累知识、自主适应新环境的算法。但受限于模型能力，这些工作多偏向理论构想或特定场景下的增量学习算法。值得一提的是，Silver et al. (2013) 等人在认知科学领域提出**“永不停歇学习”(Never-Ending Learning)的愿景，旨在构建能无限获取新知识的AI系统。这些理念上的探索进一步强调了持续学习的重要性，但真正有效的算法突破还要等待深度学习的成熟。进入2010年代，深度神经网络在图像、语音等任务上取得突破，但其遗忘现象依然明显**。Goodfellow et al. (2014) 的实证研究表明，即使现代深度网络在顺序学习多任务（如不同MNIST变换）时，仍然发生严重的性能遗忘，他们尝试用Dropout等正则策略略微缓解遗忘cs.uic.edu。这一阶段的研究重新量化了深度模型遗忘的程度，引发了学界对持续学习的关注，也为随后的关键方法发明做好了铺垫。\n深度学习时代的方法演进（2016–2020） 进入深度学习时代后，大量持续学习算法被提出。总体而言，这些方法可分为以下几类： 参数正则化 、 经验回放 （或生成回放）、 参数隔离/模块化 、以及外部记忆等arxiv.org。各类方法都有经典代表工作，我们按时间演进介绍主要方法及其贡献。\n参数正则化方法 Learning without Forgetting (LwF, 2016): Li 和 Hoiem 提出“学习不遗忘”算法arxiv.org。他们假设只能获取新任务数据，无法重温旧任务数据的现实情况，通过让模型在学习新任务时蒸馏(distillation)旧任务模型的输出分布来正则化模型参数变化arxiv.org。具体而言，在训练新任务时对旧任务模型的预测进行保持，使新模型尽可能产生与旧模型相似的输出，从而保护原有能力。LwF是知识蒸馏用于持续学习的开创性工作，实现了只用新任务数据也能较好保留旧任务性能arxiv.org。该方法在2016年ECCV发表，此后蒸馏正则化成为持续学习的重要手段之一。\nElastic Weight Consolidation (EWC, 2017): Kirkpatrick 等人（DeepMind）在PNAS 2017发表了著名的EWC算法arxiv.org。EWC通过近似计算每个参数对旧任务的重要程度（利用费舍尔信息矩阵对参数敏感度进行估计），在学习新任务的损失中添加项，惩罚对重要参数的大幅更新arxiv.org。直观来说，模型会“放慢”那些对旧任务重要参数的学习速率，以免遗忘旧知识arxiv.org。EWC在经典分类任务（如顺序MNIST）和强化学习任务（顺序Atari游戏）上验证了有效性arxiv.org。作为持续学习领域里程碑，EWC证明通过软约束参数更新，可以在一定程度上同时保持先前任务性能和新任务学习能力arxiv.org。其思路后来衍生出许多变体，例如利用更精细近似二阶信息的方法等ar5iv.org。\nSynaptic Intelligence (SI, 2017): Zenke 等人在ICML 2017提出了另一本质类似EWC的正则化方法SIarxiv.org。不同于EWC预先计算参数重要度，SI在训练过程中实时累积每个参数对损失的贡献度，结束当前任务时将其视为该参数的重要性arxiv.org。这种“智能突触”机制借鉴了生物突触强度调节的复杂性，每个突触（参数）在多个任务中累积“任务相关信息”，新任务来时利用这些信息调整学习率arxiv.org。SI在多个连续分类任务上显著降低了遗忘，同时保持了计算高效arxiv.org。与EWC相比，SI无需存储旧任务样本，同样不增加模型容量，对于资源受限的设备具有吸引力arxiv.org。但正如多数正则化方法的局限，当任务数量增多或差异较大时，单纯靠增加惩罚会导致模型学习新任务受限，需要在稳定与塑性间权衡ar5iv.orgar5iv.org。\n其他正则化进展: 除上述，2017年前后还出现了多种参数正则化方案。例如 Li et al. (2017) 的增量时刻匹配（IMM）方法通过匹配参数分布的方式融合旧新任务模型；Aljundi et al. (2018) 提出的MAS（Memory Aware Synapses）利用输出敏感度评估参数重要性。这些方法本质均为在损失函数中添加某种形式的正则项，使模型避免过度调整关键参数以保留旧知识ar5iv.orgar5iv.org。正则化方法的优点是 无需存储旧样本，内存占用低 ，易于在嵌入式/机器人等设备上实现ar5iv.org。它们的不足在于当任务间差异巨大、参数冲突严重时效果受限，而且累积过多任务后模型可能进入“过度稳定”状态难以学习新任务ar5iv.org。在具身智能场景中，由于设备算力和存储有限，正则化方法仍是常用选择之一。例如 EWC 被用于机器人连续控制任务以保护低层政策参数不被遗忘arxiv.org。但如果机器人遇到全新领域任务，正则化可能限制其适应新技能的能力，这是后续方法力图解决的问题。\n经验重放与生成式回放方法 显式经验重放 (Experience Replay): 针对持续学习，最直接的思路是 在学习新任务时重温部分旧任务的数据 ，仿佛让模型“复习”以前的知识ar5iv.org。Rebuffi 等人在CVPR 2017提出的 iCaRL 方法将这一思想与深度学习结合arxiv.org。iCaRL在每学新类别时， 保存每个旧类别少量代表样本（记忆库） ，训练时将这些旧样本与新数据一起用于更新模型，并对模型输出进行知识蒸馏以防决策边界偏移arxiv.org。通过同时学习分类新类别和回顾旧类别，iCaRL实现了深度网络在长时间增量学习许多类时，比仅新数据训练的策略遗忘显著减少arxiv.org。iCaRL开创了样本记忆回放+蒸馏结合的范式。之后许多增量学习方法沿用了“小样本记忆”思路，如 Hou et al. (2019) 的UCIR、Wu et al. (2019) 的BiC等，都在如何精选和高效利用少量旧样本上下功夫。经验重放策略也直接应用于强化学习领域——在非平稳环境中，智能体可将过去经历的轨迹保存一部分，在策略更新时混入重放，以避免策略完全偏离先前成功经验。这与深度Q网络（DQN）的经验回放缓冲理念一脉相承，只是这里目的是防遗忘而不仅是破除相关性。在机器人学习中，经验重放意味着机器人在学习新技能时定期练习已掌握的技能（通过模拟旧技能的传感器输入等），这在实践中提高了多技能机器人系统的鲁棒性。\nGradient Episodic Memory (GEM, 2017): Lopez-Paz \u0026amp; Ranzato 在 NeurIPS 2017 提出的 GEM 则进一步创新了重放的使用方式arxiv.org。与直接将旧样本混入训练不同，GEM把少量旧任务样本存入 episodic memory ，在每次参数更新时，通过约束新梯度与旧样本梯度的内积为非负，确保新任务训练不会增加旧任务损失arxiv.org。这种基于优化约束的方法保证了模型对记忆样本性能不下降，实现了一定的“向后迁移”能力：在学习新任务的同时还有可能改进旧任务表现arxiv.org。GEM开创了利用回放样本的梯度信息指导优化的思路，其后续简化版A-GEM (Chaudhry et al. 2018)降低了计算成本。对于具身智能而言，GEM这类方法的优势在于即使少量记忆也能通过优化约束起效，而且不需要明确任务边界（可以对任意过去经验施加约束）。不过其劣势是需要实时计算并存储梯度，复杂度较高，且仍需维护一个小型记忆库。\n生成式回放 (Generative Replay): 当直接存储原始旧样本受限时，另一策略是训练生成模型来产生日前学过的数据，从而实现回放。Shin et al. 在 NeurIPS 2017 提出的 Deep Generative Replay (DGR) 是该思路的里程碑arxiv.org。DGR构建了一个生成模型（如GAN或VAE）作为“记忆仿真器”，在学习新任务时利用生成模型产生日前各任务的合成样本，并与新数据混合训练Solver模型arxiv.org。在每完成一个任务后，Solver的参数固定，然后训练生成模型去拟合更新后的Solver分布，以便下次产生更新的数据分布ar5iv.org。这种双模型协同框架受到大脑“海马-新皮层”互作机制的启发，即利用快速变化的“海马体”生成回忆来训练慢更新的“皮层”网络arxiv.org。DGR实验证明，即使不保存任何真实旧样本，模型仍能通过生成模拟数据达到与有存储时相近的效果ar5iv.orgarxiv.org。生成回放的优势是 不直接占用存储真实数据 ，在隐私敏感或内存极小的设备上尤为有用arxiv.org。其缺点在于生成模型本身也面临持续学习问题（如何不忘记早期的数据分布），且训练开销较大ar5iv.org。后续不少工作改进了生成回放，如 Wu et al. (2018) 将GAN与变分特征结合（MeRGAN），Rios et al. (2018) 用生成对抗网络生成特征而非像素，提高效率等ar5iv.orgar5iv.org。生成回放还被应用到强化学习的状态生成中，例如 Caselles-Dupré et al. (2019) 提出的自触发生成回放（S-TRIGGER）用于连续学习环境状态表示ar5iv.org。总的来看，回放类方法（包括经验重放和生成回放）在各种基准上往往表现突出，被认为是目前抗遗忘最有效的范式之一。然而它们对存储或生成能力有要求，在具身智能中需权衡内存/算力和性能：对于机器人等设备，存储少量关键经验（如图像片段、关键帧）进行回放在实践中较常用，而实时训练复杂生成模型则相对少见。\n模块化架构与参数隔离方法 Progressive Neural Networks (PNN, 2016): Rusu 等人提出的渐进神经网络是持续学习的架构派代表arxiv.org。PNN在每遇到新任务时 冻结已有网络 ，并侧旁新增一组“列”网络用于学习新任务arxiv.org。同时，通过旁路连接让新任务列能够利用之前各列学到的特征（实现知识迁移）arxiv.org。这种架构确保旧任务的参数永不修改，从而彻底避免遗忘arxiv.org；而新增模块可以专门学习新任务，有充足的模型容量。PNN在Atari游戏和3D迷宫导航等一系列强化学习任务上取得优于微调的成绩，并显示出显著的前向迁移能力arxiv.org。其缺点也很明显：每增加一个任务网络规模就线性增长，在任务数很多时不切实际arxiv.org。尽管如此，PNN证明了模块隔离在避免遗忘上的有效性，许多后续方法受此启发引入可扩展或可选择激活的架构。\nPathNet (2017) 与 PackNet (2018): 为了缓解PNN网络爆炸的问题，Fernando 等人 (2017) 提出的 PathNet 利用进化算法在固定网络中为每个任务选择一条互不干扰的子网络路径，相当于在共享参数的前提下实现参数隔离。Mallya 和 Lazebnik (2018) 则提出 PackNet ，通过反复剪枝和重训练来为新任务腾出参数空间arxiv.org。具体来说，PackNet先训练初始任务模型，然后剪除一定比例不重要的参数（权重置零但保留位置），学习第二个任务时仅利用空闲参数；如此迭代，将多个任务“打包”进单个网络中arxiv.org。实验表明，在ImageNet等大型数据上，PackNet可在一个VGG模型中连续容纳多个细粒度分类任务，性能接近于单独训练arxiv.org。PackNet无需存储旧数据，也不引入新参数，因此相比PNN更高效arxiv.org。但PackNet需要预先设定剪枝比例，且剪枝过多可能损害旧任务性能，过少则限制新任务空间。后来一些变体如 “Piggyback” (Mallya, 2018) 则改为学习任务特定的掩码，更灵活地实现参数复用。总体而言，参数隔离类方法（含动态扩张和网络剪枝）通过结构上的硬约束避免了遗忘，其优势是旧知识完全保留、无干扰arxiv.org。在机器人等具身智能中，如果任务集是离散且有限的，这类方法可考虑使用。例如在多任务机器人控制中，可为每个任务分配专属网络模块或参数子集，新任务加入时扩展网络并冻结旧模块，从而保持以往技能arxiv.org。然而，在开放环境下任务可能连续涌现且无法预知数量，单纯无限扩展网络不切实际。因此近期一些工作尝试结合元学习或 条件网络 ，自动决定何时复用旧参数、何时增加新参数，以兼顾模型规模和遗忘防护。比如 Serra et al. (2018) 提出的 HAT 方法对每层参数学习可训练门控，通过门控向量的稀疏化实现在相同网络中隔离不同任务的激活区域，从而在不显著增加参数的情况下减少干扰。\n脑启发的双记忆体系: 值得注意的是，一些方法从神经科学的双重内存理论汲取灵感，将快速学习模块和稳定长时模块结合起来应对遗忘。例如 Kemker 和 Kanan (2018) 提出的 FearNet 模型采用“大脑 海马-新皮层 ”的架构arxiv.org：用一个类似海马体的小网络专门快速学习当前任务，并在适当时机（模拟睡眠）将新知识整合（consolidate）到另一个类似皮层的大网络中做长期存储arxiv.org。同时还有一个类似杏仁核的模块，根据输入判断应该用哪套记忆系统回答arxiv.org。FearNet不需存储旧样本，依赖生成式机制回忆旧类数据，达到与iCaRL相当的性能arxiv.org。这类方法实质上属于架构+回放的混合策略（因为短期网本身可看作一种内生记忆生成器）。双记忆策略对具身智能有自然的意义：机器人或代理可以配置一个“小而快”的在线学习器来及时适应新变化，同时定期将知识固化到“大而稳”的长期模型中，从而两全其美。不过如何确定巩固频率以及双网络的容量匹配仍在探索中。\n方法对比与小结 不同持续学习方法各有优劣，在具身智能场景下需要平衡选择ar5iv.org。正则化方法不需保存样本、开销低，适合嵌入式设备在线更新，但在任务变化剧烈时可能束缚新知识获取ar5iv.org。回放方法往往效果最佳，即便少量样本重放也能显著降低遗忘arxiv.org；对于机器人这种可反复与环境交互的场景，还可通过自主采样过去环境状态进行重演。然而存储真实数据可能受限于隐私或容量，而训练生成模型又对计算资源有较高要求arxiv.org。模块化/参数隔离方法彻底杜绝了遗忘，在多任务机器人系统（任务有限且可拆分）中很有价值，但在开放任务中扩展性受限arxiv.org。外部记忆和双重内存策略提供了一种折中：通过引入专门的记忆模块，模型可以在不反复调整主要网络权重的情况下查询和更新知识。例如在多人对话交互机器人中，引入一个可读写的记忆单元存储历史对话要点，有助于长期一致的对话理解。但引入记忆也增大了系统复杂度，需要设计高效的检索和写入机制。\n此外，许多先进方法不再局限于单一策略，而是混合多种机制以取长补短ar5iv.org。例如 Schwarz 等人 (2018) 提出的 Progress \u0026amp; Compress 框架将动态架构与蒸馏结合：使用Progressive Network扩展新任务列，然后通过蒸馏将新列知识压缩回主干网络，从而既避免遗忘又控制模型规模arxiv.org。再如 von Oswald et al. (2019) 的 MER 方法将元学习思想融入记忆重放，通过元训练提高模型表示对新旧任务的解耦，从而辅助减少干扰。这些综合方法在近年不断涌现，说明持续学习领域正朝着多策略融合与自动适应方向发展。\n具身智能场景中的持续学习应用 具身智能领域（如机器人、自主车辆、智能代理）为持续学习提供了最实际也最具挑战的用武之地arxiv.org。与静态数据集不同，具身智能体在物理世界中连续感知和行动，环境非平稳且任务边界往往不明确ar5iv.orgar5iv.org。以下我们重点考察持续学习方法在几个具身场景的应用进展：\n机器人视觉与物体识别： 服务机器人需要在不断变化的环境中识别新对象、适应新场景，这正是开放域的持续学习问题。为评测算法，2020年提出了OpenLORIS-Object机器人视觉数据集，包含随时间推移环境光照、视角、物距等变化的数据流sciencedirect.com。在该数据集上，Lomonaco 等人组织了持续学习挑战，促进了算法在真实机器人感知条件下的比较。一系列方法被测试：如 iCaRL 的小样本存储结合知识蒸馏策略在这种增量物体识别中取得稳健表现；又如 IROS 2020 的 Latent Replay 方法，Pellegrini et al. 提出只在特征空间保存和重放旧数据link.springer.com。具体而言，机器人摄像头图像经卷积网络得到中间表示，将这些低维激活缓存代替原始高维图像，可大幅减少存储并实现实时回放ieeexplore.ieee.org。实验表明，在OpenLORIS这种持续视觉任务中，Latent Replay比直接存图像几乎不降性能，却更高效满足机器人实时性需求ieeexplore.ieee.org。另一最新进展是 Hajizada et al. (2024) 提出的 Continually Learning Prototypes (CLP) 算法arxiv.orgarxiv.org。CLP针对机器人少样本在线学习和开放世界场景设计：它采用原型向量表征每类知识，并通过元可塑性机制动态调整每个原型的学习速率来平衡新旧知识稳定性arxiv.orgarxiv.org。同时CLP具备新类别自我检测与无监督学习能力（即机器人遇到未知物体时可判断新类别并自主创建原型学习）arxiv.orgarxiv.org。重要的是，CLP不使用任何显式回放数据且兼容神经形态芯片，实现了超低能耗下的持续学习arxiv.orgarxiv.org。这对于内存和电池有限的移动机器人具有现实意义。总的来看，在机器人视觉领域，混合使用 小样本记忆 、 特征回放 、适应性学习率等技术已取得显著效果，使机器人能逐步扩展认知能力且遗忘受控。\n人机交互与多模态学习： 具身智能体常涉及多模态感知（视觉、听觉、语言）和人机交互，这带来了持续学习的新课题。例如社交机器人需要持续学习新的对话内容、新的手势动作等。NLP领域已有针对增量学习的综述（如 Biesialska et al. , 2020link.springer.com），其中提到自然语言处理任务在持续学习中面临词汇和语义随时间演变的问题。一些方法通过动态扩充词典或嵌入空间缓解了“遗忘”早期语义的现象。对于多模态交互，Kulkarni et al. (2019) 提出在对话系统中使用弹性权重约束来保留模型早期对话技能，同时新增新领域对话意图。交互学习中一个重要方面是 用户在环（human-in-the-loop） ：机器人可通过用户反馈实时修正知识。近期有工作探索 交互式持续学习 ，如 Hazifa et al. (2022) 结合神经形态计算，利用片上在线学习快速吸收用户教授的新知识，同时通过正则保护已有知识dl.acm.org。虽然具体算法仍在早期，但这些尝试指出了方向——未来的具身智能体应能通过持续人机交互 自我进化 ，并且做到“学而不忘”。\n连续控制与强化学习： 在自主驾驶、机器人控制等连续决策场景，持续学习同样关键。例如自动驾驶车辆遇到新道路场景，需要学习新策略而不忘记基本驾驶技能。Shaheen et al. (2022) 的综述link.springer.comlink.springer.com总结了三类自主系统（无人车、无人机和移动机器人）中的持续学习挑战：模型需在在线方式从大量顺序数据中学习，且资源受限、须保障安全稳定link.springer.comlink.springer.com。一些研究采用策略蒸馏或迁移学习避免遗忘旧任务策略。如 Rusu et al. 在DeepMind的机器人实验中，用渐进网络将仿真训练的技能迁移到现实机器人上，同时保持仿真技能不丢失arxiv.org。又如 Traoré et al. (2019) 提出的 DiscoRL 框架，将旧策略压缩为策略库，再用Policy Distillation（策略蒸馏）技术在新环境中融合旧策略以加速学习，同时旧策略作为教师防止遗忘ar5iv.org。在连续控制中，策略往往以神经网络表示，类似分类任务的遗忘也会发生：新环境下调整策略网络，会导致旧环境下性能下降。为此 Rolnick et al. (2019) 提出的 CLEAR 方法，将off-policy经验重放引入强化学习的策略梯度训练，既提高新任务样本效率又维持旧任务价值函数不变。该方法在Atari游戏顺序学习中取得好结果，被视为强化学习领域对抗遗忘的有效方案之一。需要强调的是，强化学习场景中任务界限往往模糊，甚至代理可能在一个不断演变的环境中持续学习（如运营多年的家庭服务机器人，会不断遇到新任务）。这接近无任务标签 (task-agnostic)的持续学习。Aljundi et al. (2019) 针对此提出了在线持续学习方案：通过检测网络对新数据的干扰程度动态触发记忆重放（MIR）link.springer.com，以及使用梯度稀疏化挑选对旧任务干扰最大的记忆样本来更新，从而在无明确任务边界下也能抑制遗忘。此类方法在机器人持续感知与导航中具有潜力，因为现实中机器人很难知道自己何时“切换了任务”，只能根据环境变化连续调整。\n开放世界和自主适应: 具身智能体经常处于开放世界，可能遇到训练时未见过的全新情况。持续学习的终极目标是在这种开放环境中实现持续适应而不崩溃。Open-world持续学习需要综合上述技术，还涉及新知识的自主发现和 主动学习 。比如前述CLP方法引入了新类检测机制，让机器人在开放世界下识别何时需要学习新对象arxiv.orgarxiv.org。又如 Mundt et al. (2020) 探讨了结合异常检测和持续学习，使模型在检测到输入分布偏移时能触发新任务学习流程。对自主车而言，面对从未见过的道路情况（极端天气、新施工区域），如果能自动检测出“新情境”并调用持续学习模块更新模型，将大幅提高安全性。当然，这也带来安全约束下的学习稳定性问题，需要确保新学习不会在尚未充分验证时投入决策。近期一些研究主张引入 不确定性估计 （如Bayesian NN）判断模型何时需要学习新任务，以及学习后的性能变化link.springer.com。这些探索尚属前沿，但对于真正长期自主运作的智能体至关重要。\n近年新进展（2020–2025）与展望 过去五年中，持续学习领域涌现了一系列新趋势和方法，进一步提高了模型在复杂环境中的持续适应能力：\n任务无关持续学习: 越来越多工作关注在无明确任务边界、数据连续流动场景下的学习link.springer.comlink.springer.com。这更贴近现实中的机器人/代理感知流。为此，方法上强调在线更新、有限内存和即时评估。例如 2020 年的 GDumb 方法提出一种极端简单但强大的baseline：始终只训练当前模型在收到的全部数据上（存储一定量最近数据），每次新数据到来直接从头训练。这种方法虽谈不上高明，却在一些线上学习赛道表现接近更复杂的方法，提示我们需要重新审视评价指标。在可预见的将来， 线上持续学习 （Single-Pass Continual Learning）将成为研究热点，它要求算法一次遍历数据且不泄露未来信息，在边学习边推理的同时抗遗忘arxiv.orgarxiv.org。具身智能如实时视频流分析、持续语音识别都属于这种场景。 持续学习评测基准丰富化: 近年构建了许多新数据集和基准来评测持续学习算法在更复杂任务上的性能。如 CORe50、OpenLORIS 等视觉序列数据集用于评测在线物体识别link.springer.com；ACL 2021 Lifelong NLP挑战提供持续自然语言理解任务；还有不同领域的持续强化学习基准、连续无人驾驶仿真环境等。这些基准推动算法从依赖任务ID的小规模实验，走向更贴近真实的情境。评测指标也越发丰富，除了遗忘率、累计精度外，开始考虑模型的计算效率、内存开销以及在长期学习中的稳定性link.springer.comlink.springer.com。这些综合指标对于具身智能系统尤为重要，因为实际应用中资源受限且需要持续运行。 联邦持续学习与分布式学习: 在物联网和边缘计算兴起的背景下，联邦持续学习成为新方向arxiv.org。即多个分散设备（如一群机器人或智能传感器）在各自持续学习的同时，定期交流模型更新，从而在保证隐私下实现知识共享和共同进化。诸如 FedWeIT、FCL 等算法探索了如何在联邦场景下减少遗忘并高效通信arxiv.org。对具身智能来说，这意味着例如一队协作机器人的经验可以融合，使整体学习速度加快且每个体遗忘降低。该领域仍在起步，面临异步学习、设备差异等挑战，但前景值得期待。 理论分析与可解释性: 持续学习理论方面，近年有人尝试从信息论和最优化角度给出遗忘的分析框架，如用Fisher信息界定参数迁移平衡ar5iv.org。另外，对持续学习过程中的可解释性要求也在提高——在机器人应用中，理解模型为何遗忘某能力、何时需要触发新学习，对于建立用户信任很重要。一些研究利用可视化技术观察随着任务增添，网络内部表示如何演化，以寻找缓解遗忘的线索。还有工作将神经符号方法引入持续学习，以借助符号逻辑的约束保持旧知识。这些方向虽属于前沿探索，但表明社区已不仅满足于经验提升性能，也在寻求持续学习更深层的理论和可解释支撑。 综上，持续学习作为迈向真正智能系统的关键一步，近年来在算法和应用上都取得了显著进展。从最初发现问题、提出启发式对策，到如今各种融合策略在复杂环境中落地，我们离“像人一样终身学习”的AI越来越近。在具身智能领域，实现持续学习将赋予机器人和自主代理长久的自主适应能力，使其能够随着环境和任务变化不断成长，而无需频繁人工干预。展望未来，持续学习研究需要进一步结合元学习、强化学习、因果推断等范式，研发更加通用高效的算法。同时，在真实世界大规模部署持续学习系统时，还需重视 安全机制 （防止在学习过程中性能突然退化）、 伦理与隐私 （学习过程中对用户数据的处理）等问题。可以预见，随着研究的深化，持续学习将在机器人自主导航、智能助理、自动驾驶乃至通用人工智能等领域扮演日益重要的角色，推动人工智能从“静态聪明”走向“动态成长”。\n参考文献：\nMcCloskey, M. \u0026amp; Cohen, N. J. (1989). Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem . In Psychology of Learning and Motivation , vol. 24, pp. 109–165en.wikipedia.org. ( 首次揭示神经网络顺序学习遗忘问题 ) French, R. M. (1999). Catastrophic forgetting in connectionist networks . Trends in Cognitive Sciences, 3 (4):128–135cell.com. ( 灾难性遗忘综述，分析原因并讨论可能解决方案 ) Robins, A. (1995). Catastrophic forgetting, rehearsal and pseudorehearsal . Connection Science, 7 (2):123–146ar5iv.org. ( 提出伪重演方法，用随机伪样本重放旧知识 ) Goodfellow, I. et al. (2014). An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks . In ICLR 2014 . ( 实证分析深度网络遗忘现象，评估基本缓解策略 ) Li, Z. \u0026amp; Hoiem, D. (2016). Learning without Forgetting . In ECCV 2016arxiv.org. ( 知识蒸馏用于持续学习，只用新任务数据保持旧任务性能 ) Kirkpatrick, J. et al. (2017). Overcoming catastrophic forgetting in neural networks . PNAS, 114 (13):3521–3526arxiv.org. ( 提出EWC，通过弹性权重凝固保护重要参数arxiv.org ) Zenke, F. et al. (2017). Continual Learning Through Synaptic Intelligence . In ICML 2017arxiv.org. ( 提出SI算法，智能累积参数重要性减少遗忘arxiv.org ) Rebuffi, S.-A. et al. (2017). iCaRL: Incremental Classifier and Representation Learning . In CVPR 2017arxiv.org. ( 提出增量分类策略，结合样本保存和蒸馏避免遗忘arxiv.org ) Lopez-Paz, D. \u0026amp; Ranzato, M. (2017). Gradient Episodic Memory for Continual Learning . In NeurIPS 2017arxiv.org. ( 提出GEM算法，用梯度约束保证新任务不增大旧任务损失arxiv.org ) Shin, H. et al. (2017). Continual Learning with Deep Generative Replay . In NeurIPS 2017arxiv.org. ( 提出深度生成回放DGR，通过生成模型重现旧样本融合训练arxiv.org ) Rusu, A. A. et al. (2016). Progressive Neural Networks . arXiv:1606.04671arxiv.org. ( 提出渐进网络架构，扩展新列避免遗忘并实现知识迁移arxiv.org ) Mallya, A. \u0026amp; Lazebnik, S. (2018). PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning . In CVPR 2018arxiv.org. ( 通过迭代剪枝为新任务腾出容量，实现单网络多任务无遗忘arxiv.org ) Schwarz, J. et al. (2018). Progress \u0026amp; Compress: A scalable framework for continual learning . In ICML 2018arxiv.org. ( 提出进展-压缩框架，结合渐进扩展和蒸馏压缩，实现无增长持续学习arxiv.org ) Aljundi, R. et al. (2019). Online Continual Learning with Maximal Interfered Retrieval . In NeurIPS 2019 . ( 提出在线持续学习算法MIR，选择干扰最大的记忆样本回放，任务无关场景有效 ) Pellegrini, L. et al. (2020). Latent Replay for Real-Time Continual Learning . In IROS 2020link.springer.com. ( 提出在特征空间进行重放，支持机器人实时持续学习，降低存储与计算需求 ) Kemker, R. \u0026amp; Kanan, C. (2018). FearNet: Brain-Inspired Model for Incremental Learning . In ICLR 2018arxiv.org. ( 提出双内存脑启发模型，不存原始数据通过生物式记忆系统整合知识arxiv.org ) Hajizada, E. et al. (2024). Continually Learning Prototypes . arXiv:2404.00418arxiv.orgarxiv.org. ( 提出原型持续学习方法，少样本在线学习并支持开放世界新类发现，无需存储回放 ) Shaheen, K. et al. (2022). Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks . Journal of Intelligent \u0026amp; Robotic Systems, 105 (9)link.springer.comlink.springer.com. ( 面向自主系统的持续学习综述，分析算法在无人车、无人机等中的性能和挑战 ) Lesort, T. et al. (2020). Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges . Information Fusion, 58 :52–68arxiv.orgar5iv.org. ( 持续学习在机器人领域的综述，提出评测框架和跨领域方法借鉴思路 ) 引用\n[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[2312.10549] Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomyhttps://arxiv.org/abs/2312.10549Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[PDF] Continual Learning and Catastrophic Forgettinghttps://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1706.08840] Gradient Episodic Memory for Continual Learninghttps://arxiv.org/abs/1706.08840[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1805.06370] Progress \u0026amp; Compress: A scalable framework for continual learninghttps://arxiv.org/abs/1805.06370[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Towards lifelong object recognition: A dataset and benchmarkhttps://www.sciencedirect.com/science/article/abs/pii/S0031320322003004Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccLatent Replay for Real-Time Continual Learning - IEEE Xplorehttps://ieeexplore.ieee.org/document/9341460/Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccInteractive continual learning for robots: a neuromorphic approachhttps://dl.acm.org/doi/10.1145/3546790.3546791Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfcc[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfcchttps://arxiv.org/pdf/2302.00487https://arxiv.org/pdf/2302.00487Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccFederated Continual Learning for Edge-AI: A Comprehensive Surveyhttps://arxiv.org/html/2411.13740v1\n2024–2025 年具身智能持续学习新方法综述：缓解灾难性遗忘的创新策略 持续学习（Continual Learning）旨在让智能体能顺序学习多个任务而不遗忘已学知识。然而传统方法（如 EWC、iCaRL、GEM、PackNet、DGR 等）主要在静态数据上验证，在具身智能场景中效果有限。具身智能中的持续学习面临真实环境的挑战：任务顺序模糊、样本稀少、实时交互、高维感知等。这要求新机制在稳定-可塑性间取得更佳平衡，避免旧知识被覆盖（灾难性遗忘）同时高效习得新技能。以下我们综述 2024–2025 年出现的多篇新论文，每篇提供方法简介、核心创新、与传统方法的区别及其在具身场景中的优势分析。 Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation (Yuanqi Yao 等, 2025 年, arXiv) 方法简介：这项工作提出了PSPL（Primitive-level Skill Prompt Learning）方法，用于机器人操作的无重温持续学习。PSPL 将技能解构为可复用的“原语”（primitives），通过前缀提示（prefix prompts）来表示，并在两阶段训练方案中不断扩展技能库。首先进行多技能联合预训练，学习一组运动感知的技能提示（motion-aware skill prompts），提取不同技能间的共性语义与运动原语。随后在增量学习新技能时，为每个新技能添加新的前缀提示，通过与旧技能提示的跨注意力机制共享知识，从而加速新技能学习。这一提示学习策略使机器人能够在无需重放旧任务数据的情况下连续获取新技能。 **核心创新：PSPL 将提示学习（prompt learning）引入机器人持续学习领域，创新性地使用“技能前缀提示”作为可扩展的模块化单元来表示和存储技能知识。相比以往为每个新任务增添适配器的方式，PSPL 的提示可以在技能之间共享和重用，显式捕捉技能间的共性原语（包括语义和运动层面的共性）。此外提出的文本+光流联合提示查询（text-flow query）机制，将语言指令与视觉运动信息融合，用于检索适当的原语提示，从而关联语义截然不同但运动模式相似的技能。这一机制解决了仅靠文本嵌入查询时，不同技能间缺乏关联的问题。 与传统方法区别：相较于 EWC 等对网络权重施加正则的方式，PSPL 并不限制参数更新，而是通过前缀提示的模块化扩展来避免遗忘：旧技能的提示向量固定，新技能通过新增提示获取模型容量，从而防止旧知识被覆盖。这类似 PackNet 的“逐任务分配参数”思想，但PSPL的提示可以通过跨注意力与旧提示交互，实现旧新知识的共享迁移，弥补 PackNet 类方法模块隔离导致的无法迁移问题。与 iCaRL、GEM 等需要存储旧样本回放不同，PSPL 不需要任何经验回放——模型通过提示机制保留旧技能，因此在训练新任务时无需重温旧数据便可避免遗忘。相比于 LwF 这类基于知识蒸馏的方法，PSPL直接在模型内部保留了原技能的“软提示”，避免了仅凭日志概率约束可能出现的信息不足。 具身场景下的优势：首先，无重放需求降低了实际机器人系统的存储和隐私负担（无需记录旧任务视频或演示数据），非常适合那些无法无限存储过去经验的场景。其次，PSPL 的原语提示捕捉了跨任务的共性动作模式，使机器人能在少样本情况下将已学技能迁移到新任务上，例如仅观察少量演示也能通过提示召回相关原语来加速学习。再次，该方法对每个技能只新增少量提示向量，属于参数高效的扩展方式，不像全面微调那样消耗大量新参数，适合机器人有限算力的约束。最后，在 LIBERO 基准的模拟和真实操作实验中，PSPL 在前向迁移（FWT）和背向保留（BWT）指标上均达到当前最好水平，验证了其在长期任务序列中的遗忘防御和跨技能知识共享能力。 Preserving and Combining Knowledge in Robotic Lifelong Reinforcement Learning (Yuan Meng 等, 2025 年, Nature Machine Intelligence) 方法简介：该研究提出了名为LEGION的机器人终身强化学习框架 nature.com 。LEGION 构建了一个非参数贝叶斯知识空间（knowledge space），用以逐步积累和保存机器人的技能知识。具体而言，作者使用Dirichlet 过程混合模型（DPMM）来根据任务生成的表征不断拓展知识空间的簇，以容纳新任务知识，同时保留旧任务的簇不被覆盖。每当机器人从连续到来的一次性任务流中学习完一个任务，DPMM 就在知识空间中产生或调整相应的知识组分，实现对该任务知识的分离存储与渐进扩充。此外，LEGION 将语言嵌入引入任务表征，使智能体理解任务语义，从而在学习和推理中利用语言指导任务推断。在执行阶段，机器人能够将知识空间中多个任务的技能组合重新应用于长序列复合任务（例如依次执行多步操作完成“清理桌子”等目标）。 核心创新：LEGION 的突出创新在于引入贝叶斯非参模型管理知识：不同于传统固定架构，DPMM 知识空间可以动态增加新的知识组分，无需预先限定任务数量。这解决了过去结构模块化方法在任务数未知时难以扩展的问题 nature.com 。知识空间中的每个簇本质上代表一类技能或策略的“知识单元”，新任务到来时，如果其知识无法用现有簇解释，就会自动生成新簇存储，避免了旧知识的遗忘与冲突。同时，每个任务的知识以概率混合模型形式保存在连续的先验空间中，便于在需要时被识别和重新调用。另一个创新是融合语言描述提升任务区分和推理能力：语言嵌入提供了任务的高层语义提示，帮助知识空间进行任务归属推断和知识检索，使得在执行长视距任务时，智能体可以更准确地推理需要调用哪些已有技能。 与传统方法区别：LEGION 综合了多种持续学习理念但又有所突破。相较于 EWC 等正则化方法，LEGION 不直接作用于网络参数，而是在参数输出的高层空间保存知识，因而不存在权重被覆盖的问题。相比 PackNet 这类结构隔离方法，LEGION 的知识空间无需预设模块数且能持续增长，解决了传统结构方法难以应对未知数量任务的局限 nature.com 。同时，有别于纯经验回放策略（如 GEM）必须存储大量过往数据，LEGION 通过贝叶斯记忆高效概括了过去任务的精华，不需要完整保留旧样本即可实现类似回放的效果。虽然训练中仍使用有限缓冲作为强化学习的 replay（SAC 算法自身需要）, 但由于知识已存于非参空间，LEGION 即使在不增加缓冲容量的情况下依然能逐任务提高成功率。另外，LEGION 与以往假定任务明确分布范围的多任务学习不同，它能够适应非参数任务变化，处理现实中出现的新规则或新交互形式——这些情景下老方法（例如基于高斯混合的元学习）因需预设任务数量而难以胜任。 具身场景下的优势：LEGION 在真实机器人持续学习中展示出多项优势。首先，其知识保存与扩展机制使机器人几乎无遗忘地累积技能——实验表明，即使经过长时间训练再次回访早期任务，成功率仍接近初始掌握水平，证明旧技能通过知识空间得到有效保留。其次，LEGION 可以将已学知识用于新任务的快速推理和组合，体现了少样本快速迁移能力：作者展示了机器人在学习一系列基本任务后，能够在一次提示下将相关技能串联完成复杂长任务，例如“制作咖啡”，这源于知识空间支持对已学技能的自由组合。再次，LEGION 引入语言降低了感知与任务推断难度，使机器人在多模态指令下表现出色——通过语言描述任务，机器人无需大量探索即可定位对应知识簇，从而更快适应新任务语境 nature.com 。最后，在对比实验中，LEGION 在顺序任务中稳定提升成功率，显著优于“完美记忆”（无限重放）、“reservoir采样回放”和 A-GEM 等方法。这表明在数据受限、任务未知的真实环境中，LEGION 的知识空间机制比传统持续学习策略更适合维持和重用机器人知识。 LEAGUE++: Empowering Continual Robot Learning through Guided Skill Acquisition with Language Models (Zhaoyi Li 等, 2024 年, VLMNM 2024 研讨会) 方法简介：LEAGUE++ 提出了一种利用大语言模型（LLM）引导机器人持续学习技能的框架。该方法将任务与运动规划（TAMP）和深度强化学习（DRL）相结合：先由预训练的大语言模型将复杂长任务自动分解为一系列子任务操作序列（即生成操作符及步骤规划），并产生日志级别的稠密奖励信号指导每个子任务的强化学习训练。机器人通过 DRL 学习各子步骤的低层技能，同时维护一个符号化技能库记录已学的技能。当新任务到来时，LEAGUE++ 使用已有技能库中的技能作为Warm-Start（预热），加速学习新技能。整个系统在四个模拟任务域上进行了验证，包括家务操作等复杂任务场景。结果表明，LEAGUE++ 相比基线有更快的学习速度和更高的任务成功率，并能通过技能重用在新域中显著缩短收敛时间。 核心创新：LEAGUE++ 最大的创新在于将大模型的规划能力和传统强化学习有机结合，实现“边规划、边学习”。具体来说，它利用 LLM 强大的语义理解和分解能力，自动将高层指令分解为低层动作序列（解决了RL直接面对长远稀疏奖励任务时的困难），从而大幅降低了长视距任务的学习难度。其次，它提出了符号技能库概念：已学技能以可调用的符号模块形式存储，使机器人在新任务时可以检索并复用相关技能，而非从零学习。这种技能库机制相当于一种渐进专家网络：随着任务增多，库中专家技能增多，策略可以根据需要调用对应子政策，类似人类在新任务中调用过往经验。最后，LEAGUE++ 将现有预训练模型用于策略初始化（warm-start），例如视觉模型用于感知、行为克隆模型用于初始策略等。这减少了新任务的探索开销，使持续学习更高效稳定。 与传统方法区别：传统持续学习多半依赖网络自身调节参数来避免遗忘，而 LEAGUE++ 则更像一种“学习+规划”双轨方案。它没有采用 EWC/GEM 等直接在RL算法中增加遗忘惩罚，而是通过任务分解将复杂问题拆解，降低每阶段学习干扰。这种方式避免了以往RL持续学习中，不同任务策略存于同一网络导致的梯度干扰，从根本上缓解了遗忘。此外，与 PackNet 等为每任务固定网络模块不同，LEAGUE++ 通过符号调用机制在同一策略网络中执行不同技能，相当于软模块化——技能库中的技能相当于可切换的策略段，无需冻结参数就能隔离任务干扰。相比纯回放或迁移学习，LEAGUE++ 依赖 LLM 提供额外知识（如何解任务、设计奖励），扩充了信息来源：这类似人类导师提供提示，加速了学习而非仅靠算法克服遗忘。可以说，LEAGUE++ 将知识提炼（通过LLM）与技能留存（通过库）结合，超出了传统持续学习框架。 具身场景下的优势：LEAGUE++ 非常契合具身智能体执行复杂任务的需求。首先，LLM 提供的分层规划使机器人哪怕面对长且模糊的指令（例如“准备早餐”）也能一步步拆解执行，避免在长时间任务中遗忘目标或迷失——这在人类复杂指令场景尤为关键。其次，技能库的重用大幅提升了数据效率：当机器人进入新环境或任务，只要能从库中找到类似子任务，就无需从头练习，少样本即可适应。这特别适合真实机器人训练昂贵、无法大量试错的情况。第三，由于采用符号层指导和技能初始化，LEAGUE++ 在每个任务的学习过程都更快、更稳定，降低了资源消耗和试错成本。作者的实验表明，在多个复杂模拟环境（如家庭清洁、装配任务）中，LEAGUE++ 全面优于纯RL的持续学习基线。例如在新域任务上，因复用了旧域技能，学习速度显著快于从零开始的对照。这证明在真实机器人需要反复学习不同任务的长期部署中，引入LLM指导和技能库的框架能够实现效率和稳健性的兼顾，这是传统方法难以同时达到的。 Scalable and Efficient Continual Learning from Demonstration via a Hypernetwork-generated Stable Dynamics Model (Sayantan Auddy 等, 2024 年, arXiv) 方法简介：该工作针对机器人示教学习（LfD）场景提出了一个持续学习方法，利用超网络（Hypernetwork）和稳定动力学模型来保持多技能学习过程中的稳定性和不遗忘。核心思想是训练一个超网络，生成两个子网络：一个用于学习每项示范任务的轨迹动力学模型，另一个生成与之配套的李雅普诺夫稳定性函数。通过在每次引入新示范任务时，让超网络输出相应的轨迹模型并辅以稳定性约束，该方法确保机器人复现的轨迹是收敛稳定的，不会在插值或续航过程中发散。更重要的是，这种引入稳定性的方式还显著提升了持续学习性能：作者发现，相较不考虑稳定性的基线，引入李雅普诺夫函数后，机器人在序列学习多个技能时的遗忘显著减少。此外，为了提高效率，作者提出了分块超网络（chunked hypernetwork）和随机正则化策略，将训练多个任务的总时间从原先的 $O(N^2)$ 降低到 $O(N)$。整个方法在实时的机器人绘画轨迹学习任务（包括 LASA 数据集扩展到高维、和作者自建的 RoboTasks9 实验）上验证，结果显示无论在轨迹精度还是持续学习指标上都优于现有方法。 核心创新：本方法的创新点在于将鲁棒控制中的稳定性保障引入持续示教学习。通过让超网络同时学轨迹模型和Lyapunov 函数，每新增一项技能，系统不仅学会了模仿轨迹本身，还获得了对该技能的稳定约束，使其在整个状态空间具有吸引域。这解决了过去很多 LfD 方法关注模仿误差、却忽视了轨迹泛化稳定性的问题——即使没有回放旧示范，稳定性保障使旧技能的轨迹在训练新技能时不易被扰乱。其次，使用超网络作为核心架构，使得多任务知识统一存储在超网络的权重中，通过不同输入（如任务ID或上下文）生成相应任务的模型。相比传统逐任务微调网络，超网络天然具备参数共享和隔离的双重特性：共享是指不同任务的共性部分自动在超网络中得到整合，隔离是指超网络输出不同任务模型，相当于为每个任务保留了一套隐含的参数。这种方式巧妙地避免了不同任务参数直接冲突覆盖。再次，提出的随机正则项训练方法，通过每次训练仅对一个随机选取的旧任务施加约束，代替以往对所有旧任务都正则化的做法，大幅降低了训练复杂度。这一技巧保持性能不变却将训练开销线性化，提升了持续学习的可扩展性。 与传统方法区别：相比 EWC 等对参数层面的稳定约束，本方法将稳定性提升到行为层面：EWC关心参数不剧烈变化，而该方法直接确保每个技能的输出轨迹在引入新技能后依然收敛不发散，因而对遗忘的防御更直接有效。与 iCaRL/GEM 等需要存储和重放示范不同，该方法不需要对旧示范数据回放：旧技能知识蕴含在超网络权重中，加之稳定性函数的限制，新任务训练不会破坏旧技能轨迹，无需样本重温也能保护旧技能。同时，相较 PackNet 逐任务固定部分网络的硬隔离，超网络属于软隔离：所有任务共享同一个生成网络，但通过任务索引输出不同技能模型，相当于用函数生成参数，既避免干扰又保持容量节省。与之前一项采用超网络+神经ODE方法做 continual LfD 的工作相比（作者提及的最近方法），本方法增加了严格的稳定性保障，因此旧技能保真度和新技能学习效果都有明显提升。总的来说，新方法在理念上融合了控制理论和持续学习，提供了一种不同于以往任何单一范式的框架。 具身场景下的优势：在实际机器人示教应用中，该方法具有显著优势。首先，稳定性约束确保机器人在执行已学轨迹时不会因为学习了新技能而出现意外失稳或偏差，这对物理系统尤为重要——避免了因遗忘造成的运行危险，提高了安全性和可靠性。其次，由于无需反复重训旧示范，机器人能更高效地学习新技能：作者在真实机器人轨迹跟踪任务中展示，新技能加入后旧技能仍能零-shot 执行，表现几乎不下降，这意味着机器人随时可用, 不需要频繁校准旧技能。第三，该方法在高维技能扩展上表现良好，证明其可应用于涉及多自由度的复杂操作（例如机械臂同时控制位置和姿态的轨迹）。传统方法在高维连续控制上的持续学习往往不稳定，而本方法借助Lyapunov函数确保了每个技能在高维空间都是吸引子的，因此具备更强的鲁棒性。最后，在资源受限的机器人上，训练效率的线性提升非常有价值：每新增技能的训练开销不会随着技能数量指数增长，使得机器人可以规模化地持续学习几十上百个技能而不会因为训练时间过长变得不可行。综上，该方法为现实中机器人持续学习大量示教任务提供了安全、高效、稳健的解决方案。 Online Continual Learning for Interactive Instruction Following Agents (Byeonghwi Kim 等, 2024 年, ICLR 2024) 方法简介：这项工作定义了更贴近现实的交互式指令跟随持续学习场景，并提出了信心感知滑动平均（CAMA）算法来缓解遗忘。作者指出，过去大多假设智能体一开始就能获取所有训练任务的数据，而不符合机器人应持续探索、持续学习的现实。为此论文提出两个设置：(1) 行为增量学习（Behavior-IL）：机器人不断学习新指令行为；(2) 环境增量学习（Environment-IL）：在新环境中执行之前学过的指令。困难在于任务边界可能不明确，传统“数据先验”方法（如基于旧任务输出日志概率的蒸馏）需要明确任务边界和缓存旧任务信息。CAMA 则不需要任务边界：智能体在训练过程中对每个观察到的指令计算模型输出的置信度，并以滑动平均的方式更新其对旧任务的输出分布估计。当模型学习新行为时，如果对某类以前学过的指令置信度下降，CAMA 会根据之前维护的滑动平均分布对模型进行轻微调整，以拉回对旧知识的信心，从而达到持续无边界学习的目的。实验证明，在作者设计的新基准上（包含连续新增指令和环境的条件），CAMA 相较现有方法取得了显著更好的表现。 核心创新：CAMA 的创新在于引入任务无关的置信度追踪机制：传统方法如 LwF 或 iCaRL 需要存储每个旧任务的模型输出分布或实例样本，当新任务到来时通过知识蒸馏或重放维持旧任务性能。这实际隐含了已知的任务边界和任务身份。而 CAMA 不存储具体样本或明确任务ID，而是对模型输出信心做持续监控。它维护一个滑动平均估计，可视作模型对过去经验的“模糊记忆”，随着时间衰减地记录旧任务的输出倾向。当模型学新任务时，如果这种倾向发生显著漂移，表明遗忘在发生，那么CAMA依据滑动平均的差异，对模型参数进行微调修正（如调整输出层偏置等），无需知道具体哪个任务受影响，只基于置信度变化即可纠正。这种方法跳出了任务级别的框架，实现了任务无关（task-free）**的持续自稳训练。另外，CAMA 所需存储的信息量极小，仅为每个输出类别一个滑动均值，计算与存储开销极低，非常简洁却有效。 与传统方法区别：与 EWC 等正则方法需要在参数变化上加全局约束不同，CAMA 工作在输出空间：它不直接限制权重，而是看模型对以前输出的信心水平是否下降。这有点类似知识蒸馏（LwF）的思想，但不需要旧模型保存——蒸馏通常要保存旧模型输出作为固定目标，而CAMA持续更新的滑动平均本质上充当了“旧模型记忆”，避免了存储多个旧模型或大量旧样本。相比 iCaRL 维护每类样本代表来近似旧任务分布，CAMA 维护的是压缩的统计量（置信度均值），因而不需要样本库也没有额外模型，只需少量内存。与 GEM 这类基于梯度投影的方法相比，CAMA 算法更简单，不需要复杂的二次规划求解，只以移动平均做调整，在线即可执行。另外，CAMA 属于任务无边界的方法，解决了传统方法大多假定任务切换已知、或者需要在检测到切换后才能应用策略的问题。在现实机器人持续学习中，任务变化往往连续发生且未必有明显分界，CAMA 正是面向这种情况设计的。 具身场景下的优势：CAMA 所针对的交互式指令跟随场景极具代表性：机器人在不断遇到新指令、新环境的过程中持续学习，这对服务机器人等非常实际。CAMA 能在无监督任务切换的前提下，让机器人保持之前学会的指令技能。例如，一个家庭助理机器人在学会一系列语音指令后，被主人带到新房间教它新任务，CAMA 确保机器人在学新任务时不会忘记旧房间的指令执行方法。由于不需要存储大量过往数据，CAMA 也适合长期部署：机器人可以不断学习数十数百种指令而不必担心存储开销或隐私问题（无需录音或录像存档）。同时，CAMA 的实时性很高，它在模型训练时即可动态调整参数，无需离线阶段，适合机器人在线学习的需求。作者报告该方法在提出的 Behavior-IL 和 Environment-IL 基准上显著优于已有的持续学习算法，这表明它成功地解决了任务模糊情况下的遗忘问题，让机器人在变化的环境和任务中保持稳健性能。总之，CAMA 为具身智能体提供了一种轻量级但有效的持续学习机制，使其能像人一样一边执行指令、一边随着环境改变不断积累本领。 ICAL: In-Context Abstraction Learning for Multimodal Agents (Gabriel Sarch 等, 2024 年, arXiv) 方法简介：ICAL 提出了一种利用大模型自身的生成与内省能力，持续提升多模态智能体决策的方案。该方法关注这样的问题：大型语言或视觉-语言模型（LLM/VLM）虽具有强大的Few-shot能力，但需要高质量范例作提示。人为提供高质量示例既昂贵又不具通用性。ICAL 的解决方案是：让大模型从次优的示范轨迹和人类反馈中自动生成可用于提示的抽象示例。具体流程包括：给定一个全新领域的嘈杂演示（例如一个任务的视频示范，可能包含低效动作或错误），首先由视觉-语言模型对该轨迹进行解析抽象。模型会产出一个通用的程序化描述，修正了示范中的低效步骤，并添加认知注释，标注出任务涉及的关系、对象状态变化、时间子目标和关键细节等。接下来，让真实机器人/代理尝试执行这份抽象计划，在执行过程中人类可以提供自然语言反馈纠正模型的错误理解或补充知识。模型根据反馈交互式地细化之前产生的抽象。最终得到的这些抽象示例（带有语言注释和优化后的步骤），被存储起来作为内存范例。在后续决策中，大模型可以将这些示例作为提示的一部分，从而显著提升对于类似任务的决策能力。ICAL 在三个具有挑战的环境中验证：TEACh对话式指令跟随、VisualWeb互联网多模态代理、Ego4D第一人称视频动作预测，都取得了新的SOTA性能。 核心创新：ICAL 将持续学习转化为持续积累提示示例的过程，而非传统的持续调权过程。这是一个范式转变：与其反复梯度更新模型参数，ICAL 让模型自己“理解反思”示范并生成抽象的知识总结，逐步丰富模型的提示库。这样的内省式示教机制是首次提出。具体创新点包括：(1) 自动抽象：利用预训练VLM从视觉示范中提取高层语义——不像以往只关注低级动作序列，ICAL 提取了任务因果关系、对象状态变化、时间逻辑、任务构造等四类认知抽象，这些抽象比原始示范更精炼、更具概括性。(2) 人机交互细化：ICAL 不仅依赖模型自我生成，还引入人类在模型执行时给出自然语言反馈，让模型迭代改进其抽象。这种循环使得次优示范（含错误或低效部分）也能通过反馈纠正而变得有价值，突破了以往示范学习要求专家示范的限制。(3) 提示记忆库：将生成的抽象示例存入一个不断扩展的库，并使用检索增强的大模型决策——模型在推理新任务时，会从库中取出相关示例放入提示，提高决策准确率。ICAL 相当于在持续学习过程中渐进地提升模型的上下文学习能力：模型见到的新任务越多，就自行总结出越多可泛化的知识片段，下次遇到类似任务时即使不调权也能通过提示完成。实验显示，ICAL 随着学习示例增多，模型性能持续提升，呈现出真正的“持续改进”特性。 与传统方法区别：传统持续学习多在参数空间操作，如EWC约束权重变化、经验回放存数据样本等。而 ICAL 完全绕开了参数遗忘问题：模型主体参数基本保持不变（或只在最后有小幅微调作为增强），因此谈不上灾难性遗忘。取而代之，ICAL 通过不断加入新的知识提示，让模型具备解决更多任务的能力。这种基于大模型提示的持续学习不同于以往任何持续学习范式。与知识蒸馏类方法相比，ICAL 并非让模型去逼近旧模型输出，而是生成更好的知识为己所用；与模块化或正则方法相比，它无需设计模型结构拆分或损失项，利用的是LLM/VLM自身强大的Few-shot学习能力和生成能力来“吸收”经验。在需要多模态理解和指令执行的任务中，ICAL 运用了视觉和语言的结合，这比单纯依赖视觉记忆（如CV领域一些记忆网络）要丰富，也比仅语言的持续学习（如对LLM增量训练新知识）更直观：ICAL 让模型直接看视频学，也听取人类语言，得到的是跨模态的知识。这些知识用自然语言+视觉标记来存储，具有很强的可解释性和可移植性。传统持续学习很难在不调整参数情况下大幅提高性能，而ICAL 展示了另一种可能：通过增强模型输入（上下文）而非改变模型本身来实现持续学习。 具身场景下的优势：ICAL 针对的场景包括家庭机器人任务（TEACh）、网络操作代理以及视频预测，都与具身智能密切相关。它的优势在于：对于复杂开放环境，很难人工定义明确的任务边界或提供足够训练数据，而ICAL 利用了现有大型模型，只需较少示范和交互即可让模型适应新任务。因此在具身智能典型的少样本、多样任务情况下，ICAL 能快速扩展技能。此外，它不要求持续占用机器人反复训练（除了必要的人机对话反馈），大部分“学习”都在模型的推理过程中完成（让模型生成抽象并评估）——这意味着机器人可以在线学习新任务而不中断服务，通过对话方式边干边学，正如人类新手在导师指导下学习新技能一样。对于任务模糊或长时间任务，ICAL 的抽象总结能力尤为关键：机器人可以从冗长的示范中提炼要点，避免执行时被次要细节干扰。这提高了机器人在复杂任务下的鲁棒性和泛化。实验证实，ICAL 在 TEACh 基准上将先前SOTA的目标达成率提升了12.6%，在VisualWebArena中成功率从14.3%提高到22.7%，在Ego4D动作预测上击败了Few-shot GPT-4V。更重要的是，性能提升是持续的：每当学习更多示例，模型表现就有所提高。因此在长期来看，ICAL 提供了一个持续增长智能体能力的途径，而无须反复训练模型参数，大大降低了具身人工智能系统维护的成本和风险（例如不会出现传统持续学习不慎遗忘必须人工纠正的问题）。这表明，充分利用大模型强大的自监督和内省能力，或许是具身智能持续学习的一条有效新路。 MLLM-CL: Continual Learning for Multimodal Large Language Models (Hongbo Zhao 等, 2025 年, arXiv) 方法简介：MLLM-CL 提出了一个多模态大模型（视觉-语言模型）持续学习的基准和方法。该工作将持续学习划分为两种情形：(1) 领域持续学习（Domain CL）：模型依次学习遥感、医学、科学、自动驾驶、金融等不同视觉问答领域的知识；(2) 能力持续学习（Ability CL）：模型顺序学习OCR识别、数学逻辑、视觉感知、GUI操作代理等不同能力。作者发现，传统增量学习范式（每次在上一个模型参数基础上微调新任务）不适用于多模态大模型：直接用上一任务的权重初始化下一任务，会损害模型对新任务的可塑性，导致次优结果。为解决这一任务冲突问题，MLLM-CL提出了MR-LoRA方法。具体来说，在每个新任务到来时，并不在原模型权重上继续微调，而是为该任务新建一个LoRA低秩适配器，从零随机初始化。这样每个领域/能力都有自己独立的LoRA模块，避免了继承上个任务权重带来的冲突，同时LoRA只引入极少参数，保证高效。在推理时，为了自动选择对应任务的LoRA，作者设计了一个基于大模型的路由器：利用多模态大模型本身处理复杂输入的能力，输入待测试样本后，由模型生成一个路由指令，选择最合适的专家LoRA来回答。这个路由器通过在持续学习每阶段后收集少量任务样本，对大模型进行轻量few-shot微调得到。总体而言，MR-LoRA包含多LoRA专家 + 大模型路由两部分。实验结果显示，在上述多个领域和能力的持续学习任务序列中，该方法在所有任务上的平均性能和最终性能都超越了现有方法，如直接微调、参数隔离、基于提示的方法等。 核心创新：MLLM-CL 的创新在于针对大模型的持续学习提出了避免遗忘的新范式。其一，每任务新LoRA的策略突破了以往串行微调“一脉相承”导致性能下降的窘境。这种做法类似“进渐网络（Progressive Networks）”，但由于LoRA参数规模小，可以在不剧增参数的情况下无限扩展到更多任务。各任务的LoRA彼此独立，保证旧任务参数不受新任务影响，从根本上杜绝了遗忘。同时，所有LoRA附加在同一个大模型上，共享其通用表示能力，又实现了知识迁移（因为基础模型权重保留了多任务共性表示）。其二，引入大模型驱动的路由机制，这是此前持续学习中少有探索的。传统任务识别通常依赖样本的特征相似度或任务ID，而该方法让多模态大模型读入输入后自主生成路由决策。由于大模型本身掌握高层语义和复杂推理，它可以比简单特征距离更精准地选出对应领域的LoRA专家。这保证了在推理阶段对于复杂多模态输入也能正确地匹配到相应能力模块。其三，该方法还建立了系统化的多模态持续学习基准（MLLM-CL基准），涵盖不同类型迁移（领域和能力）并提供了评价指标和数据集构建流程。这填补了多模态大模型持续学习缺少评测标准的空白，为后续研究提供了平台。 与传统方法区别：MLLM-CL 在设计上融合了参数高效微调和专家路由思想。与 EWC 等全模型正则不同，它不直接约束原模型参数，而是固定主干模型，只对每任务引入少量新参数，这避免了原模型权重冲突累积。与 PackNet 类似的方法相比，MR-LoRA 也采用了“每任务额外参数”的思路，但PackNet通过剪枝固定网络容量，MR-LoRA 则用可增添的LoRA实现弹性容量，理论上任务越多仅线性增加参数，无需预留容量。和提示学习（L2P、ModalPrompt 等）相比，提示方法在多模态大模型上效果有限，而且往往需要固定提示长度、可能仍有干扰；MR-LoRA 通过独立LoRA完全隔离任务，比软提示更干净利落。还有，许多多头网络或专家混合模型需要已知任务ID才能选专家，MR-LoRA 则通过路由器实现任务自动判别，不需要人工指示任务类别。可以说，它将模块化与任务识别两个问题一起解决了。在多模态场景下（例如同时应对图片文本、多任务问答），这种方法比单纯视觉或单纯语言的持续学习方法更复杂但也更全面地考虑了输入多样性和任务多样性。 具身场景下的优势：虽然 MLLM-CL 的实验主要是多模态问答和 GUI 任务，但其思想对许多具身智能应用同样有益。比如，一个家庭服务机器人具备多个视觉语言能力（识物、对话、读屏、算术等），MR-LoRA 可以让其持续添加新能力而旧能力不衰减：每项新技能加一个LoRA，机器人就掌握了新本领，又不会遗忘以前学过的（因为以前的LoRA保留原样）。这样的能力库扩展非常符合具身AI逐步进化的需求。其次，由于LoRA模块小，机器人可以在资源受限设备上部署多个技能专家，而不会像扩增整个模型那样内存爆炸。例如针对移动设备，把不同领域的视觉问答能力拆成多个LoRA加载，按需调用，远比加载多个大模型高效。再次，路由器机制使机器人在遇到新感知输入时能自动判断调用哪种能力，这对于多模态交互场景很关键——现实中用户不会明确告知机器人“这是一道医学问题”或“现在开始OCR任务”，机器人必须自适应切换内部技能。MR-LoRA 的路由方案正是训练机器人根据输入自行选择专家。作者实验也表明，在跨领域问答中，MR-LoRA 能正确选择相应LoRA模块，最终在所有领域上同时取得高精度，相比其它方法在后期任务训练后前期任务精度大幅下降，MR-LoRA 几乎零遗忘且新任务精度也高。这证明了其稳定性和塑造性兼顾的能力，非常适合需要终身学习的多才多艺型机器人。总之，MLLM-CL 为具身智能体集成多模态大模型指明了一条持续进化的道路：通过低秩模块化扩展和智能路由，实现持续学习众多技能而性能不减。 Task-Unaware Lifelong Robot Learning with Retrieval-based Weighted Local Adaptation (Pengzhi Yang 等, 2024 年, arXiv) 方法简介：此工作面向机器人开放环境，提出了无任务标识的终身学习框架，结合检索式记忆和局部微调来提升持续学习效果。在真实世界中，机器人遇到的新任务往往没有清晰的边界或ID，且无法存储海量以往数据。为此作者的方法在机器人学习过程中维护一个情景记忆（Episodic Memory），存储每个任务的一小部分关键示例。当机器人在序列学习新技能时，主要模型仍采用常规的基于经验回放的训练（比如采用一部分记忆样本进行练习，以降低遗忘）。然而仅靠有限回放仍难免遗忘一些早期技能。因此在测试执行阶段，如果机器人遇到类似以前学过却已部分遗忘的情境，该方法会从记忆中检索最相关的过往示例，并对当前策略进行快速局部微调（local fine-tuning），以恢复在该情境下的性能。为了提高这种局部微调的效率，作者引入选择性加权机制：首先让机器人在当前情境下执行若干回合，记录其性能（例如哪些步骤出错），然后将这些失败轨迹与记忆中检索到的示范进行比对，自动衡量哪部分旧示范对当前情况帮助最大，给予这些片段更高权重来指导微调 arxiv.org 。简言之，该框架模仿人类温习知识的过程：在需要时重点复习遗忘的难点，从而高效恢复技能。整个方法适用于没有明确任务划分的开放式场景，作者在 LIBERO等操作任务基准上测试了该方法，结果表明即使任务无明显边界，机器人也能持续学到新技能并在需要时重新找回旧技能，大幅优于不采用检索适应的基线。 核心创新：该方法的创新之处在于提出了任务无关的回顾与适应机制。传统持续学习通常假定任务边界明确（如任务开始和结束）以便采取对应策略，比如任务后冻结部分网络或保存样本。然而现实中机器人可能连续不断遇到各种挑战，无法提前知道哪些属于同一任务。本文通过检索式记忆绕过了显式的任务划分：无论当前遇到的情况是否是以前练过的任务，机器人都可以基于当前观测从记忆库检索类似经验。这赋予了模型一种情景感知能力，让它能够自行判定何时需要参考旧经验。其次，提出的Weighted Local Adaptation将元学习思想引入了持续学习测试阶段：不像传统只在训练阶段防遗忘，这里在测试阶段也运行一次短暂微调，相当于在机器人执行时刻临时提高旧技能专门性。这种做法以往少见，因为通常假设模型定型后就执行不变，但在终身学习背景下，适度的测试微调可以极大提升旧技能复现效果。通过限制微调只针对检索到的示例且有选择地重点学习难点，既保证了微调幅度受控不会破坏模型总体性能，又有针对性地弥补遗忘。第三，任务无关意味着模型无需任务标签输入或边界信号，这对开放环境学习非常关键。作者利用视觉和语言嵌入的一致性作为检索键值 arxiv.org ，避免了因持续学习造成的表征漂移（通过预训练模型确保不同阶段embedding空间一致）。这保证了即使模型学了新东西，仍能用统一标准去查找旧记忆，提高了检索可靠性。 与传统方法区别：相比 EWC等在训练时防遗忘的方法，该框架将遗忘补救延伸到了测试时刻：传统模型一旦训练完部署，如果遗忘了旧技能往往束手无策；而此方法允许机器人在执行某任务前“温习一遍”相关经验，相当于给模型一个自行恢复的机会。与 GEM 等在训练时用小样本回放不同，这里的记忆检索主要服务于执行阶段的微调，而非整个训练过程持续混入，因而不会显著增加训练难度，同时又充分利用了记忆在关键时刻的价值。和 iCaRL 直接根据记忆做最近邻推断不同，该方法选择的是微调模型，因此能适应当前状况的细微差别，而不是简单模式匹配，效果更强。在没有任务ID的情况下，很多传统方法如多头网络就无法应用，而本方法完全不依赖任务标签或边界，通过检索+微调实现了隐式的任务处理。可以将其视作结合了经验回放和快速自适应：既保留少量记忆（回放理念），又在需要时通过微调快速适应（元学习理念），融合了二者优点。 具身场景下的优势：对于在动态未知环境中工作的机器人来说，该方法提供了极高的灵活性。机器人无需预先知道将面对哪些任务，也不用在模型结构上做固定分配，它可以不断遇新仍保持从容：每当遗忘可能影响当前任务时，就从记忆中找回相关信息补强自己。这很像人类在现场快速翻阅笔记确认知识点，从而表现出色。尤其在长期部署中，机器人可能几天甚至几月不执行某项技能，这种情况下直接执行可能失败，但如果允许机器人在执行前复习一下过去案例，就能大幅提升成功率。作者的方法正是提供了这种快速热身功能，让机器人在现实使用中更可靠。其次，由于只存储挑选的一小部分示范数据，记忆库很轻便，不会像保存所有数据那样不现实。并且由于采用预训练embedding统一表示，不同时间学习的经验可以共同检索，这意味着机器人可以在跨环境、跨时间的情况下整合经验，非常符合具身智能需要持续集成多方信息的特性。最后，实验中该方法在没有任务边界信息的情况下，其性能超出了使用明确任务边界且大量回放的策略，体现出开放场景适应性。总之，此方法赋予机器人一种人类般的学习策略：即使偶尔遗忘细节，通过快速翻阅过往经历又能想起来，从而在不断变化的任务挑战中保持整体能力不退化，为具身智能的长期自主学习提供了新的思路。 方法演化趋势与对比 综上所述，2024–2025 年关于具身智能持续学习的方法展现出一些共同的发展趋势与对过去方法的根本区别： 从参数保护到知识重组：过去方法（EWC 等）通过阻止参数改动来防遗忘，但在机器人复杂任务中效果不佳。新方法更强调对知识的表达与重组，如通过前缀提示、技能库、知识空间等将知识单元化，避免直接在同一参数上博弈。这些机制允许共享与隔离并存：共享的是不同任务的共性（如PSPL原语、LEGION语言嵌入），隔离的是各任务独特部分（如每任务LoRA、每技能提示）。因此新方法能在不牺牲塑性前提下保持稳定性。 从任务清晰到任务模糊：许多新工作针对任务边界不明的现实。比如任务无关检索、CAMA 的置信度调整都不需要预先知道任务什么时候切换。相比之下，传统方法大多假定任务切换明确并提供信号，现实中难以满足。新方法通过连续监测模型行为或环境，相当于赋予模型自适应觉察能力，真正做到持续学习“终身运行”而非分阶段训练。 从被动防御到主动利用：以前方法把旧知识当需要保护的内容，新方法则倾向主动利用旧知识来帮助新学习。例如PSPL利用旧技能提示加速新技能习得、LEGION 用旧知识簇推断长序列任务、LEAGUE++ 复用已学技能解决新问题。这种正向迁移思路解决了传统方法虽然防遗忘但也不擅长迁移的问题，新方法在防止遗忘的同时显著提高了前向迁移性能，使机器人越学技能越多，解决新任务反而越快。 减少对存储和重放的依赖：传统方法如经验重放在机器人上往往不可行（存储无限增长且隐私风险）。许多新方法都强调“不依赖回放”：PSPL 无需回放旧经验、CAMA 无需存样本只存统计、LEGION 只以有限回放辅以知识空间、Hypernetwork 方法不重训旧示范。即便需要，也以小规模记忆或生成器代替全量存储（如Task-Unaware方法的小记忆+检索、本质上非常有限）。这使持续学习更能适应机器人存储和计算受限的条件。 引入大模型与多模态：与以往专注单一模型训练不同，新方法勇于将预训练大模型纳入持续学习框架，如利用LLM规划子任务（LEAGUE++）、用VLM自我生成知识（ICAL）、大模型本身作为路由器和骨干（MLLM-CL）。多模态信息（语言、视觉、触觉）也被融入，如LEGION用语言辅助任务编码、PSPL用文本和光流提示、ICAL用视频+语言反馈。这些让机器人更好地理解任务和环境，也提供了额外手段缓解遗忘（例如语言描述可作为语义锚点，帮助模型回忆对应技能）。这是持续学习与大模型、多模态技术的交叉融合，标志着具身智能持续学习进入一个更智能更复杂的阶段。 小结：传统持续学习方法在具身智能情境下面临诸多挑战，如参数共用导致遗忘、任务未知导致方法失效、资源有限导致回放不可行等。2024–2025 年的一系列新方法通过全新的机制——提示网络、非参知识库、超网络稳定、符号技能库、任务无关调整、大模型自监督等，成功地解决或缓解了这些问题。在机器人连续学习复杂技能、跨模态理解指令的任务中，这些方法展现出显著优于旧方法的性能，有的甚至实现了遗忘几乎为零且持续正迁移的效果。可以预见，未来具身智能持续学习将沿着模块化+共享、智能检索+自适应、融合大模型知识的路线继续发展，使智能体更接近人类的终身学习水平，在不断变化的世界中保持学习新知识的同时不忘记旧本领。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-16/","summary":"Avoid Catastrophy forget","title":"Bug Journal 2025-06-16"},{"content":"一、Vision-Language-Action (VLA) 模型 论文 目标 创新 平台 Code 算力 总结 UniAct 将 28 种机器人异构动作映射到统一“通用动作空间”，提升跨形态迁移 动作离散化＋对比约束学习，0.5 B 参数模型优于 7 B 基线 Open-X Embodiment、Libero、Droid；64×A100 训练 GitHub 64 × A100，10 天训练 UniAct 针对不同机器人动作空间异构的问题，提出“Universal Action Space”，把 28 种平台的演示映射到一组离散原子行为，使跨形态学习成为可能。该框架通过语义对比与动作重构双重约束，让 0.5 B 参数模型在多任务操控上超越 7 B 基线。实验表明，在 OXE 与 LIBERO 任务上显著提高数据效率与泛化性。 MoManipVLA 把固定底座 VLA 策略快速迁移到移动底座 双退火搜索选基座位姿＋SLSQP 精细优化 OVMM benchmark；Hexman Echo Plus + RM65 TBD 4×RTX 3090（作者建议） MoManipVLA 设计“双退火 + SLSQP”策略，把固定底座 VLA 生成的末端轨迹快速迁移到移动底座，实现零样本导航-操控一体。方法不改动原 VLA，只在推理阶段搜索最优基座位姿并微调手臂解算器。仿真与 Hexman + RM65 实验展示跨房间场景的 70 %→89 % 成功率提升。 CoT-VLA 为 VLA 引入显式visual chain-of-thought时序规划 先自回归预测未来视觉帧，再输出短动作序列 PyBullet 6DoF 手臂模拟 TBD 8×A100（7 B 模型预训练） CoT-VLA 将“视觉链式思考”引入 VLA：模型先自回归预测未来图像帧，作为显式视觉目标，再生成短动作序列。这样的分两步推理显著提升复杂操作的时间规划能力，并在 RT-X benchmark 刷新成绩。其 7 B 版本在无需额外模态标签下超越同规模基线。 SOLAMI 3D 虚拟角色“看-说-动”社交交互 合成 SynMSI 多模态对话 + 双塔解耦发声/动作 Oculus Quest 3 VR 前端；2×H800 推理 GitHub 32×V100 预训，16×V100 指令微调 SOLAMI 是首个端到端“社交” VLA 框架，可同时生成语音与动作驱动 3D 角色与人沉浸式互动。作者合成 6.3 K 多轮多模态对话数据 SynMSI 并预训练双塔模型，显著提升动作-语音一致性。用户 VR 研究验证，其角色在情感贴合度上优于 GPT-4o + 动画基线。 PVM Revisit 系统评测 PVM 预训练策略在机器人任务中的效果 提出 SlotMIM：在非对象中心数据上也保留 object-centric 表征 Franka Kitchen / Meta-World / Habitat GitHub 8×A100 训练 论文系统比较 DINO、iBOT、MAE 等预训练方法对机器人任务影响，发现“对象中心”特征是关键。为解决非对象中心数据劣化，提出 SlotMIM：语义瓶颈 + 跨视图一致约束诱导对象显式槽位。在 Franka Kitchen、Habitat 等 8 项任务全面超越现有 PVM。 Think Small, Act Big (PPL) 终身学习中复用“动作基元”避免遗忘 Primitive Prompt+Motion-Aware Query与 Diffusion Transformer MimicGen + LIBERO (sim)；Franka Panda TBD 4×A100（预训），1×A40（增量学习） Primitive Prompt Learning 首先在多技能库中学习可重用“动作基元”提示，再在终身阶段冻结旧提示、增量插入新提示，缓解灾难遗忘。模型利用光流-文本查询选择最相关基元，引导条件扩散 Transformer 产出动作。仿真与 Franka 实测均较 PEFT + 经验回放提升 20 % +。 Phoenix 失败后自我反思并细粒度修正动作 LLM 生成“语义-动作”双反思 → Diffusion Policy 校正 RoboMimic 仿真 + UR5 实机 GitHub 单张 RTX 4070 即可微调 Phoenix 通过 MLLM 生成“语义反思+动作反思”文本，先粗调运动指令，再用条件扩散策略做高频微调，实现失败后的自我纠正。框架把泛化压力转移到 LLM 层，低阶策略仅需少量新数据迭代。RoboMimic \u0026amp; UR5 显示跨任务鲁棒性提升 25 %。 OmniManip 物体中心交互基元 + VLM 推理，实现开箱即用操控 定义 canonical 空间 + 方向约束；零样本泛化 Franka Panda + RealSense GitHub TBD 4×A6000 训练 OmniManip 引入 “Object-Centric Interaction Primitive” 作为中间空间约束，让 VLM 只需输出 3D 关键点与方向即可驱动精密操控。通过零样本 IK + 轨迹优化，在 Franka 上完成 12 项复杂任务，平均成功率 78 %。方法避免昂贵 robot-Finetune，兼具成本与泛化优势。 Domain Discrepancy Mitigation 减小人演示与机器人视觉差 Vision Adapter 对齐嵌入；双塔 CLIP 训练 RLBench；xArm 7 GitHub 4×A6000 论文揭示人演示视频与机器人视觉存在形态与尺度差异，导致直接迁移失效。作者在视觉编码器尾部插入可学习适配器，并用跨域对比损失同时拉近语义与几何分布。RLBench 与 xArm7 实验显示在 unseen-object 设定下成功率翻倍。 Object-Centric Prompt-Driven (CrayonRobo) 用彩线标注关键方向减少冗余 四色线标 + GPT 选择最相关 3 条 SAPIEN + Franka GitHub \u0026gt;40 GB VRAM 研究用四色线条在图像中显式标注接触点与位姿方向，减少语言与视觉冗余。GPT 自动从 32 候选线中挑 3 条最相关提示，送入 CLIP-LLM 预测 SE(3) 接触位姿后由 IK 执行。模拟与 Franka 真实机结果优于端到端 VLM 8–12 pp。 Robotic Visual Instruction 用箭头/圆圈视觉语言代替冗余文本 手工 15 K 图像标注 RoVI 语言；VLM 直接生成代码 UR5 / XArm6 + SAPIEN GitHub TBD 1×A40 RoVI 提出箭头、圆圈、颜色、数字四元素的视觉指令语言，解决纯语音交互空间精度不足与嘈杂场景受限问题。作者手工标注 15 K 图像并用 VLM 生成任务代码，机器人可直接按图索骥。UR5 /X-Arm6 实验展示在无语音环境中完成多步装配。 RoboGround 结合语言先验做抓取/推动等 统一 Vision-Language-Action Prompt SAPIEN；Franka Panda GitHub 8×A100（训练） RoboGround 以 grounding-mask 作为视觉中介，将对象定位与策略网络分离；同时自动合成大规模仿真数据扩展训练域。结果在 see/unseen 物体抓取、推拉任务上均超越 End-to-End Diffusion Policy。 二、Policy / Diffusion 控制 论文 目标 创新 平台 Code 算力 总结 KStar Diffuser 双臂协作轨迹生成 物理关节动态图 + 可微前向运动学 Bimanual Franka (sim \u0026amp; real) TBD 8×A100 KStar Diffuser 用机器人双臂物理拓扑构造动态时空图并融入可微正运动学，引导扩散过程生成协调关节动作。此设计在 bimanual 夹取与对接任务上比 baseline Policy Diffusion 提升 17 %。引入 kinematic loss 使收敛速度加快 35 %。 RoboPEPP 视觉-关节姿态预训练 时序预测任务 + 姿态编码器 Isaac Gym 7DoF GitHub 4×A6000 RoboPEPP 把“遮盖-预测”自监督移植到机器人图像，强迫编码器重建被 Mask 关节嵌入，从而学到物理结构感知。微调后在多数据集姿态估计误差降低 30 %，对遮挡鲁棒性最强且推理耗时最低。 Lift3D Policy 将 2D 基础模型迁移到 3D 抓取 深度 Lift 模块 + Domain Adapt - GitHub 8×A100 Lift3D 先用任务感知 MAE 为 2D 基础模型注入深度重建能力，再通过坐标映射“抬升”到点云编码，构建显式 3D 表征。方法在 ManiSkill 等 3D 任务大幅超过纯 2D 预训和显式 3D CNN 基线。 PDFactor 三视角扩散场统一多任务策略 Tri-Perspective 视图条件扩散 Meta-World GitHub 8×A6000 PDFactor 以“鸟瞰-第一视角-自由视角”三视图为条件，学习统一 Policy Diffusion Field，同步处理抓取、转动、插配多任务。多视角条件显著提高迁移，CVPR 实验成功率相较单视角提升 22 %。 Two by Two 跨任务配对装配 Pairwise Object Assembly + Transformer SAPIEN GitHub 4×A100 作者发布含 517 物体对、18 类日常装配任务的大型 2BY2 数据集，并提出等变 SE(3) 两步姿态估计方法。新方法在所有装配任务上刷新 SOTA，并在真实机器人验证兼容性。 FlowRAM Region-Aware Mamba + Flow Matching 局部注意 + OT 监督 RLBench TBD 4×A40 FlowRAM 结合 Region-Aware Mamba 感知器与 Flow-Matching 生成器，统一视觉编码与动作扩散，提高采样效率。测试显示对 occlusion 与 multi-object 情况鲁棒性显著提升。 G3Flow 3D 语义流生成多场景抓放 TSDF + Diffusion Flow Habitat GitHub 8×A100 G3Flow 用 TSDF 构建稠密场景，再以生成式 3D 语义流预测目标-手序列，实现跨场景抓-放一体。Pose-aware 设计让训练迭代减少 40 %，零样本任务成功率提升 18 %。 DexHandDiff 自适应灵巧手规划 接触感知 diff + 物理约束 Shadow Hand TBD 8×A100 DexHandDiff 引入手-物接触显式编码与能量约束到扩散规划中，避免“漂浮抓取”幽灵态。框架在 Shadow Hand 抓转与重定位七项基准刷新记录。 Tra-MoE 多域轨迹预测条件策略 动态专家 gate + BC 初始化 ManiSkill2 TBD 4×RTX 3090 Tra-MoE 通过稀疏门控专家网络吸收多域外数据，预测任意目标轨迹，并以可学习 2D mask 对齐视觉观测指导策略。实验证明在 DomainGap20 % 情况下仍维持高成功率。 AffordDP 利用可迁移 affordance 的泛化策略 Affordance Mask + Diffusion Policy LIBERO TBD 4×A6000 AffordDP 把 3D 接触点 + 轨迹定义为可迁移 affordance，并在扩散采样中注入 6D 变换引导，支持 unseen 类别操控。与原 Diffusion Policy 比，新架构在真实实验 unseen 物体上提升 30 % 成功率。 三、Grasp 论文 目标 创新 平台 Code 算力 总结 UniGraspTransformer 通用抓取蒸馏 Transformer-Distil 模型简化训练 Shadow Hand (sim) GitHub 4×A40 提出通用 Transformer 抓取网络及简化蒸馏流程，既减低训练成本，又扩展到海量手型。实验证实缩短 40 % 训练时间同时保持高成功率。 DexGrasp Anything 面向任意物体的物理感知抓取 物理引擎约束 Embedding Franka + Mujoco GitHub 8×A100 DexGrasp Anything 在训练和采样阶段显式加入物理约束，并发布 3.4 M 抓姿-15 K 物体数据集。方法在多 benchmark 抓取精度全线领先。 ZeroGrasp 零样本形状重建驱动抓取 SDF ↔ Partial Depth BP Isaac Gym GitHub 4×A6000 ZeroGrasp 联合实时 3D 重建与抓姿预测，实现 zero-shot 抓取；采用 SDF 先验避免碰撞。近实时推理 (\u0026lt; 50 ms) 在模拟和 Franka 验证效果稳定。 四、Humanoid 论文 目标 创新 平台 Code 算力 总结 Humanoid Hiking 复杂崎岖地形步行 多技能 Curriculum + Terrain Adapt Unitree H1 TBD 8×A100 LEGO-H 框架通过层次 Transformer 预测未来局部目标，并以特权学习将视觉导航与步态控制整合，使模拟 humanoid 能走崎岖山路。跨多机器人形态验证了迁移与鲁棒性。 MobileH2R 合成数据学手递物 Unity-based Synthetic Human→Robot 数据 Clearpath Ridgeback + UR5 GitHub 4×A40 MobileH2R 用 Unity 合成高质量人-机器人交接数据，使移动底座在大空间内可靠收物。Sim-to-Real 迁移到 Ridgeback + UR5，达到 90 % 交接成功率。 五、3D Vision \u0026amp; Perception 论文 目标 创新 平台 Code 算力 总结 3D-MVP 用多视图 MAE 预训 3D 表征 分离视图编码 + Objaverse 预训 RVT + ManiSkill TBD 8×A100 3D-MVP 把 MAE 扩展到多视图 point-cloud，利用 RVT 对齐 voxel-level 特征；在多任务上全面提升 3D 理解与操控性能。 VidBot 2D 野外视频 → 零样本 3D 轨迹 SfM + 粗-细阶段扩散 Hexman Echo Plus TBD 4×A6000 VidBot 从网络 2D 视频自动恢复手轨迹与目标点，再通过粗-细扩散生成 3D 交互轨迹，实现零样本部署；在 13 项家庭任务上显著超越 SOTA。 Touch2Shape 触觉条件 3D 形状扩散 Vision-Touch 融合扩散 GelSight + iCub 手 TBD 4×A100 Touch2Shape 将 GelSight 触觉嵌入 3D Diffusion，结合 RL 引导探索，实现高保真局部细节重建；在触觉+视觉形状重建基准取得最佳分数。 六、Planning \u0026amp; Reasoning 论文 目标 创新 平台 Code 算力 总结 RoboBrain 从抽象到具体统一规划 层级 Brain 模型 + 递归搜索 SAPIEN (多机器人) GitHub 8×A100 RoboBrain 融合机器人与通用多模态数据，多阶段训练 MLLM，能处理长视频 + 高分辨率图像并输出操控计划，在多任务刷新成绩。 PhysVLM 让 VLM 知道“够不着” S-P Reachability Map 注入视觉编码 PyBullet + UR3/XArm6 GitHub 8×A800 × 48h PhysVLM 预先计算 Reachability Map 并注入 SigLip + Qwen 语义空间，LLM 可回答“到不了哪里”并辅助 VoxPoser 规划；零样本实机验证成功率最高。 RoboSpatial 训练 VLM 空间推理三视角 1 M 图 + 5 K 扫描数据集 Kinova Jaco + cuRobo GitHub 8×H100 × 20–40h ROBOSPATIAL 构造百万图像 + 5 K 3D 扫描数据集，以三视角问答方式训练 VLM 空间理解；Kinova 抓放实验超越 GPT-4o。 Tartan IMU 轻量惯导定位 F-model Transformer-Tiny + IMU 重加权 Tartan Drive Hugging Face TBD 单 RTX 2080 提出轻量级 Transformer 模型仅用 IMU 即可做定位，统一导航与姿态估计；在 TartanDrive 数据集上达成 SOTA。 Code-as-Monitor 视觉编程自动生成约束检测 LLM-to-Code + 生命周期触发 Amazon AWS 码实验室 TBD CPU-only CaM 将开集故障判定统一为约束求解问题，用 VLM 生成 Python 代码实时监控。系统在三模拟器+真实场景把成功率提高 28.7 %。 七、Video \u0026amp; Representation 论文 目标 创新 平台 Code 算力 总结 TASTE-Rob 任务导向手-物交互视频生成 三阶段：DynamiCrafter → MDM → SD-Adapter SAPIEN (sim) GitHub 8×A100 三阶段视频生成管线（DynamiCrafter→MDM→SD-Adapter）让语言-场景-手姿一致，可直接用作模仿学习数据；SAPIEN 验证动作可复现。 GraphMimic 视频 → Graph-to-Graph 生成策略 Skeleton-Graph VAE RoboMimic TBD 4×A6000 GraphMimic 把视频抽象为时序场景-动作图，并训练 Graph-to-Graphs 生成器预训练策略网络，在少量下游数据时显著提升。 八、Sim2Real \u0026amp; 机器人模型 论文 目标 创新 平台 Code 算力 总结 Prof. Robot 无自碰 \u0026amp; 无静态穿模可微渲染 带 Signed Distance 渲染器 Blender + PyTorch3D GitHub 1×A40 在可微渲染中加入 Eikonal 正则化学习碰撞分类器，实现无静态 / 自碰的梯度优化动作；对比 Dr.Robot 成本相当但碰撞率降低一半。 AutoURDF 无监督点云→URDF 模型 Cluster Registration + IK 估计 RealSense 点云 GitHub 1×RTX 3090 AutoURDF 通过点云聚类配准，无监督推断关节拓扑与参数，自动生成符合主流模拟器的 URDF；在多机器人扫描数据集精度领先。 九、基准与数据集 论文 目标 创新 规模 Code / 数据 算力 总结 RoboTwin 双臂数字孪生基准 生成式 Digital Twin 105 场景 × 10 K 轨迹 GitHub 数据预处理需 8×CPU RoboTwin 使用 3D 生成与 LLM 产出多样专家示范，建立双臂数字孪生评测；提供 105 场景、10 K 轨迹，填补双臂基准空白。 Pixel-aligned RGB-NIR Stereo 近红外+RGB 对齐立体数据 同轴 NIR Stereo 标定 50 K 对 TBD 无 提出同轴 RGB-NIR 立体系统并发布多光照数据集，示范两种融合方法显著改进暗光下深度与语义性能。 RoboSense 拥挤非结构环境 egocentric 感知 3D 相机 + 激光混合注释 150 K 帧，900 min 视频 GitHub 无 RoboSense 搭建 360° 相机-LiDAR 采集平台，发布 133 K 帧、1.4 M 3D box 的拥挤非结构环境数据，并定义近场匹配指标，覆盖 KITTI 270× 标注量。 ","permalink":"https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/","summary":"CVPR 2025 Robotics Paper Summary","title":"Bug Journal CVPR2025-Summary"},{"content":"快速文章阅读 prompt, 用于找到一个自己喜欢的题目\n这篇文章要做什么，目标是什么 动机是什么 数据是从哪里来的 算力要求多少 公开代码吗 For Robotics: 现在有这一篇文章： \u0026lt;文章标题\u0026gt; 请用中文回答我： 这篇文章要做什么，目标是什么 动机是什么 数据是从哪里来的 算力要求多少 公开代码吗 模拟环境用的是什么平台 现实环境用的是什么平台 ","permalink":"https://tzj2006.github.io/bugjournal/2025-06-14/","summary":"Prompt for fast paper read","title":"Bug Journal 2025-06-14"},{"content":"Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction CVPR 2025\nFrom 人大 \u0026amp; 上海AI lab\n要达成的事情：\n让机器人能够自我反思到底是哪里做得不好，然后自我调整\n动机：\n人类可以很自然地反思：为什么失败了，为什么机器人不行呢？\n模型实现方式：\n首先，这是一个“语言指导”的RL方法。\n对于这个方法，首先由 LLM 生成一个文字指令：\n比如：现在我要移动一个杯子，我要怎么做\n然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。\n如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。\n现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL\n最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。\n对于修正指令这个步骤，模型一共会输出两条语句，分别是 \u0026ldquo;语义上的反思\u0026rdquo; (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)\n最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。\n那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。\n结果：\n更强的学习能力，更强的泛化能力。\n计算要求：\n仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行\n虚拟环境为 RoboMimic 模拟器\n使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs\nPhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability CVPR 2025\nFrom 北京交通大学 \u0026amp; 广东技术师范大学\n目标： 告诉机器人什么位置它到不了\n动机： 有时候机器人不知道一个位置到不到得了，结果把自己搞坏了\n模型流程：\n首先离线计算什么位置是机械臂能达到的。 形成一个点云 (S-P Map)\n然后用 SigLip-400M 提取图像和点云的特征\n然后把这个 embedding 和文字的 embedding 混合之后\n通过 Qwen-2.5-Instruct-3B，生成一个回答给 VoxPoser 做。\n实验设计：\n仿真：基于 CUDA 11.x 的 PyBullet 环境，六种机器人（UR5、FR5、CR5、FRANKA、UR3、XArm6），共 1.3K 问答，零样本评测。\n实机：在 UR3、XArm6 真实平台上各 10 次零样本任务，评估任务成功率。\n评估指标：\nEQA-phys：基于 LLM 评分（5 分正确、1 分错误）计算平均分； RoboVQA-val / OpenEQA：标准 BLEU-4 与准确率； 任务规划：成功率。\n结果：\nS-P Map 在很多 LLM 上都有用\nPhysVLM-3B 效果平均最好\n数据集： Zero-shot\n算力要求：\n\u0026lt; 48h * 8 * A800\n代码：\n开源\nObject-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation CVPR 2025\nFrom PKU Agibot Lab\n目标： 帮助机器人排除冗余信息干扰\n动机： 语言和视频中冗余信息过多\n模型流程：\n在图片上加一些标记 分别是：\n接触点（蓝色） 末端执行器在接触时的 z 轴方向（红色） y 轴方向（绿色） 接触后移动方向（黄色） 这些标记可能来自 Grounding-Dino + SAM 或者 ChatGPT-4o 或者 手动标记\n标记方式如下：\n均匀生成 N(32) 条线，让 GPT 选择 3 条标上颜色\n然后把这个“增强”过后的信息输入给 CLIP,然后再让 LLM 输出\u0026quot;应该在哪里，以什么角度接触\u0026quot;\n对于这个信息，我们可以和GT 做 train\n最后，我们得到了这个位置和姿态的信息，我们就可以用IK求解路径了。\n结果：\n数据集：\n模拟环境：SAPIEN + PartNet-Mobility •\t平台：使用 SAPIEN（一个支持刚体与关节物体的物理模拟器），其提供高保真动力学与渲染接口，适合零样本机器人操控研究 ￼ ￼。 •\t资产集：加载 PartNet-Mobility 中的 1,500 多个关节化 CAD 模型，涵盖抽屉、门、笔记本电脑盖等常见家电与家具 ￼。 •\t飞行夹持器（Flying Gripper）：采用 SAPIEN 中的“飞行”版 Franka Panda Gripper，使得末端执行器可在自由空间中无碰撞地移动，以便专注测试模型对提示的理解能力 ￼。 •\t摄像机随机化：在每个场景随机采样摄像机视角（水平±45°、俯仰30°–60°），并在单帧中记录 RGB 图像与 3D 末端执行器位姿，用于提示生成与训练 ￼。 •\t数据采集：共采集 ~10,000 条训练样本，过程耗时约 6–8 小时；测试集与训练集按照固定比例拆分，模拟了 Seen/Unseen 物体类别的泛化能力评估 ￼ ￼。 现实机器人平台 •\t硬件平台：使用真实的 Franka Emika Panda 机械臂（7 自由度，集成高精度力矩传感器），配合标准的 RGB-D 摄像头（如 Intel RealSense D415）采集视觉输入 ￼。 •\t执行流程：将 CrayonRobo 在模拟环境中输出的 SE(3) 接触位姿与移动方向，通过 ROS + MoveIt! 的 IK 求解与笛卡尔轨迹规划一键下发真实机器人执行 ￼。 •\t测试任务：与模拟中一致，包括“拉抽屉”“开门”“掀笔记本盖”等单步原子操作，每个任务在 5–10 个不同初始姿态下重复试验。 •\t评价指标：以二值成功率衡量：当末端执行器按照预测方向将物体部件移动超过阈值（如 5 cm）即记为成功；平均成功率达到 74%（Seen）与 72%（Unseen） ￼。 算力要求： 未知 建议 \u0026gt; 40 GB VRAM\n代码： 开源\nCheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation CVPR 2025\nFrom PKU Agibot lab\n目标： 让机器人读取说明书之后根据说明书做出正确操作\n动机： 阅读说明书\n电子产品有时候视觉信息不够，很多有着相似外观的东西可能有着不同的功能。\n所以要读说明书\n模型流程：\nOCR → GPT-4o 提取图文信息 → GPT‑4o 生成操作步骤 → SoM、Grounding‑DINO、SAM 实现视觉对齐 → FoundationPose 匹配 CAD\n最后输出{部件，操作} pair, 最后使用 VoxPoser 进行操作。\n实验设置 模拟平台：SAPIEN 0.8 + Franka Panda 飞行夹具 ￼\n数据集： PartNet-Mobility CAD 模型； CheckManual 合成说明书（已公开，可下载使用） ￼\n评估指标： 任务完成率\n现实验证： Franka + RealSense 摄像头，完成单个用例的实物测试\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\nCode availability: 开源\n结果：\n总之有 manual 效果更好\nROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics CVPR 2025\nFrom OSU and NVIDIA\n目标：\n引导 VLM 2D \u0026amp; 3D 视觉，理解空间结构。\n动机：\nVLM 目前无法理解空间结构。 原因并非 VLM 不行，而是数据不够\u0026quot;spatial\u0026quot;\n模型实现细节：\n首先是数据收集：\n输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”\n同时，生成一个俯视图，来看看哪里适合放置一个物体。\n最后在看看这个物体是否适合被放在这个地方。\n对于物体的方位，每一次会从三个角度问问题：\n以机器人为中心的视角 (第一视角) 以物体为中心的视角 (第三视角) 以世界为中心的视角 (fix-cam)\n自我中心：“从您的视角看，书在电脑的左边吗？” 以物体为中心：“从电脑的视角看，书在电脑的左边吗？” 以世界为中心：“从海拔高度看，书在电脑的上方吗？” 这样的好处是可以让 VLM 有更强的空间理解\n结果：\n在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o\n算力要求：\n20-40h * 8 * H100\n模拟环境：\nROBOSPATIAL 数据集，这包括 ScanNet, Matterport3D, 3RScan, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。\n真实环境：\nKinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。 机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。\nRobotic Visual Instruction: A New Paradigm for Human-Robot Communication CVPR 2025\nFrom IC + 上海 AI lab + UCSD \u0026hellip;\n目标：\n更好的人机交互\n动机：\n语言有很多冗余信息，那在图片中增加信息不就行了？\n模型实现方式：\n机器人视觉指令 (RoVI) RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：\n箭头：指示运动方向和轨迹 圆圈：标记感兴趣的对象或动作目标 颜色：区分多个指令或动作步骤 数字：指示操作顺序 这种视觉语言具有以下几个优点：\n空间精确性：视觉标记精确地指示3D空间中的位置和路径 时间清晰性：顺序步骤被清晰地划分 直观设计：这些符号易于人类理解和创建 跨文化实用性：视觉指令超越语言障碍 RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。\n靠手动标记了 15K 图片。。。 工作量爆炸💥 而且以后也要人手标。。。\n🤔： 为什么不能自动标，难道作者没有想过这一点吗\n但总之，现在在这个图像的基础上，VLM 会帮忙生成：\n任务的文字描述 可以执行这些任务的代码 代码中包含：\n路径 起点，终点，过程点\n然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取\n算力要求：\nNvidia A40\n现实实验设置：\nUFACTORY X-Arm 6和UR5 两台机械臂 两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。 两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。\n模拟环境设置：\n使用了SAPIEN 作为模拟器。 SIMPLER 作为基础环境。\nTASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation CVPR 2025\nFrom Xiaoguang Han\u0026rsquo;s Lab at 港中深\n目标： 优化对齐视频-人手数据集\n动机： 现在有这些问题：\n视角不一致 动作语义无法对齐 手部姿态稳定性不高 这个模型想要解决这些问题 模型流程：\n数据集构建（Sec. 3）： •\t100,856 条 1–8 秒单动作视频，静态 1080p 视角，语义指令一一对应。 粗视频生成（Stage I – Coarse Action Planner）： •\t基于 DynamiCrafter（512 × 512 分辨率），语言指令＋静态环境图→粗交互视频； •\t微调参数：batch=16, lr=5×10⁻⁵, 30K steps。 姿态优化（Stage II – MDM Refinement）： •\t使用 Motion Diffusion Model 以 60 帧手部关键点序列为条件，保证握姿一致性； •\t训练设置：batch=64, lr=1×10⁻⁴, 500K steps; 推理 10-step DDIM denoise。 最终生成（Stage III – Frame-wise Adapter）： •\t将优化后手部姿态图像与环境图和语言指令一起接入 Stable Diffusion 2 Adapter，生成最终视频； •\t训练设置：batch=32, lr=5×10⁻⁵, 30K steps。 实验设置 •\t仿真验证：在 SAPIEN 模拟平台上，用 DynamiCrafter＋MDM + Adapter 生成视频，并通过 Figure 17 示范在虚拟机器人上的模仿效果。\n模型流程：\n第一阶段：Coarse Action Planner（粗动作生成） •\t目标：在给定语言指令（如“拿起杯子并倒入水”）和静态环境图（512×512 分辨率）条件下，生成一段粗略的、合理的手–物体交互视频序列（通常 16 帧）。 •\t模型架构：基于 Video Diffusion Model（VDM），如 DynamiCrafter，使用文本–图像条件扩散网络。 •\t训练细节： •\tBatch size = 16，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t推理时使用 50-step DDIM 采样，平衡生成质量与速度。 •\t输出：一段低分辨率、含大致手部运动轨迹和物体交互的粗视频，用于后续姿态细化。\n⸻\n第二阶段：Motion Diffusion Model (MDM) Refinement（姿态细化） •\t目标：针对第一阶段生成的粗视频，从中提取手部的 3D 关键点序列（60 帧），并利用扩散模型细化运动轨迹，提升抓握姿态的连贯性与自然度。 •\t模型架构：1D 时序扩散网络，对帧间手部关键点做去噪与重建。 •\t训练细节： •\tBatch size = 64，学习率 1×10⁻⁴； •\t训练步数 500K steps； •\t推理时使用 10-step DDIM，快速得到精细关键点序列。 •\t输出：一组平滑、符合物理约束的 3D 手部关键点轨迹，用以指导下一阶段的视频生成。\n⸻\n第三阶段：Frame-wise Adapter（帧级最终生成） •\t目标：将姿态细化后的关键点与原环境图、语言指令结合，生成最终高质量、手部姿态稳定的交互视频。 •\t模型架构：基于 Stable Diffusion 2，通过 Adapter 模块将手部姿态（以可视化关节点或姿态图形式）作为条件，连同环境图与文本，一起输入扩散模型。 •\t训练细节： •\tBatch size = 32，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t采用与 Stage I 相似的 DDIM 采样流程，保证画质与动作一致性。 •\t输出：分辨率可达 512×512 的连续视频帧序列，手–物体交互清晰、握姿自然，可直接用于机器人模仿学习。\nMitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation CVPR 2025\nFrom 港科广\n目标：\n缩小人机之间的 Gap\n动机：\n人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap\n问题：\n现在有这些 Gap:\n视觉环境差异 人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。\n形态差异： 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。\n尺度和视角： 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。\n解决这一差距的传统方法分为两大类，每类都有显著的局限性：\n预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。\n在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。\n模型实现方式：\nbasically, 就是说希望用一个 Adaptor 来 fill in the gap.\n把从 human demo pretrain embedding 转换成 robot demo embedding.\n对于任务感知也是如此。\n用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。\n结果：\nAlign 一下效果变好了\n算力要求：\n4 * Nvidia A6000\n模拟环境为 RLBench\n真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头\nMoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation CVPR 2025\nFrom 北邮 + 南阳理工 + 清华\n目标：\n导航 + 空间操作\n动机：\n虽然静态的操作已经没问题了，但是若是平台移动就不太好办。\n实现细节：\n这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分\nMoManipVLA使用以下方法实现这些优化问题：\n双退火搜索算法用于基座位置寻找优化 序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案 该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：\nRGB相机进行视觉感知 关节编码器进行本体感知（感知机器人自身位置） (optional) 深度感知以增强障碍物避免\n结果：\n模拟环境：\n模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。\n真实环境（机械臂）：\n在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。\n算力：\n4 * RTX 3090\nThink Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation CVPR 2025\nFrom 上交，复旦，上海 AI lab\n目标：\n如何避免灾难性遗忘\n动机：\n有些动作有相似之处，比如递筷子和递镊子有相似之处 那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？\n模型实现细节：\nPPL 框架的核心组件包括：\n输入编码器： 本体感觉编码器：处理机器人的关节状态和夹爪姿势 视觉编码器：处理场景的 RGB 图像 光流编码器：处理光流信息以捕获运动模式 文本编码器：处理任务的语言指令\n基元提示： 跨任务共享的基本运动模式的学习表示 注入到多头自注意力层的键和值中\n终身提示 (Lifelong Prompts)： 在终身学习期间为新任务学习的特定于任务的提示 与原始提示连接以自定义模型的行为\n运动感知提示查询 (Motion-Aware Prompt Query)： 结合光流和文本指令信息 用于确定不同原始提示的相关性\n扩散Transformer (Diffusion Transformer)： 基于条件输入和提示生成机器人动作\n算力：\n论文中没有直接说明具体的GPU类型和训练时间。\n模拟环境：\n模拟实验是在基于 MimicGen 和 LIBERO 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。\n机械臂：\n真实世界的实验是在 Franka Panda 机械臂上进行的。\n数据集：\n论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。\n获取方式：\n这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。\nGenerating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision CVPR 2025\nFrom Kyoto University\n这篇论文的标题是“从自我中心视角下的动作描述生成6自由度物体操作轨迹”，其核心目标是根据文本描述和初始视觉输入来生成物体在3D空间中的操作轨迹序列。\n目标： 从自我中心视频中提取6自由度（6DoF）物体操作轨迹。 基于这些提取的轨迹和相关的动作描述，开发能够根据文本动作描述生成物体操作轨迹的模型。\n动机： 开发能够协助人类活动的交互式机器人，其中一个关键能力是让机器人能够按人类指令操作工具和物体。 训练生成此类操作轨迹的模型需要大量多样且详细的物体操作演示，但这在实际中很难大规模收集。 利用日常工作场景中的视频来提取人类演示中的各种物体操作，是一种很有前景的解决方案。 现有的3D物体操作轨迹预测方法受限于昂贵的3D注释成本，难以处理多样化的操作动作。 现有数据集（如HOT3D）中的人类运动轨迹数据有限，不足以开发能够生成物体操作轨迹的模型。\n数据来源： 训练数据： 论文使用Ego-Exo4D数据集 [30] 来构建大规模训练数据集，包含6DoF物体操作轨迹以及图像、深度图和动作描述。这个过程是自动化的，不依赖于预设的相机外部参数。\n评估数据： 论文使用现有的HOT3D [3] 数据集进行评估。HOT3D是一个用于3D手和物体跟踪的自我中心视角数据集，通过光学标记和多红外OptiTrack相机记录，提供了精确的手和物体6DoF信息。\n算力要求： 论文提到模型训练了30个epoch，批处理大小为8。优化器使用了AdamW，基础学习率为2e-5（对于LLMs）和2e-4（对于其他参数）。 为了处理旋转连续性问题，每个角度（roll, pitch, yaw）都用[cos(θ), sin(θ)]表示。 虽然论文没有直接给出具体的算力消耗数字（例如所需的GPU数量或训练时间），但从训练epoch和批处理大小来看，以及使用了BLIP-2、VILA、PointLLM和MiniGPT-3D等大型视觉语言模型作为骨干网络，可以推断出这需要相当大的计算资源。这些模型本身就比较大，并且训练涉及处理大规模视频数据。\n公开代码： 论文在摘要中明确提到了数据集和代码的链接：https://biscue5.github.io/egoscaler-project-page/。\n模拟环境和现实环境平台： 论文没有提到使用了特定的模拟环境平台。 在现实环境方面，论文主要使用了自我中心视频数据。HOT3D [3] 数据集是通过Project Aria眼镜 [21] 和Quest 3 [65] 记录的，这些是现实世界中的可穿戴设备，用于捕捉自我中心视角下的活动。这表明他们的研究是基于真实世界中的人类活动数据。\nUniversal Actions for Enhanced Embodied Foundation Models CVPR 2025 From 清华\n这篇文章介绍的 UniAct 框架，目标是解决具身基础模型在处理异构动作数据时面临的挑战，并构建一个能够在通用动作空间中操作的框架。\n主要目标：\n构建通用动作空间： 学习一种能够捕捉不同机器人通用原子行为的动作空间，从而消除机器人之间因物理形态和控制接口差异造成的动作异构性。\n实现跨形态泛化： 使得具身基础模型能够有效利用跨领域数据，并在不同的机器人形态之间实现更好的泛化控制和适应能力。\n提高模型效率： 训练一个相对较小（0.5B 参数）但性能优于更大（14倍）现有模型的具身基础模型，证明通用动作的优势。\n动机： 数据异构性挑战： 现有的大型基础模型在自然语言处理和计算机视觉领域取得了巨大成功，主要得益于海量的、多样化的互联网数据。然而，将同样的方法应用于具身智能体时，面临一个显著的挑战：不同机器人收集的动作数据存在显著的异构性，因为它们有不同的物理形态和控制接口。这种异构性严重阻碍了跨领域数据共享和通用具身基础模型的发展。\n现有解决方案的局限性： 大多数现有方法要么强制性地将不同动作空间视为等效，采用统一的离散化或归一化技术，但这可能导致动作编码的物理意义冲突；要么试图设计一个适用于各种机器人系统的物理可解释动作空间，但这需要大量人工工程，且未能充分利用不同具身动作空间之间的内在联系。\n对通用代理的需求： 开发能够处理跨任务、跨环境和跨形态泛化的通用具身基础模型，是构建通用具身智能体的一个有前景的途径。 数据来源：\nUniAct-0.5B 模型在训练时整合了来自多个开源机器人数据集的示范数据。这些数据集包括：\nOpen-X Embodiment (OXE) Libero Droid 这些数据被标准化，以包含第三人称视角观察和语言指令，同时保留了动作的异构性。总共使用了来自 28 种不同机器人形态的约 100 万个示范数据进行训练。\n算力要求： UniAct-0.5B 的训练是在 64 块 A100 GPU 上进行的，并使用了 DeepSpeed 进行优化，持续了 10 天。\n公开代码： 是的，文章中提到了项目的项目页面，通常这意味着代码是公开的： 项目页面\nSOLAMI: Social Vision‑Language‑Action Modeling for Immersive Interaction with 3D Autonomous Characters CVPR 2025\nSOLAMI 这篇文章旨在介绍一个端到端的社交视觉-语言-动作（VLA）建模框架，用于与 3D 自动角色进行沉浸式交互。\n文章的目标是构建能够感知、理解并与人类互动的 3D 自动角色，使其具备类似于人类的社交智能，通过多模态响应（语音和动作）驱动角色进行社交互动。\n这项研究的动机在于，目前的字符代理在与用户交互时，主要限于文本或语音交互，缺乏更丰富的模态。在社交互动中，沉浸感越深，人类体验越好。因此，研究人员希望构建具有更丰富模态的 3D 自动角色。此外，多模态交互数据非常稀缺，难以获取，这也促使他们开发了数据合成方法。\n数据主要来源于以下几个方面：\n交互式多模态数据（SynMSI）： 这是一个合成的多模态社交互动数据集，通过自动化流程生成，利用了现有的文本-动作数据集、基于文本的角色扮演模型和语音合成方法。SynMSI 数据集包含 6.3K 多轮多模态对话项。 运动数据：为了进行预训练阶段的运动与文本对齐，以及生成多模态数据用于指令微调，研究人员收集了包含丰富社交动作的现有数据集，例如 HumanML3D (24K 运动-文本对)、Inter-X (20K 运动-文本对和 10K 两人运动对)，以及 DLP-MoCap (2K 运动-文本对)。\n语音数据： 用于预训练阶段的语音-文本对齐，使用了 CommonVoice (150K 语音-文本对)、AnyInstruct (200K 语音-文本对和 100K 语音到语音项)，以及通过文本到语音方法（Azure TTS 和 XTTS_v2）生成的合成语音数据 (60K 语音-文本对)。\n算力要求： 在预训练阶段，SOLAMI 使用了 32 块 V100 GPU 来训练模型，批处理大小为 256。 在指令微调阶段，SOLAMI 使用了 16 块 V100 GPU，批处理大小为 48。推理时，所有模型都部署在 2 块 H800 GPU 上，并采用 vLLM 框架和异步机制来提高性能并保持公平性。\n代码： 项目的 GitHub 链接\n模拟环境在文章中没有明确提及，但实验中提到了使用 VR 界面进行用户研究，其中用户可以与各种 3D 角色进行沉浸式交互。\n现实环境指的是 VR 界面，研究人员开发了一个基于 Oculus Quest 3 前端和后端服务的 VR 界面。前端实现用户与 3D 自动角色的沉浸式交互，后端由 2 块 H800 GPU 提供算力支持。在实际使用中，VR 头显捕获用户的语音和身体动作，并将其发送到后端计算节点。\nA Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning CVPR 2025 from HKU\n这篇文章主要研究了预训练视觉模型（PVMs）在机器人学习任务中的应用，特别是视觉运动控制和感知任务。文章的目标是找出最优的预训练方法和数据来源，以提高PVMs在机器人学习任务中的表现。\n动机 文章的动机源于当前PVMs在机器人学习任务中的应用存在一些问题和局限性。尽管PVMs在传统视觉任务中表现出色，但它们在机器人学习任务中的最优配置仍不清楚。文章通过系统性的评估发现，虽然某些PVMs（如DINO和iBOT）在视觉运动控制和感知任务中表现出色，但它们在非对象中心（NOC）数据上的表现会显著下降。这种下降与它们学习对象中心表示的能力减弱密切相关。\n数据来源 文章中使用的数据集包括：\n对象中心数据集：ImageNet 场景中心数据集：COCO 网络爬取数据：CC12M 以自我为中心的数据：Ego4D 这些数据集被用来评估PVMs在不同类型的数据上的表现。\n算力要求 由于PVMs的训练和评估需要大量的计算资源，文章中提到使用了8个A100 GPU进行训练。对于某些任务，如导航任务，需要大约400M到500M步的训练和512到320个并行环境，这对计算资源提出了极高的要求。\n代码公开情况 文章中提到，他们的代码和模型是公开可用的，链接\n模拟环境 文章中使用了多个模拟环境平台进行评估，包括：\nFranka Kitchen Meta-World Habitat（用于导航任务，包含HM3D和Gibson环境） 现实环境 虽然文章主要关注模拟环境中的评估，但提到PVMs在现实环境中的应用潜力。现实环境中的平台并未在文章中具体提及，但提到了多个现实环境中的机器人学习任务和应用。\n总结 总的来说，这篇文章通过系统性的评估和实验，提出了SlotMIM方法，以有效地从NOC数据中学习对象中心的表示，并在多个任务中取得了优于现有方法的性能。文章的研究为PVMs在机器人学习任务中的应用提供了新的见解和方法。\nOmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints 这篇文章介绍了一个名为 OmniManip 的新方法，旨在实现通用机器人操作。\n这篇文章要做什么，目标是什么？ 这篇文章提出了 OmniManip，这是一种开放词汇的操控方法，旨在弥合视觉-语言模型 (VLM) 的高层推理能力与低层精确操控之间的差距。其核心目标是开发一个通用的机器人操控系统，能够通过物体中心交互基元作为空间约束，在非结构化环境中执行各种操控任务，并具有强大的零样本泛化能力。\n动机是什么？ 开发能够在非结构化环境中进行通用机器人操控的系统是一个重大挑战。虽然 VLM 在高层常识推理方面表现出色，但它们缺乏精确操控任务所需的精细 3D 空间理解能力。现有的解决方案，例如在机器人数据集上微调 VLM，面临数据收集成本高昂和泛化性差的问题。通过将机器人动作抽象为交互基元并利用 VLM 定义空间约束，是解决这些挑战的动机。\n数据是从哪里来的？ 文章中提到，为了评估 OmniManip 在真实世界场景中的操控能力，他们设计了 12 个任务来评估模型的操控能力，这些任务涵盖了各种对象和复杂环境。虽然没有明确说明具体的数据集来源，但实验部分提到了通过 OmniManip 自动生成演示数据，并收集了每项任务 150 条轨迹用于训练行为克隆策略。\n算力要求多少？ 文章中没有直接给出具体的算力要求，但提及了多个 VLM 调用会带来计算挑战，即使进行了并行处理也是如此。这暗示了该系统可能需要较高的计算资源。\n公开代码吗？ 文章中没有明确提到代码是否公开。\n模拟环境用的是什么平台？ 文章中没有提到具体使用了哪个模拟环境平台。\n现实环境用的是什么平台？ 现实环境实验平台是基于 Franka Emika Panda 机器人臂搭建的，并配备了 UMI 机械手。感知方面，使用了两台 Intel RealSense D415 深度相机，一台安装在机械手上提供第一人称视角，另一台则放置在机器人对面提供第三人称视角。\nVidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic CVPR 2025\nFrom Technical University of Munich\n目标： 大规模网络视频人类样本学习 训练 家务机器人 模型\n动机： 机器人依赖实例教学，但是做家务没那么多教学\n模型流程：\n模型概览\nVidBot 通过三大模块——3D 可交互性提取、粗阶段 affordance 预测、细阶段轨迹生成——实现从“野外”RGB 视频到机器人可执行动作的零样本迁移。首先，它利用 Structure-from-Motion 和度量深度模型，从单目视频中恢复一致的 3D 手部轨迹与接触/目标点；然后，因式分解成粗预测网络 πc（提取高层次接触点与目标点）与细预测网络 πf（基于扩散生成精细轨迹），并在推理时引入多目标、法线与避碰等可微成本进行采样指导；最后，将生成的 3D 交互轨迹直接部署于多种机器人平台，实现开“即用”的零样本操控能力 ￼ ￼。\n⸻\n3D 可交互性提取管道 1.1 数据准备 •\t视频与语言输入：给定原始 RGB 图像序列 {Ĩ0,…ĨT} 及指令 l。 •\tSfM 与深度预测：使用 SfM 系统估计相机内参 K、无尺度位姿 {TWC} 及稀疏地标，再调用度量深度基模型（如 ZoeDepth）生成密集深度 {D̂t}，实现时序一致且度量尺度的重建 ￼； •\t手-物体分割与填充：借助手—物体检测（如 [72]）与分割模型（如 [94]）提取手部与接触物体掩码，再通过视频修复生成无手帧，以消除动态遮挡对优化的影响 ￼。\n1.2 姿态与尺度优化 •\t全局尺度校正：优化全帧尺度 sg，使稀疏地标深度与预测深度对齐； •\t位姿细化：联合优化每帧位姿 TWCi 与局部尺度 si，补偿 SfM 在手—物体动态区域的误差，实现一致的 3D 重建 ￼。\n1.3 交互表示提取 •\t手部中心轨迹：将优化后各帧手部中心点恢复至首帧坐标系，插值形成平滑交互轨迹 τ̂； •\t接触与目标点采样：在首帧均匀下采样手部中心生成接触点 ĉ，在末帧提取目标点 ĝ，用于监督模型的中间预测； •\t表示定义：最终得到的 3D affordance a = {c, τ}，其中 c∈ℝNc×3 为接触点集，τ∈ℝH×3 为轨迹序列 ￼ ￼。\n⸻\n粗—细分级 Affordance 学习 2.1 模型结构因式分解\n将 affordance 模型 π({Ĩ, D̃},l) 分解为： 1.\t粗阶段 πc：从 RGB-D 图像 Ĩ→D̃ 及指令 l 中预测目标点 g 和接触点 c； 2.\t细阶段 πf：在粗阶段输出 {g,c} 及场景上下文指导下，通过扩散模型生成精细轨迹 τ ； 整个流程利用测试时可微成本（多目标到达、碰撞避让等）对采样进行引导，增强与新场景/新形体的适应性 ￼ ￼。\n2.2 粗阶段：目标与接触点预测 •\t输入预处理：使用开集物体检测器裁剪关注物体区域的 RGB-D 图像； •\t网络架构：πc 由两支网络 πgoalc 与 πcontc 组成，分别输出目标和接触热图及（目标点）深度。 •\t融合视觉特征、RoI 池化后得到的物体嵌入、语言嵌入（CLIP 特征）和位置编码，通过 Perceiver 与 Transformer 生成每像素概率分布； •\t3D 投影：根据相机内参与热图深度，将像素坐标提升到三维空间，得到 g∈ℝNg×3 与 c∈ℝNc×3 ￼。\n2.3 细阶段：基于扩散的轨迹生成 •\t条件扩散模型：πf 采用 1D U-Net 架构，输入包含轨迹状态 τk、TSDF 空间特征（由 3D U-Net 从 RGB-D 构建的体素化 TSDF 地图提取）、语言与物体嵌入等； •\t正向/反向过程：遵循扩散概率模型，逐步加入高斯噪声，再由网络学习去噪，直接回归未加噪轨迹 τ̄0； •\t测试时引导：在每个去噪步骤中加入可微成本函数——多目标到达、法线一致、碰撞避让——通过扰动引导采样，提高交互轨迹在新场景/新形体下的合理性与可执行性 ￼ ￼。\n⸻\n输入与输出 •\t输入：首帧的“修复”RGB 图像 Ĩ̃0、对应度量深度 D̃0、裁剪的物体图像 Ĩ̃0o 及语言指令 l ￼。 •\t输出：3D 接触点集 c 和交互轨迹 τ ，直接可用于机器人控制模块执行操作 ￼。 算力要求： 没说\nCode availability: 暂时没有 (2025-06-11)\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-13/","summary":"Paper review of CVPR 2025","title":"Bug Journal 2025-06-13"},{"content":"Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction CVPR 2025\nFrom 人大 \u0026amp; 上海AI lab\n要达成的事情：\n让机器人能够自我反思到底是哪里做得不好，然后自我调整\n动机：\n人类可以很自然地反思：为什么失败了，为什么机器人不行呢？\n模型实现方式：\n首先，这是一个“语言指导”的RL方法。\n对于这个方法，首先由 LLM 生成一个文字指令：\n比如：现在我要移动一个杯子，我要怎么做\n然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。\n如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。\n现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL\n最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。\n对于修正指令这个步骤，模型一共会输出两条语句，分别是 \u0026ldquo;语义上的反思\u0026rdquo; (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)\n最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。\n那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。\n结果：\n更强的学习能力，更强的泛化能力。\n计算要求：\n仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行\n虚拟环境为 RoboMimic 模拟器\n使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs\nRobotic Visual Instruction: A New Paradigm for Human-Robot Communication CVPR 2025\nFrom IC + 上海 AI lab + UCSD \u0026hellip;\n目标：\n更好的人机交互\n动机：\n语言有很多冗余信息，那在图片中增加信息不就行了？\n模型实现方式：\n机器人视觉指令 (RoVI) RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：\n箭头：指示运动方向和轨迹 圆圈：标记感兴趣的对象或动作目标 颜色：区分多个指令或动作步骤 数字：指示操作顺序 这种视觉语言具有以下几个优点：\n空间精确性：视觉标记精确地指示3D空间中的位置和路径 时间清晰性：顺序步骤被清晰地划分 直观设计：这些符号易于人类理解和创建 跨文化实用性：视觉指令超越语言障碍 RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。\n靠手动标记了 15K 图片。。。 工作量爆炸💥 而且以后也要人手标。。。\n🤔： 为什么不能自动标，难道作者没有想过这一点吗\n但总之，现在在这个图像的基础上，VLM 会帮忙生成：\n任务的文字描述 可以执行这些任务的代码 代码中包含：\n路径 起点，终点，过程点\n然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取\n算力要求：\nNvidia A40\n现实实验设置：\nUFACTORY X-Arm 6和UR5 两台机械臂 两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。 两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。\n模拟环境设置：\n使用了SAPIEN 作为模拟器。 SIMPLER 作为基础环境。\nMitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation CVPR 2025\nFrom 港科广\n目标：\n缩小人机之间的 Gap\n动机：\n人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap\n问题：\n现在有这些 Gap:\n视觉环境差异 人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。\n形态差异： 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。\n尺度和视角： 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。\n解决这一差距的传统方法分为两大类，每类都有显著的局限性：\n预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。\n在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。\n模型实现方式：\nbasically, 就是说希望用一个 Adaptor 来 fill in the gap.\n把从 human demo pretrain embedding 转换成 robot demo embedding.\n对于任务感知也是如此。\n用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。\n结果：\nAlign 一下效果变好了\n算力要求：\n4 * Nvidia A6000\n模拟环境为 RLBench\n真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头\nMoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation CVPR 2025\nFrom 北邮 + 南阳理工 + 清华\n目标：\n导航 + 空间操作\n动机：\n虽然静态的操作已经没问题了，但是若是平台移动就不太好办。\n实现细节：\n这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分\nMoManipVLA使用以下方法实现这些优化问题：\n双退火搜索算法用于基座位置寻找优化 序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案 该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：\nRGB相机进行视觉感知 关节编码器进行本体感知（感知机器人自身位置） (optional) 深度感知以增强障碍物避免\n结果：\n模拟环境：\n模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。\n真实环境（机械臂）：\n在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。\n算力：\n4 * RTX 3090\nROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics CVPR 2025\nFrom OSU and NVIDIA\n目标：\n引导 VLM 2D \u0026amp; 3D 视觉，理解空间结构。\n动机：\nVLM 目前无法理解空间结构。 原因并非 VLM 不行，而是数据不够\u0026quot;spatial\u0026quot;\n模型实现细节：\n首先是数据收集：\n输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”\n同时，生成一个俯视图，来看看哪里适合放置一个物体。\n最后在看看这个物体是否适合被放在这个地方。\n对于物体的方位，每一次会从三个角度问问题：\n以机器人为中心的视角 (第一视角) 以物体为中心的视角 (第三视角) 以世界为中心的视角 (fix-cam)\n自我中心：“从您的视角看，书在电脑的左边吗？” 以物体为中心：“从电脑的视角看，书在电脑的左边吗？” 以世界为中心：“从海拔高度看，书在电脑的上方吗？” 这样的好处是可以让 VLM 有更强的空间理解\n结果：\n在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o\n算力要求：\n20-40h * 8 * H100\n模拟环境：\nROBOSPATIAL 数据集，这包括 ScanNet, Matterport3D, 3RScan, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。\n真实环境：\nKinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。 机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。\nThink Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation CVPR 2025\nFrom 上交，复旦，上海 AI lab\n目标：\n如何避免灾难性遗忘\n动机：\n有些动作有相似之处，比如递筷子和递镊子有相似之处 那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？\n模型实现细节：\nPPL 框架的核心组件包括：\n输入编码器： 本体感觉编码器：处理机器人的关节状态和夹爪姿势 视觉编码器：处理场景的 RGB 图像 光流编码器：处理光流信息以捕获运动模式 文本编码器：处理任务的语言指令\n基元提示： 跨任务共享的基本运动模式的学习表示 注入到多头自注意力层的键和值中\n终身提示 (Lifelong Prompts)： 在终身学习期间为新任务学习的特定于任务的提示 与原始提示连接以自定义模型的行为\n运动感知提示查询 (Motion-Aware Prompt Query)： 结合光流和文本指令信息 用于确定不同原始提示的相关性\n扩散Transformer (Diffusion Transformer)： 基于条件输入和提示生成机器人动作\n算力：\n论文中没有直接说明具体的GPU类型和训练时间。\n模拟环境：\n模拟实验是在基于 MimicGen 和 LIBERO 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。\n机械臂：\n真实世界的实验是在 Franka Panda 机械臂上进行的。\n数据集：\n论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。\n获取方式：\n这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。\nGenerating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision CVPR 2025\nFrom Kyoto University\n这篇论文的标题是“从自我中心视角下的动作描述生成6自由度物体操作轨迹”，其核心目标是根据文本描述和初始视觉输入来生成物体在3D空间中的操作轨迹序列。\n目标： 从自我中心视频中提取6自由度（6DoF）物体操作轨迹。 基于这些提取的轨迹和相关的动作描述，开发能够根据文本动作描述生成物体操作轨迹的模型。\n动机： 开发能够协助人类活动的交互式机器人，其中一个关键能力是让机器人能够按人类指令操作工具和物体。 训练生成此类操作轨迹的模型需要大量多样且详细的物体操作演示，但这在实际中很难大规模收集。 利用日常工作场景中的视频来提取人类演示中的各种物体操作，是一种很有前景的解决方案。 现有的3D物体操作轨迹预测方法受限于昂贵的3D注释成本，难以处理多样化的操作动作。 现有数据集（如HOT3D）中的人类运动轨迹数据有限，不足以开发能够生成物体操作轨迹的模型。\n数据来源： 训练数据： 论文使用Ego-Exo4D数据集 [30] 来构建大规模训练数据集，包含6DoF物体操作轨迹以及图像、深度图和动作描述。这个过程是自动化的，不依赖于预设的相机外部参数。\n评估数据： 论文使用现有的HOT3D [3] 数据集进行评估。HOT3D是一个用于3D手和物体跟踪的自我中心视角数据集，通过光学标记和多红外OptiTrack相机记录，提供了精确的手和物体6DoF信息。\n算力要求： 论文提到模型训练了30个epoch，批处理大小为8。优化器使用了AdamW，基础学习率为2e-5（对于LLMs）和2e-4（对于其他参数）。 为了处理旋转连续性问题，每个角度（roll, pitch, yaw）都用[cos(θ), sin(θ)]表示。 虽然论文没有直接给出具体的算力消耗数字（例如所需的GPU数量或训练时间），但从训练epoch和批处理大小来看，以及使用了BLIP-2、VILA、PointLLM和MiniGPT-3D等大型视觉语言模型作为骨干网络，可以推断出这需要相当大的计算资源。这些模型本身就比较大，并且训练涉及处理大规模视频数据。\n公开代码： 论文在摘要中明确提到了数据集和代码的链接：https://biscue5.github.io/egoscaler-project-page/。\n模拟环境和现实环境平台： 论文没有提到使用了特定的模拟环境平台。 在现实环境方面，论文主要使用了自我中心视频数据。HOT3D [3] 数据集是通过Project Aria眼镜 [21] 和Quest 3 [65] 记录的，这些是现实世界中的可穿戴设备，用于捕捉自我中心视角下的活动。这表明他们的研究是基于真实世界中的人类活动数据。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-12/","summary":"CVPR 2025 Robotics summary","title":"Bug Journal 2025-06-12"},{"content":"PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability CVPR 2025\nFrom 北京交通大学 \u0026amp; 广东技术师范大学\n目标： 告诉机器人什么位置它到不了\n动机： 有时候机器人不知道一个位置到不到得了，结果把自己搞坏了\n模型流程：\n首先离线计算什么位置是机械臂能达到的。 形成一个点云 (S-P Map)\n然后用 SigLip-400M 提取图像和点云的特征\n然后把这个 embedding 和文字的 embedding 混合之后\n通过 Qwen-2.5-Instruct-3B，生成一个回答给 VoxPoser 做。\n实验设计：\n仿真：基于 CUDA 11.x 的 PyBullet 环境，六种机器人（UR5、FR5、CR5、FRANKA、UR3、XArm6），共 1.3K 问答，零样本评测。\n实机：在 UR3、XArm6 真实平台上各 10 次零样本任务，评估任务成功率。\n评估指标：\nEQA-phys：基于 LLM 评分（5 分正确、1 分错误）计算平均分； RoboVQA-val / OpenEQA：标准 BLEU-4 与准确率； 任务规划：成功率。\n结果：\nS-P Map 在很多 LLM 上都有用\nPhysVLM-3B 效果平均最好\n数据集： Zero-shot\n算力要求：\n\u0026lt; 48h * 8 * A800\n代码：\n开源\nObject-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation CVPR 2025\nFrom PKU Agibot Lab\n目标： 帮助机器人排除冗余信息干扰\n动机： 语言和视频中冗余信息过多\n模型流程：\n在图片上加一些标记 分别是：\n接触点（蓝色） 末端执行器在接触时的 z 轴方向（红色） y 轴方向（绿色） 接触后移动方向（黄色） 这些标记可能来自 Grounding-Dino + SAM 或者 ChatGPT-4o 或者 手动标记\n标记方式如下：\n均匀生成 N(32) 条线，让 GPT 选择 3 条标上颜色\n然后把这个“增强”过后的信息输入给 CLIP,然后再让 LLM 输出\u0026quot;应该在哪里，以什么角度接触\u0026quot;\n对于这个信息，我们可以和GT 做 train\n最后，我们得到了这个位置和姿态的信息，我们就可以用IK求解路径了。\n结果：\n数据集：\n模拟环境：SAPIEN + PartNet-Mobility •\t平台：使用 SAPIEN（一个支持刚体与关节物体的物理模拟器），其提供高保真动力学与渲染接口，适合零样本机器人操控研究 ￼ ￼。 •\t资产集：加载 PartNet-Mobility 中的 1,500 多个关节化 CAD 模型，涵盖抽屉、门、笔记本电脑盖等常见家电与家具 ￼。 •\t飞行夹持器（Flying Gripper）：采用 SAPIEN 中的“飞行”版 Franka Panda Gripper，使得末端执行器可在自由空间中无碰撞地移动，以便专注测试模型对提示的理解能力 ￼。 •\t摄像机随机化：在每个场景随机采样摄像机视角（水平±45°、俯仰30°–60°），并在单帧中记录 RGB 图像与 3D 末端执行器位姿，用于提示生成与训练 ￼。 •\t数据采集：共采集 ~10,000 条训练样本，过程耗时约 6–8 小时；测试集与训练集按照固定比例拆分，模拟了 Seen/Unseen 物体类别的泛化能力评估 ￼ ￼。\n现实机器人平台 •\t硬件平台：使用真实的 Franka Emika Panda 机械臂（7 自由度，集成高精度力矩传感器），配合标准的 RGB-D 摄像头（如 Intel RealSense D415）采集视觉输入 ￼。 •\t执行流程：将 CrayonRobo 在模拟环境中输出的 SE(3) 接触位姿与移动方向，通过 ROS + MoveIt! 的 IK 求解与笛卡尔轨迹规划一键下发真实机器人执行 ￼。 •\t测试任务：与模拟中一致，包括“拉抽屉”“开门”“掀笔记本盖”等单步原子操作，每个任务在 5–10 个不同初始姿态下重复试验。 •\t评价指标：以二值成功率衡量：当末端执行器按照预测方向将物体部件移动超过阈值（如 5 cm）即记为成功；平均成功率达到 74%（Seen）与 72%（Unseen） ￼。\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\n代码： 开源\nCheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation CVPR 2025\nFrom PKU Agibot lab\n目标： 让机器人读取说明书之后根据说明书做出正确操作\n动机： 阅读说明书\n电子产品有时候视觉信息不够，很多有着相似外观的东西可能有着不同的功能。\n所以要读说明书\n模型流程：\nOCR → GPT-4o 提取图文信息 → GPT‑4o 生成操作步骤 → SoM、Grounding‑DINO、SAM 实现视觉对齐 → FoundationPose 匹配 CAD\n最后输出{部件，操作} pair, 最后使用 VoxPoser 进行操作。\n实验设置 模拟平台：SAPIEN 0.8 + Franka Panda 飞行夹具 ￼\n数据集： PartNet-Mobility CAD 模型； CheckManual 合成说明书（已公开，可下载使用） ￼\n评估指标： 任务完成率\n现实验证： Franka + RealSense 摄像头，完成单个用例的实物测试\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\nCode availability: 开源\n结果：\n总之有 manual 效果更好\nTASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation CVPR 2025\nFrom Xiaoguang Han\u0026rsquo;s Lab at 港中深\n目标： 优化对齐视频-人手数据集\n动机： 现在有这些问题：\n视角不一致 动作语义无法对齐 手部姿态稳定性不高 这个模型想要解决这些问题 模型流程：\n数据集构建（Sec. 3）： •\t100,856 条 1–8 秒单动作视频，静态 1080p 视角，语义指令一一对应。 粗视频生成（Stage I – Coarse Action Planner）： •\t基于 DynamiCrafter（512 × 512 分辨率），语言指令＋静态环境图→粗交互视频； •\t微调参数：batch=16, lr=5×10⁻⁵, 30K steps。 姿态优化（Stage II – MDM Refinement）： •\t使用 Motion Diffusion Model 以 60 帧手部关键点序列为条件，保证握姿一致性； •\t训练设置：batch=64, lr=1×10⁻⁴, 500K steps; 推理 10-step DDIM denoise。 最终生成（Stage III – Frame-wise Adapter）： •\t将优化后手部姿态图像与环境图和语言指令一起接入 Stable Diffusion 2 Adapter，生成最终视频； •\t训练设置：batch=32, lr=5×10⁻⁵, 30K steps。 实验设置 •\t仿真验证：在 SAPIEN 模拟平台上，用 DynamiCrafter＋MDM + Adapter 生成视频，并通过 Figure 17 示范在虚拟机器人上的模仿效果。\n模型流程：\n第一阶段：Coarse Action Planner（粗动作生成） •\t目标：在给定语言指令（如“拿起杯子并倒入水”）和静态环境图（512×512 分辨率）条件下，生成一段粗略的、合理的手–物体交互视频序列（通常 16 帧）。 •\t模型架构：基于 Video Diffusion Model（VDM），如 DynamiCrafter，使用文本–图像条件扩散网络。 •\t训练细节： •\tBatch size = 16，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t推理时使用 50-step DDIM 采样，平衡生成质量与速度。 •\t输出：一段低分辨率、含大致手部运动轨迹和物体交互的粗视频，用于后续姿态细化。\n⸻\n第二阶段：Motion Diffusion Model (MDM) Refinement（姿态细化） •\t目标：针对第一阶段生成的粗视频，从中提取手部的 3D 关键点序列（60 帧），并利用扩散模型细化运动轨迹，提升抓握姿态的连贯性与自然度。 •\t模型架构：1D 时序扩散网络，对帧间手部关键点做去噪与重建。 •\t训练细节： •\tBatch size = 64，学习率 1×10⁻⁴； •\t训练步数 500K steps； •\t推理时使用 10-step DDIM，快速得到精细关键点序列。 •\t输出：一组平滑、符合物理约束的 3D 手部关键点轨迹，用以指导下一阶段的视频生成。\n⸻\n第三阶段：Frame-wise Adapter（帧级最终生成） •\t目标：将姿态细化后的关键点与原环境图、语言指令结合，生成最终高质量、手部姿态稳定的交互视频。 •\t模型架构：基于 Stable Diffusion 2，通过 Adapter 模块将手部姿态（以可视化关节点或姿态图形式）作为条件，连同环境图与文本，一起输入扩散模型。 •\t训练细节： •\tBatch size = 32，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t采用与 Stage I 相似的 DDIM 采样流程，保证画质与动作一致性。 •\t输出：分辨率可达 512×512 的连续视频帧序列，手–物体交互清晰、握姿自然，可直接用于机器人模仿学习。\nVidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic CVPR 2025\nFrom Technical University of Munich\n目标： 大规模网络视频人类样本学习 训练 家务机器人 模型\n动机： 机器人依赖实例教学，但是做家务没那么多教学\n模型流程：\n模型概览\nVidBot 通过三大模块——3D 可交互性提取、粗阶段 affordance 预测、细阶段轨迹生成——实现从“野外”RGB 视频到机器人可执行动作的零样本迁移。首先，它利用 Structure-from-Motion 和度量深度模型，从单目视频中恢复一致的 3D 手部轨迹与接触/目标点；然后，因式分解成粗预测网络 πc（提取高层次接触点与目标点）与细预测网络 πf（基于扩散生成精细轨迹），并在推理时引入多目标、法线与避碰等可微成本进行采样指导；最后，将生成的 3D 交互轨迹直接部署于多种机器人平台，实现开“即用”的零样本操控能力 ￼ ￼。\n⸻\n3D 可交互性提取管道 1.1 数据准备 •\t视频与语言输入：给定原始 RGB 图像序列 {Ĩ0,…ĨT} 及指令 l。 •\tSfM 与深度预测：使用 SfM 系统估计相机内参 K、无尺度位姿 {TWC} 及稀疏地标，再调用度量深度基模型（如 ZoeDepth）生成密集深度 {D̂t}，实现时序一致且度量尺度的重建 ￼； •\t手-物体分割与填充：借助手—物体检测（如 [72]）与分割模型（如 [94]）提取手部与接触物体掩码，再通过视频修复生成无手帧，以消除动态遮挡对优化的影响 ￼。\n1.2 姿态与尺度优化 •\t全局尺度校正：优化全帧尺度 sg，使稀疏地标深度与预测深度对齐； •\t位姿细化：联合优化每帧位姿 TWCi 与局部尺度 si，补偿 SfM 在手—物体动态区域的误差，实现一致的 3D 重建 ￼。\n1.3 交互表示提取 •\t手部中心轨迹：将优化后各帧手部中心点恢复至首帧坐标系，插值形成平滑交互轨迹 τ̂； •\t接触与目标点采样：在首帧均匀下采样手部中心生成接触点 ĉ，在末帧提取目标点 ĝ，用于监督模型的中间预测； •\t表示定义：最终得到的 3D affordance a = {c, τ}，其中 c∈ℝNc×3 为接触点集，τ∈ℝH×3 为轨迹序列 ￼ ￼。\n⸻\n粗—细分级 Affordance 学习 2.1 模型结构因式分解\n将 affordance 模型 π({Ĩ, D̃},l) 分解为： 1.\t粗阶段 πc：从 RGB-D 图像 Ĩ→D̃ 及指令 l 中预测目标点 g 和接触点 c； 2.\t细阶段 πf：在粗阶段输出 {g,c} 及场景上下文指导下，通过扩散模型生成精细轨迹 τ ； 整个流程利用测试时可微成本（多目标到达、碰撞避让等）对采样进行引导，增强与新场景/新形体的适应性 ￼ ￼。\n2.2 粗阶段：目标与接触点预测 •\t输入预处理：使用开集物体检测器裁剪关注物体区域的 RGB-D 图像； •\t网络架构：πc 由两支网络 πgoalc 与 πcontc 组成，分别输出目标和接触热图及（目标点）深度。 •\t融合视觉特征、RoI 池化后得到的物体嵌入、语言嵌入（CLIP 特征）和位置编码，通过 Perceiver 与 Transformer 生成每像素概率分布； •\t3D 投影：根据相机内参与热图深度，将像素坐标提升到三维空间，得到 g∈ℝNg×3 与 c∈ℝNc×3 ￼。\n2.3 细阶段：基于扩散的轨迹生成 •\t条件扩散模型：πf 采用 1D U-Net 架构，输入包含轨迹状态 τk、TSDF 空间特征（由 3D U-Net 从 RGB-D 构建的体素化 TSDF 地图提取）、语言与物体嵌入等； •\t正向/反向过程：遵循扩散概率模型，逐步加入高斯噪声，再由网络学习去噪，直接回归未加噪轨迹 τ̄0； •\t测试时引导：在每个去噪步骤中加入可微成本函数——多目标到达、法线一致、碰撞避让——通过扰动引导采样，提高交互轨迹在新场景/新形体下的合理性与可执行性 ￼ ￼。\n⸻\n输入与输出 •\t输入：首帧的“修复”RGB 图像 Ĩ̃0、对应度量深度 D̃0、裁剪的物体图像 Ĩ̃0o 及语言指令 l ￼。 •\t输出：3D 接触点集 c 和交互轨迹 τ ，直接可用于机器人控制模块执行操作 ￼。 算力要求： 没说\nCode availability: 暂时没有 (2025-06-11)\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-11/","summary":"Find Robotics in CVPR 2025","title":"Bug Journal 2025-06-11"},{"content":"CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models Currently on Arxiv.\nFrom Nvidia, Stanford, and MIT\n动机：\n为什么不能和 Language 一样，对 Vision 也做 CoT 呢\n实现方式：\n先生成实现这个目标的话图像应该怎么变 然后再生成动作\n实验设计：\n使用 LIBERO 四个任务组（Spatial, Object, Goal, Long），每组 10 个任务，各 50 条示范轨迹；评价指标为成功率（500 次试验，3 个随机种子）\n真实机器人：Bridge-V2（WidowX 机械臂，45k 语言标注轨迹）和 Franka-Tabletop（Panda 机械臂，10–150 条示范），分别测试视觉泛化、运动泛化、语义泛化和语言对齐四类场景；指标为成功率\n对比模型 •\tDiffusion Policy：从头训练的扩散策略，各场景均需单独训练； •\tOcto：通用 VLA，预训练于 OpenX，无 VLM 初始化； •\tOpenVLA：基于预训练 VLM 的开源 VLA； •\tSUSIE：两阶段图像编辑 + 目标条件策略。 在 LIBERO 中，CoT-VLA 平均成功率达 81.13%，高于 OpenVLA 的 76.5% 和 Diffusion Policy 的 72.4%\nRobotic Control via Embodied Chain-of-Thought Reasoning Currently on Arxiv\nFrom UCB, UWarsaw, Stanford\n动机：\n泛化能力\n实现方式：\n多步文本推理 + 框选关键物品位置 + LLM CoT 生成 \u0026ndash;\u0026gt; OpenVLA 完成所有步骤\n实验设计：\n两种设计：\n一样的物品/一样的指令/一样的视角 不同的物品/不同的指令/不同的视角 Robo-DM: Data Management For Large Robot Datasets ICRA 2025 BestPaper on Robot Learning\nfrom UCB \u0026amp; Google Deepmind\n做数据库的\n以前的数据都没压缩过，太大了; 存储，传输成本也高 这样的话加载也会很慢 这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\n他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\n最后把信息都通过 EBML file 存储\n为什么选择 EBML 呢?\n因为：\n支持嵌套结构 是自包含的，更方便复用 支持流处理，不需要一次性全部导入到内存中 支持自动时间同步 对于视频，主要选择了.H264 来压缩，显著降低了文件大小\n最后这个数据集又小又快\nAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview ICRA 2025 Best Paper on Robot Learning Finalist\nfrom Google Deepmind\n机器人打乒乓球\n打乒乓球要又快又准。所以是理想的机器人测试器\n对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\n首先用模仿学习做 base, 然后强化学习训练\n分层控制则是类似 MOE 的思路，每一个子网络都“学一种打球技术”\n然后让主网络来“选择一种打球技术”。\n控制频率：50HZ.\n并且主网络还会在 Validation 的时候 Continue Learning\n比喻：当机器人发现正手比反手好得分，那机器人就会偏向于打更多正手\n$H(s, a) = H_{offline}(s, a) + \\alpha * [R(s, a) - R_{expected}(s, a)]$\n在这里，$H(s,a)$ 是现在的偏好 $H_{offline}(s,a)$ 是训练时的偏好 $R(s,a)$ 是当前环境的奖励 $R_{expected}$ 是预期获得的奖励\n结果：\n此处，B 指 Beginner, I 指 Intermediate, A 指 Expert\n训练难度：\n2.4 Billion steps on 6k Parallel Simulators 训练出了正反手\n每一个操作需要训练 300 - 1200 Million Steps.\n但是推理难度很低，只需要一个 CPU 的 3ms CPU time 即可完成\n最终能实现 50Hz 的推理速度\nNo Plan but Everything Under Control: Robustly Solving Sequential Tasks with Dynamically Composed Gradient Descent ICRA 2025 Best Paper on robot learning finalist\nFrom University of Berlin\n部分现有方法用的是 planning 来做的机器人 manipultaion task.\n就是说比如会找到一个机器人的起点和终点，然后通过一些算法从起点移动到终点\n这样的算法会通过一些数据来训练\n但是人类在做这些 task 的时候并不会有一个 planning, 那如何不训练做这些 task 呢？\n既然现在 Gradient Descent 这么强，能不能考虑直接用 Gradient Desent 来解决这个问题呢？\n可以的：但是和传统的 Gradient Desnet 不一样的点在于：传统的 Gradient Desent 会把所有的 Gradient 全部加在一起，但是对于现在的 task, 不一定要找全局最优解，可以找当前“做哪个分解动作”最优。\n比如：现在可以让机械臂向某个轴的某个方向移动，或者让机械臂把物品抓起来。\n缺点：这个方法必须要对于每一个 task 都设计一个新的 Gradient 方向。\n优点：这个方法可以避免一些不必要的移动，并且可以根据当前状态来调整他的策略。\n实验设计：在“block world”模拟环境中和现实中推拉抽屉。\n在现实世界中会对于这个环境给予一个 干扰，看看这个模型的抗干扰能力如何。\n比较的模型是：ICRA 2020: Online replanning in belief space for partially observable task and motion problems\nPolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation ICRA 2025 Best Paper in Field and Service Robotics\n使用了多种 Modality 来增强模型的 Manipulation 能力\n使用了包括：视觉，听觉，触觉 \u0026hellip; 等 modality 的能力\n实验设计：\n耐用性 成功率 首先和一个其他的商业模型做对比，然后发现耐用性更高\n然后和自己做消融实验做对比，然后发现模态越多，效果越好\nHuman-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition ICRA 2025 Best Paper on Human-Robot Interaction\nFrom 上交 \u0026amp; UIUC\n提出了一种更高效的数据收集 \u0026amp; 训练的方式\n核心在于 Diffusion Policy 的应用。\n最开始的时候，会采集一些数据用于最初的训练\n然后在接下来的训练中，机器人会一步一步 take control, 这时人类只需要做一个大致的动作就可以了。\ne.g.\n以“拿起杯子并放到指定位置”（Pick-and-Place）任务为例：\n阶段一：接近杯子 人类操作：你只需要做一个简单的“向前移动”的手势。 智能体接管：智能体理解你的意图是“去拿杯子”，于是它会自主地、平滑地控制机械手移动到杯子正上方，并摆好最佳的抓取姿势。 关键节点 1 到达：机械手已经就位，悬停在杯子上方。第一个子任务“接近杯子”已完成。此时，智能体停下来，因为它不知道你接下来是想抓取，还是想调整位置，或是想取消任务。它在等待你的下一个指令。\n阶段二：抓取杯子 人类操作：你做一个“抓握”的手势。 智能体接管：智能体接收到“抓取”指令，于是它会自主执行精确的抓取动作，以最稳定的方式合拢机械手，握紧杯子。 关键节点 2 到达：杯子已经被成功拿起。第二个子任务“抓取杯子”已完成。现在，智能体又停下来了。它知道手里拿着杯子，但它不知道你想把杯子放到哪里去。\n阶段三：移动到目标位置并释放 人类操作：你做一个指向目标位置的“移动”手势。 智能体接管：智能体理解意图，自主地将拿着杯子的手移动到目标位置上方，然后等待你最后的指令。 人类操作：你做一个“松开”的手势。 智能体接管：智能体平稳地释放杯子。任务完成。\nVisual Chain-of-Thought Prompting for Knowledge-based Visual Reasoning AAAI 2024\nFrom MIT, UCLA, CMU\n可以作为以后的 baseline\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-10/","summary":"paper review","title":"Bug Journal 2025-06-10"},{"content":"Robo-DM: Data Management For Large Robot Datasets ICRA 2025 BestPaper on Robot Learning\nfrom UCB \u0026amp; Google Deepmind\n做数据库的\n以前的数据都没压缩过，太大了; 存储，传输成本也高 这样的话加载也会很慢 这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\n他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\n最后把信息都通过 EBML file 存储\n为什么选择 EBML 呢?\n因为：\n支持嵌套结构 是自包含的，更方便复用 支持流处理，不需要一次性全部导入到内存中 支持自动时间同步 对于视频，主要选择了.H264 来压缩，显著降低了文件大小\n最后这个数据集又小又快\nAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview ICRA 2025 Best Paper on Robot Learning Finalist\nfrom Google Deepmind\n机器人打乒乓球\n打乒乓球要又快又准。所以是理想的机器人测试器\n对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\n首先用模仿学习做 base, 然后强化学习训练\n分层控制则是类似 MOE 的思路，每一个子网络都“学一种打球技术”\n然后让主网络来“选择一种打球技术”。\n控制频率：50HZ.\n并且主网络还会在 Validation 的时候 Continue Learning\n比喻：当机器人发现正手比反手好得分，那机器人就会偏向于打更多正手\n$H(s, a) = H_{offline}(s, a) + \\alpha * [R(s, a) - R_{expected}(s, a)]$\n在这里，$H(s,a)$ 是现在的偏好 $H_{offline}(s,a)$ 是训练时的偏好 $R(s,a)$ 是当前环境的奖励 $R_{expected}$ 是预期获得的奖励\n结果：\n此处，B 指 Beginner, I 指 Intermediate, A 指 Expert\n训练难度：\n2.4 Billion steps on 6k Parallel Simulators 训练出了正反手\n每一个操作需要训练 300 - 1200 Million Steps.\n但是推理难度很低，只需要一个 CPU 的 3ms CPU time 即可完成\n最终能实现 50Hz 的推理速度\nNo Plan but Everything Under Control: Robustly Solving Sequential Tasks with Dynamically Composed Gradient Descent ICRA 2025 Best Paper on robot learning finalist\nFrom University of Berlin\n部分现有方法用的是 planning 来做的机器人 manipultaion task.\n就是说比如会找到一个机器人的起点和终点，然后通过一些算法从起点移动到终点\n这样的算法会通过一些数据来训练\n但是人类在做这些 task 的时候并不会有一个 planning, 那如何不训练做这些 task 呢？\n既然现在 Gradient Descent 这么强，能不能考虑直接用 Gradient Desent 来解决这个问题呢？\n可以的：但是和传统的 Gradient Desnet 不一样的点在于：传统的 Gradient Desent 会把所有的 Gradient 全部加在一起，但是对于现在的 task, 不一定要找全局最优解，可以找当前“做哪个分解动作”最优。\n比如：现在可以让机械臂向某个轴的某个方向移动，或者让机械臂把物品抓起来。\n缺点：这个方法必须要对于每一个 task 都设计一个新的 Gradient 方向。\n优点：这个方法可以避免一些不必要的移动，并且可以根据当前状态来调整他的策略。\n实验设计：在“block world”模拟环境中和现实中推拉抽屉。\n在现实世界中会对于这个环境给予一个 干扰，看看这个模型的抗干扰能力如何。\n比较的模型是：ICRA 2020: Online replanning in belief space for partially observable task and motion problems\nPolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation ICRA 2025 Best Paper in Field and Service Robotics\n使用了多种 Modality 来增强模型的 Manipulation 能力\n使用了包括：视觉，听觉，触觉 \u0026hellip; 等 modality 的能力\n实验设计：\n耐用性 成功率 首先和一个其他的商业模型做对比，然后发现耐用性更高\n然后和自己做消融实验做对比，然后发现模态越多，效果越好\nHuman-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition ICRA 2025 Best Paper on Human-Robot Interaction\nFrom 上交 \u0026amp; UIUC\n提出了一种更高效的数据收集 \u0026amp; 训练的方式\n核心在于 Diffusion Policy 的应用。\n最开始的时候，会采集一些数据用于最初的训练\n然后在接下来的训练中，机器人会一步一步 take control, 这时人类只需要做一个大致的动作就可以了。\ne.g.\n以“拿起杯子并放到指定位置”（Pick-and-Place）任务为例：\n阶段一：接近杯子 人类操作：你只需要做一个简单的“向前移动”的手势。 智能体接管：智能体理解你的意图是“去拿杯子”，于是它会自主地、平滑地控制机械手移动到杯子正上方，并摆好最佳的抓取姿势。 关键节点 1 到达：机械手已经就位，悬停在杯子上方。第一个子任务“接近杯子”已完成。此时，智能体停下来，因为它不知道你接下来是想抓取，还是想调整位置，或是想取消任务。它在等待你的下一个指令。\n阶段二：抓取杯子 人类操作：你做一个“抓握”的手势。 智能体接管：智能体接收到“抓取”指令，于是它会自主执行精确的抓取动作，以最稳定的方式合拢机械手，握紧杯子。 关键节点 2 到达：杯子已经被成功拿起。第二个子任务“抓取杯子”已完成。现在，智能体又停下来了。它知道手里拿着杯子，但它不知道你想把杯子放到哪里去。\n阶段三：移动到目标位置并释放 人类操作：你做一个指向目标位置的“移动”手势。 智能体接管：智能体理解意图，自主地将拿着杯子的手移动到目标位置上方，然后等待你最后的指令。 人类操作：你做一个“松开”的手势。 智能体接管：智能体平稳地释放杯子。任务完成。\n概念理解\nauto regressive Auto regressive 是一种生成方式。可以从前一个数生成下一个数。 更准确地说，$X_{t_i} = a_1X{t_1} + a_2X{t_2} + \\dots + a_{t_{i-1}}X{t_{i-1}}$.\nteacher forcing Teacher forcing 是指在训练的过程中，把真实信息放入训练\nPi 0\nflow matching\n连续的动作和离散的动作有什么区别\nGemini diffusion\n已加入 waitlist 正在测试 LLaDA 开源 Diffusion model. 目前该模型仍然无法加入 Chain of Thought 问题是：没有 Chain of Thought 的模型显著没有加入 Chain of Thought 的模型强 但是现在有一个叫做 Diffusion of Thought 的方法可以加入类似的东西\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-09/","summary":"\u003ch2 id=\"robo-dm-data-management-for-large-robot-datasets\"\u003eRobo-DM: Data Management For Large Robot Datasets\u003c/h2\u003e\n\u003cp\u003eICRA 2025 BestPaper on Robot Learning\u003c/p\u003e\n\u003cp\u003efrom UCB \u0026amp; Google Deepmind\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e做数据库的\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e以前的数据都没压缩过，太大了; 存储，传输成本也高\u003c/li\u003e\n\u003cli\u003e这样的话加载也会很慢\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"1749461291726\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461291726.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\u003c/p\u003e\n\u003cp\u003e他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461487133\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461487133.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后把信息都通过 EBML file 存储\u003c/p\u003e\n\u003cp\u003e为什么选择 EBML 呢?\u003c/p\u003e\n\u003cp\u003e因为：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e支持嵌套结构\u003c/li\u003e\n\u003cli\u003e是自包含的，更方便复用\u003c/li\u003e\n\u003cli\u003e支持流处理，不需要一次性全部导入到内存中\u003c/li\u003e\n\u003cli\u003e支持自动时间同步\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对于视频，主要选择了.H264 来压缩，显著降低了文件大小\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461874888\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461874888.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461827040\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461827040.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后这个数据集又小又快\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461984813\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461984813.png\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"achieving-human-level-competitive-robot-table-tennis-a-comprehensive-overview\"\u003eAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview\u003c/h2\u003e\n\u003cp\u003eICRA 2025 Best Paper on Robot Learning Finalist\u003c/p\u003e\n\u003cp\u003efrom Google Deepmind\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e机器人打乒乓球\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e打乒乓球要又快又准。所以是理想的机器人测试器\u003c/p\u003e\n\u003cp\u003e对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\u003c/p\u003e","title":"Bug Journal 2025-06-09"},{"content":"Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation 发表时间：15 Jun 2024\n动机 当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\n问题 1：学习难度高（需要从高维图像中学全局策略）； 问题 2：泛化差（模型可能过拟合于训练视角或场景）； 问题 3：sample efficiency 差（训练数据需求量大）\n作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的， 也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\n主要论点 提出 Local Policy Networks（LPN）： 将策略函数设计为一组 局部策略（local policy heads）； 每个 head 只负责“在自己 patch 上预测动作”； 用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）； 最终策略通过对多个 local head 输出聚合（weighted sum）得到。\n模型流程图 简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\n实验 setting: 使用数据集：RT-1（Robotics Transformer 1）:\nGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper） 数据来源 真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集 数据规模 ~130k 条实际机器人操作轨迹，覆盖 700+ 种任务 采样频率 每条轨迹包含约 50–100 帧关键帧（图像 + 动作） 场景 家庭式办公环境（桌面、水槽、地面） 物体 80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等） 语言指令 每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型） 对于每一条指令：\nRGB 图像 来自机器人头部相机的视角（尺寸通常为 240×320） 深度图（可选） 当前状态 如 gripper pose（位置 + 朝向） 语言指令 例如 “pick up the red apple and place it in the bowl” 动作标签 6-DoF 末端动作（位置增量、旋转、夹爪开合） 时间戳 当前帧在轨迹中的位置 成功标志 是否完成任务（某些版本包含） 文章通过 “加热”在数据集中的抓取位置 来 train heatmap.\n在 heatmap 中取出关键点\n之后通过神经网络预测每一个关键点的移动，加权得到机械臂移动的方向。\n通过模仿学习来算 Loss, 然后训练。\nCoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models 发表时间：13 Mar 2024\n动机 当前机器人 manipulation 研究大多集中在：\n高层任务规划（如使用 LLMs 推理“该做什么”）， 或低层控制（通过 imitation learning 或 RL 获取 end-effector 动作序列）。 然而，低层控制模块在真实世界中很难泛化，因为：\n缺乏对物体几何和功能部分的理解， 容易对新的任务和场景失效。 因此，本文希望结合 VLMs 的常识知识和几何建模能力，以“空间约束”的形式桥接语言与机器人执行之间的鸿沟，实现可泛化的低层 manipulation 能力。\n核心思想 Zero-shot! GPT-4V is all you need!\n模型流程图 首先分割场景，识别场景中有多少物体。\n简单来说就是：让 GPT 决定怎么操作这个物体。 比如说，要抓哪里，要怎么抓，物体的姿态是什么，\u0026hellip;\n最后使用一个传统路径规划算法来达成上述所有条件 一旦条件达成，任务也就完成了\n结果 MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment 发表时间: 24 Mar 2024\n动机 之前的模拟环境都太简单了，设计一个复杂的\n主要论点 设计了一个复杂的模拟环境:\n更多更复杂场景，更真实的物理引擎，更好的 Reward\nAny-point Trajectory Modeling for Policy Learning 发表时间：28 Dec 2023\n动机 数据不够用啦，我要从视频里学\n本文旨在从无动作标签的视频中提取出用于控制的轨迹，从而辅助策略学习\n主要论点 作者提出了 Any-point Trajectory Modeling (ATM) 框架：\n第一阶段：使用无动作标签的视频和现有视频追踪器，从视频中提取任意点的轨迹（即二维运动轨迹），训练一个轨迹预测模型（Track Transformer）。 第二阶段：使用轨迹预测模型，在给定任务语言描述和当前图像的情况下预测未来的轨迹，并用这些轨迹作为中间目标（subgoal）指导策略学习，从少量动作演示中学习有效的策略。 这不就是昨天的General Flow吗\n模型流程图 和昨天的那篇文章的区别在于:\n这篇不需要 Ground Truth 数据，只是视频就行，而那一篇需 如何做到的？答:用了一个 LLM Tracker 来跟踪 这篇文章是对\u0026quot;运动最显著\u0026quot;的 K(32) 个点算运动向量，那个是对每一个像素算运动向量 MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning 发表时间: 19 Oct 2024\n动机 做 RL, 还是得泛化\n过去的 embedding 太 General 了。\n思路是：如果 embedding 不分任务注意所有细节, 反而做不好 这里的泛化能忽略掉任务无关的信息\n总之，这里使用了一个 MoE 网络来处理输入。\n模型流程图 至于这个 Perturbation.. 流程如下:\n[输入图像 I] ↓ [Encoder 输出特征 z] ↓ [策略网络 → 计算 loss] ↓ [反向传播：计算 ∇_z L] ↓ [构造 δ = ε · normalized gradient] ↓ [扰动特征 z + δ → 再送策略网络训练] ↓ [更新 encoder + expert 参数] 总之就是训练的时候把 z 往成功的方向“推一下” “引导视觉 encoder 学会放大那些能带来任务成功的区域”\n结果 模拟环境：\nDMC (DeepMind Control Suite) → 如：Walker、Cheetah、Finger、Cartpole 等控制任务 Meta-World → 多任务机器人操作环境（push、reach、pick-place 等） RLBench → 视觉+控制为主的仿真机械臂任务，如插电源、开抽屉等\n真实世界任务：\n在完成 Task 的时候会干扰一下不让它完成这个 Task.\n但是最后还是成功完成了。\nTake away MoE 是什么呢？其实就是多个动态加权平均的网络。 什么是动态加权平均呢？ 就是权重是通过 $SoftMax(MLP)$ 算出来的 这样每次每个网络加权平均的权重就会不同。\nDemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025\n动机 Robust，还得是Robust。\n为什么不加数据(ο´･д･)??\n过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\n那怎么办呢(ο´･д･)??\n很简单，纯计算模拟不就是了。\n主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\n这样就不需要实际操作，但是可以直接获得大量数据。\n模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\n思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\n机器人一共有两段动作\n一段是碰到物体前的动作\n一段是碰到物体后的动作\n对于第一段，直接用一个变换矩阵变换\n对于第二段，直接规划一个新路径 (use RRT-Connect)\n现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\n如果可以用的话\n然后通过模拟环境生成这个路径的图像\n实验设定 虚拟环境：1 条 GroundTruth\n真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\n一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\n结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 + mask + 指令 + robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM + Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove 一个这样的力反馈手套\n可以链接到灵巧手上，然后展示反馈物体的力\n盲抓分辨物体\n盲眼抓杯子\nReactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation 发表时间：23 Apr 2025\n动机 人有触觉，为什么机器人不行？\n所以我们为机器人加上了触觉。\n并且用触觉来优化模型.\n可以是，触觉数据怎么来呢？🤔\n在机器人手上加上触觉组件就好了\n模型流程 方法很简单：\nimage -\u0026gt; 正常的 CV Encoder -\u0026gt; Touch Encoder -\u0026gt; (Action -\u0026gt; Touch Encoder) * N -\u0026gt; image2 \u0026hellip;\n实现效果 Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control 发表时间：June 23 2025\n动机 现在机器人的表情不够生动，希望能生动一点\n用一段语音输入进机器人\n为什么选语音？\n因为语音有语气, 语气中可能有细微的表情差别\n“我没事\u0026hellip;” “我没事！”和 “我?没事” 是有区别的\n这更适合模型生成表情.\n而现有机器人的表情做得不好，所以这个团队自己做了一个可以做更丰富表情的机器人脸\n实现方式 第一部分，有一个带有丰富语气的语音数据集 + 对应的 3D 人脸建模数据\n第二部分，为了训练模型 -\u0026gt; 电机； 随机生成了 5000 个 3D 建模 让电机去拟合这个表情\n↑以上过程都是在虚拟环境中训练完，然后搬到真实环境做的。\nTwo by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation 发表时间: 9 Apr 2025\n动机 机器人泛化能力和精确对齐的能力不行\n一一对应的插入 task 做不好\n模型流程图 一言以蔽之：先找插座位置，再预测插件位置\n这里的位置是真正的位置，分别是 Tran(三位坐标系中的位置) \u0026amp; Rot (三个轴旋转的角度)\n这样就可以计算 loss 了\n之后第二部分就是把刚才得到的 embedding 和插件的 embedding 乘起来 之后还是 predict Tran \u0026amp; Rot 算 loss.\n注：所有训练都是在虚拟环境完成，真实环境仅有 validation.\nGrasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping 发表时间: ICRA 2025\n动机 以前的方法都是以机器人为中心。 没有考虑到机器人和物块之间的相关性\n动机2 之前方法都太慢了\n动机 3 如果我们可以根据 observation 来推测出机械臂抓起物块的时刻的姿态 那我们就可以用 IK 等算法来计算这个抓取路径\n那我们就不需要数据中的路径信息了，只需要两帧，一帧开始，一帧结束\n模型流程图 先训练 Robot Encoder，用手部多配置点云对做 point-level contrastive learning，使手部特征具有结构一致性； 训练 CVAE：把训练好的 hand embedding 和 object embedding（经过 Transformer 融合）作为条件； 将 grasp pose 的手点云 输入到 CVAE 编码器，学习一个 latent 抓取表示； 用 CVAE 解码器重建抓取交互矩阵 D(R,O)，这是对 hand-object grasp 状态的结构表示； 用重建误差 + KL loss 训练整个模型。 结果 缺陷 有很多模型都只考虑了开始点和结束点的信息，没有经过 Motion Planning 这样的话如果遇到障碍物就容易出问题\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-06/","summary":"\u003ch2 id=\"leveraging-locality-to-boost-sample-efficiency-in-robotic-manipulation\"\u003eLeveraging Locality to Boost Sample Efficiency in Robotic Manipulation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2406.10615\"\u003e发表时间：15 Jun 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003e当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\u003c/p\u003e\n\u003cp\u003e问题 1：学习难度高（需要从高维图像中学全局策略）；\n问题 2：泛化差（模型可能过拟合于训练视角或场景）；\n问题 3：sample efficiency 差（训练数据需求量大）\u003c/p\u003e\n\u003cp\u003e作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的，\n也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e提出 Local Policy Networks（LPN）：\n将策略函数设计为一组 局部策略（local policy heads）；\n每个 head 只负责“在自己 patch 上预测动作”；\n用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）；\n最终策略通过对多个 local head 输出聚合（weighted sum）得到。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749017331301\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749017331301.png\"\u003e\u003c/p\u003e\n\u003cp\u003e简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\u003c/p\u003e\n\u003ch4 id=\"实验-setting\"\u003e实验 setting:\u003c/h4\u003e\n\u003cp\u003e使用数据集：RT-1（Robotics Transformer 1）:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-markdown\" data-lang=\"markdown\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据来源\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据规模\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e~130k 条实际机器人操作轨迹，覆盖 700+ 种任务\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e采样频率\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每条轨迹包含约 50–100 帧关键帧（图像 + 动作）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e场景\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e家庭式办公环境（桌面、水槽、地面）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e物体\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e语言指令\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型）\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e对于每一条指令：\u003c/p\u003e","title":"Bug Journal 2025-06-06"},{"content":"DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025\n动机 Robust，还得是Robust。\n为什么不加数据(ο´･д･)??\n过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\n那怎么办呢(ο´･д･)??\n很简单，纯计算模拟不就是了。\n主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\n这样就不需要实际操作，但是可以直接获得大量数据。\n模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\n思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\n机器人一共有两段动作\n一段是碰到物体前的动作\n一段是碰到物体后的动作\n对于第一段，直接用一个变换矩阵变换\n对于第二段，直接规划一个新路径 (use RRT-Connect)\n现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\n如果可以用的话\n然后通过模拟环境生成这个路径的图像\n实验设定 虚拟环境：1 条 GroundTruth\n真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\n一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\n结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 + mask + 指令 + robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM + Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove 一个这样的力反馈手套\n可以链接到灵巧手上，然后展示反馈物体的力\n盲抓分辨物体\n盲眼抓杯子\nReactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation 发表时间：23 Apr 2025\n动机 人有触觉，为什么机器人不行？\n所以我们为机器人加上了触觉。\n并且用触觉来优化模型.\n可以是，触觉数据怎么来呢？🤔\n在机器人手上加上触觉组件就好了\n模型流程 方法很简单：\nimage -\u0026gt; 正常的 CV Encoder -\u0026gt; Touch Encoder -\u0026gt; (Action -\u0026gt; Touch Encoder) * N -\u0026gt; image2 \u0026hellip;\n实现效果 Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control 发表时间：June 23 2025\n动机 现在机器人的表情不够生动，希望能生动一点\n用一段语音输入进机器人\n为什么选语音？\n因为语音有语气, 语气中可能有细微的表情差别\n“我没事\u0026hellip;” “我没事！”和 “我?没事” 是有区别的\n这更适合模型生成表情.\n而现有机器人的表情做得不好，所以这个团队自己做了一个可以做更丰富表情的机器人脸\n实现方式 第一部分，有一个带有丰富语气的语音数据集 + 对应的 3D 人脸建模数据\n第二部分，为了训练模型 -\u0026gt; 电机； 随机生成了 5000 个 3D 建模 让电机去拟合这个表情\n↑以上过程都是在虚拟环境中训练完，然后搬到真实环境做的。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-05/","summary":"\u003ch2 id=\"demogen-synthetic-demonstration-generation-for-data-efficient-visuomotor-policy-learning\"\u003eDemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2502.16932\"\u003e发表时间：24 Feb 2025\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003eRobust，还得是Robust。\u003c/p\u003e\n\u003cp\u003e为什么不加数据(ο´･д･)??\u003c/p\u003e\n\u003cp\u003e过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\u003c/p\u003e\n\u003cp\u003e那怎么办呢(ο´･д･)??\u003c/p\u003e\n\u003cp\u003e很简单，纯计算模拟不就是了。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\u003c/p\u003e\n\u003cp\u003e这样就不需要实际操作，但是可以直接获得大量数据。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749026226827\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749026226827.png\"\u003e\u003c/p\u003e\n\u003cp\u003e一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\u003c/p\u003e\n\u003cp\u003e思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\u003c/p\u003e\n\u003cp\u003e机器人一共有两段动作\u003c/p\u003e\n\u003cp\u003e一段是碰到物体前的动作\u003c/p\u003e\n\u003cp\u003e一段是碰到物体后的动作\u003c/p\u003e\n\u003cp\u003e对于第一段，直接用一个变换矩阵变换\u003c/p\u003e\n\u003cp\u003e对于第二段，直接规划一个新路径 (use RRT-Connect)\u003c/p\u003e\n\u003cp\u003e现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\u003c/p\u003e\n\u003cp\u003e如果可以用的话\u003c/p\u003e\n\u003cp\u003e然后通过模拟环境生成这个路径的图像\u003c/p\u003e\n\u003ch4 id=\"实验设定\"\u003e实验设定\u003c/h4\u003e\n\u003cp\u003e虚拟环境：1 条 GroundTruth\u003c/p\u003e\n\u003cp\u003e真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\u003c/p\u003e\n\u003cp\u003e一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\u003c/p\u003e\n\u003ch4 id=\"结果\"\u003e结果\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749093303146\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749093303146.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749093327428\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749093327428.png\"\u003e\u003c/p\u003e\n\u003ch4 id=\"vs-roboground\"\u003eV.S. RoboGround\u003c/h4\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003cstrong\u003e方面\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eDemoGen\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eRoboGround\u003c/strong\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e目标问题\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e数据高效、空间泛化性差的视觉模仿学习\u003c/td\u003e\n          \u003ctd\u003e多任务泛化能力差、语义-空间信息连接弱\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e核心思想\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e从少量人类演示中\u003cstrong\u003e合成大量视觉演示数据\u003c/strong\u003e用于模仿学习\u003c/td\u003e\n          \u003ctd\u003egrounding mask（掩码）作为embedding增强泛化\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e数据生成方式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像\u003c/td\u003e\n          \u003ctd\u003e构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e人类演示\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e一条\u003c/td\u003e\n          \u003ctd\u003e在仿真中自动生成，无需真实 rollouts\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e任务表征形式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e(图像帧, 末端动作)\u003cstrong\u003e对\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e图像 + mask + 指令 + robot state\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e中间表示\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eNone（直接预测动作）\u003c/td\u003e\n          \u003ctd\u003e掩码（mask）作为空间引导\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e依赖模型\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eImmitation Learning\u003c/td\u003e\n          \u003ctd\u003e利用 VLM + Grounded Perceiver 构建 mask-guided policy\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e泛化方式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e利用空间重定向与图像合成覆盖更多初始状态\u003c/td\u003e\n          \u003ctd\u003e通过 grounding masks 和多样 instruction 提升语义-空间泛化\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"doglove-dexterous-manipulation-with-a-low-cost-open-source-haptic-force-feedback-glove\"\u003eDOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"1749095773362\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749095773362.png\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-06-05"},{"content":"Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation 发表时间：15 Jun 2024\n动机 当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\n问题 1：学习难度高（需要从高维图像中学全局策略）； 问题 2：泛化差（模型可能过拟合于训练视角或场景）； 问题 3：sample efficiency 差（训练数据需求量大）\n作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的， 也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\n主要论点 提出 Local Policy Networks（LPN）： 将策略函数设计为一组 局部策略（local policy heads）； 每个 head 只负责“在自己 patch 上预测动作”； 用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）； 最终策略通过对多个 local head 输出聚合（weighted sum）得到。\n模型流程图 简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\n实验 setting: 使用数据集：RT-1（Robotics Transformer 1）:\nGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper） 数据来源 真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集 数据规模 ~130k 条实际机器人操作轨迹，覆盖 700+ 种任务 采样频率 每条轨迹包含约 50–100 帧关键帧（图像 + 动作） 场景 家庭式办公环境（桌面、水槽、地面） 物体 80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等） 语言指令 每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型） 对于每一条指令：\nRGB 图像 来自机器人头部相机的视角（尺寸通常为 240×320） 深度图（可选） 当前状态 如 gripper pose（位置 + 朝向） 语言指令 例如 “pick up the red apple and place it in the bowl” 动作标签 6-DoF 末端动作（位置增量、旋转、夹爪开合） 时间戳 当前帧在轨迹中的位置 成功标志 是否完成任务（某些版本包含） 文章通过 “加热”在数据集中的抓取位置 来 train heatmap.\n在 heatmap 中取出关键点\n之后通过神经网络预测每一个关键点的移动，加权得到机械臂移动的方向。\n通过模仿学习来算 Loss, 然后训练。\nCoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models 发表时间：13 Mar 2024\n动机 当前机器人 manipulation 研究大多集中在：\n高层任务规划（如使用 LLMs 推理“该做什么”）， 或低层控制（通过 imitation learning 或 RL 获取 end-effector 动作序列）。 然而，低层控制模块在真实世界中很难泛化，因为：\n缺乏对物体几何和功能部分的理解， 容易对新的任务和场景失效。 因此，本文希望结合 VLMs 的常识知识和几何建模能力，以“空间约束”的形式桥接语言与机器人执行之间的鸿沟，实现可泛化的低层 manipulation 能力。\n核心思想 Zero-shot! GPT-4V is all you need!\n模型流程图 首先分割场景，识别场景中有多少物体。\n简单来说就是：让 GPT 决定怎么操作这个物体。 比如说，要抓哪里，要怎么抓，物体的姿态是什么，\u0026hellip;\n最后使用一个传统路径规划算法来达成上述所有条件 一旦条件达成，任务也就完成了\n结果 MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment 发表时间: 24 Mar 2024\n动机 之前的模拟环境都太简单了，设计一个复杂的\n主要论点 设计了一个复杂的模拟环境:\n更多更复杂场景，更真实的物理引擎，更好的 Reward\nAny-point Trajectory Modeling for Policy Learning 发表时间：28 Dec 2023\n动机 数据不够用啦，我要从视频里学\n本文旨在从无动作标签的视频中提取出用于控制的轨迹，从而辅助策略学习\n主要论点 作者提出了 Any-point Trajectory Modeling (ATM) 框架：\n第一阶段：使用无动作标签的视频和现有视频追踪器，从视频中提取任意点的轨迹（即二维运动轨迹），训练一个轨迹预测模型（Track Transformer）。 第二阶段：使用轨迹预测模型，在给定任务语言描述和当前图像的情况下预测未来的轨迹，并用这些轨迹作为中间目标（subgoal）指导策略学习，从少量动作演示中学习有效的策略。 这不就是昨天的General Flow吗\n模型流程图 和昨天的那篇文章的区别在于:\n这篇不需要 Ground Truth 数据，只是视频就行，而那一篇需 如何做到的？答:用了一个 LLM Tracker 来跟踪 这篇文章是对\u0026quot;运动最显著\u0026quot;的 K(32) 个点算运动向量，那个是对每一个像素算运动向量 MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning 发表时间: 19 Oct 2024\n动机 做 RL, 还是得泛化\n过去的 embedding 太 General 了。\n思路是：如果 embedding 不分任务注意所有细节, 反而做不好 这里的泛化能忽略掉任务无关的信息\n总之，这里使用了一个 MoE 网络来处理输入。\n模型流程图 至于这个 Perturbation.. 流程如下:\n[输入图像 I] ↓ [Encoder 输出特征 z] ↓ [策略网络 → 计算 loss] ↓ [反向传播：计算 ∇_z L] ↓ [构造 δ = ε · normalized gradient] ↓ [扰动特征 z + δ → 再送策略网络训练] ↓ [更新 encoder + expert 参数] 总之就是训练的时候把 z 往成功的方向“推一下” “引导视觉 encoder 学会放大那些能带来任务成功的区域”\n结果 模拟环境：\nDMC (DeepMind Control Suite) → 如：Walker、Cheetah、Finger、Cartpole 等控制任务 Meta-World → 多任务机器人操作环境（push、reach、pick-place 等） RLBench → 视觉+控制为主的仿真机械臂任务，如插电源、开抽屉等\n真实世界任务：\n在完成 Task 的时候会干扰一下不让它完成这个 Task.\n但是最后还是成功完成了。\nTake away MoE 是什么呢？其实就是多个动态加权平均的网络。 什么是动态加权平均呢？ 就是权重是通过 $SoftMax(MLP)$ 算出来的 这样每次每个网络加权平均的权重就会不同。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-04/","summary":"\u003ch2 id=\"leveraging-locality-to-boost-sample-efficiency-in-robotic-manipulation\"\u003eLeveraging Locality to Boost Sample Efficiency in Robotic Manipulation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2406.10615\"\u003e发表时间：15 Jun 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003e当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\u003c/p\u003e\n\u003cp\u003e问题 1：学习难度高（需要从高维图像中学全局策略）；\n问题 2：泛化差（模型可能过拟合于训练视角或场景）；\n问题 3：sample efficiency 差（训练数据需求量大）\u003c/p\u003e\n\u003cp\u003e作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的，\n也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e提出 Local Policy Networks（LPN）：\n将策略函数设计为一组 局部策略（local policy heads）；\n每个 head 只负责“在自己 patch 上预测动作”；\n用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）；\n最终策略通过对多个 local head 输出聚合（weighted sum）得到。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749017331301\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749017331301.png\"\u003e\u003c/p\u003e\n\u003cp\u003e简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\u003c/p\u003e\n\u003ch4 id=\"实验-setting\"\u003e实验 setting:\u003c/h4\u003e\n\u003cp\u003e使用数据集：RT-1（Robotics Transformer 1）:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-markdown\" data-lang=\"markdown\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据来源\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据规模\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e~130k 条实际机器人操作轨迹，覆盖 700+ 种任务\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e采样频率\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每条轨迹包含约 50–100 帧关键帧（图像 + 动作）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e场景\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e家庭式办公环境（桌面、水槽、地面）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e物体\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e语言指令\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型）\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e对于每一条指令：\u003c/p\u003e","title":"Bug Journal 2025-06-04"},{"content":"Catch It! Learning to Catch in Flight with Mobile Dexterous Hands 发表时间: 16 Sep 2024\n动机 Basically, 这是之前那篇 DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。\n模型流程图 这是真机器人的样子： 一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。\n训练的过程分为两步：\n第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。 第二步是微调灵巧手让手抓住这个物体。\n最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。\n解决的难点 部署到真机器人上 一步到位 end-to-end 效果没那么好 抓不住从未见过的物体 还需要解决的难点 从未见过的物体还是不好抓 仍然没有考虑材质之类的问题 还是无法在当物体在空中时就判断物体的形状 Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own 发布时间: 4 Oct. 2023\n动机 现在机器人训练要 $10^6$ 级别的数据，这要的时间太长了。 反观人类，人类不需要这么多数据。 这可能是因为人类在训练之前就知道什么能做什么不能做。\n那么，我们能不能输入一个 policy 给机器人让它也知道什么能做什么不能做呢？\n主要论点 训练方式：Reinforcement Learning\n并非模仿学习\n作者提出了一个新框架：\nRLFP（Reinforcement Learning with Foundation Priors），其中融合三种先验： 策略先验（Policy Prior）：告诉 agent “该怎么做”。 价值先验（Value Prior）：评估当前状态“是否更接近成功”。 成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 他们在此框架下构建了具体算法 FAC（Foundation-guided Actor-Critic），把三类先验引入到 Actor-Critic 的学习流程中，并在真实机器人与模拟任务中验证了该方法的有效性。\n模型流程图 策略先验（Policy Prior）：告诉 agent “该怎么做”。 这会生成一个策略分布。 这个策略分布会作为一个 KL 正则项。 希望 Actor 生成的策略和这个策略分布不会差太远\n价值先验（Value Prior）：评估当前状态“是否更接近成功”。 这会作为 Reward 的一部分。 告诉 Robot 它是不是更接近成功了。\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 这也会作为 Reward 的一部分。 告诉 Rotbot 它是不是成功了。\n实现细节 在现实中： 策略先验（Policy Prior）：告诉 agent “该怎么做”。 本文使用了 GPT-4V 来实现这个功能：\n对于每一个单独的任务，需要重新写一个这样的 prompt, 但是模板都是一样的。\n模板如下：\nYou are a helpful robot programming assistant. Here is an image of the current environment, which includes: \u0026lt;Task Instruction\u0026gt; Your job: \u0026lt;Task Instruction\u0026gt; You may only use the following primitive skills: \u0026lt;Primitive Skills List\u0026gt; \u0026lt;Image Input\u0026gt; Please write a Python code to solve this task, however, you can only write code in this format: \u0026lt;Code Format\u0026gt; e.g. (根据文章推测的 example prompt):\nYou are a helpful robot programming assistant. Here is an image of the current environment, which includes: - A plastic bottle with a green cap (the bottle is fixed to the table) - A pink plate nearby Your job: Help a robot arm **unscrew the bottle cap** and **place it on the pink plate**. You cannot lift the bottle. You must rotate the cap **anticlockwise** to unscrew it. You may only use the following primitive skills: # Primitive Skills: # 1. move_to x y z —— move the gripper to position (x, y, z) # 2. grasp —— close the gripper to grasp # 3. release —— open the gripper # 4. rotate_anticlockwise —— rotate the gripper anticlockwise (90°) # 5. rotate_clockwise —— rotate the gripper clockwise (90°) # 6. reset —— move back to the home position \u0026lt;input image\u0026gt; Please write a Python function `code_policy()` that returns a plan list using the above skills. Be sure to: - Estimate the coordinates from the image (roughly) - Include comments to explain each step - Output only the code block and nothing else Your format should be like this: def code_policy(): plans = [ \u0026#39;move_to 0.5 0.0 0.26\u0026#39;, \u0026#39;grasp\u0026#39;, \u0026#39;rotate_anticlockwise\u0026#39;, \u0026#39;move_to 0.75 0.0 0.06\u0026#39;, \u0026#39;release\u0026#39; ] return plans Now write the code: 价值先验（Value Prior）：评估当前状态“是否更接近成功”。 使用了一个 Pretrain LLM 来 “判断好不好” VIP: Universal Visual Reward and Representation via Value-Implicit Pretraining\nVIP 是一个使用大规模离线机器人/视频数据集，目标是 通过一个 image 得到一个方程 $V(O_{t_i})$, 越大表示越成功。\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 在现实中使用 GPT-4V 来判断这个任务是否完成。\n这有浇花的时候没判断成功 (3 success in all 4 tasks)。\n浇花和狡猾脚滑谐音，做不对是正常的\n在虚拟环境中 策略先验（Policy Prior）：告诉 agent “该怎么做”。 为了证明 Policy 不需要固定的形式\n使用了 \u0026ldquo;a diffusion-based policy prior, following the UniPi [25] pipeline\u0026rdquo;\n先用扩散模型生成一个完成任务的视频，再通过一个逆动力学模型把视频帧之间的状态变化转化为动作。\n为了效率起见，使用了 开源视频扩散模型 Seer [26] 预生成视频，然后离线训练（distill）出一个策略模型（policy network）\n然而，因为模拟环境图像质量比现实差，所以生成的视频效果也不好。\n所以用了 10个视频 fine-tune 了一下。\n价值先验（Value Prior）：评估当前状态“是否更接近成功”。 Same set up.\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 这里有 Ground Truth 了，就不用 GPT-4V 了 但是，为了模拟现实中 GPT-4V 的情况，加入了一些噪声。 加入方法如下： 训练一个模型，从状态 + label 预测是否成功 这个模型不是 100% 准确。\n结果 现实世界一个小时后： 模拟世界:\nAblation study\nTake away run on 3090 GPU\nGeneral Flow as Foundation Affordance for Scalable Robot Learning 发表时间: 21 Jan 2024\n动机 一言以蔽之：机器人如何从感知（图像）中知道：“我该操作哪儿”和“怎么操作”？\n当前机器人操作学习普遍依赖：\n大量手工收集的数据； 手动定义的 affordance； 复杂的模仿学习或强化学习流程 而现在的数据：\n泛化能力差 没有统一，可以拓展的，自动的，包含语义的 embedding 主要论点 General Flow（GF） —— 一种结构化、密集的视觉场表示，表征“像素应如何在操作中流动”。\n这个和 NVIDIA DLSS 中的 OPTICAL FLOW 很类似，只不过加入了语义信息。\n模型流程图 ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors 发表时间: 30 Apr 2025\n动机 机器人操作策略泛化能力差, 能不能用 mask 的办法让机器人操作策略的泛化能力更强。\n主要论点 构建一个自动化数据生成流水线，合成高复杂度、多样化语言指令的数据集（112K 指令，24K 演示）;\n利用 GLaMM 模型和 SAM 架构生成目标对象与放置区域的精细分割 mask; 将这些 masks 融合进策略网络\n模型流程图 Part 1 数据集 现在的数据集不够好，我要弄一个新的数据集。 我有一个虚拟环境，这个环境里面有一些物体。\n那我能不能写一个脚本来自动设计一个数据集。\n为了给测试用的模型增加难度，我要在环境中添加一些相似的物体。\n那就可以在图片中找一些相似的物体出来，最好是有一项特征(如颜色)完全一致\n这时候 GPT 可以帮忙\nGPT 对这些物体有一定理解 (3视图，材质，颜色)\nGPT 的这些理解也可以加入进来帮我挑选要放入那些物体。\n有了环境信息还不够，我还要一个 Language Instruction.\n首先，我可以根据位置信息自动生成一些 rule-based Instruction: 比如把 A 移动到 B的右边\u0026hellip;\n那如果要一些更 abstract 的 Instruction, 那我可以用 GPT 生成一个 Language Instruction.\n比如说 水果 -\u0026gt; making jam.\nPart 2 New method 左边的部分和那天的 SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation 很像，都是通过一个 LLM + SAM 获取起始点和终点的信息。\n而右边的部分就是通过 强调起始点和终点 的 attention 来增强起点和终点权重。\n更详细地：通过增加了两个 Query, 分别只和起点和终点做 attention 来增强。\n最后通过一个 transformer decoder 输出离散的 action。\n训练是通过模仿学习，最小化和样本之间的差距。\n效果 计算复杂度 8 * 4090 GPU Approx. 5 days.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-03/","summary":"\u003ch2 id=\"catch-it-learning-to-catch-in-flight-with-mobile-dexterous-hands\"\u003eCatch It! Learning to Catch in Flight with Mobile Dexterous Hands\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2409.10319\"\u003e发表时间: 16 Sep 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003eBasically, 这是之前那篇 \u003cem\u003e\u003ca href=\"https://arxiv.org/abs/2310.08809\"\u003eDexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands\u003c/a\u003e\u003c/em\u003e 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1748922875088\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-03/1748922875088.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是真机器人的样子：\n一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1748922862160\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-03/1748922862160.png\"\u003e\u003c/p\u003e\n\u003cp\u003e训练的过程分为两步：\u003c/p\u003e\n\u003cp\u003e第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。\n第二步是微调灵巧手让手抓住这个物体。\u003c/p\u003e\n\u003cp\u003e最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。\u003c/p\u003e\n\u003ch4 id=\"解决的难点\"\u003e解决的难点\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e部署到真机器人上\u003c/li\u003e\n\u003cli\u003e一步到位 end-to-end 效果没那么好\u003c/li\u003e\n\u003cli\u003e抓不住从未见过的物体\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"还需要解决的难点\"\u003e还需要解决的难点\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e从未见过的物体还是不好抓\u003c/li\u003e\n\u003cli\u003e仍然没有考虑材质之类的问题\u003c/li\u003e\n\u003cli\u003e还是无法在当物体在空中时就判断物体的形状\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"reinforcement-learning-with-foundation-priors-let-the-embodied-agent-efficiently-learn-on-its-own\"\u003eReinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2310.02635\"\u003e发布时间: 4 Oct. 2023\u003c/a\u003e\u003c/p\u003e","title":"Bug Journal 2025-06-03"},{"content":"CLIP 这是 CLIP 的结构，主要是两个部分，一个是 text encoder, 用于得到 text embedding, 一个是 image encoder, 用于得到 image encodding.\nText encoder 是一个 text transformer, Image encoder 是一个 ResNet50 / ViT.\n输入数据是一张图片和它的 alternative text.\n训练的逻辑也不难理解：\n现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text, 现在我对这 N 个 pair 做两两配对。\n如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近 如果不属于同一个 pair, 那么我希望他们的 embedding 更远\n同时，我希望这个embedding的距离是有意义的，越相近的离得越近，越不同的离得越远。\n这时候我们就可以用 Cosine Similarity Loss 来比较两个 embedding 之间的距离。\nThat\u0026rsquo;s it.\n这里有个 assumption, 就是说，虽然我的数据质量不怎么样，但是我有很好的数量。\n对于每一个patch, 我有整整 32k 个图文 pair, 加起来一共 1B 个 True/False pair, 那我一定是可以学到一些东西的。\n但是这里也就是它的局限所在：32k 个图文 pair 要放到非常大量的 GPU 中才能 work, 这时设备之间的通信就成为了最大的效率瓶颈。\n但是虽然说这个模型 train 起来非常复杂，但是其实这个模型不算太大，单 GPU 就足以 inference.\n代码也非常简洁，一个简单的实现如下:\nimport clip import torch from PIL import Image device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;, device=device) image = preprocess(Image.open(\u0026#34;test.png\u0026#34;)).unsqueeze(0).to(device) # CLIP.png为本文中图一，即CLIP的流程图 text = clip.tokenize([\u0026#34;car\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;leaf\u0026#34;]).to(device) # 将这三句话向量化 with torch.no_grad(): logits_per_image, logits_per_text = model(image, text) probs = logits_per_image.softmax(dim=-1).cpu().numpy() print(\u0026#34;Label probs:\u0026#34;, probs) BLIP 这是 BLIP，大部分和之前一样：\n现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text, 现在我对这 N 个 pair 做两两配对。\n如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近 如果不属于同一个 pair, 那么我希望他们的 embedding 更远\n但是，现在我增加了一个部分：除了原本的 Contrastive learning 之外，我还要做一个图片和文字之间的 cross-attention.\n另外，原本的数据里有很多噪音。\n现在我已经初步 train 好一个图文匹配模型了。\n那我们默认在这个模型中图文匹配比较好的，就是数据中“高质量”的部分。\n这时我们再引入一个图生文模型，让模型自己学习这些“高质量”数据，然后覆盖“低质量数据”。\n这样就可以提高数据的整体质量。\nQ-Former 是轻量、任务相关、可控制的视觉语义提取器。\nSigLIP AlexNet AlexNet就是最开始的 CNN 网络\nResNet ResNet引入残差的概念，不再让 CNN 学习原始表示，而是让 CNN 学习不同层之间的差 同时，有些层的结果可以越过中间某些层直接去往更深的层。 这让更深的网络成为了可能。\nDenseNet DenseNet 则是再进一步，DenseNet 会让每层之间形成两两连接，使得网络效果更好。\nTake Away 为什么要用 Cosine Similarity:\n可大可小，既可以拉进，又可以推远；重要的是大小是有意义的，越大代表越不相近，越小代表越相近。\n单塔模型和双塔模型：\n单塔模型就是一个输入的模型；双塔输出就是有两个输入的模型。 或者说，如果一个 embedding 只过一遍模型，那就是双塔模型。 如果一个 embedding 可能要过多遍一个模型，那就是单塔模型。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-02/","summary":"Review of Three base VLA models and three basic CNNs.","title":"Bug Journal 2025-06-02"},{"content":"主流视觉-文本多模态模型技术分析 近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。\nCLIP (OpenAI, 2021) 整体架构设计： CLIP 采用 双编码器架构 ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到相同维度的向量空间lightly.ai。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型lightly.ai。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到共享的多模态嵌入空间lightly.ai。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。 模态对齐方式： CLIP通过对比学习实现视觉-语言对齐。训练时，模型给定一批图文对，学习预测哪张图像与哪段文本匹配lightly.ai。具体而言，CLIP使用 对称的跨模态对比损失 （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度lightly.ai。这种训练使图像和文本编码器产生的特征在共享空间中成对靠近lightly.ai。 输入 token 表达统一： CLIP并未显式统一图像和文本的输入表示格式。 图像和文本各有独立的token化和编码流程 ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征lightly.ai。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过独立编码+对齐空间的方式，规避了直接将图像作为序列token处理的不统一问题。 损失函数与训练策略： 损失采用 对比学习的InfoNCE损失 （实现为带温度系数的归一化softmax交叉熵）research.google。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了大批量训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化lightly.ai。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应lightly.ai。CLIP从随机初始化开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型lightly.ai。 数据集及伪标签： CLIP在一个超大规模的图文配对数据集上预训练，包含约4亿对图像-文本arxiv.org（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖自然语言的弱监督arxiv.org。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。 六大难点应对： 模态对齐困难： CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远lightly.ai。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。 token格式不统一： 采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对lightly.ai。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。 语义粒度不匹配： CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如Fine-Grained CLIP等正是受限于CLIP在局部语义对齐上的不足lightly.ailightly.ai。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。 多模态上下文保持： CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 多模态上下文 （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。 训练数据稀缺： CLIP通过大规模弱标注数据从根本上缓解了数据稀缺的问题arxiv.org。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征research.google。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。 计算开销高： 训练CLIP确实需要巨大的算力和显存，但相对来说，其双塔架构使训练可并行展开，推理时也可分别预编码图文后做相似度计算，具有一定的效率优势lightly.ai。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化lightly.ai。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现lightly.ai。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。 参考： CLIP 的论文arxiv.org详细描述了其对比预训练方法，OpenAI 的博客也提供了概述lightly.ai。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库huggingface.co。\nALIGN (Google, 2021) 整体架构设计： ALIGN（ A Large-scale ImaGe and Noisy-Text embedding ）延续了与CLIP相同的双编码器对比学习架构research.google。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：EfficientNet-L2卷积网络作为图像编码器，BERT-Large作为文本编码器，并均从随机初始化开始训练research.google。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。 模态对齐方式： ALIGN采用对比损失（normalized softmax）来训练，使匹配的图文对嵌入向量接近，不匹配的远离research.google。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以批为单位的跨模态对比训练，使模型学到强大的图文对齐表示。 输入 token 表达统一： ALIGN同样没有将图像直接离散为token序列，而是通过双通道处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出embedding空间实现统一表示research.google。 损失函数与训练策略： 使用 对比学习损失 （InfoNCE变体），在大批量上训练模型research.google。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 最小程度的过滤 ，通过数据规模来弥补噪声research.google。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。 数据集及伪标签： ALIGN的亮点在于使用了超过10亿对图像-Alt文本的超大规模数据集research.google。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 无需人工标注 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN 放宽过滤标准 ，只做了基于频率的简单过滤，最终得到约18亿对图文数据research.google。这些文本描述可能包含噪声甚至与图像无关，但研究表明 规模弥补质量 ：如此海量的数据使模型学到泛化的视觉语言表示research.google。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题research.google。 六大难点应对： 模态对齐困难： ALIGN证明了数据规模在对齐中的重要作用。通过十倍于CLIP的数据规模和强大的对比损失，模型学到了稳健的跨模态对齐能力research.googleresearch.google。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型research.google。 token格式不统一： 与CLIP类似，ALIGN通过双编码器架构回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可research.google。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。 语义粒度不匹配： ALIGN的训练目标依旧作用在 全局图像-句子层面 ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对细粒度语义的不匹配没有特殊解决方案。 多模态上下文保持： ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力research.google，未涉及多模态对话等情境。因此，ALIGN在多轮交互或长上下文问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。 训练数据稀缺： ALIGN的策略是极端扩增数据规模以消除数据稀缺瓶颈research.google。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型research.google。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 弱标注的大数据 。 计算开销高： 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展research.google。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源research.google。尽管计算开销高昂，ALIGN通过 冻结架构复杂性 （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，优先扩大数据规模比增加模型复杂度更有效research.googleresearch.google。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。 参考： ALIGN 的研究细节发表于 ICML 2021research.google。Google Research 官方博客提供了对ALIGN的通俗描述research.googleresearch.google。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。\nBLIP (Salesforce, 2022) 整体架构设计： BLIP（ Bootstrapping Language-Image Pre-training ）提出了一种 多模态混合编码-解码架构（MED） ，旨在同时支持视觉-语言的理解和生成任务lightly.ai。具体而言，BLIP的模型包含三个不同模式的子模型：① 单模态编码器 ：对图像或文本单独编码，用于提取各自模态的表示，并采用图文对比损失（ITC）对齐两种模态的全局特征lightly.ai；② 图像引导的文本编码器 ：在文本Transformer中引入跨模态注意力，让文本编码能够利用图像特征，训练时使用 图文匹配（ITM）损失来判断给定图文是否匹配lightly.ai；③ 图像引导的文本解码器 ：使用因果自注意（单向）以实现文本生成，能够在给定图像的条件下生成描述或回答，训练时使用语言模型（LM）损失lightly.ai。这三部分共享同一个视觉编码器（ViT）和大部分文本编码-解码参数，仅在自注意力层是否双向/单向上有所区别lightly.ai。这种设计使一个模型即可兼顾判别任务 （如检索、VQA判断）和 生成任务 （如图像描述）。 模态对齐方式： BLIP结合多种目标实现模态对齐和融合：(1) 图文对比学习（ITC）让图像和文本的全局表示对齐，获得CLIP类似的跨模态嵌入空间lightly.ai；(2) 图文匹配（ITM）通过引入交叉注意力的文本编码器，对图文对的匹配与否进行二分类训练，从而细粒度地对齐图像内容和文本语义lightly.ai；(3) 图像条件文本生成（LM损失）使模型学会在视觉条件下生成合适的语言输出。这三种损失共同训练，迫使模型不同层面地对齐：既有 全局对齐 （ITC保证embedding空间一致），又有 局部对齐 （ITM通过注意力机制关注图像局部来判断匹配），还有跨模态 生成对齐 （LM确保图像信息融入生成过程）。尤其ITM子任务，需要模型理解图像细节与文本句子细微差异，提高了模态对齐的精细程度。 输入 token 表达统一： BLIP并未将图像直接当作序列token交给文本Transformer，而是采用部分共享参数的编码器-解码器架构来统一多模态信息。图像首先由视觉Transformer编码为一组图像embedding序列，文本则通过词嵌入得到文本token表示。在图文交互阶段，图像embedding通过跨模态注意力机制供文本编码器/解码器读取，从而在Transformer中融合lightly.ai。由于文本端Transformer参数在编码和解码模式下共享（仅注意力方向不同），图像信息可以以一致的方式融入文本token处理流程lightly.ai。简言之，BLIP通过在Transformer中引入图像作为钥匙/值的跨注意力，将图像内容注入文本token序列的处理，使两种模态的信息在Transformer中统一表达和交互。 损失函数与训练策略： BLIP在预训练阶段联合优化三种损失lightly.ai：图文对比损失、图文匹配损失、语言建模损失。一次训练迭代中，通过共享的图像编码器提取视觉特征后，分别送入上述三种模式的网络计算损失lightly.ai。为提高效率，BLIP 让文本编码器和解码器共享参数 （仅自注意力模块不同），这样图像特征可一次读取，多头任务不会成倍增加参数lightly.ai。训练策略上，BLIP先在大规模图文数据上这样多任务预训练，然后可以微调到下游具体任务。由于同时优化多目标，需平衡各损失的权重，论文中选择了适当的超参使模型在理解和生成性能上均有提升。此外，为了利用噪声较大的网络数据，BLIP设计了 两阶段Bootstrapping策略 ：首先用预训练好的模型作为图像描述生成器（Captioner）为图像生成候选描述，然后用一个筛选器（Filter）剔除不匹配的图文对，再将清洗/补充后的数据用于进一步训练lightly.ai。这种“Captioner+Filter”流程有效降低了训练数据中的噪声，并引入了合成的伪标签描述，提高了数据利用率arxiv.org。 使用的数据集及伪标签： BLIP的预训练使用了数百万规模的公众图文对数据（如COCO Caption、Visual Genome等常用数据集的组合，以及从网上爬取的图文对）arxiv.org。相对于CLIP/ALIGN那样十亿级的弱标注数据，BLIP使用的数据规模较小但质量更高（经过一定清洗）。为了进一步扩充数据，BLIP引入 伪标签机制 ：利用自己模型生成图像描述（Captioner生成synthetic captions），再通过训练好的匹配模型（Filter）过滤噪声lightly.ai。结果是，原本嘈杂的网络图文数据被“自举”出较为可靠的新图文对，从而缓解了高质量标注数据不足的问题arxiv.org。这一过程中生成的图像描述相当于 合成标签 ，极大丰富了训练语料。在预训练后，BLIP在下游如Flickr30K检索、COCO描述、VQA等数据集上进行微调或直接评估，均取得领先性能arxiv.org。值得一提的是，BLIP的整个预训练不依赖外部标注工具（如不使用额外OCR或检测模型），完全通过多任务训练和自举数据来提高性能。 六大难点应对： 模态对齐困难： BLIP通过多重训练目标从不同层次对齐图像和文本。ITC损失提供全局嵌入对齐，ITM损失迫使模型关注细粒度关联来判断真伪配对，LM损失则确保图像信息能融入自然语言生成lightly.ailightly.ai。此外，BLIP在训练中让图像参与文本Transformer的注意力计算，直接在模型内部融合模态，这比单纯对比学习的对齐更深入。综合来看，BLIP有效缓解了模态对齐难的问题，使模型不仅对整体匹配敏感，也能对局部语义对齐做出正确判断。 token格式不统一： BLIP没有一刀切地将图像转为离散token，而是采用跨注意力融合策略保持各模态表征方式的优势。图像以连续向量形式存在，通过跨模态注意力供文本Transformer使用，实现类似“在Transformer中把图像当成一串记忆token”的效果。这种方法避免了人为定义图像token格式的问题，由模型自学怎样将图像特征融入文本语境。因此，BLIP在不显式统一输入格式的情况下，通过模型结构实现了功能上的token统一处理。 语义粒度不匹配： BLIP在架构上引入了细粒度语义对齐机制。ITM子任务要求模型判别图文是否匹配，这通常取决于对图像细节和文本词语是否对应的判断（例如一句描述中某个细节是否在图中存在）。模型通过跨注意力可以聚焦图像的局部区域来对应文本片段，从而解决了图像区域与文本词汇粒度不对齐的问题。在生成阶段，图像引导的解码器也能描述具体对象或属性，实现细粒度描述lightly.ai。因此BLIP比CLIP这类全局对齐模型更好地兼顾了细粒度的语义对齐。 多模态上下文保持： 虽然BLIP主要针对单幅图像与单段文本的配对任务，但其架构天然适用于 多轮交互的扩展 。因为BLIP的文本Transformer本质是一个语言模型，经过适当调整可以接受前文对话作为文本输入，然后结合图像生成回答。事实上，BLIP的设计理念已被后续多模态对话模型（如 InstructBLIP 等）继承，用于处理多轮对话。不过BLIP原始模型并未显示多轮对话训练。在单轮情景下，BLIP利用Transformer的长序列能力，一定程度上可以处理 更长的文本上下文 （如图像说明+问题一起作为前缀，然后生成回答）。因此在上下文保持上，BLIP通过Transformer结构具备了潜在优势，但需通过特定训练来充分发挥。 训练数据稀缺： BLIP以 自举（Bootstrapping）的方式缓解数据不足。面对高质量标注数据有限的问题，BLIP使用初步模型为大量未标注图像生成描述（相当于自动标注），再过滤噪声后加入训练arxiv.org。这种方式有效放大了训练集规模且成本低，因为生成伪标签比人工标注快得多。结果，BLIP无需像CLIP那样依赖上亿数据，就能取得优异表现arxiv.org。此外，多任务联合训练也提高了数据利用效率：同一数据同时为对比、匹配、生成三种任务服务，信息提取更加充分。总之，BLIP通过模型自生成数据+多任务学习 ，成功在有限数据下逼近甚至超越了依赖海量数据的方法。 计算开销高： BLIP的模型大小适中（基于ViT-B/16等视觉主干，文本部分与BERT-base级别相当），但同时优化三种目标确实增加了训练复杂度。不过，通过 参数共享 （文本编码器和解码器共享大部分参数）lightly.ai和 模块复用 （同一个视觉编码器和Transformer用于多任务），BLIP将训练开销控制在可接受范围。相较于为理解和生成训练两个模型，BLIP训练单个模型完成两类任务，实际上 节省了总体算力 。当然，多任务训练需要更长时间收敛，但Salesforce的实验表明收益是值得的arxiv.org。在推理阶段，BLIP可以根据任务切换模式，例如执行检索时只用编码器部分，做描述时用编码-解码器，全模型参数无需全部参与，从而 推理开销也相对可控 。综上，BLIP通过架构设计在性能和计算成本之间取得了平衡，使得大型多模态模型的训练变得更加高效。 参考： BLIP的论文发表在 ICML 2022arxiv.orgarxiv.org。官方代码已开源在 GitHubarxiv.org（salesforce/BLIP仓库），提供了预训练模型和下游任务的fine-tune实现，方便复现论文结果。\nBLIP-2 (Salesforce, 2023) 整体架构设计： BLIP-2的核心思想是利用现有的预训练模型来高效构建多模态模型arxiv.org。它冻结了图像编码器（如ViT系列）和大型语言模型（LLM，如OPT、Flan-T5等），通过引入一个轻量级的Query Transformer（Q-Former）将二者连接起来arxiv.org。架构上包括：冻结的视觉编码器-\u0026gt; Q-Former -\u0026gt; 冻结的文本生成模型。Q-Former本质是一个Transformer模块，接受视觉特征作为输入，输出一组固定数量的查询向量lightly.ai。这组向量经过投影后，作为虚拟的“视觉token”，嵌入到LLM的输入序列中，从而让LLM能够接收图像信息lightly.ai。由于LLM参数冻结，BLIP-2主要训练Q-Former和少量连接层。 模态对齐方式： BLIP-2将跨模态对齐的主要难点转移到Q-Former上。它采用两阶段训练lightly.ai：第一阶段，让Q-Former结合冻结视觉编码器进行 图文表示学习 （类似BLIP-1的方法，使用图文对比或匹配损失），使Q-Former学会提取与文本语义相关的视觉概念lightly.ai。第二阶段，将训练好的Q-Former输出连接到冻结的LLM输入embedding，利用图文对话/描述数据训练生成任务，使整个系统能够端到端地产生对应输入图像的文本lightly.ai。在对齐过程中，Q-Former充当中介：它一头通过跨注意力读取视觉特征，另一头输出的查询向量要能和LLM的语义空间对接。因此，通过专门设计的损失（如阶段一的对比/ITC+ITM，阶段二的语言建模），BLIP-2成功将视觉空间对齐到语言空间。直观来说， Q-Former学会生成“描述图像的语言向量” ，这些向量插入LLM提示中后，LLM即可理解并基于图像内容作出回答。 输入 token 表达统一： 在BLIP-2中，输入给LLM的是标准的文本token序列，但其中混入了由图像生成的 特殊嵌入向量 。具体实现是：LLM的词表中引入若干个保留位置，用来放置Q-Former生成的视觉查询向量（不一定真的映射为离散token，而是直接作为embedding）lightly.ailightly.ai。因此，从LLM角度看，它接收到了一串长度为N（固定）的“视觉token”嵌入，后面可能跟随文本token，例如问题或提示语。通过这种方式，图像信息被格式统一地并入LLM的输入序列，就好像视觉也被表示成了一组特殊的单词embedding。值得注意的是，这里的视觉token并非通过人工词典获得，而是Q-Former自由学习产生的向量。不过，对于LLM来说，无论是真实文字embedding还是视觉embedding，它都一视同仁地通过自注意力机制处理。这实现了在架构上的 输入统一 ：图像被转换成等价于文本embedding的形式，与文本共同作用于下游生成。 损失函数与训练策略： BLIP-2采用 分阶段训练策略 。阶段一使用与BLIP类似的目标（ITC对比学习、ITM匹配等）训练Q-Former，使其能够对齐视觉和文本表示lightly.ai。阶段二则固定视觉编码器和Q-Former不变（或仅微调Q-Former），仅训练将Q-Former输出喂入LLM后的生成能力lightly.ai。阶段二通常采用 语言模型损失 ：给定图像和（可选的）文本提示，让LLM输出描述或答案，与GT文本计算交叉熵损失，从而调整Q-Former和连接层使LLM的输出正确。在这一阶段，LLM本身参数冻结，所以训练信号主要作用于Q-Former，使其输出的视觉查询能被LLM高效利用。此外，BLIP-2可能使用了混合数据训练策略：既包含纯图文对话数据，也包含传统图文描述数据，以增强模型泛化能力。总结来说，第一阶段注重表示对齐，第二阶段注重生成对接arxiv.org。两个阶段结合，使模型以较低的训练成本达到对大语言模型“喂图”的效果。 使用的数据集及伪标签： BLIP-2所使用的数据包括现成的大规模图文对以及 对话式多模态数据 。阶段一使用的数据类似BLIP-1，例如COCO Caption、Visual Genome Caption以及LAION-400M等开放图文集，用于学习跨模态表示。阶段二则需要图像输入/文本输出的监督数据，如VQA问答、图像描述，以及自制的指令数据集等。由于BLIP-2本身是在2023年提出，可能利用了当时兴起的多模态指令数据（例如由GPT生成的对话）来增强模型的对话能力。关于伪标签，BLIP-2相比BLIP-1更少需要合成描述，原因是它直接利用预训练的LLM已经具备生成流畅文本的能力。相反，BLIP-2更关注 如何高效利用预训练资源 。它没有从零开始生成伪标签，而是通过降低训练需求（冻结大模型）来避免需要海量新标注数据arxiv.org。因此，除非为了特定任务，BLIP-2通常不依赖额外的伪标注数据。不过，在一些研究和开源实现中，会将BLIP-2作为基础，再用GPT-4生成的指令数据进行微调（如InstructBLIP），那属于后续fine-tuning阶段。总的来说，BLIP-2本身强调 利用已有数据与模型 ，而非采集新数据，这是一种不同于以往“大规模爬取”的范式。 六大难点应对： 模态对齐困难： BLIP-2的巧妙之处在于借助预训练模型降低对齐门槛：视觉编码器（如CLIP的ViT）本身已具有与文本对齐的表示能力，LLM则有强大的语言理解和生成能力。Q-Former经过专门训练，学习如何从图像提取出能解释文本的关键视觉概念lightly.ai。它将视觉信号压缩成几十个查询向量，使之恰好能被LLM理解。通过两阶段训练，BLIP-2成功将视觉信息嵌入LLM的上下文中，实现 模态隐式对齐 。尤其第二阶段训练，让模型生成正确描述，确保了视觉表示和语言表示在语义空间上对齐，以至于LLM可以将来自图像的embedding视作自身词汇的一部分。这解决了LLM未看过图像的难题，将跨模态对齐转换为一个中等规模Transformer训练就完成了arxiv.org。因此，BLIP-2在对齐上绕过了直接训练巨型多模态模型的难关，以更低成本达到对齐效果。 token格式不统一： BLIP-2通过 Query Transformer输出固定长度视觉token向量 ，使得图像信息以接近文本token的形式输入LLMlightly.ai。这些视觉token不是离散符号，但在Transformer中发挥的作用与普通词嵌入相同。LLM在位置嵌入上也不区分它们，这样视觉和文本序列实际上融合为一个统一的序列处理lightly.ai。因此，虽然没有显式定义图像的词汇表，BLIP-2达成了功能上的token格式统一：模型把连续视觉特征转换为离散的若干embedding插入序列。LLM可以像处理句子一样处理“图像句子”。这一设计继承了Flamingo等模型的思路，但更轻量（因为只有Q-Former承担额外计算）。 语义粒度不匹配： BLIP-2输出的视觉token向量本质上可以被视为图像的 语义摘要 。Q-Former通过训练，会针对文本任务提取图像中与语义相关的细粒度信息。例如，若任务是描述图像，Q-Former会聚焦显著对象和属性；若任务是回答问题，Q-Former会提取与问题相关的视觉线索。这种机制使图像的大量低层次像素信息被压缩，仅保留语义层面的关键内容，从而匹配LLM处理的语言粒度（概念级别）lightly.ai。因此，语义粒度的鸿沟通过Q-Former的提炼得到弥合——图像的细节被提升到语义概念后才提供给LLM。实践证明，BLIP-2能够让LLM正确识别图中具体对象并生成相应描述，说明语义层次基本匹配了语言空间arxiv.org。当然，如果图像中有非常细微的局部信息，固定数量的查询可能略有不足，但总体上BLIP-2在保持主要语义同时过滤冗余细节方面是成功的。 多模态上下文保持： BLIP-2本身不直接处理多轮对话，但由于它的输出接口是对接LLM，而LLM天然支持长上下文对话，因此BLIP-2具备扩展为多模态对话的潜力。事实上，将BLIP-2生成的视觉token视为对话的一部分，就可以实现 ChatGPT+图像 的效果。BLIP-2的论文主要评估的是单轮任务（如VQA回答、图像描述），但把它用于对话时，可以每次在提示中加入视觉token并配合已有的聊天上下文，LLM即可持续参考视觉信息进行对话。这意味着BLIP-2间接实现了 视觉上下文在多轮对话中的保持 ：视觉token可以在对话prompt中重复出现或被引用，使LLM记住之前提到的图像要点。不过，在一个会话过程中，BLIP-2通常针对每张新图像各自运行一次，不会像Flamingo那样显式处理多张图共同存在的情况。因此严格来说，BLIP-2原生支持 单图上下文保持 ，多图或连续对话需借助LLM的记忆机制来维系。 训练数据稀缺： BLIP-2的策略是 以预训练模型替代海量数据 。因为直接训练一个看图的LLM需要海量图文数据，但BLIP-2通过使用预先训练好的ViT和LLM，将主要学习任务转为训练Q-Former。Q-Former的参数规模（约1.9亿）远小于LLM，所需训练数据也相对少lightly.ai。实验表明，在已有的大模型基础上，只需在相对有限的图文数据上微调，就能达到甚至超过训练80亿参数模型（如Flamingo）的效果arxiv.org。这等于用模型知识弥补了数据量不足。此外，BLIP-2本身利用了BLIP-1时期的图文数据清洗经验，挑选高质量数据进行两阶段训练，以较小的数据量取得高性能arxiv.org。因此，对研究者而言，BLIP-2降低了训练多模态模型的数据门槛——无需爬取上亿样本，有几百万高质量样本配合预训练模型就够用。 计算开销高： 相比从头训练一个多模态Transformer（参数往往数十亿），BLIP-2的训练开销显著降低。冻结LLM和视觉编码器意味着大部分参数不需要反向传播更新，只训练Q-Former等少部分参数，使内存和算力需求下降。作者报告，BLIP-2仅有少量可训练参数却超越了一些体量大几十倍的模型arxiv.org。同时，由于分阶段训练，第一阶段可在相对小模型上完成，第二阶段虽然用LLM但只进行embedding层和Q-Former的调优，计算效率高。综合来看，BLIP-2通过迁移学习和 参数高效微调 ，极大缓和了算力需求。这也体现在推理阶段：因为LLM冻结且对话时只需将视觉token拼接输入，不增加额外推理步骤，实时性有保障。当然，BLIP-2依赖的LLM本身推理开销不低（如果LLM很大），但相较于训练一个同等大小的多模态模型，BLIP-2的总计算代价小得多。因此，在算力有限的环境下，BLIP-2提供了一种实用可行的多模态方案。 参考： BLIP-2的论文在 arXiv 发布arxiv.orgarxiv.org（ICLR 2023），详细介绍了其两阶段训练方法和在零样本VQA等任务上的性能。代码已开源在 GitHub（salesforce/LAVIS库中提供了BLIP-2实现）。BLIP-2的效果也推动了许多衍生工作（如开放对话系统 MiniGPT-4 等），这些都建立在BLIP-2提供的视觉-语言接口之上。\nGIT (Microsoft, 2022) 整体架构设计： GIT（ Generative Image-to-text Transformer ）尝试将视觉-语言任务完全统一到一个生成式Transformer框架下ar5iv.labs.arxiv.org。其架构极为简洁： 一个图像编码器 + 一个文本解码器 ，二者共同组成一个端到端的序列到序列模型ar5iv.labs.arxiv.org。图像编码器提取图像特征（采用预训练的CLIP视觉Transformer或自训练的ViT等，输出二维特征序列ar5iv.labs.arxiv.org），然后通过线性层投影并加上位置嵌入，作为文本解码器的跨注意力键值输入ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。文本解码器是标准的Transformer解码架构（多层自注意力+交叉注意力），以语言模型方式生成文本ar5iv.labs.arxiv.org。不同于许多早期方法，GIT不使用任何物体检测器或OCR模型来预处理图像，也不引入额外的多模态编码器，一切融合在Transformer解码器中完成ar5iv.labs.arxiv.org。这种纯粹的“图像到文本”架构使模型在预训练和微调阶段的结构完全一致，能够方便地泛化到各种以文本为输出的视觉任务。 模态对齐方式： GIT没有采用显式的对比对齐或ITM损失，而是通过单一的语言建模任务隐式地实现模态对齐ar5iv.labs.arxiv.org。在预训练时，模型接收图像并 直接生成整段描述文本 （或回答），训练目标是最小化生成文本与真实文本之间的交叉熵损失ar5iv.labs.arxiv.org。这种方式迫使图像编码器提取的特征必须包含生成正确文本所需的所有信息，同时解码器的交叉注意力会学习将文本词汇与相应的图像区域关联，以便正确生成。这意味着图像和文本的对齐并不是通过拉近embedding距离实现的，而是在Transformer解码过程中，通过注意力权重对齐：模型只有在正确对齐图像内容与生成词语时才能取得低损失。例如，当解码器生成单词“狗”时，跨模态注意力会自然地关注图像中狗所在的特征区域，从而将视觉语义与该单词绑定。经过大规模训练后，这种注意力驱动的软对齐形成模型内隐的模态对齐机制。值得一提的是，作者在预训练时 扩充了任务种类 ，不仅包括图像描述，还有图像问答等，这些任务都要求正确关联图像和文本才能解答，从而进一步强化了模态对齐ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。 输入 token 表达统一： GIT通过 将图像特征序列拼接进Transformer解码器的输入 ，实现了一种隐式的token统一表示ar5iv.labs.arxiv.org。具体而言，图像编码器输出经过投影变换后，作为一组“图像token”（连续向量）排列在Transformer解码器的输入序列最前ar5iv.labs.arxiv.org。紧随其后的是文本的\u0026lt;BOS\u0026gt;标记和需要生成的文本token（初始化为待预测状态）。在Transformer内部，采用一个特别的序列到序列注意力掩码ar5iv.labs.arxiv.org：文本token可以看见所有图像token和之前的文本token，而图像token之间也可以相互看到（便于图像特征全局建模）ar5iv.labs.arxiv.org。这样，Transformer解码器实际上同时处理了图像token和文本token的序列。对模型而言，图像token与普通文本embedding在同一计算图中，只是通过mask控制了注意力方向。通过这种机制，GIT无需修改Transformer结构，就实现了 图像+文本统一序列建模 ：图像被视作解码开始时的一段前缀序列。这保证了图像信息能够像前文一样参与生成过程，从而让图像上下文与文本自然融合。另外，这种方法也不需要离散化图像，只要提供足够的图像token分辨率，模型就能以连续表示处理视觉信息。 损失函数与训练策略： GIT采用 纯粹的自回归语言模型损失 。给定图像（以及可选的提示文本），让模型生成目标文本序列，计算标准的交叉熵损失来训练ar5iv.labs.arxiv.org。在预训练期间，为了让模型适应多样任务，训练数据中包含了各种形式：图像标题生成、图像问答（在这种情况下，会在图像token后加入问题文本作为前缀，然后生成答案）等ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。例如，对于VQA，输入序列是「\u0026lt;img\u0026gt;\u0026hellip;\u0026lt;img\u0026gt; 问题：\u0026hellip; 答案：」，模型学习在看到“问题”后生成正确“答案”ar5iv.labs.arxiv.org。这种统一的语言模型策略使预训练和下游任务能够共享同一套参数和目标，不需要为不同任务切换架构或损失函数。此外，作者强调扩大预训练数据和模型规模对性能至关重要ar5iv.labs.arxiv.org。他们使用了比以往更大规模的图文数据，以及训练了不同尺寸的模型（从Base到巨型）进行对比，在多个任务上取得新的SOTAar5iv.labs.arxiv.org。训练策略上没有使用教师模型或多阶段训练，而是一阶段大一统模型学尽可能多的任务。这种“无技巧（no bells and whistles）”的方法充分依赖海量数据和模型容量来获得性能ar5iv.labs.arxiv.org。 使用的数据集及合成数据： GIT的预训练数据非常广泛和庞大。微软在论文中没有公布确切的数据量，但提到**“扩大了预训练数据规模”ar5iv.labs.arxiv.org。推测他们使用了公共的大型图文数据集合集（如COYO、LAION等），以及内部收集的数据，包括图片描述和问答标注。此外，他们还将视频字幕数据扩充到模型中，使模型能处理视频（选帧作为序列的一部分）ar5iv.labs.arxiv.org。在下游微调时，GIT在12个具有挑战性的基准上测试，包括COCO、nocaps（开放词汇描述）、VizWiz（盲人拍照求助）、TextCaps（需要OCR的图片描述）、多种VQA和视频caption等arxiv.org。令人瞩目的是，GIT在TextCaps数据集上首次超越了人类表现arxiv.org，说明模型学会了相当程度的场景文本识别和理解——这归功于预训练涵盖了带文字的图像以及无需OCR模块的端到端学习。GIT并未借助合成的伪标签数据；相反，它直接在真实任务数据上大规模训练**。例如，为了让模型具备OCR能力，他们可能在预训练中加入了带文字的图像及其文字描述（如OCR-VQA等），让模型自己去学习文字区域的特征提取ar5iv.labs.arxiv.org。因此，GIT更多是通过多任务训练覆盖各种模态难点，而不是通过额外生成数据来弥补。当然，训练这样一个模型本身需要巨量的数据，但微软具备这样的资源优势。 六大难点应对： 模态对齐困难： GIT选择了端到端生成作为对齐手段。由于模型只能通过正确生成文本来降低损失，它被迫在内部对齐图像与文本。例如，Transformer解码器的交叉注意力会在训练中自动调整，使得每个生成的词与相应的图像内容关联。这种隐式对齐不需要额外的对比损失，却在模型Attention权重中形成了图像区域-文本词汇的映射关系。再加上GIT预训练涵盖问答等任务，模型学会在回答问题时关注相关图像部分，在描述时依照图像内容组织语言——这些都属于模态对齐的体现。可见，尽管没有显式对齐Loss，GIT通过任务驱动对齐实现了高质量的模态对齐ar5iv.labs.arxiv.org。模型的成功表明，只要任务设计合理，生成式训练本身就能让模型学会跨模态对齐。 token格式不统一： GIT通过序列到序列Transformer架构，巧妙地让图像和文本“同列于一个序列”。图像编码器输出一系列向量，这些向量在解码器里被视作一段上下文序列ar5iv.labs.arxiv.org。这样，虽然图像不是离散单词，但在Transformer看来，它们只是前若干个特殊的输入embedding。后续文本token可以自然地参考这些图像embedding，就如同参考句首提供的提示一样。这个设计避免了需要定义图像词典或修改模型输入结构，使 格式统一的问题迎刃而解 。换言之，Transformer模型对图像和文本一视同仁，只是通过mask控制依赖关系ar5iv.labs.arxiv.org。因此，GIT内部已经实现了对不同模态信息的格式融合，不存在单独处理再对齐的问题。 语义粒度不匹配： GIT直接使用CNN/ViT提取图像特征，并通过Transformer将其转换为语言。没有显式区域级别的对齐机制，但Transformer的交叉注意力可以细粒度地处理图像patch与词的关系。例如，模型在生成某个名词时，会极大地注意对应物体的那些视觉token，实现类似局部对齐的效果。这相当于让细粒度对齐在注意力机制中自发完成。此外，作者使用了一个trick：他们用对比学习预训练好的图像编码器ar5iv.labs.arxiv.org，保证图像特征本身具有较高级的语义表示能力（对比预训练会让相同类别/语义的图像特征聚类）。这意味着图像特征一开始就带有一定的语义概括性，减少了视觉低层细节与语言高级概念的不匹配ar5iv.labs.arxiv.org。因此，在语义粒度上，GIT通过预训练的视觉语义特征+解码器注意力两方面，较好地解决了粒度差异问题。模型的OCR能力说明它可以从小区域拼写出单词，说明精细粒度也能捕获；而在描述整图时又能抓大放小，生成整体语义，这体现了粒度上的灵活性。 多模态上下文保持： GIT的设计初衷不在对话，而在统一各种 静态视觉任务 。因此原版GIT不具备多轮对话记忆。然而，它提供了 统一的生成框架 ，理论上可以扩展对话：只需在输入序列中加入之前对话的文本，即可将历史作为上下文。而图像如果需要在对话中反复参考，可以在每轮答复时都把同样的图像token放入输入。但这会受到模型最大序列长度限制。微软没有在论文中报道对话实验，但在VQA任务里，GIT通过将“问题”作为前缀文本与图像共同输入ar5iv.labs.arxiv.org来回答，已经体现了处理图文混合上下文的能力。对于多张图像，GIT可以一次编码多张图的特征串联作为更长的图像token序列，只是论文未深入探索。这种架构天然支持多模态上下文的扩展，但需要注意计算成本会随序列长度增长。在视频场景中，作者已经验证了能处理多帧（通过给每帧加上时间嵌入再串联）ar5iv.labs.arxiv.org。所以GIT显示出一定的上下文扩展性，但要真正保持多轮对话语境，可能还需在生成策略上做些改动（如引入特殊标记区分说话人等）。总的说来，GIT为多模态上下文提供了一个统一容器，但对话管理不在其预训练范围内，需要额外设计。 训练数据稀缺： GIT依赖大规模多样化数据取得成功。它的理念是与其设计复杂模型，不如用简单模型配合巨量数据ar5iv.labs.arxiv.org。虽然作者未公开数据细节，但可以推测其使用近十亿级别的图文对进行训练（极可能包括微软内部的ALIGN-类数据或JFT系列）。通过大量数据，GIT在各任务上都达到新的高度arxiv.org。对于普通研究者而言，如此数据难以获得。但GIT证明，大模型+大数据可以在无需额外标注和复杂技巧的情况下解决很多问题。因此，GIT没有使用伪标签，它体现的是另一种思路： 以规模取胜 。这在一定程度上回避了数据稀缺，因为一旦数据够多，很多小数据集的问题都变得可以零样本解决ar5iv.labs.arxiv.org。此外，统一模型能跨任务共享知识：例如在描述任务学到的知识对VQA有帮助，这其实提高了每条数据的利用率。这种多任务迁移也缓和了单任务数据不足的情况。因此，虽然GIT本身消耗了巨大数据，但相对于分别训练多个任务专用模型，其综合效率反而更高。 计算开销高： 训练一个像GIT这样的模型（尤其是大尺寸版本）需要相当高的计算投入。微软通过大规模并行和分布式训练完成了这一过程。幸运的是，GIT架构简单统一，没有多分支，这使并行效率较高。模型参数虽多，但Transformer易于在GPU/TPU上加速。而且作者在论文中提供了不同模型规模的对比如Base、Large、Huge等ar5iv.labs.arxiv.org。在实际应用中，可根据算力选择较小的模型进行fine-tune。推理方面，由于没有双塔或额外模块，GIT生成一次回复需要完整地跑Transformer，对于长序列仍较耗时。但没有交叉模块切换开销。值得注意的是，GIT证明了 统一模型减少了重复计算 ：比起每个任务训练不同模型，一个预训练模型fine-tune各任务总计算量更小ar5iv.labs.arxiv.org。同时，它也展示了Transformer在CV任务中的威力，使GPU上的Transformer算力得以充分利用，不像以前CNN+RNN需要异构处理。所以总体看，GIT的 训练成本虽然高，但回报是一个通用模型 。随着算力的提升，这种“大一统预训练”将变得越来越现实。 参考： GIT论文发表于 2023 年CVPRarxiv.org（OpenReview提供了审稿意见）。论文附带的代码已在GitHub开源arxiv.org（microsoft/GenerativeImage2Text），方便社区使用。有关GIT的更深入讲解，可参考微软研究博客和OpenAI笔记等资源对比GIT与同类模型的设计理念。\nFlamingo (DeepMind, 2022) 整体架构设计： Flamingo是DeepMind提出的一种 少样本视觉语言模型 ，它将预训练的视觉编码器和预训练的大型语言模型结合，通过插入跨模态注意力层实现图文融合lilianweng.github.iolilianweng.github.io。具体来说，Flamingo采用了CLIP的ViT作为图像编码器（提取每张图像的一组视觉特征），采用类似GPT-3风格的大型Transformer作为文本生成模型（如Chinchilla 70B）lilianweng.github.io。在两者之间，Flamingo引入一个 Perceiver Resampler模块 ，将任意长度的视觉特征压缩成固定数量的 视觉tokens （如每张图像压缩成N≈64个token）lilianweng.github.io。然后，在语言模型的每层若干位置，插入“门控跨注意力层”，让文本流在生成过程中可以多次访问这些视觉tokenslilianweng.github.io。这些跨注意力层在语言模型层之间交织，使模型在生成每个词时，都能参考图像信息。值得强调的是，Flamingo在训练时 冻结了原有的语言模型和视觉编码器权重 ，只训练中间的新组分（包括Perceiver和跨模态层）lilianweng.github.io。这种设计确保了预训练模型的语言和视觉知识被最大程度保留，同时通过新组件实现模态融合。 模态对齐方式： Flamingo的模态对齐依赖于 预训练模型的知识+少量新的连接参数 。图像编码器CLIP本身已提供高质量的视觉表示，语言模型也有丰富的语言常识。Flamingo只训练连接部分，通过自回归语言模型目标来让视觉信息对接语言输出lilianweng.github.io。训练过程中，模型读取一串交织的图像和文本（例如一个网页内容，其中有文字和插入的图片），试图按照出现顺序预测下一个文本tokenlilianweng.github.io。这隐含地要求模型学会对齐：当遇到需要描述图像的地方，就必须利用视觉tokens提供的信息来正确地产生文字。因此，Flamingo没有明确的对比或匹配损失，而是在 序列建模过程中完成对齐 。尤其得益于CLIP提供的视觉特征空间和语言模型的语义空间都非常成熟，跨注意力层只需学会将二者关联即可。例如，Flamingo使用一个门控机制控制每个跨注意力头对视觉的依赖程度，这保证了模型不会过度依赖或忽略视觉信息，而是渐进式地融合lilianweng.github.io。经过训练，Flamingo实现了图文对齐，以至于在推理时，可以在看到图像后正确地继续对话生成相关文本。这种对齐能力在它的few-shot学习中表现突出：只需给出少量图文示例，模型就能对新图像输出合理描述或回答，表明模态对齐已经内化在模型中了lilianweng.github.io。 输入 token 表达统一： Flamingo通过 对文本序列进行特殊标记和掩码 ，实现了对图像和文本交替输入的统一处理lilianweng.github.io。他们在训练语料的文本中插入特殊标记 \u0026lt;image\u0026gt;代表图像占位符，当遇到该标记时，模型会取下一张图像的视觉tokens作为输入lilianweng.github.io。在Transformer内部，通过设计注意力mask，使得文本token只能看见最近一次出现的图像tokens以前的文本，而不能看见更早图像，以此处理多图场景lilianweng.github.io。同时，由于视觉tokens长度固定，每当有图像时，就把那N个视觉tokens嵌入序列。这样，整个输入序列可能形如：“文本段1 \u0026lt;image\u0026gt; 文本段2 \u0026lt;image\u0026gt; 文本段3\u0026hellip;”。对于Transformer来说，\u0026lt;image\u0026gt;标记只是一种指示，它实际会被替换为N个视觉embedding。最终，模型看到的是一个混合序列，其中既有文本token embedding也有视觉token embedding。Flamingo的跨注意力层保证文本可以从视觉embedding汲取信息lilianweng.github.io。总之，Flamingo实现了 在同一序列中交织图像和文本 ：在位置编码上，文本和图像embedding各据其位，模型通过mask确保因果关系正确lilianweng.github.io。这种方式处理输入使得模型能够自然地接受任意交替的多模态输入，而不需要显式地将图像转成离散标签或one-hot表示。 损失函数与训练策略： Flamingo以自回归下一个词预测作为唯一的训练目标lilianweng.github.io。训练数据是精心构造的 多模态序列 ：DeepMind构建了一个名为“M3W”（MassiveWeb）的大型数据集，从网络抓取包含图像和文字的网页片段共4300万条lilianweng.github.io。这些数据被处理成长度为256的token序列（其中可能包含最多5张图像）lilianweng.github.io。此外，Flamingo还混合了传统的 图文对数据 （如ALIGN的1.8亿图文对）和 视频-文本数据 （如从视频中抽帧及对应描述）进行训练lilianweng.github.iolilianweng.github.io。整个训练在不同数据源上采用 分布式多任务训练 ：每个batch随机抽取来自网页、多图文对、视频的样本分别计算NLL损失，再按设定权重求和优化lilianweng.github.io。这样的策略使模型同时适应多种输入形式。训练中需要注意各数据集的权重分配，作者采用均衡采样避免小数据集被忽略，同时也调整过不同任务损失的比重lilianweng.github.io。最后，通过大量算力（语言模型80B参数，加上新插入层）训练，Flamingo可以在不微调的情况下实现few-shot学习，即给定少数示例即可在16个下游任务中取得接近或超过有监督SOTA的成绩lilianweng.github.io。模型也支持进一步微调，但由于参数量巨大且新的门控层较敏感，微调需要小心调参。不过，一旦训练完成，Flamingo在多模态few-shot方面展示了卓越的能力。 使用的数据集及伪标签： 如上所述，Flamingo的主要预训练数据包括三个部分：网页多模态语料M3Wlilianweng.github.io、 图片-文本对数据 （如ALIGN 1.8B对、LAION等）lilianweng.github.io、 视频-文本数据 （如Instagram/Twitter短视频说明等，文中代号LTIP和VTP）lilianweng.github.io。M3W的构建无需人工标注，纯粹爬取网页，这可以视为引入了 大量弱监督数据 。那些网页上的文本并非专门描述图像，但模型会通过上下文学习其中关联。这有点类似伪标签，因为并非每句话都准确描述对应图像，但模型会自己找关系。另外，Flamingo并未使用生成模型来自行生产描述，它依赖真实世界的数据多样性。值得注意的是，Flamingo训练所需的监督非常少，几乎全是弱标注或无标注数据。few-shot能力使它在下游不需要大规模微调数据。因此，Flamingo充分体现了用海量弱标注数据替代高质量标注的理念。没有迹象表明Flamingo使用了由其他模型生成的伪标签数据；它更像是把互联网当作最大的标注来源，在文本和图像并存的自然场景中学习。 六大难点应对： 模态对齐困难： Flamingo借助冻结的CLIP提供良好的图文先验表示，并通过少量参数训练将其输出嵌入语言模型上下文。这意味着视觉和语言模态的大体对齐已经由CLIP和预训练LM保证，Flamingo只需学习 在具体上下文中关联 。通过跨注意力层，Flamingo学会在需要时提取视觉token信息用于生成下一个词，从而实现对齐。其few-shot性能表明，训练后模型能够快速对齐新任务的图文语义，这得益于大量多样化训练让对齐泛化良好lilianweng.github.io。换言之，Flamingo用数据多样性+强大基础模型平稳地度过了模态对齐难关。 token格式不统一： Flamingo直接在Transformer中处理交替的多模态序列，将图像表示为固定长度token插入序列，这相当于统一了输入格式lilianweng.github.io。虽然图像token不是离散符号，但它们像文本token一样有自己的位置，与前后文本共同组成序列输入Transformer。同时，引入 \u0026lt;image\u0026gt;标记作为占位符，使文本流认识到何处有图像lilianweng.github.io。这种方案无需对图像进行离散化编码，而是用连续向量表示并通过mask和标记融入序列，实现了格式统一。实验证明，这样模型可以灵活处理任意交替顺序的图文输入，这正是统一输入格式带来的好处。 语义粒度不匹配： Flamingo利用CLIP的高层视觉特征（ViT-L/14等）作为输入，这些特征本身具有较丰富的语义信息（CLIP已对齐过标签文本）。再通过Perceiver压缩，Flamingo获得一组紧凑的视觉tokens，每个可能聚合了图像若干部分信息lilianweng.github.io。这会损失一些低层细节，但保留主要语义，匹配语言模型处理的概念粒度。对于非常细的细节，如图像中的文字或小目标，Flamingo如果训练数据涵盖这类任务也能捕捉（但Flamingo主要没专门练OCR类任务，表现可能一般）。总体而言，Flamingo的设计旨在 抓主要语义 ：用几百个视觉token代表整张图lilianweng.github.io。语言模型生成注重全局语义和上下文，微观细节在few-shot场景下可能需要提示引导才能关注。不过，通过web数据训练，Flamingo也学习了不少细节（如定位照片里的物体等）。因此，它在语义粒度上采取以语义为主，细节为辅的策略，符合few-shot应用的需求。 多模态上下文保持： 这是Flamingo最大的强项之一。模型专门设计来处理 任意长度的交互式多模态上下文 。通过mask策略，Flamingo可以应对多张图和多段文本交替：保证每段文本只能看最近的图像，从而按顺序关联图像和文字lilianweng.github.io。这使模型在一个序列中可以包含多轮图文对话——实际上Flamingo天生就是支持图文混合对话的。训练中它看过网页内容的多次图文交替，因此对多模态上下文延续性有经验lilianweng.github.io。few-shot推理时，可以先给几个示例（图+问+答），模型就能在持续的多模态对话下发挥作用lilianweng.github.io。这种能力是一般模型不具备的。因此Flamingo很适合多轮对话、讲故事等需要保持上下文的场景。需要注意长序列涉及的内存和计算成本，但Flamingo通过稀疏注意力等优化应对。总的来说，Flamingo在多模态上下文保持方面达到了当时的新高度，真正实现了在Transformer中融合长上下文的多模态信息。 训练数据稀缺： Flamingo通过大规模弱标注数据和 多数据源混合 ，在没有显式人工标注的情况下取得了卓越性能lilianweng.github.io。它所需的只是网络上已有的大量图文并茂内容，而不需要额外的人工作答或描述数据（除了验证集）。这证明了利用海量的非结构化数据也能训练出强大的多模态模型。few-shot学习的优势在于，模型可以适应新任务而不需要对每个任务都有成千上万标注数据。Flamingo在16个任务上的结果显示，即使这些任务的数据对模型来说是新的，它依然靠few-shot提示达到不错效果lilianweng.github.io。这极大缓解了对监督数据的需求。因此，Flamingo的方案是 用预训练+提示学习替代下游数据 。当然，预训练本身用了43M网页和十亿级对，耗资巨大，但都是低成本获取的数据。可以说，它把收集标注的钱换成了算力钱。一旦模型训练完毕，同样权重可以few-shot解决多个任务，再也不需要逐个任务大量标注了。 计算开销高： Flamingo包含一个80B规模的语言模型（如Chinchilla 70B）和一系列新插入的层，总参数量非常高，训练消耗巨大的TPU/GPU资源。这显然是非常高的计算开销。然而，Flamingo通过 冻结大模型 ，大幅减少了需要更新的参数量lilianweng.github.io。仅训练新加的几千万参数，使得训练收敛更快、更稳定，同时避免灾难性遗忘。此外，相比从零训练80B多模态模型，这种“夹心”微调的成本要低得多。推理阶段，Flamingo的计算与一个同等大小的LM相当，外加一些跨注意力计算，可以在多卡并行生成。在few-shot时，不需要反复fine-tune，从而节省了针对每个任务微调的算力。因此，对于拥有训练超大模型能力的团队来说，Flamingo的 性价比反而不错 ：用额外\u0026lt;1B参数的代价，把一个纯语言模型变成了多模态模型。总之，Flamingo依然属于算力投入极高的模型，但在架构上做了取舍，通过参数冻结和高并行设计，把这笔开销控制在可能范围，并用其泛化能力回收了在多个任务上的成本。 参考： Flamingo的论文（Alayrac et al. 2022）可在arXiv获取lilianweng.github.iolilianweng.github.io。其中详述了模型架构和训练数据构成。DeepMind未公开Flamingo的代码，但有社区复现项目（如lucidrains的PyTorch实现）。Lilian Weng的博客对Flamingo进行了通俗讲解lilianweng.github.io。Flamingo在Few-shot VQA等任务上的表现促使后续多模态聊天模型（如OpenAI的GPT-4V）采用类似思想。\nGPT-4V (OpenAI, 2023) 整体架构设计： GPT-4V是GPT-4模型的视觉增强版本，能够接受图像和文本输入，输出文本en.wikipedia.org。虽然OpenAI并未公开GPT-4V的具体架构和参数en.wikipedia.org，“V”版的实现大致可推测为在GPT-4大型Transformer架构中融合了视觉处理模块。很可能GPT-4V采用了单一Transformer模型来同时处理图像和文本：图像通过一个卷积或ViT编码器提取特征，然后以某种形式馈入Transformer。例如，有推测称GPT-4V使用类似Flamingo的方法——一个 预训练的ViT作为图像编码器 ，将其输出作为额外的输入embedding，通过新添的跨注意力机制注入到原GPT-4的Transformer中lilianweng.github.io。也有可能GPT-4V将图像编码为若干“视觉token”直接拼接到文本token序列中处理（类似BLIP-2/GIT那样）。不管实现细节如何，GPT-4V的架构原则应是 在不大幅改变GPT-4语言能力的前提下，赋予其视觉输入通路 。因此，它很可能保留了GPT-4的大部分层和参数，仅在输入嵌入层或中间插入层增加视觉接口，使模型能够在Self-Attention中同时考虑图像和文本信息。作为一个多模态LLM，GPT-4V仍以Transformer为核心en.wikipedia.org。 模态对齐方式： GPT-4V在开发过程中应该经历了 大量多模态预训练和对齐调优 。预训练阶段，模型接受图文混合数据，学习以生成下一个token为目标（无论下一个是文字还是需要根据图像产生的文字）。这种训练会驱动模型自动建立图像与文本语义的映射关系。由于GPT-4本身非常强大，GPT-4V可能仅需较少的额外数据就能学会模态对齐。然而OpenAI可能使用了多种辅助手段：包括 对比损失 （确保图像相关的文本embedding靠近）或者多模态一致性约束等，但具体未知。可以肯定的是，GPT-4V经过了 强化学习人类反馈（RLHF）的对齐环节en.wikipedia.org：人工反馈不仅针对文本回答质量，也包括对视觉理解正确性的评价。这种人工调教确保模型在视觉问答中对齐人类期望。例如，人类监督会奖赏模型正确描述图像、严惩胡编乱造，从而促使模型更好地学习视觉-语言对齐关系。总的说来，GPT-4V的模态对齐来自两部分 ：一是模型大规模多模态训练的自我监督对齐（让模型预测正确的多模态输出而被迫对齐），二是 对抗性和人类反馈微调 （纠正不准确的对齐，如图像内容误解）以达成人类满意的对齐度。最终结果是GPT-4V在各种视觉描述、问答任务上表现出强大的理解力和对齐度，甚至可以准确解释复杂图片、阅读图中文字并将之融入答案——这说明其视觉语义已与语言很好地结合。 输入 token 表达统一： 从用户接口看，GPT-4V接受的输入是图像（像素形式）和文本，自然语言以token形式进入，图像则以文件上传形式进入API。但在模型内部，必须将图像转化为与文本token可交互的表示。根据业界经验，GPT-4V可能采用两种方式之一：其一， 离线视觉编码+前缀嵌入 。即通过一个CNN/ViT将图像转成一串embedding，然后在Transformer输入端用特殊标记占位，将这些embedding作为“视觉前缀”插入。这类似BLIP-2和GIT的策略，让视觉embedding在Transformer序列中，与后续文本共同处理。其二， 中途插入跨模态层 ，即模型运行过程中，当需要处理图像时，调用一个微型视觉Transformer将图像转成键值供专门的跨注意力层使用（类似Flamingo做法）。无论哪种，最终效果是 模型看到了一系列向量表示，部分来自图像，部分来自文本 ，并通过统一的注意力机制处理它们。因此，GPT-4V实现了 视觉信息向等价文本向量的转换 ：这些向量可能没有离散token对应，但Transformer无差别对待它们，把它们当作上下文的一部分。OpenAI也定义了GPT-4V的token计费方式：图像按一定像素大小折算成若干token成本platform.openai.com, 这暗示他们内部将图像信息映射为了固定数量的embedding，相当于一些token。这与输入统一表示的思路一致。此外，GPT-4V支持在对话中多次输入图像，模型通过聊天记忆可以连续参照多幅图像。这种灵活性也表明输入的图像已经嵌入Transformer上下文，模型可以在内部“记住”它，就像记住前文一样。因此可以说，GPT-4V在实现上做到了图像和文本输入的格式统一，至少从Transformer的视角来看是一致的序列信息流。 损失函数与训练策略： GPT-4V的训练包括两个阶段： 预训练（Self-Supervised）和对齐微调（Supervised + RLHF） en.wikipedia.org。预训练损失是标准的 因果语言建模损失 ，扩展到多模态场景，即给定之前的文本token和图像embedding，预测下一个文本tokenen.wikipedia.org。这一步可能使用了大量图文对数据和合成任务数据，让模型具备基础视觉理解和描述能力。接下来，OpenAI对GPT-4V进行了 监督微调 ，包括让模型跟随指令、可靠回答问题、避免不当输出等。这一步使用有人类标注答案的图像问答数据和对话数据，损失为交叉熵对标参考答案。最后还有 RLHF阶段 ，通过人类反馈训练一个奖励模型，对模型回答质量评分，再用策略梯度或近端策略优化调整模型参数，使之产生更符合人类期望的回答en.wikipedia.org。在RLHF中，人类会比较两版对同一图像问题的回答优劣，以训练奖励模型。这确保GPT-4V不仅正确，还要解释清楚、详尽并遵守安全守则。训练策略方面，GPT-4V很可能采用了 混合训练 ：例如让模型在大约80%时间学习纯文本任务（以不损害其语言能力），20%时间学习带图像的任务，以逐渐融合视觉能力而不遗忘语言能力。这符合OpenAI对GPT-4统一多模态模型的描述，称其在巨量算力下进行预测性能的平稳扩展arxiv.org。因此，GPT-4V训练过程相当复杂，但核心损失仍是让模型预测正确的输出序列（文本），只是过程中施加了各种人类知识和偏好约束。 使用的数据集及伪标签： OpenAI未公开GPT-4V使用的数据细节en.wikipedia.org。推测其预训练数据包含 互联网爬取的大规模图文对 （如可能使用LAION、ALIGN数据，或者自建的10亿级别数据集），涵盖多样领域。还可能有 OCR场景数据 （扫描文档及文本）、 图表数据 、网页截图和说明等，因为GPT-4V表现出识别文档、读表格、看图编程等广泛能力。监督微调阶段，他们可能编纂了一个多模态指令数据集，类似InstructGPT，但带图像：比如让标注员提供图像并提问，写出高质量参考答案。这部分数据可能较小（数万到数十万对），但涵盖不同任务（描述、定位、分类、推理等）。此外，社区猜测OpenAI可能利用GPT-4自身生成了一部分训练数据（即“判师”策略），但官方未证实。相较于开源做法（如LLaVA用GPT-4生成对COCO的问答作为训练集），OpenAI有资源直接人工标注，所以GPT-4V的关键数据更可能是人类精标而非伪标签。唯一确定的是，GPT-4V 融合了多源数据 ：文本数据（与GPT-4共享）、图像+文本数据，以及人类反馈数据en.wikipedia.org。这种多阶段、多样本训练使模型具有极其广泛的视觉语言知识。基于效果推断，GPT-4V肯定见过各种真实世界图像场景，包括照片、插画、截图、漫画等，也了解了不少常见视觉任务的问答格式。这正是其在未知图片上一样游刃有余的原因。 六大难点应对： 模态对齐困难： GPT-4V可被视为目前模态对齐最成功的例子之一。OpenAI通过 统一模型训练+精细对齐调优 ，使得GPT-4V在视觉和语言之间建立了深度联系。模型能将图像中的元素转换成文字描述或用于推理，说明跨模态概念高度统一。例如，它可以看图进行幽默理解、数学分析，这意味着不仅低层语义对齐，高层推理也对齐了。相比CLIP等需下游配对的模型，GPT-4V内部产生了 端到端的对齐 ：一幅图像输入，其内部生成的表征能直接触发与之对应的知识和词汇。RLHF过程中，人类引导模型关注正确区域、忠实描述，进一步强化了 精准对齐 （比如不编造不存在的物体）。因此，对以前悬而未决的模态对齐难题，GPT-4V以超级规模训练+人类校正的方法给出了答案：几乎可以对任意复杂图文实现正确对齐。 token格式不统一： GPT-4V在接口上依然区分图像上传和文本输入，但在模型内部已经实现了格式统一。如上推测，图像被编码成embedding插入Transformer，相当于模型看到的是统一的向量序列，其中没有本质区别区分来源。OpenAI甚至提供了一个token折算方法来计价图像，这暗示他们定义了一个统一本质的token空间包含图像platform.openai.com。GPT-4V也许没有明确的视觉词表，但通过扩展embedding层，模型接受了一批额外的向量（视觉patch的embedding或者Resampler输出）作为“视觉token”。Transformer处理自注意力时，对这些向量和普通文字embedding执行相同的矩阵计算。因此可以说，GPT-4V在实现上 消除了模态输入格式差异 ，达到了真正的多模态Transformer形态。这也是为什么用户能对它自由提问“图中有什么字”或“这个人是谁”，模型像读文字一样“读”图。这种统一在OpenAI的技术报告中虽未明说，但从其行为特征和架构趋势能推断出来。 语义粒度不匹配： GPT-4V展现出处理各种粒度语义的能力，从辨认具体细节（如图中小字、微小物品）到理解抽象场景（如人物关系、场景氛围）。这表明模型采用了高分辨率的视觉表征和 强大的分层理解 。一种可能方式是多级特征：基础ViT提供细粒度patch特征，然后Transformer多层逐步汇总，像人类视觉系统一样先看细节再理解整体。此外，OpenAI可能特意在训练集中加入了一些需要细粒度识别的任务（OCR、细分类），迫使模型关注局部细节。同时，大语言模型部分拥有强大的上下文推理能力，能从细节推导整体意义。这两方面结合，使GPT-4V能较好地弥合视觉像素级信息与语言概念级信息之间的鸿沟。例如，对一张复杂的漫画，模型既能识别面部表情这样的细节，又能归纳出搞笑之处这样的高层语义。可以认为，GPT-4V通过多尺度注意力解决了语义粒度不匹配：低层注意力抓取细节，高层Self-Attention整合语义，并在输出时选择恰当的语言粒度表述。 多模态上下文保持： GPT-4V本质是ChatGPT的扩展版，因而天然具备对话上下文记忆能力。用户可以在一次对话中连续上传多张相关图像并配以提问，模型能够参考对话历史和所有已提供的图像信息来回答。比如，用户先上传一家谱照片问“这是谁？”，再上传另一张照片问“他和前面那人是什么关系？”，GPT-4V可以基于前文记忆，将两图人物联系起来回答。这说明模型内部对多轮图像和文本都建立了表示，并通过对话状态维持了跨轮次的多模态上下文。OpenAI很可能在微调阶段加入了这类多轮、多图对话的数据，使模型学会使用 \u0026lt;image_n\u0026gt;引用之前的图像。在推理实现上，ChatGPT系统会给每张图一个编号，将其embedding保存在对话状态，后续提问如果引用，模型就会重新利用。这种机制虽未明示，但从体验上看GPT-4V确实支持相当长的多模态对话。因此，它在多模态上下文保持上达到了目前最强水平：既能处理长文本对话，又能记忆多张图像的内容并综合推理。这一能力是之前模型（如Flamingo）few-shot模拟的更高级形式，因为GPT-4V经过明确的对话格式训练和强化，对话管理更加可靠。 训练数据稀缺： 对于普通研究者来说，高质量大规模多模态数据稀缺是难点，但OpenAI通过自身积累和合作，可能获取了十分丰富的数据。GPT-4V可以被视为以数据和算力硬碰硬解决问题的典型。它用规模（模型、数据）换性能，不太依赖小技巧。值得注意的是，虽然OpenAI未公布数据，但推测很大一部分来自现有开放数据（LAION、COCO、Visual Genome等）以及定制采集的数据（比如购买版权图片、内部生成的数据等）。此外，人类标注在对齐阶段起了决定性作用，这是另一种形式的数据： 专家知识数据 。OpenAI投入了大量人力去微调模型的行为，使得最终模型的能力远超仅靠原始数据训练的版本。这相当于通过人类反馈来弥补数据集不足之处——对于一些模型自己难以领会的任务，人类示范和偏好指导提供了额外信息。这种做法开创了用少量高质量人工数据引导海量机器学习的范式。简而言之，GPT-4V应对数据稀缺的方案在于： 一手抓“大”（扩展预训练数据广度），一手抓“精”（收集人类高质量指令/反馈数据） 。两者结合，使模型既见多识广，又合乎人意。 计算开销高： GPT-4V毫无疑问是在极其庞大的算力支持下训练的。传闻GPT-4基础模型参数在数千亿以上，训练消耗数千万美元级别GPU成本。加入视觉模态后，训练复杂度进一步提高。不过OpenAI通过一些工程手段控制了成本：据报道，他们使用了训练性能预测方法，在较小模型上估计大模型表现，从而少走弯路arxiv.org。另外采用混合精度、模型并行、流水线并行等技术提高效率。模型结构上，使用统一Transformer而非多分支，可以充分利用成熟的Transformer优化器和加速器。这些都帮助缓解了计算压力。在推理阶段，GPT-4V同样需要强大算力支持，但OpenAI通过托管API方式，用优化过的推理服务器提供服务，单次调用成本对于终端用户来说隐藏在付费中。可以说，GPT-4V目前的计算开销不是一般机构能承担的，但它也展示了高投入带来高性能的路线。随着硬件进步和可能的压缩蒸馏技术，未来GPT-4V的成本有望下降。就当前而言，OpenAI通过自身资源攻克了这一难题，对外提供一个无需本地计算就能调用的强大多模态模型，这在客观上绕过了许多用户对算力的需求。 参考： GPT-4 的技术报告en.wikipedia.orgen.wikipedia.org提到其多模态能力和训练方法，但未披露细节。维基百科也指出OpenAI未公布GPT-4的架构和数据en.wikipedia.org。尽管如此，我们可以参考类似的研究（如Google PaLM-E、DeepMind Flamingolilianweng.github.io）来推测GPT-4V的设计思路。OpenAI的GPT-4发布博客openai.com和官方FAQ也提供了一些线索（如图像计费折算）。目前没有公开的GPT-4V代码或模型，但已有一些开源项目（如MiniGPT-4、LLaVA等）尝试复现其部分功能，可供了解实现原理。总的来说，GPT-4V代表了当前多模态模型技术的前沿，将视觉和语言能力融合达到了前所未有的高度。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-01/","summary":"\u003ch1 id=\"主流视觉-文本多模态模型技术分析\"\u003e主流视觉-文本多模态模型技术分析\u003c/h1\u003e\n\u003cp\u003e近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。\u003c/p\u003e\n\u003ch2 id=\"clip-openai-2021\"\u003eCLIP (OpenAI, 2021)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e整体架构设计：\u003c/strong\u003e CLIP 采用 \u003cstrong\u003e双编码器架构\u003c/strong\u003e ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到\u003cstrong\u003e相同维度\u003c/strong\u003e的向量空间\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到\u003cstrong\u003e共享的多模态嵌入空间\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模态对齐方式：\u003c/strong\u003e CLIP通过\u003cstrong\u003e对比学习\u003c/strong\u003e实现视觉-语言对齐。训练时，模型给定一批图文对，学习\u003cstrong\u003e预测哪张图像与哪段文本匹配\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。具体而言，CLIP使用 \u003cstrong\u003e对称的跨模态对比损失\u003c/strong\u003e （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。这种训练使图像和文本编码器产生的特征在共享空间中\u003cstrong\u003e成对靠近\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e输入 token 表达统一：\u003c/strong\u003e CLIP并未显式统一图像和文本的输入表示格式。 \u003cstrong\u003e图像和文本各有独立的token化和编码流程\u003c/strong\u003e ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过\u003cstrong\u003e独立编码+对齐空间\u003c/strong\u003e的方式，规避了直接将图像作为序列token处理的不统一问题。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e损失函数与训练策略：\u003c/strong\u003e 损失采用 \u003cstrong\u003e对比学习的InfoNCE损失\u003c/strong\u003e （实现为带温度系数的归一化softmax交叉熵）\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了\u003cstrong\u003e大批量\u003c/strong\u003e训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,2023%2C%20in%20their\"\u003elightly.ai\u003c/a\u003e。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=insensitive%20to%20the%20capacity%20of,the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。CLIP从\u003cstrong\u003e随机初始化\u003c/strong\u003e开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据集及伪标签：\u003c/strong\u003e CLIP在一个超大规模的图文配对数据集上预训练，包含约\u003cstrong\u003e4亿对图像-文本\u003c/strong\u003e\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖\u003cstrong\u003e自然语言的弱监督\u003c/strong\u003e\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e六大难点应对：\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003e模态对齐困难：\u003c/em\u003e  CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003etoken格式不统一：\u003c/em\u003e  采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e语义粒度不匹配：\u003c/em\u003e CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如\u003cstrong\u003eFine-Grained CLIP\u003c/strong\u003e等正是受限于CLIP在局部语义对齐上的不足\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=SigLIP%3A%20Optimising%20the%20loss%20function,for%20better%20scaling\"\u003elightly.ai\u003c/a\u003e。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e多模态上下文保持：\u003c/em\u003e CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 \u003cstrong\u003e多模态上下文\u003c/strong\u003e （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e训练数据稀缺：\u003c/em\u003e CLIP通过\u003cstrong\u003e大规模弱标注数据\u003c/strong\u003e从根本上缓解了数据稀缺的问题\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e计算开销高：\u003c/em\u003e 训练CLIP确实需要巨大的算力和显存，但相对来说，其\u003cstrong\u003e双塔架构\u003c/strong\u003e使训练可并行展开，推理时也可分别预编码图文后做相似度计算，\u003cstrong\u003e具有一定的效率优势\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,Training%E2%80%9D%2C%20propose%20to\"\u003elightly.ai\u003c/a\u003e。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=implementation%20is%20numerically%20unstable%2C%20and,additional%20bias%20terms%2C%20and%20calculations\"\u003elightly.ai\u003c/a\u003e。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e参考：\u003c/strong\u003e CLIP 的论文\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e详细描述了其对比预训练方法，OpenAI 的博客也提供了概述\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库\u003ca href=\"https://huggingface.co/docs/transformers/en/model_doc/clip#:~:text=CLIP%20uses%20an%20image%20encoder,the%20same%20number%20of\"\u003ehuggingface.co\u003c/a\u003e。\u003c/p\u003e\n\u003ch2 id=\"align-google-2021\"\u003eALIGN (Google, 2021)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e整体架构设计：\u003c/strong\u003e ALIGN（ \u003cstrong\u003eA Large-scale ImaGe and Noisy-Text embedding\u003c/strong\u003e ）延续了与CLIP相同的\u003cstrong\u003e双编码器对比学习架构\u003c/strong\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：\u003cstrong\u003eEfficientNet-L2卷积网络\u003c/strong\u003e作为图像编码器，\u003cstrong\u003eBERT-Large\u003c/strong\u003e作为文本编码器，并均从随机初始化开始训练\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模态对齐方式：\u003c/strong\u003e ALIGN采用\u003cstrong\u003e对比损失（normalized softmax）\u003cstrong\u003e来训练，使匹配的图文对嵌入向量接近，不匹配的远离\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以\u003c/strong\u003e批为单位的跨模态对比\u003c/strong\u003e训练，使模型学到强大的图文对齐表示。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e输入 token 表达统一：\u003c/strong\u003e ALIGN同样没有将图像直接离散为token序列，而是通过\u003cstrong\u003e双通道\u003c/strong\u003e处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出\u003cstrong\u003eembedding空间\u003c/strong\u003e实现统一表示\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e损失函数与训练策略：\u003c/strong\u003e 使用 \u003cstrong\u003e对比学习损失\u003c/strong\u003e （InfoNCE变体），在\u003cstrong\u003e大批量\u003c/strong\u003e上训练模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 \u003cstrong\u003e最小程度的过滤\u003c/strong\u003e ，通过\u003cstrong\u003e数据规模\u003c/strong\u003e来弥补噪声\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据集及伪标签：\u003c/strong\u003e ALIGN的亮点在于使用了\u003cstrong\u003e超过10亿对图像-Alt文本\u003c/strong\u003e的超大规模数据集\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 \u003cstrong\u003e无需人工标注\u003c/strong\u003e 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN \u003cstrong\u003e放宽过滤标准\u003c/strong\u003e ，只做了基于频率的简单过滤，最终得到约\u003cstrong\u003e18亿对\u003c/strong\u003e图文数据\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20this%20work%2C%20we%20follow,text%20pairs\"\u003eresearch.google\u003c/a\u003e。这些文本描述可能包含噪声甚至与图像无关，但研究表明 \u003cstrong\u003e规模弥补质量\u003c/strong\u003e ：如此海量的数据使模型学到泛化的视觉语言表示\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e六大难点应对：\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003e模态对齐困难：\u003c/em\u003e ALIGN证明了\u003cstrong\u003e数据规模\u003c/strong\u003e在对齐中的重要作用。通过\u003cstrong\u003e十倍于CLIP的数据规模\u003c/strong\u003e和强大的对比损失，模型学到了稳健的跨模态对齐能力\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,L2\"\u003eresearch.google\u003c/a\u003e。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003etoken格式不统一：\u003c/em\u003e 与CLIP类似，ALIGN通过\u003cstrong\u003e双编码器架构\u003c/strong\u003e回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e语义粒度不匹配：\u003c/em\u003e ALIGN的训练目标依旧作用在 \u003cstrong\u003e全局图像-句子层面\u003c/strong\u003e ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对\u003cstrong\u003e细粒度语义\u003c/strong\u003e的不匹配没有特殊解决方案。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e多模态上下文保持：\u003c/em\u003e ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e，未涉及多模态对话等情境。因此，ALIGN在\u003cstrong\u003e多轮交互\u003c/strong\u003e或\u003cstrong\u003e长上下文\u003c/strong\u003e问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e训练数据稀缺：\u003c/em\u003e ALIGN的策略是\u003cstrong\u003e极端扩增数据规模\u003c/strong\u003e以消除数据稀缺瓶颈\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 \u003cstrong\u003e弱标注的大数据\u003c/strong\u003e 。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e计算开销高：\u003c/em\u003e 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。尽管计算开销高昂，ALIGN通过 \u003cstrong\u003e冻结架构复杂性\u003c/strong\u003e （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，\u003cstrong\u003e优先扩大数据规模\u003c/strong\u003e比增加模型复杂度更有效\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e参考：\u003c/strong\u003e ALIGN 的研究细节发表于 ICML 2021\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20,We\"\u003eresearch.google\u003c/a\u003e。Google Research 官方博客提供了对ALIGN的通俗描述\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。\u003c/p\u003e","title":"Bug Journal 2025-06-01"},{"content":"主要动机 目前有三大挑战：数据稀缺、泛化能力不足，以及在复杂物理环境中效果不好 作者希望通过引入大规模语言及VLA模型在自然语言处理和计算机视觉中的成功经验解决这个问题.\n主要论点 在预训练的视-语言模型（VLM，本文采用 PaliGemma）基础上，增加一个“动作专家”（action expert），通过条件流匹配（diffusion policy）生成高频、连续的动作序列（每秒可达50 Hz）\n模型流程图 收集数据 -\u0026gt; 训练$\\phi_0$ -\u0026gt; Zero-Shot/微调/Fine-tune\n数据来自7种不同的机器人，68个不同的任务，总计 10k小时。\n每一个单独机械臂的自由度在 6-7 (有些机器人有多机械臂)\n输入有 3 块，分别是：Image, Language, and State\nImage 用 400M Pretrained VIT SigLip 得到 embedding, 一共会有up to 3 个 Image， 所以最会有 3 个 embedding (没有就填 0)\nLanguage 用 2.6B Pretrained LLM Gemma, 得到 embedding\n最后是关节信息，最多会有 18 个(没有就填 0)。\n之后运用 Diffusion Policy 来生成每一步的动作\n生成的是 $p(A_t|O_t)$, 即，在给定条件(环境下)，每一个动作的概率。\n这时给一个 chunk 内的真实动作加噪音，然后让模型学习如何去噪。\n方式如下：\n随机采一个噪声级别 $\\tau$，采一个高斯噪声 $\\varepsilon$； 构造带噪动作块 $A_t^\\tau = \\tau A_t + (1 - \\tau)\\varepsilon$； 用观测 $o_t$ 与 $A_t^\\tau$ 输入网络，预测去噪速度场 $v_\\theta(A_t^\\tau, o_t) \u0026lt;-\u0026gt; \\varepsilon - A_t$； 以 $\\left| v_\\theta - (\\varepsilon - A_t) \\right|^2$ 作为监督信号； 推理的时候就会把这个过程分成 10-20步，每一步去噪一点点，最终得到一个动作 注：\u0026ldquo;这个过程\u0026quot;指的是随机生成一个噪声，然后从这个噪声去噪的过程\nCode Inference def create_trained_policy( train_config: _config.TrainConfig, # 训练配置，包含模型定义、数据配置等 checkpoint_dir: pathlib.Path | str, # 检查点目录：存放已训练模型参数和归一化统计信息的路径 *, repack_transforms: transforms.Group | None = None, # 可选的“重打包”预处理组——在所有其他 transform 之前应用 sample_kwargs: dict[str, Any] | None = None, # 传递给 policy.sample_actions 的参数字典 default_prompt: str | None = None, # 默认提示词，如果输入数据中没有 prompt，则注入该默认值 norm_stats: dict[str, transforms.NormStats] | None = None, # 归一化统计信息（均值、方差或分位数），若未提供则从 checkpoint 中加载 ) -\u0026gt; _policy.Policy: \u0026quot;\u0026rdquo;\u0026quot; 从训练好的检查点创建并返回一个可交互的 Policy 对象。\nArgs: train_config: 用于创建模型和数据流水线的训练配置。 checkpoint_dir: 存储模型参数和归一化信息的目录路径。 repack_transforms: （可选）在所有其他数据变换之前应用的变换组。 sample_kwargs: （可选）调用 sample_actions 方法时使用的关键字参数。 default_prompt: （可选）注入到输入数据中的默认提示词。 norm_stats: （可选）归一化统计信息；如果未提供，会尝试从 checkpoint 加载。 \u0026quot;\u0026quot;\u0026quot; # 如果外部没有传入 repack_transforms，则使用一个空的 transforms.Group repack_transforms = repack_transforms or transforms.Group() # 下载 checkpoint_dir = download.maybe_download(str(checkpoint_dir)) # 怀疑是这个地方卡住了，正在测试 logging.info(\u0026quot;Loading model...\u0026quot;) # 从 checkpoint 的 params 文件中恢复模型参数，并用 jnp.bfloat16 精度加载到模型中 model = train_config.model.load( _model.restore_params(checkpoint_dir / \u0026quot;params\u0026quot;, dtype=jnp.bfloat16) ) # 使用训练配置中的 data 部分构建数据流水线（包括 asset 路径、transform 定义等） data_config = train_config.data.create(train_config.assets_dirs, train_config.model) # 如果调用方未提供归一化统计信息，则从 checkpoint 中加载 if norm_stats is None: # 确保 data_config 中配置了 asset_id，否则无法定位归一化文件 if data_config.asset_id is None: raise ValueError(\u0026quot;Asset id is required to load norm stats.\u0026quot;) # 从 checkpoint_dir/assets/\u0026lt;asset_id\u0026gt; 文件夹加载归一化统计信息 norm_stats = _checkpoints.load_norm_stats(checkpoint_dir / \u0026quot;assets\u0026quot;, data_config.asset_id) # 构造并返回 Policy 对象 return _policy.Policy( model, # 定义输入端的 transform 流水线 transforms=[ *repack_transforms.inputs, # 首先应用重打包变换 transforms.InjectDefaultPrompt(default_prompt), # 注入默认 prompt（如有） *data_config.data_transforms.inputs, # 然后是数据阶段的预处理（如裁剪、编码） transforms.Normalize(norm_stats, # 使用加载的统计信息做归一化 use_quantiles=data_config.use_quantile_norm), *data_config.model_transforms.inputs, # 最后是模型期望的输入 transform（如维度调整、拼接） ], # 定义输出端的 transform 流水线，用于将模型输出反向映射回原始数据格式 output_transforms=[ *data_config.model_transforms.outputs, # 模型输出后先做反向 transform（如反维度调整） transforms.Unnormalize(norm_stats, # 反归一化 use_quantiles=data_config.use_quantile_norm), *data_config.data_transforms.outputs, # 数据阶段的后处理（如解码、去补齐） *repack_transforms.outputs, # 最后应用重打包的输出变换 ], sample_kwargs=sample_kwargs, # 传给 sample_actions 的运行时参数 metadata=train_config.policy_metadata, # 附带的元数据 ) 创新点 VLM+流匹配的融合：首次将预训练视觉-语言骨干与流匹配（flow matching）动作生成相结合，实现高频连续动作预测。 跨平台预训练：采用跨样本（cross-embodiment）训练，将来自单臂、双臂及移动操纵器的多样化数据统一到同一模型中。 两阶段训练配方：借鉴大规模语言模型的“预训练–后训练”流程，预训练学会恢复与泛化行为，后训练习得高效、精炼策略。 动作专家模块：在 Transformer 上增设专门处理机器人状态与动作的子网络，相当于一种混合专家（mixture-of-experts）设计，提高对连续动作的建模能力 解决的难点 数据稀缺：以往专用策略仅依赖于任务特定的少量数据，难以涵盖错误恢复或未见场景；π0 通过多任务多平台数据缓解了此问题。 泛化与鲁棒性：先前的自回归离散动作方法（如 OpenVLA）不支持高频动作分块，难以处理精细操控；π0 的流匹配架构可生成连续、高精度动作，提升了对复杂任务的适应能力。 多阶段任务：传统方法往往针对单一任务设计，难以扩展到折叠衣物、装箱等涉及多步骤和语义推理的场景；π0 可直接通过语言提示或与高层策略结合，完成复杂多阶段流程 还需要解决的难点 预训练数据组成与加权：如何选择和加权最有助于下游任务的数据仍然未知。 任务可靠性：部分下游任务（尤其与预训练差异大者）仍存在不稳定性，需要更多高质量后训练数据。 跨领域通用性：尚不清楚该框架能否推广到更异质的机器人领域（如自主驾驶、步态运动等）。 资源需求：大规模预训练对算力和示教数据的需求极高，实际部署成本仍是瓶颈 Take away tmux CUDA_VISIBLE_DEVICES=num pdb\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-31/","summary":"\u003ch4 id=\"主要动机\"\u003e主要动机\u003c/h4\u003e\n\u003cp\u003e目前有三大挑战：数据稀缺、泛化能力不足，以及在复杂物理环境中效果不好\n作者希望通过引入大规模语言及VLA模型在自然语言处理和计算机视觉中的成功经验解决这个问题.\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e在预训练的视-语言模型（VLM，本文采用 PaliGemma）基础上，增加一个“动作专家”（action expert），通过条件流匹配（diffusion policy）生成高频、连续的动作序列（每秒可达50 Hz）\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1748596153733\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-31/1748596153733.png\"\u003e\u003c/p\u003e\n\u003cp\u003e收集数据 -\u0026gt; 训练$\\phi_0$ -\u0026gt; Zero-Shot/微调/Fine-tune\u003c/p\u003e\n\u003cp\u003e数据来自7种不同的机器人，68个不同的任务，总计 10k小时。\u003c/p\u003e\n\u003cp\u003e每一个单独机械臂的自由度在 6-7 (有些机器人有多机械臂)\u003c/p\u003e\n\u003cp\u003e输入有 3 块，分别是：Image, Language, and State\u003c/p\u003e\n\u003cp\u003eImage 用 400M Pretrained VIT SigLip 得到 embedding, 一共会有up to 3 个 Image， 所以最会有 3 个 embedding (没有就填 0)\u003c/p\u003e\n\u003cp\u003eLanguage 用 2.6B Pretrained LLM Gemma, 得到 embedding\u003c/p\u003e\n\u003cp\u003e最后是关节信息，最多会有 18 个(没有就填 0)。\u003c/p\u003e\n\u003cp\u003e之后运用 Diffusion Policy 来生成每一步的动作\u003c/p\u003e\n\u003cp\u003e生成的是 $p(A_t|O_t)$, 即，在给定条件(环境下)，每一个动作的概率。\u003c/p\u003e\n\u003cp\u003e这时给一个 chunk 内的真实动作加噪音，然后让模型学习如何去噪。\u003c/p\u003e\n\u003cp\u003e方式如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e随机采一个噪声级别 $\\tau$，采一个高斯噪声 $\\varepsilon$；\u003c/li\u003e\n\u003cli\u003e构造带噪动作块 $A_t^\\tau = \\tau A_t + (1 - \\tau)\\varepsilon$；\u003c/li\u003e\n\u003cli\u003e用观测 $o_t$ 与 $A_t^\\tau$ 输入网络，预测去噪速度场 $v_\\theta(A_t^\\tau, o_t) \u0026lt;-\u0026gt; \\varepsilon - A_t$；\u003c/li\u003e\n\u003cli\u003e以 $\\left| v_\\theta - (\\varepsilon - A_t) \\right|^2$ 作为监督信号；\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e推理的时候就会把这个过程分成 10-20步，每一步去噪一点点，最终得到一个动作\n\u003cem\u003e注：\u0026ldquo;这个过程\u0026quot;指的是随机生成一个噪声，然后从这个噪声去噪的过程\u003c/em\u003e\u003c/p\u003e","title":"Bug Journal 2025-05-31"},{"content":"Docker 的安装和调试 Docker相当于一台虚拟机。安装之后就可以在这台虚拟机上跑代码了。\n安装方式：\n首先上 Dockerhub 挑选一个心仪的 docker, 下面以 nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 为例：\n然后运行以下代码：\ndocker run -it -rm\\ --name \u0026lt;your-instance-name\u0026gt; \\ --network host \\ nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 \\ /bin/bash 注：这里的 -it 指的是打开一个可交互界面，-rm 指的是用后删除\n这时候就会自动下载 docker 并打开一个 bash 来用。\n现在你会发现这个虚拟机里面什么都没有，所以就需要 apt-get install\n另外，如果你的宿主机器的根目录比较小，想要挂载一个硬盘的话，就在 docker run 中间加上：\n-v /path/to/large/storage:/somepath \\ 这样就可以在 somepath 下挂载这个硬盘了。\n注：不能挂载在根目录下，必须挂载在一个文件夹下\n这里在测试的时候建议加上 -rm,这样不会产生很多个休眠中的 docker 但是在要频繁使用的时候不建议使用 -rm, 而是就让 docker休眠就好。\n一些比较常用的 docker 指令： # 启动 docker docker start \u0026lt;容器ID或名字\u0026gt; # 关闭 docker docker stop \u0026lt;容器ID或名字\u0026gt; # 重启 docker docker restart \u0026lt;容器ID或名字\u0026gt; # 删除容器 docker rm \u0026lt;容器ID或名字\u0026gt; # 进入容器 docker exec -it \u0026lt;容器ID或名字\u0026gt; bash # 查看正在运行的容器 docker ps # 查看所有容器（包括停止的） docker ps -a # 列出本地镜像 docker images # 删除镜像 docker rmi \u0026lt;镜像ID或名字\u0026gt; # 挂载目录 docker run -v /host/path:/container/path # 增加环境变量 docker run -e HTTP_PROXY=http://localhost:10086 代理的使用 这里使用的是 xray。\nxray 是这样运行的：\n./xray run -c config.json 运行之后你就可以看到哪个端口放开了，就可以在哪个端口上使用代理,比如 port: 10086。\n这时候如果你想使用代理就需要：\nexport HTTP_PROXY=http://localhost:10086 export HTTP_PROXY=http://localhost:10086 这样你的下载就会走代理辣。\n非常重要 (大坑) apt-get install 对代理的要求较高，没那么稳定的代理会很稳定的挂，报 Error 503 Service Unavailable. 这时候就直接换清华源就行了，别使用代理了，等之后下别的再用。\n代理的使用 之后就是装 git, 装conda, 装 python \u0026hellip;\n装 git:\napt update \u0026amp;\u0026amp; apt-get install git 装 conda:\napt update \u0026amp;\u0026amp; apt install -y curl cd /Path curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p /Path/miniconda3 /Path/miniconda3/bin/conda init source ~/.bashrc 装 python:\nconda --version conda create -n \u0026lt;yourname\u0026gt; python=\u0026lt;yourversion\u0026gt; -y conda activate \u0026lt;yourname\u0026gt; 注意：不要在~/.bashrc 中添加 source ~/.bashrc，而是就运行一遍 source ~/.bashrc 就可以了\nPDF压缩 gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/default -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf input.pdf ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-30/","summary":"Docker 的安装和调试","title":"Bug Journal 2025-05-30"},{"content":"DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 动机 现有机器人在静态任务（比如说开门，拿方块，玩魔方）上已经做得很好了，但是在动态任务(比如接住一支笔)上做得不行。 所以现在希望能够解决：\u0026ldquo;灵巧手动态抛接物体\u0026rdquo; 这个问题。\n主要论点 作者提出一个新的强化学习框架 LTC (Learning-based Throwing-Catching) 来操控灵巧手完成抛接动作\n利用压缩后的点云特征感知物体； 基于PPO算法的Actor-Critic策略学习； 引入Lyapunov稳定性准则 和 Intrinsic Advantage 提高捕捉稳定性与学习效率； 模型流程图 PointCloud V2 获取物体点云 -\u0026gt; PCA 压缩点云信息 后面K-Means 优化的线索\n输入观察得到的信息concat 上点云输入\n然后使用 PPO 算法优化\n简单来说，Actor 负责做一个动作，Critic 负责判断这个动作好不好，PPO则会让策略和策略之间的连贯性更强。\n另外，为了增加系统的稳定性，作者引入了一个Lyapunov 函数（经典控制理论中用来衡量系统稳定性）来让这个系统更加稳定。\n这时候有 3 个值：第一个是 原本 PPO 算法中算出来的值，第二个是 Critic 对于动作价值的预测值，第三个是Lyapunov 函数的值\n最后通过加权平均，得到最后的A_all 用于优化 actor.\n创新点 首个实现任意物体灵巧抛接的学习方法，尤其在手部侧握极不稳定条件下成功； 引入Lyapunov稳定性引导的优势估计，显著提升捕捉的稳定性； 点云 + PCA 压缩特征用于泛化物体类型，训练期间加入物理属性随机扰动； 提出混合优势估计，结合 PPO 优势、Lyapunov 稳定优势、Intrinsic 优势； 通过仿真-现实迁移设计，包括扰动鲁棒性验证和实际机器人平台部署规划。 解决的难点 引入系统稳定性约束，避免学习出高回报但不稳定的动作策略； 物体点云感知 + 训练过程中的域随机化增强泛化能力； 之前方法的缺点 大多为静态任务（如抓握、拼积木、开门）； RL方法在动态任务中效果极差（之前成功率几乎为0）； 缺乏对动态稳定性的建模或对多物体、多姿态的泛化能力。 还需要解决的难点 对复杂形状和姿态扰动的鲁棒性仍有限； 仅仅在模拟环境中验证； 复杂场景（如多机器人配合抛接）。 Take away 说到动机，想问一下学长为什么想做具身智能呢？ Mojuco 引擎是一个可以运行的虚拟环境\n对于这一段的 reinforcement learning, 这里的意思是：S 是机器人的状态，A 是机器人可以做的动作的集合，p 是环境，R 是奖励函数，$\\rho_0$是一个随机的初始状态分布，$\\gamma$是奖励因子：最终的奖励类似：$R = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} \\dots$ 而现在我们要做的是:选择一些动作让 $E[R]$ 尽可能大。\n在这里，PPO 看起来很复杂，但是其实不难：\n简单来说，Actor 负责做一个动作，Critic 负责判断这个动作好不好，PPO则会让策略和策略之间的连贯性更强。\n这里的“连贯性更强指的是”：新旧策略在面对同一个 state 的情况下做某一个 action 的概率之间的比值不会超过$(1 - \\theta, 1 + \\theta)$\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-29/","summary":"Paper review 2025-05-29","title":"Bug Journal 2025-05-29"},{"content":"SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation 发表时间：24 Jan 2025\n主要论点 这篇文章提出了一种新的模仿学习框架 SKIL（Semantic Keypoint Imitation Learning），通过结合视觉基础模型自动提取“语义关键点”（semantic keypoints），使得机器人在仅有少量示范的条件下，依然能完成具有泛化能力和复杂步骤的操作任务（如挂毛巾、折叠布料等）。该方法显著降低了训练所需的数据量，并且在未见过的物体与场景中也表现出优越的泛化能力。\nHow can we reduce sample complexity to enable robots to learn data-efficient and generalizable manipulation tasks?\n模型流程图 创新点 自动语义关键点提取： 借助如 DiFT 等视觉基础模型与 SAM，对参考图像中的目标区域进行聚类，自动生成语义关键点，不需要人工标注或专门训练。 语义关键点描述符（descriptor）设计： 结合相似度向量（cosine similarity）与 3D 坐标构建表示，每个关键点携带语义和空间信息。 结合 Diffusion Policy 输出动作序列： 使用 Transformer 编码器和扩散模型作为动作头，实现连续动作输出。 支持跨主体学习（Cross-embodiment）： 提出的 SKIL-H 模块允许利用人类视频（无动作标签）进行辅助训练，提高数据效率和泛化性。 Ensemble 推理策略： 在推理阶段进行关键点子集 dropout 与多次采样求中位数，从而降低视觉匹配误差带来的动作抖动。\nbb*SKIL-H: ** 可以把第三人称视角人的视频转换为辅助训练的数据集\n使用一个 frozen 的通用关键点检测器（如 SAM-Track 或 VIT tracker），输出每一帧的人体相关关键点（如手、手指等）。 画出这些关键点之间的轨迹 将人类演示中关键点的表示映射到机器人操作空间中的目标关键点 直接用 transformer 编码 解决的难点 利用视觉大模型提取对任务有语义意义的关键点，极大降低了状态空间维度； 通过高质量、稀疏但语义强的关键点建模，提升少样本学习能力； 使用 Transformer + Diffusion Policy 构建策略网络，强化连续动作输出的能力； 提供了一种利用人类演示辅助学习的方法，不依赖标注动作。 之前方法的缺点 慢：比如ACT要收集上万个数据; GenGP 用的是完整的语意场，有太多信息了 只预测关键帧，而不是连续的动作 (注：我认为这里说不定关键帧可能好一点) 预测连续动作的模型没有 generalizability 还需要解决的难点 关键点提取质量依赖于视觉基础模型能力： 如在 “Bulb Assembly” 等精度要求极高的任务中，DiFT 模型提取的关键点不够精确，导致失败。 忽略环境障碍与场景信息： 当前关键点只从目标对象上提取，无法感知障碍物等环境元素，可能导致安全问题。 固定视角与姿态： 当前工作主要依赖固定的第三视角 RGBD 摄像头，泛化到第一视角或动态视角仍有挑战。 动作表示维度有限： 尚未充分拓展到高自由度控制（如仿人手）、复杂轨迹规划等更广泛的应用场景。 泛化问题：这个模型并不是 zero-shot,而是每个 task 都单独 train 了一个 model\nTake away semantic keypoints: 语义关键点 指的是一个物体和操作相关的哪一个部分。比如杯子上的把手\ncosine similarity 可以获得不同关键点之间的相似度，相当于某种位置信息；之后可以把真正的位置信息也嵌入进去。\nDiffusion Policy: 这种方法使得 multi-model 变得可行\noff-the-shelf tracking models: 约等于已经开源的动作跟踪实现\n数据集 Meta World\nDATA SCALING LAWS IN IMITATION LEARNING FOR ROBOTIC MANIPULATION 发表时间：24 Oct 2024 修改于：12 Feb 2025\n主要论点 这篇文章研究了模仿学习中的数据规模定律(data scaling laws) 在机器人操作任务中的适用性，核心问题是：\n在不同环境和不同物体下增加演示数据量，是否能提升策略的泛化能力，进而使单任务策略在新环境和新物体上零样本部署成为可能？\n作者通过在真实世界收集超 4 万条人类演示、1.5 万次机器人 rollout，基于 Diffusion Policy 训练策略，在多个任务上发现：\n泛化性能随着训练环境/物体/环境-物体组合数量近似呈幂律增长，即：$y = \\alpha x ^ \\beta$, 其中，y 是用 normalized score 来衡量的泛化性能，x 是样本的多样性。 同一个物体或环境上多收集数据的效果远远不如增加多样性； 实证证明仅需 32 个不同环境-物体组合、每个 50 个演示，就能训练出成功率超 90% 的策略。 模型流程图 创新点 首次系统性提出并验证模仿学习中机器人操控的“scaling law” 测试了 环境泛化 + 物体泛化； 提出了一个高效数据采集策略：以环境和物体的多样性优先，不盲目增加演示数量； 在多项任务上（Pour Water、Mouse Arrangement、Fold Towels、Unplug Charger）进行大规模真实机器人验证； 对视觉模型和动作模型的扩展进行了 ablation study，验证视觉编码器更关键。 解决的难点 数据量大 还需要解决的难点 只验证了四个任务，任务种类仍有限，无法保证适用于所有任务 只使用了 Diffusion Policy，未评估不同学习算法对 scaling law 的依赖性差异 未来还将验证 Reinforcement Learning 的 scaling law. Take away 为了获得更好的泛化能力，采集更多不同的环境 ($# \u0026gt; 16$) \u0026gt; 在同一个环境中堆砌多个物体样本\nLearning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving 发表时间：22 Jan 2025\n主要论点 这篇文章提出了一种新颖的自我训练方法 LEPA（Learning to Plan before Answering），训练 LLM 在生成具体解答前，先生成高层次的抽象“计划”。这些“计划”是通用的元知识（meta-knowledge），可以指导模型更有效地推理和解题。\n具体流程如下： 1.\t模型先生成anticipatory plan（预测性计划）：对问题的大致解题策略。 2.\t再基于这个计划生成具体解答。 3.\t如果解答错误，模型会进行 自我反思（self-reflection），调整 plan 并重新解题，直到成功或达到最大尝试次数。 4.\t最终用 plan + solution 对模型进行监督微调（Supervised Fine-Tuning）。\n模型流程图 创新点 引入 anticipatory plan：首次在自我训练中加入计划作为元知识，帮助 LLM 更清晰地组织解题路径。 计划 + 解答联合训练：不仅学习答案，还学习如何规划解题路径，提高泛化性。 自我反思机制：错误时能分析原因并优化 plan，不再依赖外部标签。 信息隔离设计：计划不能包含具体答案细节，避免 LLM 投机取巧。 可与 RL 兼容扩展：初步展示了 LEPA 与 RL（REINFORCE）结合后的性能提升。 之前方法的缺点 没有引导模型形成通用的解题策略 → 泛化能力差； 错误的修改方式（如修改最终答案而非解题路径）容易产生假阳性答案； 缺少系统性规划与反思机制 。 解决的难点 自我训练生成的数据质量不足、泛化能力差，尤其是在复杂的数学和推理任务上表现不佳。\n之前的方法无法抽象出问题：比如，在做物理题时先抽象出公式，然后再代数运算。\n还需要解决的难点 计划与答案不一致问题：计划可能无法完全约束模型生成的推理步骤； 复杂度与推理选择权平衡：对于简单问题，可能不需要计划，LEPA 可能浪费计算； 计划的质量仍依赖 LLM 自身能力 ，弱模型难以产生有用计划； RL 优化仍是初步探索，与更强 RL 算法结合仍是未来方向； Take away Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving 主要论点 本文提出一种针对四足机器人“难以在仿真中准确建模”的目标（如总电池功耗）进行优化的数据驱动微调方法。核心思想是：\n先用预训练策略在现实中采集数据； 训练一个“测量模型”来预测那些仿真环境中缺失或误差较大的目标（如总电流）； 将这个模型集成到仿真中，作为奖励函数优化新策略； 通过仿真+现实评估的迭代更新，逐步提升策略在真实世界中的表现。 他们以“降低四足机器人总功耗”为目标，在 Unitree Go1 上进行实证研究，结果显示电池功耗减少了 24-28%。\n模型流程图 创新点 数据驱动的测量模型作为奖励代理：直接从现实数据学习一个预测真实目标的模型（如总电流），替代传统的机械功或热功率等代理指标。 目标不可建模时的微调框架：将测量模型集成进仿真中，使得原本难以仿真的目标也可以用于训练优化。 层级策略选择机制：候选策略先在仿真中初选，再在现实中精评，保留最优用于下次迭代。 迭代式收集数据 + 微调：不断积累现实数据、优化测量模型、提升策略性能，达到持续优化的效果。 解决的难点 传统 sim-to-real 方法依赖物理仿真器（如 MuJoCo），但这些仿真器通常无法准确建模：\n电池电流/电压（尤其是 PMSM 电机复杂控制） 步态噪声 电机过热等非刚体物理特性 之前方法的缺点 使用代理目标 (如机械功) 不够准确，甚至误导优化过程； 现实直接训练 RL 效率低，只适用于简单任务； 以往方法不支持针对新目标进行微调（例如预训练时没有考虑节能目标，无法直接迁移）。 还需要解决的难点 需大量真实数据才能收敛（约 38 万个样本）； 测量模型存在分布偏移（Out-of-Distribution）问题； 实验仅在 Unitree Go1 和平地环境下进行，缺乏跨机器人、复杂地形验证； 没有测试对其他 hard-to-simulate 目标（如声音、热量）的迁移适应性。 Take away Revisit when doing a robot dog related task.\n关注文章的 Motivation\n关注文章的 Motivation 是怎么和方法联系起来的\n关注文章是如何卖出去的\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-28/","summary":"2025-05-28 论文阅读笔记","title":"Bug Journal 2025-05-28"},{"content":"Today\u0026rsquo;s problem 2894. Divisible and Non-divisible Sums Difference\nIntuition We are given an integer n and a divisor m, and we want to compute the difference between:\nThe sum of numbers from 1 to n that are not divisible by m. The sum of numbers from 1 to n that are divisible by m. This means we want to partition the numbers 1 to n into two groups based on divisibility by m, sum each group, and return the difference.\nApproach We can solve this problem using two methods:\nMethod 1: Formula-Based Use the formula for the sum of the first n natural numbers: n * (n + 1) // 2 to get the total sum. Count how many numbers from 1 to n are divisible by m: k = n // m. The divisible numbers are: m, 2m, ..., km, and their sum is m * (1 + 2 + ... + k) = m * (k * (k + 1) // 2). Subtract the divisible sum from the total to get the sum of non-divisible numbers, then subtract. Method 2: Brute-Force Iteration Iterate from 1 to n. If the number is divisible by m, add it to num2. Otherwise, add it to num1. Return the difference num1 - num2. Complexity Time complexity:\nMethod 1: $O(1)$ (constant time using formulas) Method 2: $O(n)$ (linear time iteration) Space complexity:\nBoth methods: $O(1)$ (only a few variables used) Code class Solution: def differenceOfSums(self, n: int, m: int) -\u0026gt; int: # Method 1: Formula-Based total_sum = n * (n + 1) // 2 k = n // m divisible_sum = m * (k * (k + 1) // 2) return total_sum - divisible_sum class Solution: def differenceOfSums(self, n: int, m: int) -\u0026gt; int: # Method 2: Brute-Force Iteration num1, num2 = 0, 0 for i in range(1, n + 1): if i % m == 0: num2 += i else: num1 += i return num1 - num2 Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-27/","summary":"Traverse and Mathmatics!","title":"LeetCode Daily Question 2025-05-27"},{"content":"模板 请仔细阅读这篇文章，并告诉我： 1. 这篇文章的动机是什么，要解决什么问题 2. 这篇文章大概讲了什么 3. 这篇文章的创新点是什么 4. 这篇文章解决了什么问题，之前的人为什么不能解决 5. 这篇文章还有什么问题没解决 6. 这篇文章有什么需要我注意的点 7. 这篇文章是如何做实验的，setting 是什么 8. 这篇文章的算力要求是多少，多少卡运行了多久，用了什么数据集，是不是可以公开获取的，模型代码呢，能不能公开获取 如果这篇文章提出了一个模型，那请告诉我： 1. 这个模型的输入是什么 2. 输出是什么 3. 输入和输出数据经过了什么处理 4. 这个模型是如何处理输入和输出数据的 动机 主要论点 模型流程图 创新点 解决的难点 之前方法的缺点 还需要解决的难点 Take away ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-27/","summary":"A template for reading AI papers","title":"Bug Journal 2025-05-27"},{"content":"Today\u0026rsquo;s problem 2131. Longest Palindrome by Concatenating Two Letter Words\nIntuition There are in total two ways to form a palindrome.\na string that has an inverse string in the list a string that is a palindrome itself. In this case, the string that is palindrome can only exisit in the middle of the palindrome. Approach Therefore, we can use a hash to solve this problem. Note that we will first run test 1 before test 2. If there is an inverse string in the list, then put that string and the current string into the list.\nThen test whether this string is a palindrome itself.\nComplexity Time complexity: $O(N)$, N is the length of words.\nSpace complexity: $O(N)$, N is the length of words.\nCode class Solution: def longestPalindrome(self, words: List[str]) -\u0026gt; int: cnt = Counter(words) ans = 0 sp = 0 for word, t in cnt.items(): # print(word, t) if word[0] == word[1]: ans += (t - t % 2) sp |= (t % 2) else: ans += min(t, cnt[word[::-1]]) return (ans + sp) * 2 Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-25/","summary":"Hash!","title":"LeetCode Daily Question 2025-05-25"},{"content":"Today\u0026rsquo;s problem 2942. Find Words Containing Character\nIntuition Do what the question ask.\nApproach Do what the question ask, find the string in every word in words array.\nComplexity Time complexity: $O(N \\times M)$, N is the length of words array, M is the length of each word. Space complexity: $O(N \\times M)$, N is the length of words array, M is the length of each word. Code class Solution: def findWordsContaining(self, words: List[str], x: str) -\u0026gt; List[int]: ans = [] for i, word in enumerate(words): if x in word: ans.append(i) return ans Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-24/","summary":"Do what the question ask","title":"LeetCode Daily Question 2025-05-24"},{"content":"Today\u0026rsquo;s problem 3068. Find the Maximum Sum of Node Values\nImportant: all the methods below are based on this fact: xor even times equals xor zero times. Method 1: Tree DP Intuition and Approach In this problem, if we only consider one direction, e.g., from root to leaf, then the process will not have after effect (later decisions will not affect previous ones). Therefore, we can use DP to solve this problem.\nThe hardest part is the definition of the dp. As we have a prerequisite of a direction, a better way to define the dp formula is to exclude the effect of current node. Also, for each node, there are two status, as described above, each node can either xor odd times or even times.\nTherefore, we have our DP definition. $dp[x][0/1]$ means the largest value the children of x can achieve when the node x is changed (1) or unchanged (0).\nNow, for each child c of node x, we can do two operations: either do xor for both node x and c, or do not do xor for neither x nor c.\nThe dp formula of these two operations will be: (Note: the priority of $\\oplus$ is lower than $+$, so it is very important to add a parentheses.)\nDo the xor operation $dp[x][0] = max(dp[x][0] + dp[c][0] + nums[c], dp[x][0] + dp[c][1] + (nums[c] \\oplus k))$. $dp[x][1] = max(dp[x][1] + dp[c][0] + nums[c], dp[x][1] + dp[c][1] + (nums[c] \\oplus k))$. NOT do the xor operation $dp[x][0] = max(dp[x][1] + dp[c][1] + nums[c], dp[x][1] + dp[c][0] + (nums[c] \\oplus k))$. $dp[x][1] = max(dp[x][0] + dp[c][0] + nums[c], dp[x][0] + dp[c][0] + (nums[c] \\oplus k))$. Note that the dp[x][0] and dp[x][1] should be renewed at the same time.\nMoreover, another important thing is the initialization of the dp array. For all $dp[x][1]$, we will give it a value of $-inf$, so that we can avoid the case when c is a leaf node and the number is $\\oplus$ with k contributes to the $dp[x]$ array.\nThe final result will be $max((dp[0][0] + nums[0]), (dp[0][1] + (nums[0] ^ k)))$\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp = [[0 for _ in range(2)] for _ in range(n)] for i in range(n): dp[i][1] = -10_000_000_000 edge = [[] for _ in range(n)] for x,y in edges: edge[x].append(y) edge[y].append(x) def dfs(x, fa): for to in edge[x]: if to == fa: continue dfs(to, x) c0 = max(dp[to][0] + nums[to], dp[to][1] + (nums[to] ^ k)) c1 = max(dp[to][0] + (nums[to] ^ k), dp[to][1] + nums[to]) dp[x][0], dp[x][1] = max(dp[x][0] + c0, dp[x][1] + c1), max(dp[x][1] + c0, dp[x][0] + c1) dfs(0,-1) return max((dp[0][0] + nums[0]), (dp[0][1] + (nums[0] ^ k))) Method 2: Tree DP with better memory Intuition and Approach In the previous code, we find that the $dp[x]$ will only use two times. Once in calculating the result of $dp[x]$, once in calculating the result of $dp[fa]$.\nTherefore, we can return the value of $dp[x][0]$ and $dp[x][1]$ to avoid the extra space of the dp array.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(1)$. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) edge = [[] for _ in range(n)] for x,y in edges: edge[x].append(y) edge[y].append(x) def dfs(x, fa): dp0,dp1 = 0,-1e9 for to in edge[x]: if to == fa: continue c0, c1 = dfs(to, x) dp0, dp1 = max(dp0 + c0, dp1 + c1), max(dp0 + c1, dp1 + c0) return max(dp0 + nums[x], dp1 + (nums[x] ^ k)), max(dp0 + (nums[x] ^ k), dp1 + nums[x]) return dfs(0,-1)[0] Important: all the methods below are based on this fact: there are always a path between two nodes on a tree. Therefore, we can $\\oplus$ all the nodes on this path, resulting the $\\oplus$ of any two nodes on the tree. Method 3: DP without tree Intuition and Approach For each node, we have two status, whether to $\\oplus$ k or not. Therefore, the definition of the DP array will be: $dp[i][0/1]$ means whether there are odd (1) or even (0) $\\oplus$ k operations when traversing to the ith node.\nWe then have the formular:\nWhen this node $\\oplus$ with k: $dp[i][0] = max(dp[i-1][0] + nums[i], dp[i-1][1] + (nums[i] ^ k))$ When this node do not $\\oplus$ with k: $dp[i][1] = max(dp[i-1][1] + nums[i], dp[i-1][0] + (nums[i] ^ k))$ Note that there are always even $\\oplus$ operations, so the answer would be $dp[n-1][0]$.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp = [[0 for _ in range(2)] for _ in range(n)] dp[0][0] = nums[0] dp[0][1] = (nums[0] ^ k) for i in range(1, n): dp[i][0] = max(dp[i-1][0] + nums[i], dp[i-1][1] + (nums[i] ^ k)) dp[i][1] = max(dp[i-1][0] + (nums[i] ^ k), dp[i-1][1] + nums[i]) return dp[-1][0] Method 4: DP without tree with better memory Intuition and Approach Same as Method 2, we also find out that the dp[i] formular only use twice. In this case, we can use two variables instead of the whold array to have a better memory usage.\nAlso, the $max$ operations is too slow in python, so a better way is to use if else equations instead of max.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(1)$. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp0, dp1 = 0, -10_000_000_000 for i in range(n): a = nums[i] b = a ^ k new_dp0 = dp0 + a if dp0 + a \u0026gt; dp1 + b else dp1 + b new_dp1 = dp0 + b if dp0 + b \u0026gt; dp1 + a else dp1 + a dp0, dp1 = new_dp0, new_dp1 return dp0 Method 5: Greedy algorithm Intuition and Approach Another way to look at this method without of tree is using greedy algorithm. Because we know that we can $\\oplus$ k as long as we can find a pair of nodes, we can use greedy algorithm to find the pairs that has the most differences after $\\oplus$ k.\nThat is, we can first $\\oplus$ every element with k, calculating the difference between the new array and the previous array, then find all the pairs that has a difference that is larger than zero, then we get our answer.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: ans = sum(nums) diff = [(x ^ k) - x for x in nums] cnt,l,r = 0,inf,-inf for x in diff: if x \u0026gt; 0: cnt += 1 if x \u0026lt; l: l = x ans += x else: if r \u0026lt; x: r = x if cnt % 2 == 1: ans += max(-l, r) return ans I don\u0026rsquo;t know why using sort to do greedy algorithm is so neat and fast. Just as the one in the official solution.\nAdvertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-23/","summary":"5 Solutions in one question!","title":"LeetCode Daily Question 2025-05-23"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/zero-array-transformation-iii/\nIntuition This question requires us to find the number of sections that is \u0026ldquo;useful\u0026rdquo;, or in other words, the smallest number of sections that is enough to make the array a zero array.\nThe key to solve this question is to change a view of how we look at this problem: if we take each element in the array seperately, then we can use greedy algorithm to solve this problem.\nThe thing is, if we look at each element seperately, i.e., to make the entire array a zero array, we must make each element zero.\nIn this case, for each element, the best way is to find a section that contains this elemnt, while it has a further tail. This is because a furtuer tail means to cover more elements in the future, which will be always better compared with the sections that has a shorter tail.\nTherefore, now we need a data structure to store the current \u0026ldquo;farest tail\u0026rdquo;. This data structure need to add element dynamically and delete the largest item, when a heap will be the best way to store the \u0026ldquo;tail\u0026rdquo;.\nApproach Therefore, we can form our algorithm.\nFirst, we need to sort the array using the left end of the query as keyword. In this case, we can find which queries has a left end that is to the left of our current index. Then we need to create a difference array to deal with the section add operation; a heap to store the right end of the queries; and an index to show where we are currently at when we traverse all the queries. The next step is to traverse the number array: for each element in the number array, we first need to push all the queries that has the left end that is less the current index. This makes all the elements in the heap potentially available to use to decrease the current element. Then we will deal with the current element, finding all the available queries for the current element, then deal with the section decrease operation. (In this case, we ensure every operation is valid by checking that the endpoint of the heap top is larger or equal to the index, so that the left end of the array will be less or equal to the current index, and the right end of the array will be larger or equal to the current index. Therefore, we guarentee that the operation is valid). Finally, if the number is still larger than 0, we will return -1; otherwise, we will return the remaining element in h, which is all the unused elements. Complexity Time complexity: $O(N \\times log(m))$\nSpace complexity: $O(N + M)$\nCode class Solution: def maxRemoval(self, nums: List[int], queries: List[List[int]]) -\u0026gt; int: queries.sort(key = lambda x: x[0]) diff = [0] * (len(nums) + 1) h = [] idx, now = 0,0 for i in range(len(nums)): while idx \u0026lt; len(queries) and queries[idx][0] \u0026lt;= i: heappush(h, -queries[idx][1]) idx += 1 now += diff[i] while h and now \u0026lt; nums[i] and -h[0] \u0026gt;= i: now += 1 diff[-heappop(h) + 1] -= 1 if now \u0026lt; nums[i]: return -1 return len(h) For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-22/","summary":"Greedy Algorithm, look at each element seperately!","title":"LeetCode Daily Question 2025-05-22"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-smallest-number-from-di-string/\nIntuition This problem let us find a sequence of numbers under some constrains.\nFirst we observe that the N, the length of the sequence, is very small. Thereofre, we can use a dfs to traverse all possible situations and get the result.\nAnother way to do that is using a stack. When the array is increasing, the smallest way to get a valid array is to traverse all the numbers, filling in the smallest number possible. When the array is decreasing, becuase we still want the smallest array, we need to put the smallest number available, which should be also larger to the next number. In this case, the smallest number we can put at current place is the position + the number of consequtive \u0026lsquo;D\u0026rsquo;s afterward. Therefore, we can use a stack to temporarily sotre the number we are traversing, and add it back to the current array when we meet a \u0026lsquo;I\u0026rsquo; or reach to the end of the array.\nApproach For Solution 1, the dfs solution, all we need to do is to traverse all the solutions and get the first one that fullfills the requirement.\nFor Solution 2, the stack solution, when we meet \u0026lsquo;I\u0026rsquo;, we can put \u0026ldquo;idx + 1\u0026rdquo; to our current array, then fill all the elements in a stack into our current array in reverse order, then flush the stack. When we meet \u0026lsquo;D\u0026rsquo;, we can put \u0026ldquo;idx + 1\u0026rdquo; to our stack for our future use.\nTrick DFS: Because we are required to find the smallest valid sequence, so the first sequence that is not None is our target. This means that we can return this answer as soon as we get a valid result.\nStack: Here I intentionally add a \u0026ldquo;D\u0026rdquo; to the end of the sequence. Intuitively speaking, the last element is the largest element in the sequence, so to put it into our current sequence, it requires a \u0026ldquo;D\u0026rdquo; operation. In this case, if the original last character is \u0026lsquo;I\u0026rsquo;, then we can directlly put the largest number to the end of our original sequence, which is the same as a \u0026lsquo;D\u0026rsquo; operation. This is because the \u0026lsquo;I\u0026rsquo; operation will flush the stack, so there will only be one element in the stack, making it the same whether adding as a normal sequence or a inverted sequence. If the original last character is \u0026lsquo;D\u0026rsquo;, then the largest character should be at the position whether the consequtive sequence of \u0026lsquo;D\u0026rsquo; starts. In this case, this means that there should be a \u0026lsquo;D\u0026rsquo; operation to put this number into the right position. Though this trick makes the code more tidy and elegant, it sacrifices readability, which is not encouraged.\nComplexity Time complexity for dfs solution: $O(N!)$, N is the length of the sequence.\nTime complexity for stack solution: $O(N)$, N is the length of the sequence.\nSpace complexity for dfs solution: $O(N)$, N is the length of the sequence.\nSpace complexity for stack solution: $O(N)$, N is the length of the sequence.\nCode class Solution: def smallestNumber(self, pattern: str) -\u0026gt; str: arr = [] n = len(pattern) + 2 def dfs(arr): if len(arr) == n - 1: return arr now = len(arr) - 1 res = None if pattern[now] == \u0026#39;I\u0026#39;: for i in range(arr[now] + 1, n): if i not in arr: res = dfs(arr + [i]) if res is not None: return res else: for i in range(1, arr[now]): if i not in arr: res = dfs(arr + [i]) if res is not None: return res return res for i in range(1, n): ans = dfs([i]) if ans is not None: return \u0026#39;\u0026#39;.join(map(str, ans)) # return ans return None class Solution: def smallestNumber(self, pattern: str) -\u0026gt; str: arr = [] n = len(pattern) + 2 def dfs(arr): if len(arr) == n - 1: return arr now = len(arr) - 1 res = None if pattern[now] == \u0026#39;I\u0026#39;: for i in range(arr[now] + 1, n): if i not in arr: res = dfs(arr + [i]) if res is not None: return res else: for i in range(1, arr[now]): if i not in arr: res = dfs(arr + [i]) if res is not None: return res return res for i in range(1, n): ans = dfs([i]) if ans is not None: return \u0026#39;\u0026#39;.join(map(str, ans)) # return ans return None For more solutions, please visit My blog.\n","permalink":"https://tzj2006.github.io/leetcode/2025-02-18/","summary":"DFS beats 100% and O(N) stack with trick","title":"LeetCode Daily Question 2025-02-18"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/set-matrix-zeroes/\nIntuition This question asks us to change the rows and columns to 0 if there exists an 0 in the row or column. Therefore, we can store the rows and columns and then change all these rows and colums to zero.\nApproach Store all the columns and rows that contains 0 Change all these columns and rows Complexity Time complexity: $O(N \\times M)$, N is the length of the array, M is the width of the array.\nSpace complexity: $O(N \\times M)$, N is the length of the array, M is the width of the array.\nCode class Solution: def setZeroes(self, matrix: List[List[int]]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify matrix in-place instead. \u0026#34;\u0026#34;\u0026#34; change_row_idx = set([]) change_col_idx = set([]) # Note that here I use set to avoid recording the same row or column multiple times. for i in range(len(matrix)): for j in range(len(matrix[0])): if matrix[i][j] == 0: change_row_idx.add(i) change_col_idx.add(j) for i in range(len(matrix)): for j in range(len(matrix[0])): if i in change_row_idx or j in change_col_idx: matrix[i][j] = 0 For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-21/","summary":"Do what the question asks!","title":"LeetCode Daily Question 2025-05-21"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/zero-array-transformation-i/\nIntuition This problem means to decrease one in a range (l, r) for each query. To deal with the change of a range, we can consider prefix sum. Note that the question says \u0026ldquo;Select a subset of indices\u0026rdquo;, this means that we do not necessarily need to minus 1 for all indices in the range. In this case, because we only care whether the final array is a zero array or not, so instead of testing whether the final array is zero or not, we can test whether the final array is less or equal to zero or not becuase of the subset mentioned in the question.\nApproach For each query, we can add 1 to the difference array at l and add -1 to the difference array at r + 1. Then when we calculate the final answer, we can use the prefix sum to add them up and get the change of the array. Finally, when we want to know whether the final array is zero array or not, we can add the difference array to the original array and test whether each index is less or equal to zero or not to get the answer.\nComplexity Time complexity: $O(N + M)$, N is the length of the original array, M is the length of the query.\nSpace complexity: $O(N)$.\nCode class Solution: def isZeroArray(self, nums: List[int], queries: List[List[int]]) -\u0026gt; bool: diff = [0] * (len(nums) + 1) for l, r in queries: diff[l] -= 1 diff[r + 1] += 1 for i in range(len(nums)): if i \u0026gt; 0: diff[i] += diff[i-1] if nums[i] + diff[i] \u0026gt; 0: # print(i, nums[i], diff[i]) return False return True For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-20/","summary":"Use Chafen!","title":"LeetCode Daily Question 2025-05-20"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/type-of-triangle\nIntuition Do what the question asks.\nApproach Do what the question asks.\nComplexity Time complexity: $O(1)$\nSpace complexity: $O(1)$\nCode class Solution: def triangleType(self, nums: List[int]) -\u0026gt; str: nums.sort() if nums[0] + nums[1] \u0026lt;= nums[2]: return \u0026#34;none\u0026#34; elif nums[0] == nums[1] == nums[2]: return \u0026#34;equilateral\u0026#34; elif nums[0] == nums[1] or nums[1] == nums[2]: return \u0026#34;isosceles\u0026#34; return \u0026#34;scalene\u0026#34; For more Solutions, please visit my website.\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-19/","summary":"Do what the question ask!","title":"LeetCode Daily Question 2025-05-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/painting-a-grid-with-three-different-colors/\nIntuition In this question, we find that m is relatively small compared with n. As $m \\le 5$, and $n \\le 1000$. Then we may consider to enumerate all solutions for a column, then elaborate it to the whole matrix. When elaborating it to the whole matrix, we figure out a thing: whatever a column is painted, the only influence is its next column, while future columns will not influence previous columns. This makes DP possible.\nApproach Therefore, here is our approach:\nFirst, we need to find out how many valid patterns are there in a column. Therefore, we can perform a dfs to search for all possible combinations. Second, we need to know which two patterns can be in adjcent columns, so we enumerate through each pair of patterns, and then test whether they can be in adjcent rows or not. Third, we use DP to elaborate from one column to the next. In this case, the DP formular will be: $DP[col][case_x] = \\sum DP[col-1][case_y]. \\forall \\text{casex and casey can be in two adjcent columns}$. Finally, all we need to do is to add up all the cases of the final column of DP to get our answer. Complexity Time complexity: $O(3^{2m} \\times n)$\nSpace complexity: $O(3^{2m})$\nCode class Solution: def colorTheGrid(self, m: int, n: int) -\u0026gt; int: pat = [] col = [0, 1, 2] def dfs(x, s): if x == m: pat.append(s) return for i in col: if x == 0 or s[x - 1] != i: dfs(x + 1, s + [i]) dfs(0, []) # till this step, we find all valid patterns for a column and store it in the pattern list. l = len(pat) valid = [[True for _ in range(l)] for _ in range(l)] for i in range(l): for j in range(i + 1, l): for k in range(m): if pat[i][k] == pat[j][k]: valid[i][j] = False break # till this step, we find all the pattern pairs that is valid. dp = [[0 for _ in range(l)] for _ in range(n)] mod = 1_000_000_007 for i in range(l): dp[0][i] = 1 # for column 0, each pattern is valid. for i in range(1, n): for x in range(l): for y in range(x + 1, l): if valid[x][y]: dp[i][x] = (dp[i][x] + dp[i-1][y]) % mod dp[i][y] = (dp[i][y] + dp[i-1][x]) % mod # we elaborate to the next column according to the DP formula. ans = 0 for i in range(l): ans = (ans + dp[-1][i]) % mod # finally, we add up all the answers. return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-05-18/","summary":"First DFS then DP!","title":"LeetCode Daily Question 2025-05-18"},{"content":"Mac 监控： 磁盘信息：\nbrew install smartmontools smartctl -a disk0 效果： CPU GPU占用信息：\nbrew install macmon macmon 效果展示： ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-17/","summary":"\u003cp\u003eMac 监控：\n磁盘信息：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ebrew\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esmartmontools\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esmartctl\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e \u003cspan class=\"n\"\u003edisk0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e效果：\n\u003cimg alt=\"1\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-17/1.png\"\u003e\u003c/p\u003e\n\u003cp\u003eCPU GPU占用信息：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ebrew install macmon\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emacmon\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e效果展示：\n\u003cimg alt=\"2\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-17/2.png\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-05-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/letter-tile-possibilities/description/\nIntuition In this question, we want to know how many different tiles we can generate.\nApproach Therefore, we can use backtracking to enumerate all the solutions.\nComplexity Time complexity: $O(2^N)$, N is the length of the sequence.\nSpace complexity: $O(N)$, N is the length of the sequence.\nCode class Solution: def numTilePossibilities(self, tiles: str) -\u0026gt; int: counter = defaultdict(int) for ch in tiles: counter[ch] += 1 def dfs(counter): total = 0 for ch in counter: if counter[ch] == 0: continue # Choose character total += 1 counter[ch] -= 1 total += dfs(counter) counter[ch] += 1 # backtracking return total return dfs(counter) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-17/","summary":"\u003col start=\"1079\"\u003e\n\u003cli\u003eLetter Tile Possibilities\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-the-lexicographically-largest-valid-sequence/description/\nIntuition This question requires you to find a solution according to the requirements.\nApproach Note that $N \\le 20$, so we can use a brute search to find the answer.\nComplexity Time complexity: $O(N^N)$, N is the same definition as the question.\nSpace complexity: $O(N)$, N is the same definition as the question.\nCode Normal dfs solution class Solution: def dfs(self, pos, n, vis, pls): if pos == 2 * n - 1: return pls # if we enumerate to the end of the sequence, then we can return the answer. if pls[pos] != 0: return self.dfs(pos + 1, n, vis, pls) # if we place a number in the current position, then we can move to the next position. for i in range(n, 1, -1): # we enumerate from large to small so that we can get the largest sequence. if vis[i]: continue # if we use this number, then pass. if pos + i \u0026lt; 2 * n - 1 and pls[pos + i] == 0: pls[pos] = i pls[pos + i] = i vis[i] = True # put the number into the slot if it is available. ret = self.dfs(pos + 1, n, vis, pls) if ret is not None: return ret vis[i] = False pls[pos] = 0 pls[pos + i] = 0 if vis[1]: return None vis[1] = True pls[pos] = 1 ret = self.dfs(pos + 1, n, vis, pls) if ret is not None: return ret vis[1] = False pls[pos] = 0 # special check for 1 becuase 1 only puts into one slot. return None def constructDistancedSequence(self, n: int) -\u0026gt; List[int]: pls = [0] * (2 * n - 1) # pls is the sequence that we place numbers vis = [0] * (n+1) # visit is the sequence we test whether a number exists in the current sequence or not. return self.dfs(0, n, vis, pls) Faster solution for future use Note that the solution will not change when we input the same number, therefore, we can just store the answer we get and output it for every query.\nclass Solution: def constructDistancedSequence(self, n: int) -\u0026gt; List[int]: ans = [ [1], [2,1,2], [3,1,2,3,2], [4,2,3,2,4,3,1], [5,3,1,4,3,5,2,4,2], [6,4,2,5,2,4,6,3,5,1,3], [7,5,3,6,4,3,5,7,4,6,2,1,2], [8,6,4,2,7,2,4,6,8,5,3,7,1,3,5], [9,7,5,3,8,6,3,5,7,9,4,6,8,2,4,2,1], [10,8,6,9,3,1,7,3,6,8,10,5,9,7,4,2,5,2,4], [11,9,10,6,4,1,7,8,4,6,9,11,10,7,5,8,2,3,2,5,3], [12,10,11,7,5,3,8,9,3,5,7,10,12,11,8,6,9,2,4,2,1,6,4], [13,11,12,8,6,4,9,10,1,4,6,8,11,13,12,9,7,10,3,5,2,3,2,7,5], [14,12,13,9,7,11,4,1,10,8,4,7,9,12,14,13,11,8,10,6,3,5,2,3,2,6,5], [15,13,14,10,8,12,5,3,11,9,3,5,8,10,13,15,14,12,9,11,7,4,6,1,2,4,2,7,6], [16,14,15,11,9,13,6,4,12,10,1,4,6,9,11,14,16,15,13,10,12,8,5,7,2,3,2,5,3,8,7], [17,15,16,12,10,14,7,5,3,13,11,3,5,7,10,12,15,17,16,14,9,11,13,8,6,2,1,2,4,9,6,8,4], [18,16,17,13,11,15,8,14,4,2,12,2,4,10,8,11,13,16,18,17,15,14,12,10,9,7,5,3,6,1,3,5,7,9,6], [19,17,18,14,12,16,9,15,6,3,13,1,3,11,6,9,12,14,17,19,18,16,15,13,11,10,8,4,5,7,2,4,2,5,8,10,7], [20,18,19,15,13,17,10,16,7,5,3,14,12,3,5,7,10,13,15,18,20,19,17,16,12,14,11,9,4,6,8,2,4,2,1,6,9,11,8] ] return ans[n - 1] ","permalink":"https://tzj2006.github.io/leetcode/2025-02-16/","summary":"This is an NP Complete question","title":"LeetCode Daily Question 2025-02-16"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/sort-colors/\nIntuition In this case, we know that there are only three elements in the list, so we can use bucket sort to solve this problem.\nApproach All we need is to use a bucket to calculate the number of times each number exists, then we put these numbers into the array.\nComplexity Time complexity: $O(N)$, N is the length of the array. Space complexity: $O(Num)$, Num is the number of different numbers. Code class Solution: def sortColors(self, nums: List[int]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; # nums.sort() cnt = [0,0,0] for num in nums: cnt[num] += 1 cnt[1] += cnt[0] cnt[2] += cnt[1] cur = 0 for i in range(len(nums)): while i \u0026gt;= cnt[cur]: cur += 1 nums[i] = cur ","permalink":"https://tzj2006.github.io/leetcode/2025-05-17/","summary":"\u003col start=\"75\"\u003e\n\u003cli\u003eSort Colors\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-05-17"},{"content":"Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (ACT Algorithm) 主要论点 ALOHA 是一个开放源代码的低成本双臂远程操作硬件系统，整体成本低于 20,000 美元，主要由现成的机器人组件和少量 3D 打印部件组成。该系统支持精细、动态和接触丰富的任务，如穿拉链、乒乓球颠球和链条组装等。\n为了应对模仿学习中政策误差累积的问题，研究人员提出了 ACT 算法。该算法基于 Transformer 架构，采用条件变分自编码器（CVAE）框架，通过预测动作序列（即“动作块”）而非单步动作，减少了有效的预测范围，从而提高了学习效率和稳定性。\n在六个现实世界的精细操作任务中，如打开透明调味杯盖和插入电池等，ALOHA 系统仅通过约 10 分钟的示范数据（约 50 次演示）就实现了 80% 至 90% 的成功率，展示了其在低成本硬件上的高效学习能力。\n创新点 低成本高性能：ALOHA 系统在成本控制的同时，仍能执行复杂的双臂操作任务，降低了高精度机器人研究的门槛。\n动作块预测 ：ACT 算法通过预测动作序列，减少了政策误差的累积，提高了模仿学习的稳定性和效率。\n快速学习能力：系统仅需少量的演示数据即可学习复杂任务，展示了高效的学习能力。\n解决的难点 高精度双臂系统价格昂贵，限制研究和数据获取 。\n模仿学习中长序列预测误差累积问题严重 。\n设计低于 $20,000 的双臂系统 ALOHA + 高效 Transformer 模仿学习方法 ACT。\n通过动作 chunking 预测提升长序列动作稳定性。\n还需要解决的难点 硬件精度限制 ：尽管系统成本低廉，但硬件精度的限制可能影响在更复杂任务中的表现。\n泛化能力：系统在面对未见过的任务或环境时的泛化能力仍需进一步提升。\n实时性能 ：在实际应用中，如何确保系统的实时响应能力和稳定性是一个挑战。\nAutoregressive Action Sequence Learning for Robotic Manipulation 主要论点 论文的核心思想是将机器人动作表示为序列数据，并通过自回归序列建模生成动作序列。为此，作者提出了两项关键技术：\nChunking Causal Transformer (CCT)：该模型扩展了传统因果变换器的单步预测能力，支持在一个步骤中预测多个动作“块”。这种方法提高了对不同控制频率任务的适应性，并通过减少自回归步骤提高了效率 Autoregressive Policy (ARP)：基于CCT，作者设计了ARP架构，用于生成混合动作序列，解决多种机器人操作任务。该架构在Push-T、ALOHA和RLBench等多种机器人操作环境中进行了评估，结果显示ARP作为通用架构，在所有测试基准中匹配或超越了特定环境下的最新技术，同时在计算和参数规模上更为高效 简而言之，CCT 可以预测未来的多个动作 创新点 多动作块预测机制：CCT模型引入了预测多个动作块的能力，使其能够处理不同类型和频率的动作数据，提高了模型的灵活性和效率\n混合动作序列设计：通过将不同类型的动作（如关节位置、2D像素坐标和末端执行器姿态）混合在一个序列中，并为每种动作类型使用不同的块大小，增强了模型对复杂任务的适应能力\n通用策略架构：ARP架构作为一个通用的策略架构，在多个不同的机器人操作环境中表现出色，显示出其广泛的适用性和高效性\n解决的难点 自回归策略效率低: 传统每次只预测一个动作的自回归方法效率低，难以适配高频任务。\n动作混合表示困难：连续值与离散值混合表示在序列学习中不易统一建模。\n还需要解决的难点 动作数据的异质性 ：机器人动作数据通常包括连续值和离散值，如何有效地将这些异质数据表示为序列，并进行建模，是一个挑战。\n高频控制任务的建模 ：现有的自回归架构在处理高频控制任务时存在限制，如何扩展模型以支持高频控制任务，需要进一步研究\n混合动作序列的生成与优化 ：在生成包含多种动作类型的混合序列时，如何确保各动作类型之间的协调性和整体序列的最优性，是一个需要解决的问题\nπ0: A Vision-Language-Action Flow Model for General Robot Control 主要论点 π₀模型的核心是将预训练的视觉-语言模型（VLM）与流匹配架构相结合，形成一个统一的视觉-语言-动作（VLA）模型，用于通用机器人控制。该模型通过在多个灵巧机器人平台（包括单臂、双臂和移动操纵器）的大型多样化数据集上进行训练，学习从视觉和语言输入到动作输出的映射关系。\n模型的训练分为两个阶段：\n预训练阶段 ：在大规模多样化的数据集上进行训练，学习通用的感知和语言理解能力。 微调阶段：在特定任务的高质量数据上进行微调，以提高在特定任务上的性能。 该模型在多个任务上进行了评估，包括折叠衣物、清洁桌子和组装盒子等，展示了其在零样本学习、语言指令遵循和新技能获取方面的能力。\n创新点 流匹配架构 ：引入流匹配技术生成连续的动作分布，适用于高频率和灵巧的任务。\n跨机器人平台训练 ：结合多种机器人类型的数据进行训练，使模型能够适应不同的机器人配置和动作表示。\n高效推理机制 ：模型设计允许高效的推理过程，通过缓存和重用注意力键值对来减少计算量，适应实时控制的需求。\n解决的难点 VLA模型对高频、连续动作建模困难。\n泛化能力差，多机器人平台适应性弱。\n引入流匹配（flow matching）机制，生成连续动作分布，替代离散token生成方式。\n在多机器人、多任务、多平台上训练实现跨平台泛化。\n还需要解决的难点 数据的多样性和质量：虽然模型在多个平台和任务上进行了训练，但如何进一步提高数据的多样性和质量，以增强模型的泛化能力，仍是一个挑战。\n高频动作控制的稳定性 ：在高频率控制任务中，如何确保模型生成的动作序列的稳定性和准确性，需要进一步研究。\n模型的可扩展性和部署 ：如何将该模型部署到实际的机器人系统中，并确保其在不同硬件平台上的性能和效率，是实现其实际应用的关键。\nFAST: Efficient Action Tokenization for Vision-Language-Action Models (Chunking) 主要论点 传统的VLA模型在处理连续的机器人动作信号时，通常采用逐维、逐时间步的简单分箱（binning）策略进行离散化。然而，这种方法在面对高频率、精细操作任务时表现不佳，主要原因在于连续动作之间的强相关性导致模型难以有效学习。\n为解决这一问题，作者提出了FAST（Frequency-space Action Sequence Tokenization）方法，其核心思想包括：\n离散余弦变换（DCT） ：将连续的动作序列转换到频域，捕捉动作信号的主要频率成分，从而减少时间上的冗余信息。（关键帧技术） 量化与字节对编码（BPE） ：对DCT系数进行量化，并采用BPE进行压缩，生成信息密度更高的离散动作标记序列。 此外，作者还推出了 FAST+ ，一个在100万个真实机器人动作轨迹上训练的通用动作标记器，能够适用于多种机器人类型和控制频率的动作序列\n创新点 频域压缩的动作离散化：首次将DCT应用于机器人动作序列的离散化，有效减少了时间上的冗余信息，提高了模型对高频率动作的学习能力。\n通用动作标记器FAST+ ：通过在大规模、多样化的机器人动作数据上训练，FAST+实现了对不同机器人平台和任务的广泛适应性，减少了对特定任务手工设计标记器的需求。\n显著提升训练效率**** ：与传统的扩散模型相比，采用FAST的自回归VLA模型在训练时间上减少了多达5倍，同时在多个任务上达到了相当甚至更优的性能。\n解决的难点 传统动作token推理慢 。\n无法处理高频控制任务和长序列建模 。\n提出 DCT + BPE 的动作频域压缩方法（FAST），大幅压缩动作序列长度，提升效率。\n预训练通用tokenizer（FAST+）跨机器人平台迁移能力强。\n还需要解决的难点 高频动作的精确重建 ：虽然FAST在压缩动作序列方面表现出色，但在某些需要高精度控制的任务中，如何确保压缩后的动作序列能够准确还原原始动作，仍需进一步研究。\n（可不可以设置不同的专家模型，交由模型来判断应该使用原始序列还是压缩后的序列）\n与其他模型架构的兼容性 ：FAST主要与自回归VLA模型结合使用，其在其他类型的模型架构（如非自回归模型）中的表现和适应性尚待探索。\n实时控制的延迟问题 ：在实际机器人控制中，动作的生成和执行需要满足实时性要求，FAST在实际部署中可能面临延迟带来的挑战。\nπ0.5: a Vision-Language-Action Model with Open-World Generalization (More data, multimodel) Basically, 更强的π0。\n更强的 model, 更多的数据\n更强的开放世界泛化能力\nOpenVLA: An Open-Source Vision-Language-Action Model (远程连接 + Chunking + Dino) 主要论点 OpenVLA 是一个拥有 70 亿参数的开源 VLA 模型，基于 Llama 2 语言模型，并结合了 DINOv2 和 SigLIP 的预训练视觉特征。该模型在 Open X-Embodiment 数据集中的 97 万个真实机器人操作轨迹上进行了训练，涵盖了多种机器人形态、任务和场景。OpenVLA 能够直接控制多种机器人，并通过参数高效的微调方法快速适应新的机器人配置\n创新点 开源性与可访问性 ：OpenVLA 是首个完全开源的 VLA 模型，提供了模型检查点、微调笔记本和 PyTorch 训练代码，支持在 Open X-Embodiment 数据集上进行大规模训练。\n融合视觉编码器：模型采用了融合 DINOv2 和 SigLIP 特征的视觉编码器，结合了空间和语义信息，以增强模型对视觉输入的理解能力。\n高效的动作离散化方法：OpenVLA 通过将连续的机器人动作映射到离散的标记上，并使用语言模型的分词器进行处理，提高了模型的训练和推理效率。\n强大的泛化能力 ：在 29 个任务和多种机器人形态上，OpenVLA 的任务成功率比封闭模型 RT-2-X（55B 参数）高出 16.5%，同时参数数量减少了 7 倍。\n解决的难点 缺乏可公开访问、端到端训练的开源VLA模型 。\n跨机器人统一控制策略训练成本高 。\n基于 Llama2 + DINOv2 构建的70亿参数模型开源并附完整工具链。\n使用统一token表示动作（通过BPE），促进模块化、可复用性和可迁移性。\n还需要解决的难点 对未见机器人形态的泛化能力有限 ：OpenVLA 在预训练数据中未包含的机器人形态上，零样本泛化能力有限，需要通过微调适应新的机器人配置。\n动作离散化的精度问题 ：尽管采用了高效的动作离散化方法，但在某些需要高精度控制的任务中，如何确保离散化后的动作能够准确还原原始动作，仍需进一步研究。\n实时控制的计算需求：在实际机器人控制中，动作的生成和执行需要满足实时性要求，OpenVLA 在实际部署中可能面临计算资源和延迟的挑战。\nHi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models (LLM分层) 主要论点 Hi Robot 系统采用了两层策略结构，分别对应心理学中丹尼尔·卡尼曼提出的“系统1”（快速、直觉反应）和“系统2”（慢速、深度推理）模型：\n高层策略（System 2） ：利用预训练的视觉-语言模型（VLM），对复杂的自然语言指令进行解析，结合视觉观察，生成一系列中间步骤的低层语言命令。这一层具备推理能力，能够处理多阶段任务，并根据用户的实时反馈进行调整。 低层策略（System 1） ：基于 π₀ 模型，执行高层策略生成的原子级命令。该层专注于具体动作的执行，如“抓取杯子”，并能根据实时的视觉和状态信息进行快速反应。 该系统在三种不同的机器人平台上进行了测试，包括单臂、双臂和双臂移动机器人，任务涵盖清理杂乱的桌面、制作三明治和杂货购物等，展示了其在处理复杂任务和适应用户反馈方面的能力。\n创新点 分层架构设计：通过将任务解析与动作执行分离，Hi Robot 能够更有效地处理复杂指令和动态反馈，提高了系统的灵活性和适应性。\n“自言自语”机制 ：高层策略在生成低层命令前，会进行内部的语言推理过程，类似于人类在执行复杂任务前的思考过程，增强了系统的推理能力。\n实时反馈整合：系统能够在任务执行过程中接受并理解用户的实时语言反馈，如“那不是垃圾”，并据此调整当前的行为策略。\n广泛的任务适应性 ：通过在多种机器人平台和任务上的测试，验证了该系统在处理多样化任务和环境中的泛化能力。\n解决的难点 单一层次VLA模型无法处理长时序、开放式自然语言指令 。\n缺乏对人类实时反馈的理解和响应能力 。\n引入分层策略（System 1/2）模拟人类推理流程，实现任务分解与动作控制解耦。\n支持语言实时中断和调整（如用户打断、重说等）。\n还需要解决的难点 高层与低层策略的协同：确保高层生成的命令能够被低层准确执行，尤其是在面对未见过的任务或环境时，仍需进一步优化两层之间的接口和协同机制。\n实时性与计算资源的平衡：高层策略的推理过程可能带来计算延迟，如何在保证系统实时响应的同时，维持高层策略的推理深度，是一个需要权衡的问题。\n数据多样性与泛化能力 ：尽管系统在多个任务上表现良好，但在面对极端或未见过的指令和环境时，其泛化能力仍需通过更多样化的数据训练进行提升。\nDiffusion Policy: Visuomotor Policy Learning via Action Diffusion (Diffusion Policy) 主要论点 传统的机器人策略学习方法在处理多模态动作分布和高维动作空间时常面临挑战。为此，作者提出了 Diffusion Policy ，将机器人视觉-动作策略表示为条件去噪扩散过程（Conditional Denoising Diffusion Process）\n该方法的核心思想是：\n训练阶段：在动作空间中添加噪声，并训练模型预测如何从噪声中恢复出原始动作序列。 推理阶段：从随机噪声开始，逐步去噪，生成符合当前视觉观察的动作序列。 通过这种方式，Diffusion Policy 能够有效建模多模态动作分布，适应高维动作空间，并在多个机器人操作任务中表现出色。\n创新点 多模态动作建模能力 ：Diffusion Policy 能够自然地处理多种可能的动作路径，适应复杂任务中的多样性。 高维动作空间适应性：通过在整个动作序列上进行建模，方法在处理高维动作空间时表现稳定，避免了传统方法在高维空间中的不稳定性。 稳定的训练过程：相比于传统的能量模型（Energy-Based Models），Diffusion Policy 避免了对归一化常数的估计，提升了训练的稳定性。 闭环控制机制：结合递归视野控制（Receding Horizon Control），实现了在任务执行过程中的动态重规划，提高了系统的鲁棒性。 解决的难点 能量模型训练不稳定、归一化常数难估计 。\n无法处理多模态动作空间（例如多个解法、连续空间）。\n用扩散模型建模动作序列的分布，天然适配多模态分布。\n在多个真实世界任务中验证其高鲁棒性与学习稳定性。\n还需要解决的难点 推理速度 ：由于扩散过程需要多步去噪，推理速度较慢，可能限制了在实时控制任务中的应用。\n与其他模型的集成 ：如何将 Diffusion Policy 与其他感知或语言模型有效集成，以处理更复杂的任务，仍需进一步研究。\nRT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE 主要论点 RT-1 模型的核心目标是实现一个通用的机器人控制策略，能够处理多种任务、对象和环境。为此，研究团队收集了一个包含 13 台机器人、历时 17 个月、涵盖 700 多个任务、共计 13 万个操作示例的大规模数据集。这些数据包括机器人在实际环境中执行任务的图像序列、自然语言指令和对应的动作序列。\nRT-1 的架构包括以下关键组件：\n图像编码器 ：使用预训练的 EfficientNet-B3 模型处理输入图像，并通过 FiLM 层融合语言指令，提取与任务相关的视觉特征。 TokenLearner 模块 ：对图像特征进行压缩，生成一组紧凑的 token，提高模型的推理效率。 Transformer 模型 ：接收图像和语言的 token 输入，输出离散化的动作 token，控制机器人的执行。 在训练过程中，RT-1 通过模仿学习（Imitation Learning）方法，从收集的示例中学习任务的执行策略。模型能够以每秒 3 次的频率进行闭环控制，直到任务完成或达到预设的时间步数。\n创新点 大规模多任务学习 ：RT-1 在一个包含 13 万个示例、700 多个任务的大规模数据集上进行训练，展示了 Transformer 架构在机器人控制中的强大能力。\n统一的输入输出表示 ：将图像、语言指令和动作统一表示为 token 序列，使得模型能够处理多模态输入，并生成相应的动作输出。\n高效的推理机制：通过 TokenLearner 模块对图像特征进行压缩，显著提高了模型的推理速度，满足实时控制的需求。\n强大的泛化能力 ：RT-1 在未见过的任务、环境和对象上表现出色，展示了其在零样本学习和迁移学习方面的潜力。\n解决的难点 大规模跨任务学习难以整合语言、视觉、动作三模态数据 。\n模型在未见任务和环境上的泛化能力弱 。\n在 130,000+ 真实机器人操作轨迹上训练的 Transformer 模型。\nTokenLearner + FiLM结构实现多模态融合，提升跨任务泛化。\n还需要解决的难点 对新任务的泛化能力有限：尽管 RT-1 在多种任务上表现良好，但在面对完全未见过的任务时，其泛化能力仍有待提升。\n对复杂操作的适应性：当前模型主要针对相对简单的操作任务，对于需要高精度和复杂操作的任务，其性能尚未验证。\n实时性与计算资源的平衡：虽然模型在推理速度上有所优化，但在资源受限的实际部署环境中，如何进一步提高实时性仍是一个挑战。\nOpen-TeleVision: Teleoperation with Immersive Active Visual Feedback 主要论点 Open-TeleVision 系统结合了 VR 设备与机器人控制，允许操作者通过 VR 头显实时感知机器人的立体视觉环境，并将自身的手臂和手部动作映射到机器人上，实现如同“身临其境”的操作体验。\n系统的核心组件包括：\n主动视觉反馈 ：机器人头部配备可动的立体 RGB 摄像头，能够根据操作者的头部运动调整视角，提供实时的第一人称3D 观察。 动作映射机制 ：通过逆运动学（IK）算法和 dex-retargeting 技术，将操作者的手部关键点转换为机器人关节角度，实现精确的动作控制。 远程操作能力 ：系统支持通过互联网进行远程控制，操作者无需与机器人处于同一地点。 在实验中，研究团队使用该系统在两个不同的人形机器人（Unitree H1 和 Fourier GR-1）上完成了包括罐头分类、罐头插入、毛巾折叠和物品卸载等四项长时序、精细操作任务，并成功部署了模仿学习策略。\n创新点 沉浸式第一人称视角：通过主动立体视觉技术，操作者能够以第一人称视角直观感知机器人周围环境，增强了空间感知能力和操作**直觉。\n高精度动作映射：结合逆运动学和 dex-retargeting 技术，实现了操作者动作到机器人动作的高精度映射，支持多指灵巧手的控制。\n远程操作与数据采集 ：系统支持远程操作，操作者可以跨地域控制机器人，并收集高质量的操作数据，促进模仿学习的发展。\n解决的难点 传统远程遥操作缺乏空间沉浸感与操作精度 。\n低质量遥操作数据不利于模仿学习 。\n主动式立体视觉反馈 + VR 映射手部运动实现高保真远程控制。\n为高质量模仿学习数据采集（精细双臂、多指操作）提供有效工具。\n还需要解决的难点 my Questions 如何评估一个机械臂的能力呢？ 高频动作，怎么样算高频呢？\n为什么说机器的精度很重要，到底有多重要呢？\n输出给机器臂的值可以是连续的而不是离散的吗？对应 pi 0\n在不同机器人上训练的难点是？\n为什么需要端到端模型呢？\n机器人上的设备能支持多大的模型运行呢？\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-14/","summary":"Summary of Robotics papers","title":"Bug Journal 2025-05-25"},{"content":" 如何控制机器人的位置？机器人控制算法的输出是什么？是机械臂的位置，速度，加速度的值吗。另外，动作分布 50Hz 的意思是？机械臂的移动是一个分布吗 多模态数据对齐的时候有什么难点，需要网络有什么样的特性呢？ 一般来说泛化能力是如何实现的，加入噪声吗？ 想法：可不可以用视频数据做一个增强：比如用 LLM 总结视频里的手都做了什么，然后以这个总结为 prompt 告诉机械臂要做什么\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-09/","summary":"遇到的问题","title":"Bug Journal 2025-05-09"},{"content":"Citation Content mainly from here: 具身智能基础技术路线.\nPart1: 场景理解 这一部分主要是机器人对环境输入的理解\n希望让机器人识别出环境中比较重要的部分， 比如：要抓起的物件\n检测分割 检测分割的算法如：SAM, Open-VOC Detection 可以比较好地检测和分割输入中需要的部分和其他部分\n多模态分割 多模态大模型可以理解更加复杂的语言，比如“汽车旁的穿蓝色衣服的男人”， 并且可以做到像素级别。\nprompt 的多样化可以让分割更有针对性，模型可以更好地理解和定位\nPart2: 数据收集和引导 视频学习 优点：不需要遥控，数据量更广\n缺点：需要让机器学习视频中的动作，训练难度更高\nVR遥控 优点：可以让人手模拟，并且可以“手把手”教\n缺点：数据量少，需要每一种新情况都需要手动模拟\n环境模拟 优点：环境搭建完成之后就有无限的数据集\n缺点：环境搭建复杂，并且无法模拟特殊情况\n动作执行 生成式模仿学习 把每一个时间点的信息丢进去学习\nDiffusion policy 在每一个时间点之间做 diffusion\nAffordance 输入每一个 region可以被如何操作：比如输入一个瓶子可以被夹起来\n这样的话就不再是low level joint, 而是控制器实现的目标\nQ\u0026amp;A from LLM 将 prompt 拆解，并且可以在文本中提取出机器人最后要做的事情\nLanguage Correction 在人类观察的时候用语言/语音帮助机器人更好地完成任务\n世界模型 预测下一步会发生什么\n我对具身智能的理解 实际上是一个 Agent, 输入是用户的指令以及整个环境。\n输出是对环境做出一些改变，比如让机器人举起一个杯子。\n所以有这些需要做的：识别输入，做出反应，强化学习\nRandom Thoughts 新的LLM生成方式：token -\u0026gt; 大纲 -\u0026gt; 句子 -\u0026gt; 文章 pyramid\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-08/","summary":"初识具身智能","title":"Bug Journal 20250508"},{"content":"Talk 6: scDesign3: Semi-synthetic Negative \u0026amp; Positive Control 数据不够的时候需要一些 simulated data\n那这时候 simlator 就需要 interpretable \u0026amp; realistic (real data characteristic \u0026amp; contains ground truth data)\ne.g. single cell RNA simulator -\u0026gt; 考虑 gene gene correlation -\u0026gt; 考虑别的 cell types and omics -\u0026gt; RNA count to RNA read\n这个可以用来给数据预处理+降噪\nTalk 7: spacial omics data 理解 low-rank property of Hi-C chromatin contact maps.\n数据上有 3 维（question what type of data / or do you have ground truth of the location of the spatial data? are the model predicting the location or the gene expression or both of them? ）\n但是问题是这个数据很 sparce. e.g. 5kb 的数据就已经是丢失了 99.5%的数据了\n所以就用了一些技术来降噪\u0026amp;还原\n之前的 AI模型：训练完了之后就没有用生物相关的信息来预测我们想要的\n现在的 AI 模型，可以自己思考来相处一个 hypothsis，并且验证他\ne.g. 用 interatctions\n要解决的问题：\nblack box ML models, overwhelming hypothesis, false positive\nML model -\u0026gt; intreatcion 可能的值\n现在提出一个预测的可能的可行性\n这个值是 false positive / total accept 值越低越好\n那如何预测 false positive 呢？这样，对于每一个 feature, 分开预测\n然后再liangliang 预测，然后看结合了之后是好了还是坏了\n还有一个办法，就是用元数据 + 假数据\n如果元数据和假数据得出来的差距大，那就说明这个元数据没什么用\n否则就说明这个有用\nIntegration of histology and multi-plex for understanding pancreatic cancer.\nimage are usually 2D, that is not connected to other tissues.=m telling us there is not spatial relationship.\nquestion is every slice a slice near another.\nSpacial transcriptomics platforms (PASTA))\n太稀疏啦\n怎么办呢？用 model concentrate 喽\n但是这些 model 并不一定准（你想，要用 500gene去补 20k,太难了）\n并且这些 model 没有用 cell type / pathway 信息\npathway loss + gene similarity + pathway expression + location (这不就是 STHD吗)\n但是这里预测的不是 cell type,是 gene expression\nTalk 8: AI in medication AI 可以很好的数据，但是要求很高\n所以要用很聪明的方法使用这些 AI\n另外，预测出了结果之后要做什么呢？\nAI 还可以用来提供数据\n生成式 AI VAE \u0026amp; Transformer (需要有 meaningful order/sequence)\nDiffusion model: forward (加噪音) and backward (去噪音) （不需要 data 是 ordered, 但是 sampling 需要很多计算资源）\n如果 black box prediction model 不正确，我应该如何 make a valid inference?\nEnd to end scalable integrative analysis ","permalink":"https://tzj2006.github.io/bugjournal/2025-03-28/","summary":"\u003ch1 id=\"talk-6-scdesign3-semi-synthetic-negative--positive-control\"\u003eTalk 6: scDesign3: Semi-synthetic Negative \u0026amp; Positive Control\u003c/h1\u003e\n\u003cp\u003e数据不够的时候需要一些 simulated data\u003c/p\u003e\n\u003cp\u003e那这时候 simlator 就需要 interpretable \u0026amp; realistic (real data characteristic \u0026amp; contains ground truth data)\u003c/p\u003e\n\u003cp\u003ee.g. single cell RNA simulator -\u0026gt; 考虑 gene gene correlation -\u0026gt; 考虑别的 cell types and omics -\u0026gt; RNA count to RNA read\u003c/p\u003e\n\u003cp\u003e这个可以用来给数据预处理+降噪\u003c/p\u003e\n\u003ch1 id=\"talk-7-spacial-omics-data\"\u003eTalk 7: spacial omics data\u003c/h1\u003e\n\u003cp\u003e理解 low-rank property of Hi-C chromatin contact maps.\u003c/p\u003e\n\u003cp\u003e数据上有 3 维（question what type of data / or do you have ground truth of the location of the spatial data? are the model predicting the location or the gene expression or both of them? ）\u003c/p\u003e","title":"Bug Journal 2025-03-28"},{"content":"MCBIOS Conference Day 1:\nTalk 1: sharing data: https://datacommons.cancer.gov/ 介绍了很多数据：特点是比较多，比较新，并且user-friendly. 还有 NIH founding 可以 use start-up server.\nhttps://computational.cancer.gov/ 介绍了很多模型：都是用来预处理数据的\n比如有一个 AI-based toolbox (类似scanpy) 可以预处理所有数据\n还有 Automated Data Collection\nlink TBD\nLLM 翻译诊断结果\nTalk 2: Write code with Github Copilot. 你能用这个干嘛 当然是写代码啦，还能干嘛（\n好处是可以直接在你的 IDE 里面生成(虽然现在 ChatGPT.app 也可以了，(反正都是一家的[doge])\n比如说你可以先写一段注释来让 Copilot 生成你想要的代码\n然后选中这段代码并且点击旁边的小星星来让 Copilot 更改这段代码\n用于重复的项目效果更佳。比如分离数据集\n修bug还挺好用的(虽然有时候越修越多[doge])\n修改代码的语言：比如把 R code 换成 python code.\n还可以 explain what the code is doing (by using /explain).\n用 /doc 来写注释\n要不要用这个 想用就用，只是这个效果不一定好罢了[doge]\nTalk 3: Graph is a link between spatial omics applications and pixel graph | cell graph | spot graph\n有 graph 就有 matrix\nfrequency 的大小决定了是否 pattern（？）\nspatial variable gene thereshold\nTalk 4: Some random ideas 结合两个 LLM, e.g. Gemini \u0026amp; ChatGPT\n在小程序中内置一个 LLM 让它当 agent.\nTalk 5: scPerb style vector 可以说是一个 noise, 然后我们希望一个 neural network 能学习到这个 noise. 并且这个学习的过程是 cell type specific 的。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-03-27/","summary":"\u003cp\u003eMCBIOS Conference Day 1:\u003c/p\u003e\n\u003ch1 id=\"talk-1-sharing-data\"\u003eTalk 1: sharing data:\u003c/h1\u003e\n\u003ch2 id=\"httpsdatacommonscancergov\"\u003e\u003ca href=\"https://datacommons.cancer.gov/\"\u003ehttps://datacommons.cancer.gov/\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e介绍了很多数据：特点是比较多，比较新，并且user-friendly. 还有 NIH founding 可以 use start-up server.\u003c/p\u003e\n\u003ch2 id=\"httpscomputationalcancergov\"\u003e\u003ca href=\"https://computational.cancer.gov/\"\u003ehttps://computational.cancer.gov/\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e介绍了很多模型：都是用来预处理数据的\u003c/p\u003e\n\u003cp\u003e比如有一个 AI-based toolbox (类似scanpy) 可以预处理所有数据\u003c/p\u003e\n\u003cp\u003e还有 Automated Data Collection\u003c/p\u003e\n\u003cp\u003elink TBD\u003c/p\u003e\n\u003cp\u003eLLM 翻译诊断结果\u003c/p\u003e\n\u003ch1 id=\"talk-2-write-code-with-github-copilot\"\u003eTalk 2: Write code with Github Copilot.\u003c/h1\u003e\n\u003ch2 id=\"你能用这个干嘛\"\u003e你能用这个干嘛\u003c/h2\u003e\n\u003cp\u003e当然是写代码啦，还能干嘛（\u003c/p\u003e\n\u003cp\u003e好处是可以直接在你的 IDE 里面生成(虽然现在 ChatGPT.app 也可以了，(反正都是一家的[doge])\u003c/p\u003e\n\u003cp\u003e比如说你可以先写一段注释来让 Copilot 生成你想要的代码\u003c/p\u003e\n\u003cp\u003e然后选中这段代码并且点击旁边的小星星来让 Copilot 更改这段代码\u003c/p\u003e\n\u003cp\u003e用于重复的项目效果更佳。比如分离数据集\u003c/p\u003e\n\u003cp\u003e修bug还挺好用的(虽然有时候越修越多[doge])\u003c/p\u003e\n\u003cp\u003e修改代码的语言：比如把 R code 换成 python code.\u003c/p\u003e\n\u003cp\u003e还可以 explain what the code is doing (by using /explain).\u003c/p\u003e","title":"Bug Journal 2025-03-27"},{"content":"Today\u0026rsquo;s Problem https://leetcode.com/problems/check-if-grid-can-be-cut-into-sections\nIntuition This question asks about merging sections. In this case, if we smash it into 1D array, it just means \u0026ldquo;Is there more than two gaps inside the section?\u0026rdquo;\nApproach Therefore, we can sort the list, and then iterate the whole list and see whether there is a gap between the section we already iterated and the new section. If there is, then add 1, else, merge this new section to our old section. The thing is that you need to do it twice.\nComplexity Time complexity: $O(N)$, N is the length of rectangles. Space complexity: $O(1)$. Code class Solution: def checkValidCuts(self, n: int, rectangles: List[List[int]]) -\u0026gt; bool: N = len(rectangles) def get_res(a,b): rectangles.sort(key = lambda x: (x[a], x[b])) gapCnt,maxPos,l = 0,1,0 while(l \u0026lt; N): while(l \u0026lt; N and rectangles[l][a] \u0026lt; maxPos): maxPos = max(maxPos, rectangles[l][b]) l += 1 if l == N: break else: gapCnt += 1 maxPos = rectangles[l][b] # print(a,l) if gapCnt \u0026gt; 1: return True return False return get_res(0,2) or get_res(1,3) ","permalink":"https://tzj2006.github.io/leetcode/2025-03-25/","summary":"\u003col start=\"3394\"\u003e\n\u003cli\u003eCheck if Grid can be Cut into Sections\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-03-25"},{"content":"Intuition In this question, we find that if we flip a coin twice, then it is the same as flipping it zero times. Moreover, the only three ways to flip a coin is: Flip this coin Flip the coin before Flip the coin before this coin This means that if we pass this coin, we can no longer flip this coin. Approach In this case, we can iterate from front to end and flip every coin that is 0, and check whether the whole array is 1 at the end. Why is this method correct then? First we know that we cannot flip a coin after we pass this coin.\nThis means that we must flip this coin. If we do not flip this coin, then this coin will remain 0, which does not satisfy the quetion. Therefore, this step is required, missing this step will not give us the array we need.\nWe can also prove that this will lead us to the result for every array that can achieve this step. Because you have to flip this coin no matter what operation you did.\nComplexity Time complexity: $O(N)$, N is the length of the array. Space complexity: $O(1)$. Code class Solution: def minOperations(self, nums: List[int]) -\u0026gt; int: cnt = 0 for i in range(len(nums) - 2): if nums[i] == 0: cnt += 1 nums[i] = 1 nums[i + 1] = 1 - nums[i + 1] nums[i + 2] = 1 - nums[i + 2] if nums[-1] == 1 and nums[-2] == 1: return cnt return -1 ","permalink":"https://tzj2006.github.io/leetcode/2025-03-19/","summary":"\u003col start=\"3191\"\u003e\n\u003cli\u003eMinimum Operations to Make Binary Array Elements Equal to One I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-03-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-punishment-number-of-an-integer/description/\nIntuition Note that all the combinations of a number that is less than $1000^2$ is $2^5 = 32$. Which means that if we use a dfs to decide whether to choose to break from one interval or not cost at most 32 for one number.\nSince we only have $N \\leq 1000$, we can solve it with brute method.\nApproach Use a dfs to check whether it cawn be a punishment number or not.\nComplexity Time complexity: $O(N\\times 2^{log(N)})$, N is the same representation as the description.\nSpace complexity: $O(1)$.\nCode class Solution: def punishmentNumber(self, n: int) -\u0026gt; int: def check(x, now, s, nows, cnt): if now == 0: return (s + nows) == x if s \u0026gt; x: return False flag = check(x, now // 10, s, nows + now % 10 * (10 ** cnt), cnt + 1) if flag: return True flag |= check(x, now // 10, s + nows, now % 10, 1) return flag ans = 0 for i in range(n + 1): if check(i, i*i, 0, 0, 0): ans += i * i return ans class Solution: def punishmentNumber(self, n: int) -\u0026gt; int: punishmentNumbers = [0, 1, 9, 10, 36, 45, 55, 82, 91, 99, 100, 235, 297, 369, 370, 379, 414, 657, 675, 703, 756, 792, 909, 918, 945, 964, 990, 991, 999, 1000] ans = 0 for x in punishmentNumbers: if x \u0026gt; n: return ans ans += x * x return ans Result of normal solution: Result of fastest solution: ","permalink":"https://tzj2006.github.io/leetcode/2025-02-15/","summary":"\u003col start=\"2698\"\u003e\n\u003cli\u003eFind the Punishment Number of an Integer\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-15"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/product-of-the-last-k-numbers/description/\nIntuition Situation 1: if there is no 0: In this case, we can just use a prefix multiplication to do what the question asks. Situation 2: if there is 0: Then it would be 0! Therefore, all we need is to just check whether there is a 0 in the last k numbers of the stream. If yes, them just return 0.\nApproach Use an array to store the multiplication prefix. Check whether there is a zero in the last k streams. Complexity Time complexity: $O(Q)$, Q means the number of operations.\nSpace complexity: $O(Q)$, Q means the number of operations.\nCode class ProductOfNumbers: def __init__(self): self.q = [] self.mul = 1 def add(self, num: int) -\u0026gt; None: self.mul *= num self.q.append(self.mul) if num == 0: self.q = [] self.mul = 1 def getProduct(self, k: int) -\u0026gt; int: if k \u0026gt; len(self.q): return 0 if k == len(self.q): return self.mul return self.mul // self.q[-k - 1] # Your ProductOfNumbers object will be instantiated and called as such: # obj = ProductOfNumbers() # obj.add(num) # param_2 = obj.getProduct(k) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-14/","summary":"\u003col start=\"1352\"\u003e\n\u003cli\u003eProduct of the Last K Numbers\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-14"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-operations-to-exceed-threshold-value-ii/description\nIntuition Do what the question ask using heap.\nApproach Get the minimum two from the list using heap. Put back value $2 * min(x, y) + max(x, y)$ to the heap. If the value you get is all larger or equal to k, then it is all done. Complexity Time complexity: $O(N\\times log(N))$, N is the length of the sequence.\nSpace complexity: $O(1)$, by using heapify, there is no external storage.\nCode class Solution: def minOperations(self, nums: List[int], k: int) -\u0026gt; int: ans = 0 heapify(nums) x = heappop(nums) while(len(nums) \u0026gt; 0 and x \u0026lt; k): y = heappop(nums) nx = min(x, y) * 2 + max(x, y) heappush(nums, nx) x = heappop(nums) ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-13/","summary":"\u003col start=\"3066\"\u003e\n\u003cli\u003eMinimum Operations to Exceed Threshold Value II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-13"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/max-sum-of-a-pair-with-equal-sum-of-digits/description\nIntuition In this question, we can simulate the process described in the question, and then get the answer.\nApproach First write a count function that counts the sum of every integer. Use a dictionary to store the top two values. Compare the current number with two numbers stored in the dictionary. Update ans (initialized by -1), note that we need at least two numbers in the dictionary before we can update the answer. Complexity Time complexity: $O(Nlog(M))$, N is the length of the sequence, M is the maximum number.\nSpace complexity: $O(N)$.\nCode class Solution: def maximumSum(self, nums: List[int]) -\u0026gt; int: def cnt(x): res = 0 while(x \u0026gt; 0): res += x % 10 x //= 10 return res dic = dict() ans = -1 for x in nums: nx = cnt(x) if nx in dic: if x \u0026gt; dic[nx][0]: dic[nx][1] = dic[nx][0] dic[nx][0] = x if dic[nx][1] \u0026gt; 0: ans = max(ans, dic[nx][0] + dic[nx][1]) elif x \u0026gt; dic[nx][1]: dic[nx][1] = x ans = max(ans, dic[nx][0] + dic[nx][1]) else: dic.update({nx: [x, 0]}) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-12/","summary":"\u003col start=\"2342\"\u003e\n\u003cli\u003eMax Sum of a Pair With Equal Sum of Digits\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/remove-all-occurrences-of-a-substring/description/\nIntuition In this case, we need to delete all the occurance of \u0026ldquo;part\u0026rdquo; in \u0026ldquo;s\u0026rdquo;. Therefore, we can check whether \u0026ldquo;s\u0026rdquo; contains \u0026ldquo;part\u0026rdquo;. Then we can delete it from the string.\nApproach Method 1. You can use a stack to do that. When you detect that your stack input a string that is the same to \u0026ldquo;part\u0026rdquo;, then we can delete the string from the stack.\nMethod 2. You can use the python function to find and delete \u0026ldquo;part\u0026rdquo; from the original string S.\nComplexity Time complexity: $O(N\\times M)$, N is the length of s, M is the length of part.\nSpace complexity: $O(1)$\nCode class Solution: def removeOccurrences(self, s: str, part: str) -\u0026gt; str: st = [] N = len(part) for ch in s: st.append(ch) if len(st) \u0026gt;= N: flag = True for i in range(1, N + 1): if st[-i] != part[-i]: print(st[-i], part[-i]) flag = False break if flag: for i in range(N): st.pop() return \u0026#39;\u0026#39;.join(st) Real Python class Solution: def removeOccurrences(self, s: str, part: str) -\u0026gt; str: while part in s: s = s.replace(part,\u0026#34;\u0026#34;,1) return s ","permalink":"https://tzj2006.github.io/leetcode/2025-02-11/","summary":"\u003col start=\"1910\"\u003e\n\u003cli\u003eRemove All Occurrences of a Substring\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-11"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/clear-digits/description/\nIntuition We need to pop the character before the digit in this question.\nApproach Therefore, all we need is just to utilize a stack.\nComplexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, N is the length of the string.\nCode class Solution: def clearDigits(self, s: str) -\u0026gt; str: st = [] for ch in s: if \u0026#39;0\u0026#39; \u0026lt;= ch and ch \u0026lt;= \u0026#39;9\u0026#39;: st.pop() else: st.append(ch) return \u0026#39;\u0026#39;.join(st) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-10/","summary":"\u003col start=\"3174\"\u003e\n\u003cli\u003eClear Digits\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-10"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-number-of-bad-pairs/description/\nIntuition In this question, we find that we need to find the pairs that has the same distance as the difference of nums.\nIn this case, if we distract the distance between these two numbers, then they would be the same.\nThat is: $j - i = nums[j] - nums[i] \\Longrightarrow nums[j] - j = nums[i] = i$.\nThen the question would be easy: we just subtract the index of every number in the list, and then found how many pairs of i,j in the nums array that has the same number.\nWe then subtract these counts from the total counts of answer.\nApproach Count all pairs of i,j; that is, $N \\times (N - 1)$, N is the length of the array. Subtract all the nums[i] by i. Count how many pairs of i,j has the same number. Subtract these i,j pairs from the original answer. Complexity Time complexity: $O(N)$. N is the length of the array.\nSpace complexity: $O(N)$. N is the length of the array.\nCode class Solution: def countBadPairs(self, nums: List[int]) -\u0026gt; int: nums = [nums[i] - i for i in range(len(nums))] cnt = Counter(nums) N = len(nums) ans = N * (N - 1) // 2 for v in cnt.values(): ans -= v * (v - 1) // 2 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-09/","summary":"\u003col start=\"2364\"\u003e\n\u003cli\u003eCount Number of Bad Pairs\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-09"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/design-a-number-container-system/description/\nIntuition In this question, we need to find the smallest index of the number in the list. Note that the index may change when dealing with the list. Because we need the smallest index, so we need a sorted datastructure. Trick: We can get a lazy tag that stores the number of each index, so that we can pop only when we find our current answer does not fullfill the requirement.\nApproach Store a dictionary that stores the sorted sequence of the numbers and the index. Store a dictionary of index and numbers pair. Check whether the answer is valid in the find function. Complexity Time complexity: $O(Q\\ times log(N))$, Q is the time of query, N is the size of the dictionary.\nSpace complexity: $O(Q)$, Q is the time of query.\nCode class NumberContainers: def __init__(self): self.lst = dict() self.idx = dict() def change(self, index: int, number: int) -\u0026gt; None: if number not in self.lst: self.lst.update({number: []}) heappush(self.lst[number], index) self.idx.update({index: number}) def find(self, number: int) -\u0026gt; int: if number not in self.lst: return -1 while self.lst[number]: currIndex = self.lst[number][0] if self.idx[currIndex] != number: heappop(self.lst[number]) else: return currIndex return -1 # Your NumberContainers object will be instantiated and called as such: # obj = NumberContainers() # obj.change(index,number) # param_2 = obj.find(number) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-08/","summary":"\u003col start=\"2349\"\u003e\n\u003cli\u003eDesign a Number Container System\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-number-of-distinct-colors-among-the-balls/description\nIntuition All we need to know is how many colors left in the list. Then we can store how many balls do one color have. If there are 0 balls left, then col_cnt -= 1. If there appears a new color, then col_cnt += 1.\nApproach Store a buket for every ball and every color. Change the color of one ball. If there are 0 balls left, then col_cnt -= 1. If there appears a new color, then col_cnt += 1. Complexity Time complexity: $O(N)$, N is the length of the query.\nSpace complexity: $O(N)$, N is the length of the query.\nPotential follow up question Now I want to change the color of a section? For example, now the imput change into (x,y,z), changing the color of the balls from x to y (inclusive) to z. Then tell me how many balls have distinct colors?\nCode class Solution: def queryResults(self, limit: int, queries: List[List[int]]) -\u0026gt; List[int]: col = 0 ans = [] visCol = dict() balCol = dict() for (x, y) in queries: if x in balCol: visCol[balCol[x]] -= 1 if visCol[balCol[x]] == 0: col -= 1 balCol.update({x: y}) if (y not in visCol) or (visCol[y] == 0): col += 1 visCol.update({y: 1}) else: visCol[y] += 1 ans.append(col) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-07/","summary":"\u003col start=\"3160\"\u003e\n\u003cli\u003eFind the Number of Distinct Colors Among the Balls\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-07"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/tuple-with-same-product/description/\nIntuition In this question, all we need to do is to find the tuple that the numbers in the tuple has the same multiplication value. For example, if there are n pairs of numbers that has the same multiplication value, then the result would be $(^2_n) \\times 4$. This is because we can pick any two pair from the set and form a tuple.\nThe question was, why can you prove that this two pairs are distinct?\nThis is because the original array is distinct. This means that there are no duplicated numbers in the original array. Therefore, if $a \\times b = c \\times d$, we know that $a \\ne c$, then we can now that $b \\ne d$.\nApproach Therefore, all we need to do is to iterate through the whole list and find all tuples that has the same multiplication.\nTrick Among four dictionaries, defaultdict, Counter, dict, and {}, dict has the fastest speed.\nComplexity Time complexity: $O(N ^ 2)$, N is the length of the array.\nSpace complexity: $O(N)$, we need to store the whole array.\nCode class Solution: def tupleSameProduct(self, nums: List[int]) -\u0026gt; int: nums.sort() cnt = dict() N = len(nums) for i in range(N): for j in range(i + 1, N): tmp = nums[i] * nums[j] if tmp not in cnt: cnt[tmp] = 1 else: cnt[tmp] += 1 ans = 0 for v in cnt.values(): ans += 4 * (v) * (v - 1) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-06/","summary":"\u003col start=\"1726\"\u003e\n\u003cli\u003eTuple with Same Product\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-06"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-one-string-swap-can-make-strings-equal/description\nIntuition Do what the question ask.\nApproach Do what the question ask.\nCheck that whether there are 0 or exactly 2 position that is different. Check that whether swapping can solve the question. Complexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(1)$.\nCode class Solution: def areAlmostEqual(self, s1: str, s2: str) -\u0026gt; bool: fst = -1 sec = -1 for i in range(len(s1)): if s1[i] != s2[i]: if fst == -1: fst = i elif sec == -1: sec = i else: return False if fst == -1 and sec == -1: return True if fst == -1 or sec == -1: return False if s1[fst] == s2[sec] and s1[sec] == s2[fst]: return True return False ","permalink":"https://tzj2006.github.io/leetcode/2025-02-05/","summary":"\u003col start=\"1790\"\u003e\n\u003cli\u003eCheck if One String Swap Can Make Strings Equal\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-05"},{"content":". \u0026ndash;\u0026gt; find all characters except \u0026lsquo;\\n\u0026rsquo;.\n^ \u0026ndash;\u0026gt; find all the start of the string\n$ \u0026ndash;\u0026gt; find all the end of the string.\n[\u0026hellip;] find a character inside the [].\n[^] find characters not inside the [].\n\\. find the special characters.\n\\d all numbers \u0026lt;=\u0026gt; [0-9]\n\\D all not numbers \u0026lt;=\u0026gt; [^0-9]\n\\s all space characters\n\\S all not space characters\n\\w all letter, num, and . [a-z-A-Z0-9].\n\\W all not letter num and _.\n\\b find side.\nfind find at least once ? find 0 times or once.\n{n} exact n times.\n{n,} at least n times.\n{n, m} at least n times, at most m times.\n| mean or.\n(\u0026hellip;) find all.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-02-04/","summary":"\u003cp\u003e. \u0026ndash;\u0026gt; find all characters except \u0026lsquo;\\n\u0026rsquo;.\u003c/p\u003e\n\u003cp\u003e^ \u0026ndash;\u0026gt; find all the start of the string\u003c/p\u003e\n\u003cp\u003e$ \u0026ndash;\u0026gt; find all the end of the string.\u003c/p\u003e\n\u003cp\u003e[\u0026hellip;] find a character inside the [].\u003c/p\u003e\n\u003cp\u003e[^] find characters not inside the [].\u003c/p\u003e\n\u003cp\u003e\\. find the special characters.\u003c/p\u003e\n\u003cp\u003e\\d all numbers \u0026lt;=\u0026gt; [0-9]\u003c/p\u003e\n\u003cp\u003e\\D all not numbers \u0026lt;=\u0026gt; [^0-9]\u003c/p\u003e\n\u003cp\u003e\\s all space characters\u003c/p\u003e\n\u003cp\u003e\\S all not space characters\u003c/p\u003e\n\u003cp\u003e\\w all letter, num, and \u003cem\u003e. [a-z-A-Z0-9\u003c/em\u003e].\u003c/p\u003e","title":"Bug Journal 2025-02-04"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-ascending-subarray-sum/\nIntuition Same as the problem yesterday, the only difference is changing count to sum.\nApproach Same as the problem yesterday, the only difference is changing count to sum.\nComplexity Time complexity: $O(N)$, N is the length of the array.\nSpace complexity: $O(1)$\nCode class Solution: def maxAscendingSum(self, nums: List[int]) -\u0026gt; int: ans, tmp, pre = nums[0], nums[0], nums[0] for num in nums[1::]: if num \u0026gt; pre: tmp += num else: ans = max(ans, tmp) tmp = num pre = num return max(tmp, ans) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-04/","summary":"\u003col start=\"1800\"\u003e\n\u003cli\u003eMaximum Ascending Subarray Sum\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-04"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/longest-strictly-increasing-or-strictly-decreasing-subarray/\nIntuition Do what the question ask.\nApproach Do what the question ask.\nComplexity Time complexity: $O(N)$, N is the length of the array.\nSpace complexity: $O(1)$.\nCode class Solution: def longestMonotonicSubarray(self, nums: List[int]) -\u0026gt; int: cntI, cntD = 1,1 ans = 1 pre = nums[0] for x in nums[1::]: if x \u0026gt; pre: cntI += 1 ans = max(ans, cntD) cntD = 1 elif x \u0026lt; pre: cntD += 1 ans = max(ans, cntI) cntI = 1 else: ans = max(ans, cntI, cntD) cntI = 1 cntD = 1 pre = x return max(ans, cntI, cntD) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-03/","summary":"\u003col start=\"3105\"\u003e\n\u003cli\u003eLongest Strictly Increasing or Strictly Decreasing Subarray\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-03"},{"content":"Why not use Apple to run LLM: https://github.com/user-attachments/assets/e03bd9e6-0174-44b0-8c99-9a1ab88eeef2\n","permalink":"https://tzj2006.github.io/bugjournal/2025-02-02/","summary":"Speed of M4 Pro","title":"Bug Journal 2025-02-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-array-is-sorted-and-rotated/description/\nIntuition If it is shifted, then it must contain a full original list if we concate two nums together.\nTrick Append the original list to the back of itself it is somehow a ring.\nApproach Concat nums at the end of itself. Then test whether there are at least N non decreasing numbers. Complexity Time complexity: $O(N)$, N is the length of the array\nSpace complexity: $O(1)$\nCode class Solution: def check(self, nums: List[int]) -\u0026gt; bool: n = len(nums) nums += nums cnt = 1 pre = nums[0] for num in nums[1::]: if num \u0026gt;= pre: cnt += 1 else: cnt = 1 pre = num if cnt \u0026gt;= n: return True return False ","permalink":"https://tzj2006.github.io/leetcode/2025-02-02/","summary":"\u003col start=\"1752\"\u003e\n\u003cli\u003eCheck if Array Is Sorted and Rotated\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/divide-nodes-into-the-maximum-number-of-groups/description/\nIntuition The key of this question is $|y - x| = 1$. We know that for a step further, the parity must change. Therefore, when we encounter a case when the parity has a problem (for example, a loop with three nodes and three edges), we would return -1. Otherwise, all we can do is to iterate the starting point of the graph to find which point is the best starting point.\nTherefore, we can have our approach:\nWe can use a dfs to find whether there is a parity issue or not. We can use a bfs to find the starting point. Questions: why using bfs to find the starting point?\nThis is because we need to add a point to the next group if it has an edge connecting the current point and the next point.\nApproach We can use a dfs to find whether there is a parity issue or not. We can use a bfs to find the starting point. Complexity Time complexity: $O(N^2)$\nSpace complexity: $O(N^2)$\nCode class Solution: def magnificentSets(self, n: int, edges: List[List[int]]) -\u0026gt; int: vis = [0] * (n + 1) bvis = [0] * (n + 1) e = [[] for _ in range(n + 1)] for x,y in edges: e[x].append(y) e[y].append(x) ans = 0 clock = 0 def bfs(x): nonlocal clock clock += 1 bvis[x] = clock q = deque([(x,1)]) res = 1 while(len(q) \u0026gt; 0): now, dis = q.popleft() res = max(res, dis) for to in e[now]: if bvis[to] == clock: continue bvis[to] = clock q.append((to, dis + 1)) return res cur = 0 def dfs(x): nonlocal cur cur = max(cur, bfs(x)) # print(cur) tmp = 0 for to in e[x]: if vis[to] == 0: if vis[x] == 0: print(\u0026#34;Warning!\u0026#34;, x) vis[to] = -vis[x] tmp += dfs(to) else: if vis[to] != -vis[x]: return -1 return tmp for i in range(1, n + 1): if vis[i] == 0: cur = 0 vis[i] = 1 if dfs(i) \u0026lt; 0: return -1 # print(\u0026#34;out: \u0026#34;, cur) ans += cur # print(bfs(5)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-30/","summary":"\u003col start=\"2493\"\u003e\n\u003cli\u003eDivide Nodes Into the Maximum Number of Groups\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-30"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/special-array-i/description/\nIntuition Do what the question ask.\nApproach Iterate through the array, then check whether the two number has the are both odd or even or not.\nComplexity Time complexity: $O(N)$\nSpace complexity: $O(1)$\nCode class Solution: def isArraySpecial(self, nums: List[int]) -\u0026gt; bool: for i in range(1, len(nums)): if (nums[i] - nums[i-1]) % 2 == 0: return False return True ","permalink":"https://tzj2006.github.io/leetcode/2025-02-01/","summary":"\u003col start=\"3151\"\u003e\n\u003cli\u003eSpecial Array I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-01"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/making-a-large-island/description/\nIntuition In this problem, we need to flip a 0 into a one to calculate the largest connected block. Now, if we can calculate the size of the connected block in the 4 directions of a 0 in the grid, then we just need to add them up and we are all set.\nApproach First we need to calculate the size of each connected block and give a label to each connected block so that we are not adding the same connected block twice.\nThen we iterate all the 0, flip its result is the sum of the unique connected blocks around it in 4 directions.\nComplexity Time complexity: $O(N^2)$\nSpace complexity: $O(N^2)$\nCode class Solution: def largestIsland(self, grid: List[List[int]]) -\u0026gt; int: islandCount = [0,0] dx = [0,0,1,-1] dy = [1,-1,0,0] n = len(grid) m = len(grid[0]) def dfs(x, y, cnt): grid[x][y] = cnt islandCount[cnt] += 1 for i in range(4): nx = x + dx[i] ny = y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or grid[nx][ny] != 1: continue dfs(nx, ny, cnt) cnt = 1 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 1: cnt += 1 islandCount.append(0) dfs(i, j, cnt) ans = max(islandCount) for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 0: tmp = 1 vis = set([]) for k in range(4): nx = i + dx[k] ny = j + dy[k] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or grid[nx][ny] in vis: continue tmp += islandCount[grid[nx][ny]] vis.add(grid[nx][ny]) ans = max(ans, tmp) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-31/","summary":"\u003col start=\"827\"\u003e\n\u003cli\u003eMaking A Large Island\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-31"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/redundant-connection/description/\nIntuition Note that in a ring, every node have more than one index. Therefore, if we delete all the index that has one index, the remaining points will be a ring.\nApproach Note that the label starts from 1 and ends at n, you would like to decrease index by one.\nComplexity Time complexity: $O(N)$\nSpace complexity: $O(N)$\nCode class Solution: def findRedundantConnection(self, edges: List[List[int]]) -\u0026gt; List[int]: N = len(edges) du = [0] * (N) E = [[] for _ in range(N)] for x,y in edges: x -= 1 y -= 1 du[x] += 1 du[y] += 1 E[x].append(y) E[y].append(x) q = deque([]) for i in range(N): if du[i] == 1: q.append(i) # print(du) while(len(q) \u0026gt; 0): x = q.popleft() du[x] = 0 for to in E[x]: if du[to] \u0026gt; 0: du[to] -= 1 if du[to] == 1: q.append(to) # print(du) loop = set([]) for i in range(N): if du[i] \u0026gt; 0: loop.add(i) for i in range(N - 1, -1, -1): x,y = edges[i] if x - 1 in loop and y - 1 in loop: return [x, y] return None ","permalink":"https://tzj2006.github.io/leetcode/2025-01-29/","summary":"\u003col start=\"684\"\u003e\n\u003cli\u003eRedundant Connection\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-29"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-number-of-fish-in-a-grid/description/\nIntuition Find the size of the connected blocks.\nApproach Iterate through the grid to add the size to a connected block, then find the maximum size of the connected block.\nComplexity Time complexity: $O(N\\times M)$, we will visit every point exactly once.\nSpace complexity: $O(1)$, if you do not count the original grid.\nCode class Solution: def findMaxFish(self, grid: List[List[int]]) -\u0026gt; int: ans = 0 dx = [0,0,1,-1] dy = [1,-1,0,0] for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 0: continue tmp = grid[i][j] grid[i][j] = 0 q = deque([(i,j)]) while len(q) \u0026gt; 0: x,y = q.popleft() for k in range(4): nx = x + dx[k] ny = y + dy[k] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= len(grid) or ny \u0026gt;= len(grid[0]) or grid[nx][ny] == 0: continue tmp += grid[nx][ny] grid[nx][ny] = 0 # print(i,j,nx, ny, tmp) q.append((nx, ny)) ans = max(ans, tmp) print(grid) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-28/","summary":"\u003col start=\"2658\"\u003e\n\u003cli\u003eMaximum Number of Fish in a Grid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-28"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/course-schedule-iv/description/\nIntuition In this problem, we need to find whether a point is the father of another point or not. In this case, we can simply use one set to store all the fathers of a point and path that set to all its children.\nApproach Use a dfs to iterate all the points. Create a set for every point, then pass it to its children. Complexity Time complexity: $O(N^3 + Q)$. We would at visit each edge at most once. The passing of a set is $O(N)$. So the final time complexity would be $O(N^3 + Q)$.\nSpace complexity: $O(N^2) + Q$. We need to store a set for every point and we also need to store the answer.\nCode class Solution: def checkIfPrerequisite(self, numCourses: int, prerequisites: List[List[int]], queries: List[List[int]]) -\u0026gt; List[bool]: edges = [[] for _ in range(numCourses)] prereq = [set([_]) for _ in range(numCourses)] for x, y in prerequisites: edges[y].append(x) def dfs(x): for to in edges[x]: if len(prereq[to]) == 1: dfs(to) prereq[x] = prereq[x] | prereq[to] for i in range(numCourses): dfs(i) ans = [] for x, y in queries: if x in prereq[y]: ans.append(True) else: ans.append(False) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-27/","summary":"\u003col start=\"1462\"\u003e\n\u003cli\u003eCourse Schedule IV\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-27"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-employees-to-be-invited-to-a-meeting/description/\nIntuition Here we have a directed graph with a ring. To get the ring, we can use topological sort. That is, to find the point which has index 0, and delete the point and its corresponding point.\nBy doing so, the remaining points in the graph are all in a ring.\nIn this problem, there are two ways to put everyone in a seat:\na ring where everyone has its favorite person on his left hand site. when two person are each other\u0026rsquo;s favorite person, they themselves can form a complete ring while people who like them can form a list that points to them. Such structure is special because everyone can find his favorite person without forming a complete ring. Therefore, there could be multiple structures in the room. Approach Use topological sort to find all the rings in the graph. Find all the special case when two people are each others\u0026rsquo; favorite. Return the max size of a ring or return the max size of that multiple structures. Complexity Time complexity: $O(N)$\nSpace complexity: $O(N)$\nCode class Solution: def maximumInvitations(self, favorite: List[int]) -\u0026gt; int: N = len(favorite) du = [0] * N l = [1] * N for x in favorite: du[x] += 1 q = deque([]) for i in range(N): if du[i] == 0: q.append((i, 1)) while(len(q) \u0026gt; 0): x, leng = q.popleft() to = favorite[x] du[to] -= 1 l[to] = max(l[to], leng + 1) if du[to] == 0: q.append((to, leng + 1)) vis = [0] * N def dfs(i): to = favorite[i] vis[i] = 2 if vis[to] == 2: return 1 return dfs(to) + 1 ans = 0 res = 0 for i in range(N): if du[i] != 0 and vis[i] == 0: tmp = dfs(i) # print(i, tmp) if tmp == 2: # print(i, favorite[i], l[i], l[favorite[i]]) res += l[i] + l[favorite[i]] else: ans = max(ans, tmp) return max(ans, res) ","permalink":"https://tzj2006.github.io/leetcode/2025-01-26/","summary":"\u003col start=\"2127\"\u003e\n\u003cli\u003eMaximum Employees to Be Invited to a Meeting\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-26"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/make-lexicographically-smallest-array-by-swapping-elements/description/\nIntuition In this question, we are doing a sorting process that only difference less than limit can swap. Therefore, there forms \u0026ldquo;groups\u0026rdquo;. In a group, two sequential number has a difference less than limit. In this case, if we sort the numbers in a group, then we are all done. Approach We need to get the groups. We need to sort the array first. Then if the difference between two numbers are bigger than limit, then it would belong to two different groups. Then we sort the result for each group. Complexity Time complexity: $O(N\\times log(N))$, N is the length of the array.\nSpace complexity: $O(N)$\nCode class Solution: def lexicographicallySmallestArray(self, nums: List[int], limit: int) -\u0026gt; List[int]: sorted_nums = [] for idx, x in enumerate(nums): sorted_nums.append((x, idx)) sorted_nums.sort() # First we sort the array groups = [] tmp = [] for i in range(len(nums)): if i \u0026gt; 0 and sorted_nums[i][0] - sorted_nums[i-1][0] \u0026gt; limit: tmp.sort() groups.append(tmp) tmp = [] tmp.append(sorted_nums[i][1]) tmp.sort() groups.append(tmp) # Then we form groups idx = 0 pos = 0 ans = [0] * len(nums) for i in range(len(nums)): if pos == len(groups[idx]): pos = 0 idx += 1 ans[groups[idx][pos]] = sorted_nums[i][0] pos += 1 # Then we sort the groups return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-25/","summary":"\u003col start=\"2948\"\u003e\n\u003cli\u003eMake Lexicographically Smallest Array by Swapping Elements\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-25"},{"content":"Some times we want to install a jupyter kernel for our server. Then we would like to use ipykernel\npip install ipykernel ipython kernel install --user --name= To know the PID of a jupyter notebook, you can use:\nimport os os.getpid() ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-24/","summary":"ipykernel, PID","title":"Bug Journal 2025-01-24"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-eventual-safe-states/description/\nNote Outgoing edges means any edge of this point, even if this point connects to itself.\nIntuition The only points that are terminate are the points that have no edges.\nThen we have to find which point connect to these terminate points.\nTherefore, we can use recursive (or any LIFO algorithms) to solve this question.\nApproach Use a DFS. If we find a point that has no outgoing edges, then its a terminate point. If we find a self-loop, all points in the loop are not safety. If a point only connects to safty points, then it is safety. Complexity Time complexity: $O(N)$, all points will be visited only once. Space complexity: $O(N)$. Code class Solution: def eventualSafeNodes(self, graph: List[List[int]]) -\u0026gt; List[int]: n = len(graph) safety = [-1] * n vis = [0] * n ans = [] def dfs(x): if safety[x] != -1: return safety[x] if vis[x] == 1: safety[x] = 0 return 0 vis[x] = 1 if len(graph[x]) == 0: safety[x] = 1 return 1 res = 0 for to in graph[x]: res += dfs(to) if res == len(graph[x]): safety[x] = 1 else: safety[x] = 0 return safety[x] for i in range(n): if(dfs(i) == 1): ans.append(i) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-24/","summary":"\u003col start=\"802\"\u003e\n\u003cli\u003eFind Eventual Safe States\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-24"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-servers-that-communicate/\nIntuition Do what the question ask.\nApproach First count the number of computers in each row and each column. Then count whether a computer has another computer that has the same row or column with it.\nComplexity Time complexity: $O(NM)$, N, M, are the length and the width of the grid.\nSpace complexity: $O(NM)$\nCode class Solution: def countServers(self, grid: List[List[int]]) -\u0026gt; int: cntR = [0] * len(grid) cntC = [0] * len(grid[0]) for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j]: cntR[i] += 1 cntC[j] += 1 ans = 0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] and (cntR[i] \u0026gt; 1 or cntC[j] \u0026gt; 1): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-23/","summary":"\u003col start=\"1267\"\u003e\n\u003cli\u003eCount Servers that Communicate\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-23"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/map-of-highest-peak/description/\nIntuition Because the maximum absolute value of the height difference between two adjacent grid is 1, and the height of water gird is 0. This means that the answer is just the manhattan distance to the nearest water grid.\nApproach Use a BFS to find the nearest manhattan idstance to a water grid.\nComplexity Time complexity: $O(NM)$, N, M are the length and width of the grid.\nSpace complexity: $O(NM)$\nCode class Solution: def highestPeak(self, isWater: List[List[int]]) -\u0026gt; List[List[int]]: q = deque() ans = [[2005 for _ in range(len(isWater[0]))] for _ in range(len(isWater))] for i in range(len(isWater)): for j in range(len(isWater[0])): if isWater[i][j] == 1: q.append((i,j)) ans[i][j] = 0 dx = [0,0,1,-1] dy = [1,-1,0,0] while q: x,y = q.popleft() for i in range(4): nx = x + dx[i] ny = y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= len(isWater) or ny \u0026gt;= len(isWater[0]) or ans[nx][ny] \u0026lt;= ans[x][y] + 1: continue ans[nx][ny] = ans[x][y] + 1 q.append((nx, ny)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-22/","summary":"\u003col start=\"1765\"\u003e\n\u003cli\u003eMap of Highest Peak\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-22"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/grid-game/description/\nIntuition This is a special case where there are only two rows. Moreover, it is also important to note that all number in the grid.\nSince the robot cannot go back whenever we choose to go down, this means that for both robots, there is only one chance to go to the second row.\nIf robot one goes to the second row at index $k$, what will happen?\nNow, in this case, the only numbers not 0 are the numbers that has index less than k in the second row and the numbers that has index more than k in the first row.\nTherefore, to maximize the result for robot 2, it gets to choose to get the numbers in the first row or in the second row because it cannot get back to the first row when it choose to get to the second row.\nApproach Now, all we have to calculate is the sum of all the numbers after index k in the first row, and the sum of all the numbers before index k in the second row.\nTrick Now we have a trick of prefix sum to solve this problem.\nThe sum of all numbers after index in the first row k can be calculated by the sum of all numbers after index k - 1, by subtracting $grid[k][0]$. The sum of all numbers before index in the second row k can be calculated by the sum of all numbers after index k - 1, by adding $grid[k][1]$. The required sum of the first row is always decreasing, while the required sum of the second row is always increasing. Therefore, when $max(sum1, sum2) \u0026gt; presentAns$, we can break the loop as now sum2 \u0026gt; sum1 and will continue increase. Therefore, the answer will not decrease anymore. (Here sum1 means the required sum of row1, sum2 means the required sum of row2, and presentAns means the answer we get at present point when we iterate to index k).\nComplexity Time complexity: $O(N)$, N is the length of the gird.\nSpace complexity: $O(1)$, we only store a few variables.\nCode class Solution: def gridGame(self, grid: List[List[int]]) -\u0026gt; int: x,y = sum(grid[0][1:]), 0 ans = x for i in range(1, len(grid[0])): x -= grid[0][i] y += grid[1][i - 1] if ans \u0026gt;= max(x,y): ans = max(x,y) else: return ans return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-21/","summary":"\u003col start=\"2017\"\u003e\n\u003cli\u003eGrid Game\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-21"},{"content":"Today\u0026rsquo;s Problem https://leetcode.com/problems/first-completely-painted-row-or-column/description\nIntuition Do what the question ask.\nTrick We can store a count array for each row and column, so that we can know how many block painted in any row or column.\nApproach Store the position of each number in the gird. Add one count in each row and column every print. If the print lead to a row or column that is all painted, then output i. Complexity Time complexity: $O(NM)$, N,M are the length and width of the grid.\nSpace complexity: $O(NM)$, we need to store the index of each number.\nCode class Solution: def firstCompleteIndex(self, arr: List[int], mat: List[List[int]]) -\u0026gt; int: col = [0] * (len(arr) + 1) row = [0] * (len(arr) + 1) for i in range(len(mat)): for j in range(len(mat[0])): col[mat[i][j]] = j row[mat[i][j]] = i cntR,cntC = [0] * len(mat), [0] * len(mat[0]) for i, x in enumerate(arr): cntR[row[x]] += 1 cntC[col[x]] += 1 if cntR[row[x]] == len(mat[0]) or cntC[col[x]] == len(mat): return i return -1 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-20/","summary":"\u003col start=\"2661\"\u003e\n\u003cli\u003eFirst Completely Painted Row or Column\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-20"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/trapping-rain-water-ii/description/\nIntuition In yesterday\u0026rsquo;s problem, we talked about the situation when we may consider using graph based methods to solve problems.\nNow we can apply that criteria to this question: in this question, we found that the water can flow from four dirctions, which means that there are aftereffects.\nTherefore, we need to apply graph based methods.\nFor each block, how many water it can store depends on the height difference between it and its lowest neighbor (image a wood bucket with edge heights you may see in psychology classes).\nTo clearify, here, water wall can also be s type of wall that contributes to the height.\nTo know the height of the walls, we first need to genereate a wall. But where is it?\nThe first wall you may consider is the outmost edge of the graph (such as the one shown in case two where stores water using the outmost edge).\nThen we can find the lowest place on the wall to create more walls inside. That is, if its neighbor is higher than the point on the wall, then the point inside will become a new componenet of the wall. Otherwise, that inside point can store enough water to create a water wall as high as the current point.\nBecause we are using the lowest place on the wall, so all other parts of the wall would be higher or equal to the point, which means that the height of the wall is the upper bond of how many water can be stored inside the wall.\nApproach In this case, we can use a priority queue to find the point of the wall efficiently.\nThen follow the algorithm described above:\nCreate the initial wall\nloop:\nfind the lowest point on the wall\ncreate new walls or new water walls\nend loop\nsum up all addition height of water walls\nComplexity Time complexity: $O(NM\\times log(NM))$\nSpace complexity: $O(NM)$\nCode class Solution: def trapRainWater(self, heightMap: List[List[int]]) -\u0026gt; int: if len(heightMap) \u0026lt; 3 or len(heightMap[0]) \u0026lt; 3: return 0 dx = [0,0,1,-1] dy = [1,-1,0,0] vis = [] n, m = len(heightMap), len(heightMap[0]) q = [] for i in range(n): heappush(q, (heightMap[i][0], i, 0)) heappush(q, (heightMap[i][-1], i, m - 1)) vis.append((i, 0)) vis.append((i, m-1)) for i in range(1, m - 1): heappush(q, (heightMap[0][i], 0, i)) heappush(q, (heightMap[-1][i], n - 1, i)) vis.append((0, i)) vis.append((n-1, i)) vis = set(vis) ans = 0 while len(q) \u0026gt; 0: h, x, y = heappop(q) for i in range(4): nx, ny = x + dx[i], y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or (nx,ny) in vis: continue if heightMap[nx][ny] \u0026lt; h: ans += h - heightMap[nx][ny] heappush(q, (max(h, heightMap[nx][ny]), nx, ny)) vis.add((nx, ny)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-19/","summary":"\u003col start=\"407\"\u003e\n\u003cli\u003eTrapping Rain Water II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-cost-to-make-at-least-one-valid-path-in-a-grid/\nIntuition Consider a question:\nHere, there is a graph, each edge has a value 1 or 0, and you should travel from point 0 to point N, what is the shortest path?\nIn this case, you would quickly think of graph algorithms such as Dijkstra, SPFA, or even BFS.\nBut what if I tell you that the question above is the exact same question as what we are solving this quesion? Can you quickly think of the transition between the setting in the question and the setting in this more simple version?\nNow, some of you may think that this problem may be DP question: we have a grid, we may be able to write a DP formular\u0026hellip;\nBut wait, the most important prerequisite of DP is aftereffect. To run a DP, you must make sure that there is not aftereffect. In our situation, because we may need to go from right to left, from down to top, so aftereffect exists. Therefore, we cannot use DP in this question.\nApproach Now, to transfer our question to the question above, we only need to iterate through the graph and create an edge between a point to its neighbor, if this is the neighbor it is pointing at, then the value of the edge will be 0, otherwise it would be 1.\nSome of you may consern the correctness of this solution, as there is also a limitation that \u0026ldquo;You can modify the sign on a cell one time only\u0026rdquo;.\nHowever, the situation is, this graph has not negative edges, which means that your result will always increase if you go through more points. Therefore, you will not even vist the same point more than 1 time, so it is impossible for the solution you get to change the sign of a cell more than 1 time.\nNow, run your Dijkstra (or other shortest path algorithms), and you are all set!\nComplexity Time complexity: $O(N\\times M\\times log(N\\times M))$\nSpace complexity: $O(N\\times M)$\nCode class Solution: def minCost(self, grid: List[List[int]]) -\u0026gt; int: dx = [0,0,1,-1] dy = [1,-1,0,0] n,m = len(grid), len(grid[0]) edges = [[] for i in range(n * m)] def cordinate2d21d(x,y): return x * m + y for i in range(len(grid)): for j in range(len(grid[0])): pos = cordinate2d21d(i, j) for idx in range(4): nx = i + dx[idx] ny = j + dy[idx] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m: continue npos = cordinate2d21d(nx, ny) if idx + 1 == grid[i][j]: edges[pos].append([npos, 0]) else: edges[pos].append([npos, 1]) dis = [inf] * (n * m) dis[0] = 0 q = [(0,0)] while q: d, x = heappop(q) for to, v in edges[x]: if d + v \u0026lt; dis[to]: dis[to] = d + v heappush(q, (dis[to], to)) return dis[cordinate2d21d(n-1, m-1)] ","permalink":"https://tzj2006.github.io/leetcode/2025-01-18/","summary":"\u003col start=\"1368\"\u003e\n\u003cli\u003eMinimum Cost to Make at Least One Valid Path in a Grid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-18"},{"content":"Part Ollama Sometimes someone may already open an ollama server on the HPC. In this case, we need to open a new ollama server, otherwise both of use will run at a slower speed.\nIn this case, what can we do?\nOpen a new personal port!\ne.g:\nenv OLLAMA_MODELS=/orange/qsong1/zt81.duke/Models OLLAMA_HOST=127.0.0.1:11451 ollama serve ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-17/","summary":"Ollama","title":"Bug Journal 2025-01-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/neighboring-bitwise-xor/description/\nIntuition Based on the information of yesterday\u0026rsquo;s problem, we know that $a\\space xor\\space a = 0$. Therefore, $\\large{XOR}_{i=0}^{n} \\small derived[i]$ has to be 0, because all the $original$ offsets. Here, n means the last index of the sequence. Now lets prove that if $\\large{XOR}_{i=0}^{n} \\small derived[i] = 0$ enables us to create the whole $original$ sequence. Let $original[0]=0$, $original[k] = original[k-1]\\space xor\\space derived[k]$. Now all we need to prove is $original[n]\\space xor\\space original[0]=derived[n]$, that is, $original[0] = original[n]\\space xor\\space derived[n]$. According to the formular above, $original[n] = \\large{XOR}_{i=0}^{n-1} \\small derived[i]\\space xor\\space original[0]$ Because $\\large{XOR}_{i=1}^{n} \\small derived[i] = 0$, so $original[n]\\space xor\\space derived[n] = original[0]\\space xor\\space \\large{XOR}_{i=0}^{n} \\small derived[i] = 0 = original[0]$. Therefore, this sequence of $original$ is valid. Trick Now we want to know whether the sequence itself has a xorsum 0 or not. Now, because it is a binary sequence, we can put all 0s together and 1s together, so that now by xor all the 0 and 1s, we find that the result is just the count of 1s. If the count is odd, then the result would be 1, otherwise it would be 0. Therefore, the easiest way to solve this question is to sum up everything in the sequence and check whether it is odd or even. Approach Sum up everything in the sequence and check whether it is odd or even.\nComplexity Time complexity: $O(N)$, N is the length of the sequence.\nSpace complexity: $O(1)$, no other variables stored.\nCode class Solution: def doesValidArrayExist(self, derived: List[int]) -\u0026gt; bool: return (sum(derived) \u0026amp; 1) == 0 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-17/","summary":"\u003col start=\"2683\"\u003e\n\u003cli\u003eNeighboring Bitwise XOR\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/bitwise-xor-of-all-pairings/description/\nIntuition According to the question, the formular of the output would be: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} \\large{XOR}{\\small j = 1}^{\\small m} nums1[i]\\space xor \\space nums2[j] $$ where $\\large{XOR}$ means the operation that xor from $i$ to $n$. Here, $n$ means the length of $nums1$, and $m$ means the length of $nums2$. According to the properties of xor, xor satisfies the law of commutation and the law of association (more information can be seen here), so we can change the formular to: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} (nums1[i]^m) \\space xor \\space \\large{XOR}{\\small j = 1}^{\\small m} (nums2[j]^n) $$ According to the property that $A\\space xor A = 0$, we now know that the result would be: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} (nums1[i]^{m % 2}) \\space xor \\space \\large{XOR}{\\small j = 1}^{\\small m} (nums2[j]^{n % 2}) $$\nApproach Iterate two arrays and apply the formular above.\nComplexity Time complexity: $O(N + M)$, N is the length of nums1, M is the length of nums2.\nSpace complexity: $O(1)$, no more space is used.\nCode class Solution: def xorAllNums(self, nums1: List[int], nums2: List[int]) -\u0026gt; int: ans = 0 if len(nums1) % 2 == 1: for x in nums2: ans ^= x if len(nums2) % 2 == 1: for x in nums1: ans ^= x return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-16/","summary":"\u003col start=\"2425\"\u003e\n\u003cli\u003eBitwise XOR of All Pairings\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-16"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimize-xor/description/\nXOR This is the True-False Diagram for $XOR$: In this diagram, we can find that if A and B is the same, then $A\\space XOR\\space B = 0$; else, $A\\space XOR\\space B = 1$.\nIntuition For the first requirement in the question, \u0026ldquo;same number of set bits\u0026rdquo; means \u0026ldquo;same number of bit 1 in the number\u0026rdquo;.\nWhy? This is because the number of 0 in a number can be infinity, while only the number of bit 1 is finite.\nTherefore, we need to count how many 1s are there in number2.\nNow, based on the $XOR$ Diagram we know that to make a number after doing $XOR$, we need to put a one in the same position where number1 has a 1, so that we can decrease it to 0 after doing $XOR$.\nIn this case, we want the \u0026ldquo;decreased\u0026rdquo; 1s from top to down to minimize the result.\nIf there are more 1s in number2 than number1, we would have to add new 1s to the result.\nIn this case, we want to \u0026ldquo;add\u0026rdquo; 1s from down to top to minimize the result.\nApproach Use $bit_count()$ function to count the 1s in num2. Iterate from top to down to decrease 1. Iterate from down to top to add 1. Complexity Time complexity: $O(log_2(N))$, N means the number.\nSpace complexity: $O(1)$, only some variables are stored\nCode class Solution: def minimizeXor(self, num1: int, num2: int) -\u0026gt; int: cnt,ans = num2.bit_count(), 0 for i in range(31, -1, -1): if cnt \u0026gt; 0 and (num1 \u0026amp; (1 \u0026lt;\u0026lt; i)) \u0026gt; 0: ans += (1 \u0026lt;\u0026lt; i) cnt -= 1 for i in range(31): if cnt \u0026gt; 0 and (ans \u0026amp; (1 \u0026lt;\u0026lt; i)) == 0: ans += (1 \u0026lt;\u0026lt; i) cnt -= 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-15/","summary":"\u003col start=\"2429\"\u003e\n\u003cli\u003eMinimize XOR\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-15"},{"content":"Part module load When you want to use something on the server, please first check whether it is on the server or not.\nUse this command:\nmodule avail Then load using this command:\nmodule load [your model name] Important:\nYou can always email rescomputing@duke.edu to get support.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-14/","summary":"\u003ch2 id=\"part-module-load\"\u003ePart module load\u003c/h2\u003e\n\u003cp\u003eWhen you want to use something on the server, please first check whether it is on the server or not.\u003c/p\u003e\n\u003cp\u003eUse this command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emodule avail\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen load using this command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emodule load \u003cspan class=\"o\"\u003e[\u003c/span\u003eyour model name\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eImportant:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou can always email \u003ca href=\"rescomputing@duke.edu\"\u003erescomputing@duke.edu\u003c/a\u003e to get support.\u003c/p\u003e","title":"Bug Journal 2025-01-14"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-prefix-common-array-of-two-arrays/description/\nIntuition In this question, the only three ways that the answer will increase one is:\n$A[i] == B[i]$. $A[i]$ appears in $B$. $B[i]$ appears in $A$. Approach Iterate through the Array, then check for those three situation. Note that situation 1 conflicts with situation 2 \u0026amp; 3. That is, if cnt is add by 1 through situation 1, then situation 2 \u0026amp; 3 will not increase cnt. But situation 2 \u0026amp; 3 could increase cnt. Complexity Time complexity: $O(N)$, N is the length of the list.\nSpace complexity: $O(N)$, because we need to store a set.\nCode class Solution: def findThePrefixCommonArray(self, A: List[int], B: List[int]) -\u0026gt; List[int]: ans = [] cntA = set([]) cntB = set([]) cnt = 0 for i in range(len(A)): if A[i] == B[i]: cnt += 1 else: if A[i] in cntB: cnt += 1 if B[i] in cntA: cnt += 1 cntA.add(A[i]) cntB.add(B[i]) ans.append(cnt) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-14/","summary":"\u003col start=\"2657\"\u003e\n\u003cli\u003eFind the Prefix Common Array of Two Arrays\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-13"},{"content":"When facing this problem, you can stop using GPU and change to CPU and check the error.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-12/","summary":"\u003cp\u003eWhen facing this problem, you can stop using GPU and change to CPU and check the error.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250112224658415\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-01-12GPUError.jpg\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-01-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-length-of-string-after-operations/\nIntuition In this question, each character is independent. Therefore, we can deal with one character a time. If one character has odd count, than we can delete from left to right and remain 1 character at the end. Otherwise, we will left 2 characters at the end. Approach Therefore, all we need to do is the count each character, and then test whether it has odd count or even count.\nComplexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, I stored a counter.\nCode class Solution: def minimumLength(self, s: str) -\u0026gt; int: cnt = Counter([ch for ch in s]) ans = 0 for x,v in cnt.items(): if v % 2 == 0: ans += 2 else: ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-13/","summary":"\u003col start=\"3223\"\u003e\n\u003cli\u003eMinimum Length of String After Operations\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-13"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-a-parentheses-string-can-be-valid/\nIntuition To make a parentheses string, we need to find a \u0026lsquo;(\u0026rsquo; for every \u0026lsquo;)\u0026rsquo; on its left side, and a \u0026lsquo;)\u0026rsquo; for every \u0026lsquo;(\u0026rsquo; on its right hand side. We can fulfill the first requirement first as we are iterating from left to right.\nWhen we encounter a \u0026lsquo;)\u0026rsquo;, there are three ways to find him a \u0026lsquo;(\u0026rsquo;, one is a \u0026lsquo;(\u0026rsquo; that is already existing, another is to find him a \u0026lsquo;)\u0026rsquo; that is unlocked, and the last way is to turn him into a \u0026lsquo;(\u0026rsquo; if it is unlocked itself.\nNow, assume that we have find all \u0026lsquo;)\u0026rsquo; a \u0026lsquo;(\u0026rsquo;, then we now need to find all \u0026lsquo;(\u0026rsquo; a \u0026lsquo;)\u0026rsquo;. Now, the only way to find a \u0026lsquo;(\u0026rsquo; on its right side is to find a \u0026lsquo;(\u0026rsquo; that is unlocked, because all \u0026lsquo;)\u0026rsquo; is matched with a \u0026lsquo;(\u0026rsquo;.\nApproach First of all, not that is the length is odd, then it could never be a parentheses string, so please just return False.\nWe can store two arrays, one $anyBracket$ that stores the index all unlocked brackets, and another $openBracket$ stroing all \u0026lsquo;(\u0026rsquo;.\nNow we iterate the whole string from left to right, here are some situations we would meet:\nThis is a unlocked bracket: Then we could put it into our $anyBracket$ stack. This is a \u0026lsquo;(\u0026rsquo;: Then we could put it into our $openBracket$ stack. This is a \u0026lsquo;)\u0026rsquo;: Then we need to find him a \u0026lsquo;(\u0026rsquo;. First we would like to find him a \u0026lsquo;(\u0026rsquo; in our $openBracket$ stack, which will also finish the task that helps a \u0026lsquo;(\u0026rsquo; to find a \u0026lsquo;)\u0026rsquo;. Then if our $openBracket$ stack is empty, then we will find him a \u0026lsquo;(\u0026rsquo; in our $anyBracket$ stack by either a \u0026lsquo;(\u0026rsquo; or a \u0026lsquo;)\u0026rsquo; that is unlocked. If both our $openBracket$ stack and our $anyBracket$ stack are empty, then return False, because we cannot find a \u0026lsquo;(\u0026rsquo; for him. Now we might left some \u0026lsquo;(\u0026rsquo; that is unmatched.\nThen we can iterate every \u0026lsquo;(\u0026rsquo; in our $openBracket$ stack to find whether there is a \u0026lsquo;(\u0026rsquo; or \u0026lsquo;)\u0026rsquo; in our $anyBracket$ stack that has a larger index than our current \u0026lsquo;(\u0026rsquo;.\nIf there is, then we successfully find him a \u0026lsquo;)\u0026rsquo;, congratulations! Otherwise we cannot find him a \u0026lsquo;)\u0026rsquo;, which leads to return False Now if there are even number in our $anyBracket$ stack (which will always be the case because we have already did the singularity test above), please return True, then you are all set!\nImportant Trick Why we need a stack for $openBracket$ and $anyBracket$? Because in this situation, a \u0026lsquo;)\u0026rsquo; will always match to the nearest \u0026lsquo;(\u0026rsquo; on its left hand site, which means we need a FIFO (First in First out) data structure to get \u0026ldquo;the nearest object\u0026rdquo;. Complexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, we stored the indexs of the brackets.\nCode class Solution: def canBeValid(self, s: str, locked: str) -\u0026gt; bool: if len(s) % 2 == 1: return False openBracket = [] anyBracket = [] for i in range(len(s)): if locked[i] == \u0026#39;0\u0026#39;: anyBracket.append(i) else: if s[i] == \u0026#39;(\u0026#39;: openBracket.append(i) else: if len(openBracket) \u0026gt; 0: openBracket.pop() elif len(anyBracket) \u0026gt; 0: anyBracket.pop() else: return False if len(anyBracket) \u0026lt; len(openBracket): return False idx = len(anyBracket) - 1 for i in range(len(openBracket) - 1, -1, -1): if anyBracket[idx] \u0026lt; openBracket[i]: return False idx -= 1 return idx % 2 == 1 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-12/","summary":"\u003col start=\"2116\"\u003e\n\u003cli\u003eCheck if a Parentheses String Can Be Valid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-k-palindrome-strings/description/\nIntuition A palindrome can have at most one character with an odd count. Therefore, to create $k$ palindrome strings, there must be at most $k$ characters in string $s$ with an odd count.\nAdditionally, since there are at most 26 different characters, if $26 \\leq k$, the result must be true. However, if the length of $s$ is less than $k$, it would be false.\nTo prove this, let the number of characters with odd counts be $cntO$, and let the count of all remaining characters be $2 \\times cntE$.\nIf all character counts are even, we can always create palindrome strings as long as $k \\leq N$, where $N$ is the total length of $s$. This is because we can place one character on the leftmost side of a palindrome string and its duplicate on the rightmost side, preserving the palindrome structure.\nSince $k \\leq N$, it follows that $k \\leq cntO + 2 \\times cntE$. Thus, $k - cntO \\leq 2 \\times cntE$. This implies that all characters with odd counts can be used to form $cntO$ palindrome strings.\nNow, we have already proved that if all character counts are even, we can always create palindrome strings as long as $k \\leq N$, where $N$ is the total length of $s$. So in this case, if $cntO \\le k$, the result would be ture, otherwise, it would be false.\nFinally, the question reduces to the proposition that $k \\leq N$ when all characters have even counts, which is always true.\nTrick Since there are at most 26 different characters, if $26 \\leq k$, the result must be true.\nApproach Now we only need to calculate the occurence of every character and test whether the odd-count characters are less or equal to $k$ or not.\nComplexity Time complexity: $O(N)$, while N is the length of the string.\nSpace complexity: $O(1)$, while the count of 26 characters are stored.\nCode class Solution: def canConstruct(self, s: str, k: int) -\u0026gt; bool: cnt = [0] * 26 if len(s) \u0026lt; k: return False if k \u0026gt; 25: return True # The code that makes the code run very fast. for ch in s: cnt[ord(ch) - ord(\u0026#39;a\u0026#39;)] += 1 x = 0 for i in range(26): if cnt[i] % 2 == 1: x += 1 return x \u0026lt;= k ","permalink":"https://tzj2006.github.io/leetcode/2025-01-11/","summary":"\u003col start=\"1400\"\u003e\n\u003cli\u003eConstruct K Palindrome Strings\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-11"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/word-subsets/\nIntuition The brute method would take $O(N^2\\times (L+D))$ time, while $N$ is the length of the words, $L$ is the length of each word, and $D$ is the size of the dictionary (that is, 26 characters). However, $N \\le 10^4$, which means that we cannot use brute method. Then we found out that we do not need to compare all $N$ words, instead, we only need to compare the maximum of the occurence of each characters in words2. For example, if \u0026lsquo;a\u0026rsquo; appears 3 times in $words2[1]$, 2 times in $words2[2]$, 4 times in $words2[3]$, then \u0026lsquo;a\u0026rsquo; must appears at least 4 times in a $words1[i]$ to add one to the answer. Therefore, all we need is to count the occurence of each character in word1, and count the maximun occurence of each character in every word2.\nApproach Count the occurence of each character in word1, and count the maximun occurence of each character in every word2.\nComplexity Time complexity: $O(N \\times (L + D))$, while $N$ is the length of the words, $L$ is the length of each word, and $D$ is the size of the dictionary (that is, 26 characters).\nSpace complexity: $O(N \\ times D)$.\nCode class Solution: def wordSubsets(self, words1: List[str], words2: List[str]) -\u0026gt; List[str]: wordcnt1, wordcnt2 = [], [0] * 26 a = ord(\u0026#39;a\u0026#39;) for word in words1: cnt = [0] * 26 for ch in word: x = ord(ch) - a cnt[x] += 1 wordcnt1.append(cnt) for word in words2: cnt = [0] * 26 for ch in word: x = ord(ch) - a cnt[x] += 1 for j in range(26): wordcnt2[j] = max(wordcnt2[j], cnt[j]) ans = [] for i in range(len(words1)): flag = True for k in range(26): if wordcnt1[i][k] \u0026lt; wordcnt2[k]: flag = False break if flag: ans.append(words1[i]) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-10/","summary":"\u003col start=\"916\"\u003e\n\u003cli\u003eWord Subsets\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-10"},{"content":"Part Matplotlib If you want to make the word in matplotlib the a word that could be edited in Adobe illustration, please use the code below at the top of your code:\nmpl.rcParams[\u0026#34;pdf.fonttype\u0026#34;] = 42 mpl.rcParams[\u0026#34;ps.fonttype\u0026#34;] = 42 ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-09/","summary":"\u003ch2 id=\"part-matplotlib\"\u003ePart Matplotlib\u003c/h2\u003e\n\u003cp\u003eIf you want to make the word in matplotlib the a word that could be edited in Adobe illustration, please use the code below at the top of your code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003empl\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ercParams\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;pdf.fonttype\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003empl\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ercParams\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;ps.fonttype\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Bug Journal 2025-01-09"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/counting-words-with-a-given-prefix/description\nIntuition Do what the question ask!\nApproach The question yesterday can give us some insight of how to solve this question with minimal code.\nComplexity Time complexity: $O(N\\times L)$, N is the length of the words, L is the length of a single word.\nSpace complexity: $O(1)$, only some variables are stored.\nCode class Solution: def prefixCount(self, words: List[str], pref: str) -\u0026gt; int: ans = 0 for word in words: if word.startswith(pref): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-09/","summary":"\u003col start=\"2185\"\u003e\n\u003cli\u003eCounting Words With a Given Prefix\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-09"},{"content":"Part SSH To use ssh for UF server, eduVPN app must be downloaded.\nThe tutorial is in this link.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-08/","summary":"\u003ch2 id=\"part-ssh\"\u003ePart SSH\u003c/h2\u003e\n\u003cp\u003eTo use ssh for UF server, eduVPN app must be downloaded.\u003c/p\u003e\n\u003cp\u003eThe tutorial is in \u003ca href=\"https://docs.rc.ufl.edu/access/federated_login/?h=eduvpn#eduvpn-connection\"\u003ethis link\u003c/a\u003e.\u003c/p\u003e","title":"Bug Journal 2025-01-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-prefix-and-suffix-pairs-i/description/\nIntuition Do what the question ask!\nApproach Iterate two times and check the suffix and prefix of the string.\nComplexity Time complexity: $O(N^2\\times L)$, N is the length of the words, and L is the length of the string.\nSpace complexity: $O(1)$\nCode class Solution: def countPrefixSuffixPairs(self, words: List[str]) -\u0026gt; int: def checkpre(str1, str2): if len(str1) \u0026gt; len(str2): return False return str1 == str2[:len(str1)] def checksuf(str1, str2): if len(str1) \u0026gt; len(str2): return False return str1 == str2[len(str2) - len(str1):] ans = 0 for i in range(len(words)): for j in range(i+1, len(words)): if checkpre(words[i],words[j]) and checksuf(words[i], words[j]): ans += 1 return ans class Solution: def countPrefixSuffixPairs(self, words: List[str]) -\u0026gt; int: ans = 0 for i in range(len(words)): for j in range(i+1, len(words)): if words[j].startswith(words[i]) and words[j].endswith(words[i]): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-08/","summary":"\u003col start=\"3042\"\u003e\n\u003cli\u003eCount Prefix and Suffix Pairs I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/string-matching-in-an-array/description/\nIntuition Do what the question ask.\nApproach First sort the words by its length, then iterate all string that has a larger string length. If the current string is a substring of the new string, then put it into the answer list.\nActually you do not need to sort the array, but the sort would accelerate the process.\nComplexity Time complexity: $O(N^2\\times L)$, N is the length of the word list, L is the length of the word.\nSpace complexity: $O(N)$, because we need to store the answer.\nCode class Solution: def stringMatching(self, words: List[str]) -\u0026gt; List[str]: ans = [] words.sort(key = lambda x: len(x)) # This lambda is very import in python for i in range(len(words)): for j in range(i + 1, len(words)): if words[i] in words[j]: ans.append(words[i]) break return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-07/","summary":"\u003col start=\"1408\"\u003e\n\u003cli\u003eString Matching in an Array\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-07"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-number-of-operations-to-move-all-balls-to-each-box/description\nIntuition For each point, is it easy to find out that the answer of that point is the sum of the distance between i and all the 1s.\nThat is: $ans[i]=\\sum_{j=0}^{n-1} boxes[j] \\times abs(j-i)$.\nNow, imagine that there is a pointer moving left to right from 1 to n, calculating the result.\nWe can find that for $ans[i]$ and $ans[i+1]$, the difference would be the number of 1s from 1 to i minus the number of 1s from i+1 to n.\nThat is: $ans[i+1] - ans[i] = \\sum_{j=0}^{i} boxes[j] - \\sum_{j=i+1}^{n-1} boxes[j]$. Therefore, by calculating the number of 1s on the left hand side of i and the number of all 1s in the sequence, we can calculate all answers by $O(N)$.\nApproach First we need to calculate $ans[0]$ and the number of all 1s in the sequence by using the equation $ans[0]=\\sum_{j=1}^{n} boxes[j] \\times j$. Therefore, we need to iterate through the whole sequence. Next we need to calculate $ans[i+1] - ans[i] = \\sum_{j=0}^{i} boxes[j] - \\sum_{j=i+1}^{n-1} boxes[j]$ for every i from 1 to n-1. Because $ans[i+1] - ans[i] = \\sum_{j=1}^{i} boxes[j] - \\sum_{j=i+1}^{n} boxes[j] = 2 \\times \\sum_{j=0}^{i} boxes[j] - \\sum_{j=0}^{n-1} boxes[j]$. Therefore, all we have to. do is to count the 1s in our iteration to our answer, then apply the fomular above.\nComplexity Time complexity: $O(N)$, N is the size of the boxes.\nSpace complexity: $O(N)$, as we need to store our answer.\nCode class Solution: def minOperations(self, boxes: str) -\u0026gt; List[int]: now = 0 cnt1 = 0 for i in range(len(boxes)): if boxes[i] == \u0026#39;1\u0026#39;: now += i cnt1 += 1 ans = [now] * len(boxes) now_cnt1 = 0 for i in range(1, len(boxes)): if boxes[i-1] == \u0026#39;1\u0026#39;: now_cnt1 += 1 ans[i] = ans[i-1] + 2 * now_cnt1 - cnt1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-06/","summary":"\u003col start=\"1769\"\u003e\n\u003cli\u003eMinimum Number of Operations to Move All Balls to Each Box\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-06"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/shifting-letters-ii/description/\nIntuition We need to write a datastructure to achieve a multi-range addition and a one-time query. Therefore, we can use difference Array and prefix sum.\nApproach In a difference array, we only count the difference of the edges of the change zone. For example, if we add the range $[l,r]$ by $k$, we only pay attention to point l and point r: we add the difference array $diff[l]$ by $k$, and then add the difference array $diff[r + 1]$ by $-k$. Now, if we calculate the prefix sum $s$ in range $[l,r]$, we find that the effect of addition $k$ will be added only in range $[l,r]$ in $s$. Therefore, by using the difference array and the prefix sum, we can deal with the one change in $O(1)$. Now, since we want to change the character, we can first change it into ASCII code, then subtract by the code of \u0026lsquo;a\u0026rsquo;. Then we can use a module of 26 to acheive the effect of \u0026ldquo;character rotation\u0026rdquo;.\nComplexity Time complexity: $O(N + C)$, N is the length of the string, c is the number of changes.\nSpace complexity: $O(N)$, we need to store the difference array.\nCode class Solution: def shiftingLetters(self, s: str, shifts: List[List[int]]) -\u0026gt; str: dif = [0] * (len(s) + 1) for l,r,delta in shifts: if delta == 0: dif[l] -= 1 dif[r + 1] += 1 else: dif[l] += 1 dif[r + 1] -= 1 a = ord(\u0026#39;a\u0026#39;) cnt = 0 ans = \u0026#34;\u0026#34; for i in range(len(s)): cnt += dif[i] ans += chr((ord(s[i]) - a + cnt) % 26 + a) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-05/","summary":"\u003col start=\"2381\"\u003e\n\u003cli\u003eShifting Letters II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-05"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/unique-length-3-palindromic-subsequences/description/\nIntuition Because the question requires the count of unqiue subsequence. Therefore, the largest count is $26*26 = 676$. Therefore, all we have to do is to count how many unique characters are there between two same characters.\nApproach First we need to calculate the first and last occurance of a character. Then we need to iterate between l and r to count how many unique characters are there between l and r.\nComplexity Time complexity: $O(kN)$, k is the number of unique characters, here it means 26 different character. N is the length of the string.\nSpace complexity: $O(N)$\nCode class Solution: def countPalindromicSubsequence(self, s: str) -\u0026gt; int: st = [inf] * 26 en = [-1] * 26 a = ord(\u0026#39;a\u0026#39;) for i,ch in enumerate(s): nch = ord(ch) - a st[nch] = min(st[nch], i) en[nch] = max(en[nch], i) ans = 0 for x in range(26): if en[x] \u0026lt;= st[x]: continue # print(en[x], st[x], x) vis = set([]) for i in range(st[x] + 1, en[x]): vis.add(s[i]) ans += len(vis) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-04/","summary":"\u003col start=\"1930\"\u003e\n\u003cli\u003eUnique Length-3 Palindromic Subsequences\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-04"},{"content":"Part Ollama To run ollama serve on UF that stores models on large storage, please use:\nenv OLLAMA_MODELS=/orange/qsong1/zt81.duke/Models ollama serve To list all the ollama environment, please use:\nollama list Then check whether all the models pulled are in this list.\nNote that now when dealing with ollama input, multiple role input may cause potential error.\nPart Model Tuning What I did today:\nSet up llama for Sprax task.\nTest1: llama3.2 with reasoning. Result: Nearly everything outputs \u0026ldquo;Sensitive\u0026rdquo; (For both cells that has sensitive and resistent label).\n![image-20250103204922830](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20250103204922830.png)\nGuess: May because there are too many genes that mislead the result of the LLM.\nDecrease the number of genes considered Result: However, the model still outputs \u0026ldquo;Sensitive\u0026rdquo; for nearly every cells.\nTry not using reasoning Result: Very unstable. Sometimes the result is very good, but in most case, it is very bad.\nTry using llama3.3 70B Result: Not much better than guess.\n![image-20250103205231023](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20250103205231023.png)\nFuture work Train MLP Train an MLP using the label, and let LLM to distinguish the embedding.\nIn this case, we can tell whether the LLM is useless or the prompt is useless.\nCustomize tokenizer Change one cell to one token\nCheck Pathway Check whether the LLM is saying nonsense or saying things right.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-03/","summary":"run ollama on the server \u0026amp; model tuning","title":"Bug Journal 2025-01-03"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/number-of-ways-to-split-array/description/\nIntuition We need to calculate $sum(a[0] \\space to\\space a[i])$ and $sum(a[i+1]\\space to\\space a[n])$ according to the question. Since $sum(a[0]\\space to\\space a[i + 1]) = sum(a[0]\\space to\\space a[i]) + a[i+1]$ and $sum(a[i+1]\\space to\\space a[n]) = sum(a[0]\\space to\\space a[n]) - sum(a[0]\\space to\\space a[i])$. Therefore, we can new two variables. One $now$ that stores $sum(a[0]\\space to\\space a[i])$ and add $a[i+1]$ to it every iteration, one $summ$ that stores $sum(a[0]\\space to\\space a[n])$.\nApproach In this case, we only need to compare $now$ and $summ - now$. Then count all i that apply.\nComplexity Time complexity: $O(N)$, N is the length of the sequence.\nSpace complexity: $O(1)$, two new varables are stored.\nCode class Solution: def waysToSplitArray(self, nums: List[int]) -\u0026gt; int: now,summ, ans = 0, 0, sum(nums) for num in nums[:-1]: now += num if now \u0026gt;= summ - now: ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-03/","summary":"\u003col start=\"2270\"\u003e\n\u003cli\u003eNumber of Ways to Split Array\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-03"},{"content":"Part Hugo Part Upgrade The best method to upgrade Hugo is to use the exact same method you use when you install Hugo.\nFor example, when you use homebrew to install Hugo, the please use brew to upgrade Hugo.\nThe command of this is:\nbrew upgrade brew upgrade hugo Part config Always the best guides:\nhttps://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\nSometimes the config may have errors.\nFor example, if the baseUrl config does not work, add this to the config file:\nrelativeURLs: false canonifyURLs: true Moreover, we can change what we want to show on the main page, for example, the menu bar:\nmenu: main: - identifier: bugJournal name: bugJournal url: /bugJournal/ weight: 10 - identifier: leetcode name: leetcode url: /leetcode/ weight: 20 - identifier: posts name: posts \u0026amp; notes url: /posts/ weight: 30 To change which part of passage to show on the main page:\nparams: mainSections: - bugJournal - leetcode - posts ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-02/","summary":"update hugo to fix bugs","title":"Bug Journal 2025-01-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-vowel-strings-in-ranges/description/\nIntuition We need to cacluate the sum of a section, while the sum of the any section remains constant for every query. Therefore we can use prefix sum.\nApproach In one iteration, we can identify whether $words[i]$ is the a vowel string or not. In this case, we can apply a prefix sum to calculate the number of vowel strings between index 1 and index i. Therefore, when we want to calculate the number of vowel strings between index l and index r, we can just use $num(1\\space to\\space r) - num(1\\space to\\space l-1)$ as our result.\nTrick In python, $list[-1]$ means the final index of the list. Therefore, we can add a [0] at the end of our prefix sum list to avoid null index.\nComplexity Time complexity: $O(k\\times N + Q)$, while k is the number of vowels, N is the length of the words, Q is the length of the queries.\nSpace complexity: $O(N)$, because we stored a new list.\nCode class Solution: def vowelStrings(self, words: List[str], queries: List[List[int]]) -\u0026gt; List[int]: sumWords = [0] * (len(words) + 1) vowels = set([\u0026#39;a\u0026#39;,\u0026#39;e\u0026#39;,\u0026#39;i\u0026#39;,\u0026#39;o\u0026#39;,\u0026#39;u\u0026#39;]) for idx, word in enumerate(words): sumWords[idx] = sumWords[idx-1] if word[0] in vowels and word[-1] in vowels: sumWords[idx] += 1 ans = [] for x,y in queries: ans.append(sumWords[y] - sumWords[x-1]) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-02/","summary":"\u003col start=\"2559\"\u003e\n\u003cli\u003eCount Vowel Strings in Ranges\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-02"},{"content":"Part Hugo How to create a Hugo website This is a good tutorial.\nHowever, the themes may contain bugs. (The best way is to download another theme!)\nLanguage Code Error: Some theme use site.LanguageCode and others use site.Lang.LanguageCode Code Highlight Error: Change it to .post-content pre code { word-break: normal !important; white-space: pre !important; } Note 2025.01.03\nAll these error are caused by the version conflict of Hugo, go, and the theme. Update everything to the latest version solves all the problem.\nPart Conda Now I install conda under /orange/qsong1/zt81.duke/miniconda3\nTherefore, to use conda, please use:\ncd /orange/qsong1/zt81.duke/miniconda3 source miniconda3/bin/activate ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-01/","summary":"create hugo website \u0026amp; Install condo","title":"Bug Journal 2025-01-01"},{"content":"Part ChatGPT 4o ChatGPT 4o API demo from openai import OpenAI model_use = \u0026#34;gpt-4o-2024-08-06\u0026#34; client = OpenAI(api_key=\u0026#34;Your-API-key\u0026#34;) completion = client.beta.chat.completions.parse( model=model_use, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Extract the event information.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;}, ], ) event = completion.choices[0].message.parsed Note: I tried to use model \u0026ldquo;gpt-4o\u0026rdquo; but failed.\nHow to create ChatGPT API Key Log in to openai Use the search bar to search \u0026ldquo;API keys\u0026rdquo; Create a new secret key (Shown only once, invisible after closing the tab) Go to billing to add some credit to the account Part UniTox ChatGPT Read from fda.gov Read the label of the drug we are interested in from a .csv file.\nRead the .html file or the .pdf file on the page\nCreate a summary of the .html and .pdf files by ChatGPT\nUse the summary generated by ChatGPT to let ChatGPT decide whether the drug is toxic or not and how toxic the drug is.\nInitial Prompt: Provide a summary of all the parts of the drug label that discuss cardiotoxicity risks and cardiotoxic reactions for this drug. In your summary of each sentence, clearly state whether the drug itself was associated with or caused the cardiotoxicity risk. Output1 Toxidity Score Prompt: Given the above information about a drug, answer \u0026#39;was this drug associated with No Cardiotoxicity, Less Cardiotoxicity, or Most Cardiotoxicity?\u0026#39; Now, answer with just one word: No, Less or Most. Output1 (Summary) OUtput2 Toxidity Test Prompt: Given the above information about a drug, answer \u0026#39;was this drug associated with Cardiotoxicity?\u0026#39; Now, answer with just one word: Yes or No. Output1 Output3 \u0026lt;-\u0026gt; compare GT ![image-20241230175906492](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20241230175906492.png)\nPart Llama Part Ollama First open an ollama server on the server:\nml ollama # activate ollama ollama serve # open ollama server To use ollama in python: (demo)\npip install ollama from ollama import chat, Client, ChatResponse client = Client(host=\u0026#39;http://localhost:11434\u0026#39;) model_use = \u0026#34;llama3.2\u0026#34; completion = client.chat( model=model_use, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Extract the event information.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;}, ], ) completion[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] ","permalink":"https://tzj2006.github.io/posts/llm-study/","summary":"\u003ch2 id=\"part-chatgpt-4o\"\u003ePart ChatGPT 4o\u003c/h2\u003e\n\u003ch3 id=\"chatgpt-4o-api-demo\"\u003eChatGPT 4o API demo\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eopenai\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eOpenAI\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emodel_use\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;gpt-4o-2024-08-06\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eOpenAI\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eapi_key\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Your-API-key\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecompletion\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebeta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecompletions\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparse\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003emodel_use\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Extract the event information.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecompletion\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparsed\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: I tried to use model \u0026ldquo;gpt-4o\u0026rdquo; but failed.\u003c/p\u003e\n\u003ch3 id=\"how-to-create-chatgpt-api-key\"\u003eHow to create ChatGPT API Key\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eLog in to \u003ca href=\"https://platform.openai.com/docs/overview\"\u003eopenai\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eUse the search bar to search \u0026ldquo;API keys\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eCreate a new secret key (Shown only once, invisible after closing the tab)\u003c/li\u003e\n\u003cli\u003eGo to billing to add some credit to the account\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"part-unitox-chatgpt\"\u003ePart UniTox ChatGPT\u003c/h2\u003e\n\u003ch3 id=\"read-from-fdagov\"\u003eRead from fda.gov\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eRead the label of the drug we are interested in from a .csv file.\u003c/p\u003e","title":"LLM Study"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-score-after-splitting-a-string/description\nIntuition Do what the question asks.\nApproach Do what the question asks. Iterate from the back to the front to count how many 1s, iterate from the front to the back to count how many 2s. To speed up the process, we can first count how many 1s are there in the whole sequence, then use another iteration to count the remaining 1s in the sequence by doing a subtraction. Same to the question in Jan.03.2025, the number of 1s in index i + 1 to n = the number of 1s in the sequence - the number of 1s in index 1 to i.\nComplexity Time complexity: $O(N)$, N is the length of s.\nSpace complexity: $O(1)$, only a few new variables are stored.\nCode class Solution: def maxScore(self, s: str) -\u0026gt; int: num1 = 0 for ch in s: if ch == \u0026#39;1\u0026#39;: num1 += 1 now0, now1, ans = 0, 0, 0 for ch in s[:-1]: if ch == \u0026#39;0\u0026#39;: now0 += 1 else: now1 += 1 ans = max(ans, now0 + num1 - now1) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-01/","summary":"\u003col start=\"1422\"\u003e\n\u003cli\u003eMaximum Score After Splitting a String\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-01"},{"content":" 笔记本的 RAM 在关闭屏幕后还耗电吗 markdown 插入图片无法在网站上自动显示 Random 中加一个 checkbox desktop video 英文版 √ 孤波算法是什么 desktop video 多语言切换 √ 是什么成就了一个奢侈品？ Desktop Video 锁屏界面播放 M4 pro V.S. M3 Max ","permalink":"https://tzj2006.github.io/random/","summary":"\u003col\u003e\n\u003cli\u003e笔记本的 RAM 在关闭屏幕后还耗电吗\u003c/li\u003e\n\u003cli\u003emarkdown 插入图片无法在网站上自动显示\u003c/li\u003e\n\u003cli\u003eRandom 中加一个 checkbox\u003c/li\u003e\n\u003cli\u003edesktop video 英文版 √\u003c/li\u003e\n\u003cli\u003e孤波算法是什么\u003c/li\u003e\n\u003cli\u003edesktop video 多语言切换 √\u003c/li\u003e\n\u003cli\u003e是什么成就了一个奢侈品？\u003c/li\u003e\n\u003cli\u003eDesktop Video 锁屏界面播放\u003c/li\u003e\n\u003cli\u003eM4 pro V.S. M3 Max\u003c/li\u003e\n\u003c/ol\u003e","title":"Random Ideas"}]