[{"content":"Linux 服务器下的指令 Docker 的安装和调试 Docker相当于一台虚拟机。安装之后就可以在这台虚拟机上跑代码了。\nDocker 的安装 Docker 的安装方式： 首先上 Dockerhub 挑选一个心仪的 docker, 下面以 nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 为例：\n然后运行以下代码：\ndocker run -it -rm\\ --name \u0026lt;your-instance-name\u0026gt; \\ --network host \\ nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 注：这里的 -it 指的是打开一个可交互界面，-rm 指的是用后即焚\n这时候就会自动下载 docker 并打开一个 bash 来用。\n现在你会发现这个虚拟机里面什么都没有，所以就需要 apt-get install\n另外，如果你的宿主机器的根目录比较小，想要挂载一个硬盘的话，就在 docker run 中间加上：\n-v /path/to/large/storage:/somepath \\ 这样就可以在 somepath 下挂载这个硬盘了。\n注：不能挂载在根目录下，必须挂载在一个文件夹下\n如果要用 GPU 语法和 CUDA_VISIBLE_DEVICES类似:\n--gpus all # 全部gpu --gpus 0,1,2,3 # 只用gpu 0,1,2,3 如果要加入系统变量, 比如 proxy\n-e HTTPS_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; 注意：image name (比如这里就是 nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04) 要放在最后一行\n这里在测试的时候建议加上 -rm,这样不会产生很多个休眠中的 docker 但是在要频繁使用的时候不建议使用 -rm, 而是就让 docker休眠就好。\n不知道我以前在这里写的啥 应该是说，反正休眠的 docker 可以通过 docker start 唤醒，如果不是必要就别删了呗\ndocker 的调试 常用的 docker 指令： # 启动 docker docker start \u0026lt;容器ID或名字\u0026gt; # 关闭 docker docker stop \u0026lt;容器ID或名字\u0026gt; # 重启 docker docker restart \u0026lt;容器ID或名字\u0026gt; # 删除容器 docker rm \u0026lt;容器ID或名字\u0026gt; # 进入容器 docker exec -it \u0026lt;容器ID或名字\u0026gt; bash # 查看正在运行的容器 docker ps # 查看所有容器（包括停止的） docker ps -a # 列出本地镜像 docker images # 删除镜像 docker rmi \u0026lt;镜像ID或名字\u0026gt; # 挂载目录 docker run -v /host/path:/container/path # 增加环境变量 docker run -e HTTP_PROXY=http://localhost:10086 代理的使用 这里使用的是 xray。\nxray 是这样运行的：\n./xray run -c config.json 运行之后你就可以看到哪个端口放开了，就可以在哪个端口上使用代理,比如 port: 10086。\n这时候如果你想使用代理就需要：\n输入几行代码\n这样你的下载就会走代理辣。\n非常重要 (大坑)\napt-get install 对代理的要求较高，没那么稳定的代理会很稳定的挂，报 Error 503 Service Unavailable. 这时候就直接换清华源就行了，别使用代理了，等之后下别的再用。\ndocker初始化 #!/bin/bash set -e echo \u0026#34;🔧 更新 apt 并安装基础工具...\u0026#34; apt update \u0026amp;\u0026amp; apt install -y \\ vim \\ git \\ curl \\ wget \\ ca-certificates \\ software-properties-common \\ build-essential \\ htop \\ unzip \\ tmux \\ sudo \\ lshw \\ libgl1-mesa-glx \\ libegl1-mesa-dev \\ libosmesa6-dev \\ patchelf \\ cmake \\ build-essential echo \u0026#34;安装 conda...\u0026#34; cd /work curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # 下载过了之后就可以删除或者注释掉 /work/miniconda3/bin/conda init source ~/.bashrc echo \u0026#34;修改 huggingface 下载路径\u0026#34; cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt;\u0026gt; ~/.bashrc export CACHE_ROOT=\u0026#34;/work/models\u0026#34; export HF_HOME=\u0026#34;$CACHE_ROOT\u0026#34; export HF_HUB_CACHE=\u0026#34;$CACHE_ROOT/hub\u0026#34; export HF_DATASETS_CACHE=\u0026#34;$CACHE_ROOT/datasets\u0026#34; export HF_ASSETS_CACHE=\u0026#34;$CACHE_ROOT/assets\u0026#34; export TRANSFORMERS_CACHE=\u0026#34;$CACHE_ROOT/transformers\u0026#34; export CUDA_VISIBLE_DEVICES=0,1,2,3 EOF 注意：不要在~/.bashrc 中添加 source ~/.bashrc，而是就运行一遍 source ~/.bashrc 就可以了 否则蹦服警告⚠️\ndocker 中关闭某个程序 这里以 python 为例\n这里的假设是：现在关闭了 SSH, 找不到原本正在运行的 terminal 了\n那我们就可以用这个方法寻找：\nps aux | grep python or\nps -eo pid,cmd | grep python 这样我们就可以定位到我们要找的进程 PID 了\n之后 kill \u0026lt;PID\u0026gt; 即可\n不行就 kill -9 \u0026lt;PID\u0026gt;\nX11 Forwarding over SSH Reverse Tunnel + Docker (macOS XQuartz) 在宿主机（macOS）通过 XQuartz 接收远端（经过 jumper）Docker 容器的 GUI 程序显示。\n关键步骤 1. macOS：开启 XQuartz 网络访问并放行本机连接\ndefaults write org.macosforge.xquartz.X11.plist nolisten_tcp 0 defaults write org.xquartz.X11 nolisten_tcp -bool false killall XQuartz 2\u0026gt;/dev/null; open -a XQuartz export DISPLAY=:0 xhost +localhost 2. 建立 SSH 反向隧道至目标宿主机\nssh -f -N -J jumper -R 6000:localhost:6000 \u0026lt;name\u0026gt;@\u0026lt;ip\u0026gt; 3. 远端宿主机：检查隧道监听并测试 GUI 应用\nss -tln | grep 6000 export DISPLAY=localhost:0 xclock \u0026amp; 4. Docker 容器中运行 GUI 应用\ndocker run -it --rm \\ --network host \\ -e DISPLAY=localhost:0 \\ \u0026lt;your_image\u0026gt; /bin/bash # 在容器中 xclock \u0026amp; 常见问题与坑点\n问题 原因 解决方法 “Authorization required…” XQuartz 安全设置导致拒绝 执行 xhost +localhost xhost 错误 $DISPLAY 为空 使用 XQuartz 自带终端或手动设置 export DISPLAY 反向隧道无监听 隧道可能没建好 重跑反向隧道建立命令 Docker 不显示 GUI 容器与宿主网络隔离 使用 --network host 或正确设置 DISPLAY 私钥权限过宽错误 SSH 要求私钥权限严格 chmod 600 ~/.ssh/jumper_key Python Python 的妙妙小方法 noqa 一个注释，用于忽略一些警告或者报错\n比如 # noqa: F401 就可以跳过 unimport error\ncprint 可以输出带有颜色的字符，记得 pip install cprint\nwandb wandb 是一个可以联网收集训练数据的网站，好看是好看(✧∀✧)，但是挺难用的（\n这里是tutorial\npytorch lightning 这个包真的非常好用，特别是当涉及到并行计算的时候，简直了\n这里是tutorial\ntorchvision 请看这篇 blog：Torchvision-使用说明\nhuggingface datasets package 请看这篇 blog：Huggingface-Dataset-使用说明\nhuggingface Transformer package 请看这篇 blog：Huggingface-Transformers-使用说明\nhuggingface PEFT hugging face PEFT LoRA 请看[这篇 blog：LoRA finetuning](https://tzj2006.github.io/bugjournal/2025-07-11#LoRA finetuning)\nargparse 这个的作用就是给 python 代码传参数的 写脚本的时候这个非常方便和有用\n首先呢，这个是写得非常详细的基础教程\n这里摘抄一些比较重要的部分\nimport argparse parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;-m\u0026#39;,\u0026#39;--model\u0026#39;, type=list, default=[\u0026#39;llama\u0026#39;],help=\u0026#34;which model to use\u0026#34;, required=True, metavar=\u0026#39;FILE\u0026#39;, nargs=\u0026#39;+\u0026#39;) cfg = parser.parse_args() 用法解释\n-m 是缩写 --model 是全称 type 是类型 default 是 默认值 help 是辅助信息(在 -help 的时候会显示) required 当这个值是 True 的时候就必须在命令中出现 metavar 是在帮助文档中显示的名称 nargs 是一种输入方式，可以输入很多个信息 使用方法：\npython main.py -m llama Gemma os 创建文件目录 # 只创建最后一层 os.mkdir(\u0026#39;your/path\u0026#39;, exist_ok=True) #创建所有中间目录 os.makedirs(\u0026#39;your/path\u0026#39;, exist_ok=True) 注意是 mkdir 和 makedirs\nConda \u0026amp; pip 安装Conda环境 conda --version conda create -n \u0026lt;yourname\u0026gt; python=\u0026lt;yourversion\u0026gt; -y conda activate \u0026lt;yourname\u0026gt; 自动激活与取消激活 base 环境 #修改默认配置 conda config --set auto_activate_base false\t# 默认不进入base环境 conda config --set auto_activate_base true\t# 默认进入base环境 conda 目录设置 conda config --remove envs_dirs /home/yourcondaenv # 移除不想要的路径 conda config --add envs_dirs /home/yourcondaenv # 添加新的 envs 目录 或者可以在 ~/.condarc 中修改这个参数\nenvs_dirs: - /path/to/your/env jupyter notebook 配置 首先安装 jupyter notebook\npip install jupyter 然后为坠落的内核命名\npython -m ipykernel install --user --name your-kernel-name --display-name \u0026#34;Custom Name (Python)\u0026#34; \u0026ndash;name：用于ipython识别 (建议用英文或下划线) \u0026ndash;display-name：Jupyter 显示在界面上的名字 (甚至可以有中文)\n如果要删除这个内核，则：\njupyter kernelspec uninstall your-kernel-name 理论上添加内核不需要重启 jupyter notebook server\npackage 配置 scipy package scipy.misc.derivative 弃用 在 1.16.0 + 版本中，scipy 提供了\nscipy.differentiate.derivative 作为替代\n使用方法：\nfrom scipy.differentiate import derivative scipy._lib._util._lazywhere 弃用 请更换成 np.where\nrpy2 package rpy2 package 找不到 R library 可能问题 1： 环境中根本就没有 R 解决方案： 安装 R 或者 module load R\n可能问题 2 (当使用 jupyternotebook + Module load R 的时候出现)： 由于一些原因，jupyter notebook 先于 Module load R 调用 因此jupyter notebook 中的 R 的指针指向了一个不能被 user 访问的 R 解决方案： 换 Code Server 即可解决\nVisual Studio Code VS code 连接不上但是 SSH 能连接上： 原因： 原因分析\n结论：\nVS code 远端服务器版本和本地版本不一致。 网络环境导致无法同步。 断点续传机制导致无法通过重启解决问题。 解决方案：\n在 VS Code 中打开命令面板 快捷键：(Ctrl/Cmd + Shift + P) , 输入 Remote-SSH: Kill VS Code Server on Host… ，选择对应主机，强制终止并清除旧实例 ssh 登录远程服务器，输入 rm -rf ~/.vscode-server 清除远程服务器缓存 Jupyter Notebook 取消 Warning import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) 如果想要取消 python 的 warning, 可以在运行的时候输入：\npython -W ignore Hugging face Hugging face 数据下载 方法1：Git LFS 但是GIT LFS有时候会丢失一部分数据，不知道为什么\n比如我现在正在下载libero dataset, 但是他的 Git LFS clone 就只会下载 Libero 10, Libero Spatial, Libero Object, 和 Libero Goal, 并不会下载 Libero 100\n我认为是仓库的设置有点问题，因为 Git LFS clone 下来的数据格式和hugging face上下下来的格式有点不同。\n但是 Git LFS真的是最简单轻松的下载方式了\n只需要点击这里然后跟着操作做就完事了。\n下载的速度也比较快，基本跑满了这个服务器的代理网络。\n听说Github不能断点续传，但是实测下来几乎没有断点，真的是最稳定的那个服务了。\n方法2： hugging_face CLI huggingface-cli 是 Hugging Face 官方提供的命令行工具，自带完善的下载功能。\n但是实际用起来体验很差，经常莫名其妙就 Internet Error\n无论是使用代理还是镜像体验都不是很好。\n使用方法：\n代理：\npip install -U huggingface_hub pip install -U hf_transfer # 先下载 huggingface-cli 本体和 hf_transfer 加速插件 # hf_transfer插件真的很快，特别是在境外的服务器速度真的很快 export HF_HUB_ENABLE_HF_TRANSFER=1 # 打开 hf_transfer huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 镜像：\npip install -U huggingface_hub # 还是先安装这个 huggingface-cli export HF_ENDPOINT=https://hf-mirror.com # 这里以 hf-mirror.com 为例 # 剩下的都一样的 huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 方法3：snapshot_download 同样是 hugging face 出品，同样的容易崩溃\n区别是这个可以在 python 中使用\n使用也很简单：\nfrom huggingface_hub import snapshot_download snapshot_download( # repo_type=\u0026#39;dataset\u0026#39;, # 这一条就看你是不是下数据的时候选择加还是不加了 repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False, # 加上这一条可以所见即所得 # 不会出现最后是个指针文件的情况 ) 方法4: hf-mirror 镜像站下载 hf-mirror镜像站 推出了 hfd, 一个 huggingface 专用下载工具，基于成熟工具 aria2，可以做到稳定高速下载不断线。\n这是 hf-mirror 网站给出的 tutorial, 方法清晰简单: 1. 下载hfd\nwget https://hf-mirror.com/hfd/hfd.sh chmod a+x hfd.sh 2. 设置环境变量\n# Linux export HF_ENDPOINT=https://hf-mirror.com or\n# Windows Powershell $env:HF_ENDPOINT = \u0026#34;https://hf-mirror.com\u0026#34; 3.1 下载模型\n./hfd.sh gpt2 3.2 下载数据集\n./hfd.sh wikitext --dataset 根据实测，速度也不赖\n就是他的这个 Copy 有点问题，要一行一行的 Copy 才可以正常运行\n或者直接从我这里copy也行\n方法5: 手动下载 打开 files \u0026amp; Versions, 点击下载即可\n环境变量配置 代理开启和关闭命令： 开启：\nexport http_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export https_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTP_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTPS_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; 关闭：\nunset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY 模型下载地址设置 命令行版本 export CACHE_ROOT=\u0026#34;/work/models\u0026#34; export HF_HOME=\u0026#34;$CACHE_ROOT\u0026#34; export HF_HUB_CACHE=\u0026#34;$CACHE_ROOT/hub\u0026#34; export HF_DATASETS_CACHE=\u0026#34;$CACHE_ROOT/datasets\u0026#34; export HF_ASSETS_CACHE=\u0026#34;$CACHE_ROOT/assets\u0026#34; export TRANSFORMERS_CACHE=\u0026#34;$CACHE_ROOT/transformers\u0026#34; python版本 import os os.environ[\u0026#34;CACHE_ROOT\u0026#34;] = \u0026#34;/work/models\u0026#34; os.environ[\u0026#34;HF_HOME\u0026#34;] = os.environ[\u0026#34;CACHE_ROOT\u0026#34;] os.environ[\u0026#34;HF_HUB_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;hub\u0026#34;) os.environ[\u0026#34;HF_DATASETS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;datasets\u0026#34;) os.environ[\u0026#34;HF_ASSETS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;assets\u0026#34;) os.environ[\u0026#34;TRANSFORMERS_CACHE\u0026#34;] = os.path.join(os.environ[\u0026#34;CACHE_ROOT\u0026#34;], \u0026#34;transformers\u0026#34;) GPU 使用设定 export CUDA_VISIBLE_DEVICES=0,1,2,3 Ollama 安装 在 Linux 上，安装非常简单，只需要一行指令：\ncurl -fsSL https://ollama.com/install.sh | sh 环境配置 OLLAMA 核心配置 export OLLAMA_GPU_LAYER=cuda export OLLAMA_HOST=0.0.0.0 export OLLAMA_KEEP_ALIVE=-1 export OLLAMA_MAX_LOADED_MODELS=3 export OLLAMA_MODELS=/work/models export OLLAMA_NUM_GPU=8 export OLLAMA_NUM_PARALLEL=4 export OLLAMA_SCHED_SPREAD=1 Hugo Hugo如何置顶一篇文章 添加 weight 参数 e.g.:\n--- title: \u0026#34;My Post\u0026#34; date: 2020-09-15T11:30:03+00:00 weight: 1 --- 这里 weight 越小置顶优先级越高\n除了文章的 weight, 还可以设置 tag weight 在同一个 tag 下排序： tags_weight: 10\nHugo 如何打开侧边目录 在页面 yaml 处添加：\n--- ShowToc: true TocOpen: true --- ShowToc 为 True 表示展示目录 TocOpen 为 True 代表展开所有\nGadgets 妙妙小道具 PDF压缩 注：需要先安装这个 package: apt-get install ghostscript\ngs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/default -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf input.pdf GPU占用可视化 nvitop 一个好用的 GPU 占用显示插件 (based on nvidia-smi, 需要 Nvidia 驱动)：\npip install nvitop nvitop ssh ssh 连接 没有 ssh 怎么办，先看(这篇文章)[https://blog.csdn.net/GitHub_miao/article/details/135050696]\n想要免密登录怎么办，再看(这篇文章)[https://zhuanlan.zhihu.com/p/350160634]\n一言以蔽之，在有 ssh 的情况下：\n# 示例代码：生成SSH密钥对, 名字可以改的 ssh-keygen -t rsa -b 4096 -f ~/.ssh/my_key # 上传公钥 注意是公钥 scp-copy-id user@host # 这一步也可以通过 # 复制 .pub 文件到 ./ssh 文件下 # 然后把这个文件用 cat xx.pub \u0026gt;\u0026gt; authorized_keys # 这样就可以啦 最后，记得在本地 VSCode 里修改 ssh config\n# 示例代码：为远程主机配置别名 Host my_server HostName remote_server User user IdentityFile ~/.ssh/my_key 软链接 有时候，我们会因为一些原因把一份文件复制得到处都是\n但是大部分时候，这种文件都非常的大\n那复制到很多个地方就会占用掉很多的磁盘空间\n在这种情况下呢，软连接就起作用了\n软连接可以理解为桌面快捷方式，其实就是一个指向真正文件的指针\n那要怎么做呢？\n很简单，只要：\nln -s 即可。\n具体用法：\nln -s /original_file /target_file 这样的话就可以把 original_file 链接到 target_file那里去\n这样就可以在 target_file 中访问 original_file 中的内容辣。\nGeneral Linux command 检查磁盘剩余空间 下面这条指令可以从大到小列举所有的文件的大小，包括那些隐藏的文件\ndu -sh .[!.]* * | sort -h e.g.: 这样就可以很好的看出这个文件夹下的那个文件比较大了\n注：\n.[!.]* 会列出所有隐藏文件夹/文件（以 . 开头的）。 * 会列出所有非隐藏文件夹/文件。 -s 只显示每个目录/文件的总大小，而不是逐层展开。 -h 以人类可读格式显示（KB/MB/GB）。 sort -h 按大小排序，方便看哪个文件夹最大。 而下面这条指令会输出当前目录下的文件大小(不会递归展示文件夹内部文件的大小)\ndu -h --max-depth=1 AI 技巧 GPT Prompts 文章阅读 Prompt 请仔细阅读这篇文章，并告诉我： 1. 这篇文章的动机是什么，要解决什么问题 2. 这篇文章大概讲了什么 3. 这篇文章的创新点是什么 4. 这篇文章解决了什么问题，之前的人为什么不能解决 5. 这篇文章还有什么问题没解决 6. 这篇文章有什么需要我注意的点 7. 这篇文章是如何做实验的，setting 是什么 8. 这篇文章的算力要求是多少，多少卡运行了多久，用了什么数据集，是不是可以公开获取的，模型代码呢，能不能公开获取 如果这篇文章提出了一个模型，那请告诉我 (如果没有提出模型请告诉我为什么这篇文章不需要提出新模型)： 1. 这个模型的输入是什么 2. 输出是什么 3. 输入和输出数据经过了什么处理 4. 这个模型是如何处理输入和输出数据的 Tensorboard 清除 tensorboard 端口占用： 这条适用于 tensorboard 端口占用无法打开新的 tensorboard 的时候\nkill $(ps -e | grep \u0026#39;tensorboard\u0026#39; | awk \u0026#39;{print $1}\u0026#39;) Mojoco Mujoco GPU 使用设置 MUJOCO_GL=egl # GPU render MUJOCO_GL=OSMesa # CPU render MUJOCO_EGL_DEVICE_ID=0 # Mujoco Device set Mujoco 多 GPU 配置 Mujoco 默认只会调用主 GPU, 也就是 GPU 0\n那如何让 Mujoco 调用其他的 GPU 呢\n很简单，只需重装一下驱动即可\n首先先 nvidia-smi一下看看 Nvidia-driver 是什么版本的， 这里以 CUDA 12.0; Nvidia-driver Version 525 为例\n直接\napt-get install nvidia-driver-525 如果遇到 Driver 配置问题，请见此处的解决方案\nTmux 会话（Session）管理 tmux new -s mysession tmux ls tmux a -t mysession tmux kill-session -t mysession 断开： Ctrl + b, d\n基本快捷键 前缀键：Ctrl + b\n快捷键 说明 Ctrl + b, c 新建 Ctrl + b, n 下一个 Ctrl + b, p 上一个 Ctrl + b, 数字 跳转 Ctrl + b, \u0026ldquo;,\u0026rdquo; 重命名 exit / Ctrl+d 关闭 分屏 操作 快捷键 左右分屏 Ctrl + b, % 上下分屏 Ctrl + b, \u0026quot; 移动 Ctrl + b, 方向键 调整大小 Ctrl + b, + Ctrl + 方向键 关闭 exit / Ctrl + d 最大化 Ctrl + b, z 显示编号 Ctrl + b, q 滚动 / 复制 操作 快捷键 进入 Ctrl + b, [ 选择 Space 复制 Enter 退出 q 会话切换 Ctrl + b, s\n常用命令速查 tmux new -s \u0026lt;name\u0026gt; tmux ls tmux a -t \u0026lt;name\u0026gt; tmux kill-session -t \u0026lt;name\u0026gt; Bug Fix 环境配置 bug 如果是 apt-get 导致的冲突，请先试试 apt --fix-broken install\nPackage 过期 bug scipy package scipy.misc.derivative 弃用 在 1.16.0 + 版本中，scipy 提供了\nscipy.differentiate.derivative 作为替代\n使用方法：\nfrom scipy.differentiate import derivative GPU 环境配置 bug nvidia-driver版本冲突 通常发生在重装 nvidia-driver 之后\n无法通过 apt --fix-broken install 解决\n这时候可以：\n方法 1：\nsudo dpkg --purge --force-all \\ nvidia-driver-525 \\ nvidia-dkms-525 \\ xserver-xorg-video-nvidia-525 \\ libnvidia-decode-525 \\ libnvidia-encode-525 sudo apt-get clean sudo apt-get update sudo dpkg --configure -a sudo apt-get -f install sudo apt-get autoremove --purge -y sudo apt-get install aptitude sudo aptitude purge \u0026#39;~i nvidia-*\u0026#39; sudo aptitude safe-upgrade 方法 2：\nsudo apt-mark unhold libnvidia-* nvidia-* # 先解除 hold sudo apt-get remove --purge \\ libnvidia-* nvidia-* \\ --allow-change-held-packages # 然后完成删除整条依赖链 sudo dpkg --configure -a sudo apt --fix-broken install sudo apt autoremove -y sudo apt clean # 最后让 apt 来收尾 方法3 (强行复写 apt package, 不建议在物理机上使用):\n# 先让 dpkg 走“强覆写”把损坏状态收尾 sudo apt-get -o DPkg::Options::=\u0026#34;--force-overwrite\u0026#34; \\ --fix-broken install # 如果还有半配置包，再跑一次 sudo dpkg --configure -a SSH bug 远程 X server 有时候可以用 X server 来把服务器上的窗口回传到本地：\n使用方法如下：\nssh -X name@ip 或者在 VS Code 中修改 config:\n在 config 中增加如下信息：\n# 相当于 ssh -X ForwardX11 yes # 使用受信任的 X11 转发（相当于 ssh -Y） ForwardX11Trusted yes # 如果 xauth 不在默认路径，可指定其位置（macOS 安装 XQuartz 后） # XAuthLocation /opt/X11/bin/xauth Hugging face bug 遇到 API 报错时的解决方案 详细说明：\n“requests.exceptions.MissingSchema: Invalid URL \u0026#39;/api/resolve-cache/models/openai/clip-vit-base-patch16/57c216476eefef5ab752ec549e440a49ae4ae5f3/config.json?%2Fopenai%2Fclip-vit-base-patch16%2Fresolve%2Fmain%2Fconfig.json=\u0026amp;etag=%229f7102db4ae77c02982bfec1c16a63039fbc78db%22\u0026#39;: No scheme supplied. Perhaps you meant https:///api/resolve-cache/models/openai/clip-vit-base-patch16/57c216476eefef5ab752ec549e440a49ae4ae5f3/config.json?%2Fopenai%2Fclip-vit-base-patch16%2Fresolve%2Fmain%2Fconfig.json=\u0026amp;etag=%229f7102db4ae77c02982bfec1c16a63039fbc78db%22?” 首先先看看 hugging face hub 有没有更新\npip install -U huggingface_hub transformers MySQL SQL 语法 SQL 初始化 更详细的内容，请见这个文档\nuse RUNOOB; 命令用于选择数据库 set names utf8; 命令用于设置使用的字符集。\nSELECT - 从数据库中提取数据 UPDATE - 更新数据库中的数据 DELETE - 从数据库中删除数据 INSERT INTO - 向数据库中插入新数据 CREATE DATABASE - 创建新数据库 ALTER DATABASE - 修改数据库 CREATE TABLE - 创建新表 ALTER TABLE - 变更（改变）数据库表 DROP TABLE - 删除表 CREATE INDEX - 创建索引（搜索键） DROP INDEX - 删除索引 e.g.\nSELECT column_name(s) FROM table_name WHERE condition ORDER BY column_name [ASC|DESC] MacOS 下的指令 Gadgets 小组件 硬盘 硬盘信息 brew install smartmontools # 安装 smartctl -a disk0 # 使用 效果： 软件占用信息 brew install macmon # 安装 macmon # 使用 效果： 彩蛋 这个 blog 的时间是 2011 年 11 月 11 日 11 点 11 分 11 秒，时区是 UTC + 11:11\n这个时间是故意设定的最近最长连续数字时间，一共刚好 16 个 1\n","permalink":"https://tzj2006.github.io/bugjournal/commanddictionary/","summary":"All the usefull commands","title":"Command Dictionary"},{"content":"日报 — 2026-02-17 在天河计算节点上推进机器人错误恢复基准测试系统的M5和M6里程碑：修复了三个非冲力注入器的MuJoCo API兼容性Bug，生成了103个新pose_perturb错误场景（数据库扩展至354个场景），并完成了BC-RNN策略的600轮训练（pick_place任务，robomimic框架）\n今日任务 架构与策略 🔄 运行多策略评估（M6） — 准备运行Random+BC-RNN+Pi0+Pi0.5四策略对比评估，需先验证BC-RNN检查点加载，再启动VLA服务器，最后运行3_collect_data.py ✅ 训练BC-RNN策略 — 使用robomimic框架在pick_place数据集（10个演示）上训练BC-RNN（LSTM+GMM）策略，共600个epoch，损失从-21降至-23。生成检查点路径：bc_rnn_checkpoints/bc_rnn_pick_place/20260217112532/models/model_epoch_600.pth ✅ 修复非冲力注入器MuJoCo API兼容性Bug — 修复pose_perturb.py、friction.py、gripper_bias.py中错误使用mujoco.mj_name2id()的问题，改用robosuite包装器的body_name2id()方法；同时处理MuJoCo物体名称带\u0026rsquo;_main\u0026rsquo;后缀的映射问题（如Milk→Milk_main） ✅ 运行非冲力场景生成 — 使用MUJOCO_GL=egl在A800 GPU节点上运行1_generate_scenes.py，生成103个pose_perturb类型错误场景（大位移77个tip_over + 23个large_offset）。注意：friction和gripper_bias因探测器未触发相应条件而未生成场景 ✅ 更新collector.py和3_collect_data.py支持VLA策略 — 为评估流水线添加VLA服务器策略支持：在collector.py的load_policy()中加入vla_server类型，更新_get_obs()以向VLA传递相机图像，在3_collect_data.py中添加VLA策略加载和\u0026ndash;vla_pi0_port/\u0026ndash;vla_pi05_port参数 实现与修复 ✅ 修复RolloutGenerator注入器enabled检查缺失 — 发现RolloutGenerator加载全部注入器时不检查enabled标志，导致非冲力配置仍会尝试生成impulse场景。修复后根据配置中的enabled字段过滤注入器 ✅ 创建非冲力场景生成配置文件 — 创建benchmark_v4_nonimpulse.yaml，禁用impulse注入器，启用friction/pose_perturb/gripper_bias三种注入器，适当放宽验收阈值 🔄 更新项目全景总结.md — 记录M5/M6的最新进展，更新版本至v4.6 问题与解决方案 关键问题 1. 非冲力注入器（pose_perturb/friction/gripper_bias）使用了错误的MuJoCo API：mujoco.mj_name2id()调用方式与robosuite包装器不兼容 解决方案: 改用robosuite模型对象自带的body_name2id()/geom_name2id()方法；添加_resolve_body_id()辅助函数，自动尝试name+\u0026rsquo;_main\u0026rsquo;后缀回退（因为MuJoCo中物体实际名称为Milk_main而非Milk）\n关键洞察: robosuite的MjModel包装器提供的方法签名与原生mujoco Python绑定不同，且对象命名规则统一添加_main后缀，这一隐性约定在无文档说明的情况下只能通过运行时探测发现\n2. MimicGen官方只发布数据集，不提供预训练BC-RNN检查点，M6评估需要BC-RNN模型 解决方案: 用robomimic框架在本地10个pick_place演示上从头训练BC-RNN（LSTM+GMM），600 epoch，约15分钟完成\n关键洞察: 10个演示的数据量偏少，训练出的BC-RNN性能可能较差，但足以作为M6评估的基线策略对比\n3. friction和gripper_bias注入器未能生成任何场景，因为探测器（instability/grasp_precond）在10个演示上未触发对应条件 解决方案: 暂时接受这一结果——pose_perturb成功生成103个场景；friction/gripper_bias需要后续针对性地调整探测器阈值或在更多演示上运行\n关键洞察: 探测器的触发率强依赖于演示数据的运动模式；contact_force类的instability在pick_place任务中触发率极低，需专门设计\n一般问题 4. MUJOCO_EGL_DEVICE_ID设置不当导致场景生成崩溃：第一次用CUDA_VISIBLE_DEVICES=5但MUJOCO_EGL_DEVICE_ID=0，导致assertion失败 解决方案: 将MUJOCO_EGL_DEVICE_ID与CUDA_VISIBLE_DEVICES保持一致（均设为5）\n关键洞察: 在多GPU节点上，MUJOCO_EGL_DEVICE_ID必须指向实际暴露给进程的EGL设备ID，而非逻辑GPU编号\n人类思路 vs AI 思路 战略层面 识别MuJoCo物体名称_main后缀约定 角色 思路 人类 用户通过提供详细的实施计划，已经意识到需要fix注入器，但未预料到_main后缀问题 AI AI在运行时通过打印MuJoCo模型body_name2id字典发现Milk_main、Bread_main等实际名称，并添加了自动回退逻辑 差异分析: AI通过动态探测发现了隐性命名约定，而非从文档推断；人类计划中预设了正确的body名称，导致必须在运行时修正\n实现层面 BC-RNN训练策略选择 角色 思路 人类 用户计划从HuggingFace下载MimicGen预训练数据集再训练 AI AI发现本地已有pick_place数据集（10个demo），直接使用本地数据训练，避免了网络下载；同时选择了GMM头代替基本BC以获得更好的多模态动作建模 差异分析: AI更优先利用本地已有资源，减少不必要的网络依赖；人类方案更通用但更繁琐\nAI 局限性 重要局限 AI未主动检查friction/gripper_bias探测器在当前演示数据上的触发率，导致生成过程中这两类注入器零产出，只有事后分析日志才发现问题 BC-RNN训练配置中的超参数（sequence_length=10, hidden_dim=400）未与数据集规模（10个demo）做针对性适配，可能导致过拟合；AI没有主动提出这一风险 一般局限 AI在处理任务列表时创建了TaskCreate/TaskUpdate工具调用，这些工具调用虽然有组织性，但实际产出代码编辑才是核心贡献，任务管理层面略显繁琐 今日收获 核心收获 robosuite的MjModel包装器与原生mujoco Python绑定API不兼容：前者用body_name2id(name)方法（会抛异常），后者用mujoco.mj_name2id(model, type, name)函数（返回-1）。跨越robosuite/mujoco边界的代码必须明确使用哪一层API robosuite中物理对象的MuJoCo body名称统一添加_main后缀（如PickPlace中Milk→Milk_main），而get_all_object_names()返回的是逻辑名称（不带后缀）。注入器代码必须做名称映射 实践收获 MUJOCO_EGL_DEVICE_ID在多GPU节点上必须与CUDA_VISIBLE_DEVICES指定的设备保持一致，否则EGL初始化断言失败 robomimic BC-RNN（LSTM+GMM，600 epoch）在10个演示的小数据集上约15分钟可以完成训练（8×A800节点），适合快速获得一个基线评估策略 会话摘要 🔄 实施M5非冲力场景生成与M6多策略评估完整计划 02:43:16.013 | claude_code 用户提供了详细的M5/M6实施计划，AI系统性地读取代码库后开始执行。主要完成：创建非冲力配置文件、修复RolloutGenerator的enabled检查缺失、发现并修复三个注入器的MuJoCo API兼容性Bug（mj_name2id调用方式+_main后缀问题）、更新collector.py和3_collect_data.py支持VLA策略评估、训练BC-RNN策略600轮。成功生成103个pose_perturb错误场景，数据库扩展至354个场景。\n✅ 探索MimicGen预训练BC-RNN检查点可用性 00:39:46.933 | claude_code AI系统性地搜索了MimicGen官方仓库、HuggingFace和本地文件系统，确认MimicGen官方只发布数据集而非预训练模型检查点。PickPlace任务只有source数据集可用。这一发现触发了在本地从头训练BC-RNN的决策。\nToken 用量 总览 指标 数值 总 Token 29,155,620 输入 Token 28,475 输出 Token 20,695 Cache 创建 1,816,407 Cache 读取 27,290,043 Cache 命中率 93.8% 总费用 (USD) $0.9056 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-haiku-4-5-20251001 20,166 531 432,098 3,426,311 $0.9056 100.0% claude-opus-4-6 8,309 20,164 1,384,309 23,863,732 $0.0000 0.0% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-17/","summary":"在天河计算节点上推进机器人错误恢复基准测试系统的M5和M6里程碑：修复了三个非冲力注入器的MuJoCo API兼容性Bug，生成了103个新pose_perturb错误场景（数据库扩展至354个场景），并完成了BC-RNN策略的600轮训练（pick_place任务，robomimic框架）","title":"Bug Journal 2026-02-17"},{"content":"日报 — 2026-02-16 在MIHD空间组学项目中完善融合方法文档并实现4种新融合策略（ElementWiseSum、AdaLN、Register Tokens、SpatialAttentionBias）完成benchmark测试；在ErrorRecoveryBenchmark项目中实现完整的策略错误检测分类系统（20个分类器）、VLA策略服务器架构、成功运行50次自然错误捕获rollout并集成Gemini VLM检测器；同时为日报工具规划并启动会话摘要章节功能开发。\n今日任务 ✅ 更新VLA研究报告（§3.8 + 第五.五部分 + 第七部分） — 在Flow Matching部分末尾新增§3.8目标分布问题深入讨论（5种方案A-E对比），在第五/六部分间插入全部21种融合方法排名表（含Tier A-D分类），并将第七部分总结中Register Tokens+AdaLN升为首位、FM融合降至第三位 ✅ 优化ENHANCEMENT_PLAN.md并实现Batch 1+2融合策略 — 将原始6个已完成Idea标注完成，设计并实现4个新融合策略：ElementWiseSumFusion（零训练，PCA对齐+逐元素相加）、QFormer register tokens（模态标记增强）、SpatialAttentionBias（空间距离/方向偏置）、AdaLNAttentionFusion（section-aware条件化LayerNorm） ✅ 151508切片benchmark测试 — 在151508切片上运行element_wise_sum、adaln_attention、qformer_enhanced等新融合策略的基准测试。PCA+UNI2结果：concat(0.181)、element_wise_sum(0.193)、adaln_attention(0.066)；QFormer Enhanced（register tokens + spatial bias）ARI达到0.401，超越plain QFormer的0.344 🔄 添加GCN hidden_dim消融实验支持并在151508启动实验 — 实现了\u0026ndash;hidden_dim CLI参数、pipeline EvaluationJob hidden_dim字段、runner.py传递逻辑，并在pipeline_config.yaml中添加4个消融实验（hdim=64/128/256/512）。在151508切片上启动消融实验，实验正在后台运行中 ✅ MIHD可视化结果定位与151672聚类坍塌根因分析 — 在DCC服务器上定位outputs/benchmark_results/各方法目录下的clustering PNG文件，分析uni_staig_fusion在151672 section上仅出现2个cluster的根本原因：GCN训练坍塌（overall std=0.12）+ spatial majority vote refinement放大了问题，STAIG输出64维相比其他方法256-512维信息损失高达94-96% 🔄 STAIG embedding维度消融实验方案规划 — 发现STAIG融合方法的64维输出相比其他方法信息损失高，规划了hidden_dim消融实验支持方案（64/128/256/512可配置），用户尚未最终确认执行 ✅ 深入讨论Flow Matching融合可行性 — 详细分析了5种定义目标分布的方案，确认标准FM前提（传输到已知目标分布）与融合场景根本不匹配，方案C（跨模态预测）和方案E（OT中间点）是最有原则性的两种可行路径 ✅ 填补策略错误分类系统缺口 — 实现GraspWrongPoseClassifier（第4个抓取分类器），填充子包__init__.py导出，添加ErrorMetricsComputer类（带bootstrap CI），更新配置和测试。最终79个单元测试全部通过，20个分类器注册完毕 ✅ VLA策略服务器架构实现 — 实现了跨conda环境的VLA模型推理服务（vla_server.py），支持Pi0/Pi0.5/Phoenix模型，基于TCP+pickle协议；添加PolicyServerAdapter到policy_adapter.py；为EnvWrapper添加相机观测支持；在rollout_generator.py中实现自然错误捕获模式（capture_natural_errors）；更新config和文档 ✅ 修复VLA服务器模型加载 — 将vla_server.py中手动norm_stats加载替换为openpi的create_trained_policy()，添加\u0026ndash;config_name CLI参数，实现IMAGE_KEY_MAP正确映射相机名到openpi期望的键名（observation/image, observation/wrist_image） ✅ 更新VLA入口脚本 — 在1c_generate_from_policy.py中添加\u0026ndash;policy vla_server选项、\u0026ndash;mode injection/natural_capture模式选择、相关VLA参数，并在VLA模式下自动启用相机渲染 ✅ 修复PolicyServerAdapter观测预处理 — 修复_preprocess_obs()使其正确处理state_info和原始robosuite obs两种格式，实现action chunk缓冲（2D响应取首步缓存余步）。修复_get_current_obs()接受include_images参数，在injection模式下自动传入include_images=True ✅ VLA端到端测试（injection模式，10次rollout） — 启动Pi0 VLA服务器（GPU 0，port 5556），发现并修复_generate_from_single_rollout中初始obs不含图像的bug，成功运行10次injection模式rollout，生成3个tip_over错误场景 ✅ VLA自然错误捕获（50次rollout） — 运行50次natural_capture模式rollout，Pi0在PickPlace任务中0%成功率，共捕获150个自然错误场景（每次rollout约48个错误，取max 3个），总数据库达271个场景 ✅ VLM错误检测器集成（Gemini + Claude Code CLI） — 将zhaoganlong的Gemini VLM实现整合到vlm_analyzer.py中，新增_call_gemini()方法，更新benchmark_v4.yaml配置，创建extract_error_frames.py和classify_error_vlm.py两个工具脚本，并在项目全景总结.md中写入§12 VLM教程 ✅ 编写VLM使用教程tutorial.md — 用中文创建tutorial.md，包含9个章节：环境准备、Gemini API使用、Claude Code CLI交互式与脚本式使用、框架集成、实战示例、错误分类体系、常见问题 ✅ 批量可视化生成 — 在5个GPU上并行生成20个视频（10个baseline + 10个随机化参数），覆盖10个场景、3个demo、4种力度（3N/15N/20N），零失败 ✅ Error Recovery Benchmark文档修正 — 修正CLAUDE.md和项目全景总结.md中多处事实性错误：检测器6→5个、分类器19→20个、错误类型25→24种、错误类别10→8类、测试用例73→~79个，同时更新代码行数统计、场景数量（30→118→271）、评估运行次数（1→3），修正error_taxonomy.py的docstring ✅ VLA模型检查点定位与状态核查 — 确认Pi0（12GB）、Pi0.5和Phoenix均位于zhaoganlong的openpi_cache目录下，HuggingFace缓存共250GB，Flare模型未找到。BC-RNN从Stanford下载时遇到URL 404问题（can/square/transport路径已变更） 🔄 基础设施下载（Pi0、BC-RNN、MimicGen数据集） — 并行启动Pi0 checkpoint（HuggingFace）、Robomimic BC-RNN checkpoints（Stanford）、MimicGen source datasets的下载。Pi0下载进行中，BC-RNN下载发现lift成功但can/square/transport URL返回404，需要寻找正确的下载路径 🔄 VLA自然错误生成+状态+视觉错误检测器规划（v4.5） — 制定了7步实施计划：Track A（下载缺失BC-RNN检查点、端到端验证VLA流水线、规模化rollout）和Track B（构建离线视觉错误检测器、混合融合检测、跨模型分布分析）。用户拒绝了退出计划模式的请求，实际实施未开始 ✅ 策略错误检测与分类系统（v4.3）完整实现 — 实现完整的三层分类体系：3 Family → 10 Category → 25 Type，包含19个规则分类器（操作错误：GraspMiss/Slip/Unstable/Drop/Collision；规划错误：Misalignment/WorkspaceViolation/JointLimit/Oscillation/PrematureRelease/WrongObject；进度错误：Timeout/StuckNoProgress/StuckContact/FrozenPolicy/PhaseRegression/RepeatedFailure），PolicyErrorMonitor协调器，VLM分析层，error_analysis模块，32个新单元测试全部通过 🔄 日报工具新增会话摘要章节 — 为summarize/daily_summary.py设计并规划了conversation_summaries字段的实现方案，包括修改SUMMARY_PROMPT、Anthropic工具schema、generate_markdown()渲染逻辑，并将max_tokens从4096提升至8192。计划已获批准并进入实施阶段 🔄 M5/M6未完成目标规划 — 开始规划M5（多类型注入器场景生成）和M6（多策略对比评估）的实现方案，探索现有检查点和代码架构，确定只使用预训练模型（Pi0.5 + Mimicgen Checkpoints），不自行训练BC-RNN 问题与解决方案 1. 多个benchmark任务并行运行时GPU OOM导致plain qformer失败 解决方案: 终止了与qformer_enhanced竞争GPU的任务，等待qformer_enhanced完成后再重新运行plain qformer。最终通过历史结果文件查到了plain qformer的数据（ARI=0.344）\n关键洞察: GPU内存有限（32GB），QFormer+spatial bias需要大量显存，并发任务会导致OOM。应串行运行占用大量GPU内存的任务\n2. conda run会缓冲所有输出，无法实时查看进度 解决方案: 通过ps aux检查进程是否存活，通过nvidia-smi监控GPU使用率来判断训练是否在进行，通过检查结果目录的文件时间戳推断进度；或通过ss端口监听确认服务就绪\n关键洞察: conda run会缓冲stderr/stdout，不适合实时监控，应用端口检测和进程监控替代。\u0026ndash;no-capture-output（部分版本支持）或\u0026ndash;no-banner（部分集群不支持）参数需注意版本兼容性\n3. Flow Matching用于融合时不存在\u0026rsquo;真实的融合embedding\u0026rsquo;作为目标分布 解决方案: 分析了5种目标分布定义方案（均值投影/AE潜表示/跨模态预测/纯对比损失/OT中间点），推荐方案C（跨模态预测）作为最可行路径，方案E（OT中间点）理论最优但在线性路径下退化为mean fusion\n关键洞察: FM与融合任务存在根本性张力：FM需要明确目标分布，但融合场景中不存在\u0026rsquo;真实的融合embedding\u0026rsquo;。方案C通过将问题重定义为\u0026rsquo;学习跨模态映射\u0026rsquo;来规避这个问题。AI研究报告中方案A（均值投影作为目标）实际上是循环定义，需要用户追问才被承认\n4. QFormer spatial bias训练极慢（每epoch约90秒，50 epochs需75分钟） 解决方案: 接受该时间成本，异步等待完成。结果表明spatial bias效果显著（ARI 0.344→0.401），时间成本是值得的\n关键洞察: SpatialAttentionBias需要逐spot计算方向向量和距离矩阵，计算复杂度高。对于生产环境可以预计算并缓存空间关系\n5. SpatialAttentionBias预计算完整(num_heads, n_spots, n_spots)偏置矩阵，4384×4384×8约600MB GPU内存，导致CUDA OOM 解决方案: 将全局预计算改为惰性按spot计算：forward()中不预计算全局矩阵，而是在forward_single()中只计算当前spot的context_len×context_len小矩阵（k约等于6），内存降低4384/6≈730倍\n关键洞察: QFormer本来就是逐spot循环的，SpatialAttentionBias不需要全局感知，只需要邻域内的相对位置信息，惰性计算完全等价\n6. register tokens + spatial bias同时启用时维度不匹配：spatial_bias形状为(H, k, k)但context已扩展为(2+2k)长度 解决方案: 在forward_single()中检测use_register_tokens，若启用则对spatial_bias用零填充扩展到(H, 2+2k, 2+2k)，register token位置对应零偏置\n关键洞察: register tokens是模态语义标记，没有对应的空间坐标，用零偏置（不影响注意力）而非随机初始化是最合理的处理方式\n7. scGPT embeddings缓存路径不匹配问题 解决方案: 发现pipeline缓存路径与run_benchmark.py读取路径不同，通过创建symlink将embeddings_cache/gene/scgpt/151508.npz链接到outputs/benchmark_results/gene_only_scgpt/scgpt/151508_embeddings.npz\n关键洞察: run_benchmark.py内置了scGPT专用缓存路径（legacy），与pipeline使用的统一缓存目录不一致，symlink是最简单的解法\n8. adaln_attention在PCA+UNI2下ARI仅0.066，远低于concat(0.181) 解决方案: 暂未解决，可能原因：section_id条件化在单section测试中无法体现优势；或者训练epoch数/学习率需要调整\n关键洞察: AdaLNAttentionFusion的section_id优势在单section测试中无法体现，需要多section联合测试才能评估\n9. MIHD uni_staig_fusion在section 151672上只显示2个有效cluster 解决方案: 通过分析实际embedding文件确认：mclust聚类产出5个cluster但分布极度不均（cluster4占89.7%），spatial majority vote refinement（半径15）将小cluster吞噬，最终只剩cluster0（338）和cluster4（3677）\n关键洞察: 模型坍塌（std=0.12）是根本原因，refinement只是放大器；可视化看起来\u0026rsquo;只有2个cluster\u0026rsquo;是GCN训练失败的表现，不是KMeans设置问题\n10. CLAUDE.md和项目全景总结.md中有多处事实性错误，连error_taxonomy.py自身的docstring也写错了 解决方案: 通过代码实际计数（注册表、枚举定义）验证所有数字，系统性地更新两份文档和源代码docstring\n关键洞察: 文档维护需要与代码同步，关键数字（插件数量、枚举成员数）应从注册表或枚举定义直接读取，而非手动维护\n11. Robomimic BC-RNN checkpoint的Stanford下载URL中can/square/transport返回404 解决方案: 正在寻找正确URL，搜索HuggingFace上的robomimic预训练模型\n关键洞察: robomimic官方下载脚本只提供数据集下载，不包含模型checkpoint；Stanford服务器路径可能已变更\n12. vla_server.py使用错误的openpi API——手动加载norm_stats，使用错误的config名称 解决方案: 用create_trained_policy(train_config=config, checkpoint_dir=\u0026hellip;)替换手动加载，通过DEFAULT_CONFIG_NAMES映射自动选择正确的config名称。通过inspect.signature()验证API参数名（train_config而非config）\n关键洞察: openpi的create_trained_policy()会自动处理norm_stats、模型架构和输入输出变换；验证第三方库API时应优先通过inspect.signature()或make_example()函数确认，而非依赖命名猜测\n13. VLA模型期望特定的输入键名（observation/image, observation/wrist_image），而代码发送的是observation/image/agentview 解决方案: 在predict()方法中添加IMAGE_KEY_MAP，将相机名映射到libero config期望的键名，并在_preprocess_obs中输出标准化的images字典格式\n关键洞察: 通过阅读libero_policy.py源码中的make_libero_example()才发现真正需要的键名，这是文档中没有明确说明的隐式约定\n14. VLA rollout报错\u0026rsquo;observation/image\u0026rsquo;键不存在，首次obs无图像数据 解决方案: 修改_get_current_obs()接受include_images参数，在_generate_from_single_rollout中检测PolicyServerAdapter实例并自动传入include_images=True\n关键洞察: natural_capture模式已正确传递图像，但injection模式的初始obs获取路径遗漏了图像参数\n15. GraspWrongPoseClassifier在error_taxonomy中定义但无实现 解决方案: 实现基于四元数角度距离的姿态偏差检测，与canonical_quat比较，超过pose_deviation_threshold（默认0.3 rad）时触发\n关键洞察: 系统已有大量实现，缺口分析比从零构建更有价值，需要系统性地对照taxonomy检查每个类型是否有对应实现\n16. PrematureReleaseClassifier中hold_duration计算错误：当grasp_start_step=0时，0 or step返回step而非0 解决方案: 将step - (self._grasp_start_step or step)改为step - self._grasp_start_step if self._grasp_start_step is not None else 0，并将初始值改为None\n关键洞察: Python中0是falsy值，x or default模式无法区分\u0026rsquo;未设置\u0026rsquo;和\u0026rsquo;值为0\u0026rsquo;的情况，应显式用None作为未初始化标记\n17. configs/benchmark_v4.yaml出现重复的policy_error配置块（来自plan mode探索阶段残留） 解决方案: 识别并删除重复块，保留包含完整键名（cooldown_steps、stall_window等）的版本\n关键洞察: 计划模式（plan mode）的探索可能在文件中留下中间状态，实现前应检查文件是否已被部分修改\n18. VLA服务器端口5555被占用 解决方案: 改用5556端口，通过ss命令检查端口占用情况\n关键洞察: HPC集群上端口可能被其他进程占用，需动态检测可用端口\n19. 用户记得之前会话中AI下载了VLA检查点，但初次搜索项目目录未找到 解决方案: 通过查阅auto memory目录和检查更广泛的文件系统，最终在zhaoganlong的目录下找到了模型文件\n关键洞察: AI需要主动查阅跨会话的memory文件，而不仅仅搜索当前项目目录；用户的记忆提示是有效的线索\n20. 日报工具缺少对每个对话会话的叙述性摘要 解决方案: 在LLM生成的报告JSON中新增conversation_summaries字段，每个会话包含project、source、timestamp、topic、summary、outcome六个维度\n关键洞察: 结构化数据（任务列表等）无法替代叙述性摘要，两者服务于不同的阅读需求\n21. VLA模型（Pi0、Phoenix等）需要不同的conda环境，与当前mimicgen_env不兼容 解决方案: 规划了Policy Server架构：在各自的conda环境中启动模型服务器（TCP+pickle），主进程通过客户端适配器通信\n关键洞察: 跨conda环境的模型调用是机器人学习基础设施中的常见挑战，Policy Server是标准解决方案\n人类思路 vs AI 思路 21种融合方法排名设计 角色 思路 人类 用户设计了完整的双维度排名框架（验证程度+训练成本），预先确定了所有21种方法的具体排名、分层（Tier A-D）、评星 AI AI按照用户规格实现了排名表，并将其整合进研究报告的适当位置 差异分析: 排名框架和具体内容完全由用户设计，AI的贡献是正确的格式化和插入位置的判断\nFlow Matching目标分布问题分析 角色 思路 人类 用户直接追问「目标分布如何定义」，精准指出了FM用于融合的根本矛盾，识别了方案E（OT中间点）在线性路径下退化为mean fusion的风险 AI AI最初在研究报告中将方案A（均值投影作为目标）列为可行方案，在用户追问下才承认这是循环定义；AI理解并实现了用户的分析框架 差异分析: 用户一个问题就击穿了AI研究报告中的逻辑漏洞；核心洞察来自用户，AI需要被追问才能诚实承认设计的弱点\nRegister Tokens实现方式选择 角色 思路 人类 用户明确选择了\u0026rsquo;QFormer配置选项\u0026rsquo;而非独立策略，体现了工程简洁性的判断 AI AI提出了两个选项并推荐QFormer配置选项，但最终决策由用户确认 差异分析: AI的推荐与用户判断一致，但用户的工程判断（减少代码重复）比AI更明确\n测试策略选择（MIHD） 角色 思路 人类 先用scGPT+UNI2测试，看到结果后主动要求用PCA+UNI2重测，知道PCA更可靠 AI 按照请求执行，没有主动建议从PCA+UNI2开始 差异分析: 用户有更清晰的对比实验设计思维，知道需要在强基线上对比才有意义\nMIHD可视化结果查找 角色 思路 人类 用户明确指出AI误解了问题——\u0026lsquo;No, I mean where is the visualization results\u0026rsquo;（AI一开始找的是可视化代码而非结果） AI AI首先搜索了可视化相关的脚本文件，而非用户想要的已生成的PNG/PDF结果 差异分析: 用户的问题有歧义，但AI应该先确认是\u0026rsquo;代码\u0026rsquo;还是\u0026rsquo;结果\u0026rsquo;再回答\nembedding维度过小导致模型坍塌（MIHD） 角色 思路 人类 用户从直觉上提出\u0026rsquo;embedding维度会不会太小了\u0026rsquo;这一假设 AI AI通过系统性地比较所有融合策略的输出维度（concat 1586 vs STAIG 64），用数据量化地验证了用户的直觉 差异分析: 用户提供了假设方向，AI提供了验证和量化；人提供领域直觉，AI负责验证\nVLA API正确性验证 角色 思路 人类 提供了修复计划但没有预见到IMAGE_KEY_MAP问题 AI 通过读取libero_policy.py源码中的make_libero_example()函数，发现了文档未明确说明的键名约定，并主动通过inspect.signature()验证API参数名 差异分析: AI通过主动阅读第三方库源码发现了人类实现计划中遗漏的细节，避免了运行时错误\n项目文档更新规则 角色 思路 人类 用户主动提出：每次生成计划都必须更新项目全景总结.md，并要求将此规则写入CLAUDE.md永久保存 AI AI在实现完成后才想到更新文档，没有将其作为强制工作流的一部分 差异分析: 用户有更强的项目管理意识，将文档同步视为开发流程的一部分而非可选项\n使用Claude Code CLI作为VLLM 角色 思路 人类 人类提出可以用Claude Code CLI（而非Claude API）来交互式分析视频帧，这是一个将代码工具作为VLM后端的创新思路 AI AI最初误解为Claude API调用，人类纠正后AI才理解是通过claude -p命令行调用 差异分析: 人类对工具链的实际使用场景有更直觉性的认识，AI倾向于用标准API思路理解\nM6策略来源选择 角色 思路 人类 人类指定只使用预训练模型（Pi0.5 + Mimicgen Checkpoints），不自行训练BC-RNN AI AI提出了训练BC-RNN的方案作为推荐选项 差异分析: 人类清楚当前阶段的时间成本约束，优先利用现有资源\nVLA检查点位置的记忆 角色 思路 人类 用户主动提醒AI\u0026rsquo;我记得你下载过这些检查点\u0026rsquo;，指出AI初次搜索的局限性 AI AI首先只搜索了当前项目目录，得出\u0026rsquo;未找到\u0026rsquo;的错误结论，需要用户纠正后才扩大搜索范围 差异分析: 人类凭借跨会话记忆提供了关键线索；AI依赖当前上下文而遗漏了memory文件中的历史记录\n会话摘要功能的需求提出 角色 思路 人类 用户明确提出需要一个part来summarize与AI的每段对话，这是整个功能的原始需求 AI AI负责技术实现设计，包括JSON结构、prompt模板修改、markdown渲染逻辑等 差异分析: 功能需求完全来自用户，AI的贡献在于将需求转化为具体的技术方案\nAI 局限性 无法独立识别GPU并发任务会OOM的风险，需要等实际失败后才能诊断 对conda run输出缓冲问题判断不够快，尝试了多次tail命令才意识到是缓冲机制 awk命令中的感叹号转义处理出错（\u0026rsquo;!\u0026lsquo;导致语法错误），需要用Python替代 等待plain qformer时未意识到它使用了200 epochs而非50 epochs，导致低估等待时间 Flow Matching目标分布问题的核心洞察来自用户计划文档，AI没有独立发现这个根本矛盾；研究报告中方案A是循环定义，需要被追问才承认 无法主动预见SpatialAttentionBias全局预计算会导致OOM，需要运行失败后才发现问题 QFormer enhanced训练时间估计不准，未提前警告用户可能需要\u0026gt;1小时 adaln_attention在单section测试下表现差未被提前分析 run_evaluation()函数签名不熟悉，连续出现override参数错误和data_root缺失错误，需要多次尝试 在查找\u0026rsquo;可视化结果\u0026rsquo;时误解为\u0026rsquo;可视化代码\u0026rsquo;，需要用户纠正 搜索robomimic BC-RNN checkpoint的正确下载URL时遇到困难，外部URL结构发生变化 在ExitPlanMode时多次被用户拒绝，计划的语言或内容不符合用户预期（用户希望用中文） 在实现VLA服务器时，基于对openpi库的推断编写API调用代码而非验证实际接口 在写单元测试时，推断阈值数值而非从配置文件中读取，导致test_compute_severity期望值错误 计划模式探索留下的中间文件修改（partial edits）在实现阶段导致重复代码块 对HPC集群共享存储规范不了解，在计划中使用了他人（zhaoganlong）的缓存路径 跨会话记忆局限：在初次搜索时未能主动查阅memory目录，需要用户明确提醒 将Claude Code CLI误理解为Claude API，需要人类明确纠正 对\u0026rsquo;Mimicgen Checkpoints\u0026rsquo;的含义理解有歧义，需要额外探索才能确定 今日收获 空间组学融合中\u0026rsquo;选对encoder+空间先验 \u0026gt; 复杂融合算法\u0026rsquo;：UNI2 vision encoder质量直接影响所有下游融合 QFormer+Register Tokens+SpatialBias组合（50 epochs）的ARI 0.401高于plain QFormer（200 epochs）的0.344，更少训练+更好归纳偏置比更多训练更有效 Flow Matching用于融合任务存在根本性目标分布问题，方案C（跨模态预测）是最可行路径，方案E（OT中间点）在线性路径下退化为mean fusion STPath验证的ElementWiseSumFusion（零训练）ARI 0.199略优于Concat 0.181，简单方法也有价值 STAIG仍是151508上的最强策略（ARI 0.500），但QFormer Enhanced已成为最有竞争力的替代方案（ARI 0.401） GCN hidden_dim=64可能是模型坍塌的原因之一，消融实验将验证128/256/512的效果 AdaLNAttentionFusion的section_id条件化优势需要多section联合测试才能体现，在单section benchmark上可能反而不如无条件化的concat SpatialAttentionBias应该设计为惰性计算（按spot计算邻域偏置），而非预计算全局矩阵 scGPT embeddings在DLPFC数据集上的zero-shot质量（ARI0.115）显著低于PCA（ARI0.288），多模态融合无法弥补弱gene encoder的不足 Python的x or default模式：当x可能为0时应使用x if x is not None else default，否则0会被当作falsy触发默认值 跨conda环境的Python进程通信最简方案：TCP socket + pickle，无需任何额外依赖，适合HPC集群环境 计划模式探索可能对文件造成部分修改，实现前应先读取文件确认当前状态，避免重复添加内容 大型系统（19个分类器）的测试策略：先确认每个分类器的真阳性和真阴性各一个case，再测试边界条件和集成行为 软件功能完成（代码可运行）与工程可用（端到端验证）之间存在重要差距，尤其对于依赖外部模型的组件 Pi0（LIBERO检查点）在PickPlace任务上0%成功率，主要失败模式是关节极限接近（47%）和抓取姿势错误（30%）——VLA模型的跨任务迁移能力存在明显局限 50次VLA rollout中每次约检测到48个错误，实际保留3个（max_errors_per_rollout=3），错误检测器非常敏感，未来可调整阈值 zhaoganlong的GeminiClient实现支持ChatAnywhere代理（国内网络可用），无需直连Google API openpi的create_trained_policy()会自动处理norm_stats和输入变换，无需手动管理；正确参数名是train_config而非config pi0_libero config期望的输入键名是observation/image和observation/wrist_image，不是observation/image/{cam_name}格式 多阶段项目规划中，应将集成测试（end-to-end validation）作为规模化之前的必要步骤 auto memory目录是跨会话信息传递的重要资源，应在每次会话开始时主动查阅 日报工具的conversation_summaries字段设计：project用可读名称、outcome反映实际完成状态、叙述控制在2-4句 文档中的插件数量统计应该自动从注册表生成，手动维护容易出错 会话摘要 MIHD ✅ 更新VLA研究报告：添加§3.8目标分布问题和21种融合方法排名 04:37:59.162 | claude_code 用户提供了详细计划要求在VLA研究报告中添加两个重要章节。AI读取报告文件后进行了三处修改：在§3.7后插入§3.8（Flow Matching目标分布问题，含5种方案A-E对比）、在第五/六部分间插入21种融合方法排名表（含Tier A-D分类）、更新第七部分总结（Register Tokens+AdaLN升为首位，FM降至第三）。所有修改通过grep验证章节结构完整。\n✅ 实现Batch 1+2新融合策略：ElementWiseSum、AdaLN、Register Tokens、SpatialAttentionBias 04:49:28.128 | claude_code 用户提供了详细的实现计划（基于VLA研究报告），要求实现4种新融合策略。AI按计划完成了全部实现：ElementWiseSumFusion（零训练，PCA对齐后直接相加）、QFormer Register Tokens（可学习模态标记）、AdaLNAttentionFusion（section_id条件化对比学习）、SpatialAttentionBias（空间位置注意力偏置）。过程中修复了register tokens + spatial bias维度不匹配bug，并通过Python语法检查和模块导入测试验证了实现正确性。\n✅ 优化ENHANCEMENT_PLAN.md并设计Batch 1+2融合策略实施方案 05:00:00.000 | claude_code 用户要求基于研究报告优化增强计划并分批实现融合策略。AI通过三个并行子Agent探索了代码库、研究报告内容和pipeline基础设施。AI设计了4个新融合策略的实施方案并询问用户两个关键决策：Register Tokens实现方式（配置选项 vs 独立策略）和实施范围（Batch 1 vs 1+2）。用户选择了QFormer配置选项和Batch 1+2一起做。\n🔄 在151508上测试新融合策略(scGPT+UNI2→PCA+UNI2)，并讨论Flow Matching融合可行性 01:13:35.771 | claude_code 首先在scGPT+UNI2上测试了concat/element_wise_sum/adaln_attention，发现scGPT缓存路径不匹配问题并通过symlink修复，adaln_attention（ARI=0.113）最优。用户随后要求用PCA+UNI2重测，结果concat(0.181)/element_wise_sum(0.193)/adaln_attention(0.066)。QFormer Enhanced测试因OOM和训练时间过长（\u0026gt;1小时）被中断，修复OOM后仍在等待结果。最后深入讨论了Flow Matching目标分布问题，用户精准指出AI报告中方案A的逻辑缺陷。\n✅ 验证新融合策略已实现并运行151508 benchmark 06:00:00.000 | claude_code 验证了Batch 1+2所有代码已存在（ElementWiseSumFusion、SpatialAttentionBias、AdaLN等），随后在151508切片上运行基准测试。element_wise_sum（ARI 0.199）和adaln_attention（ARI 0.159）结果偏低，但qformer_enhanced（register tokens + spatial bias）表现优异（ARI 0.401 vs plain qformer 0.344）。因GPU并发OOM问题，plain qformer比较结果通过历史记录获取。\n🔄 MIHD可视化结果定位与151672 section聚类坍塌根因分析 22:26:09.691 | claude_code 用户询问可视化结果位置，AI先找到了可视化代码后被纠正。在outputs/benchmark_results/各方法目录下找到PNG结果，发现adaln_attention和element_wise_sum只有1张图（未跑完）。深入分析151672 section上uni_staig_fusion只出现2个cluster的原因：GCN embedding坍塌（overall std=0.12）+ spatial refinement放大问题。用户提出embedding维度可能过小的假设，对比发现STAIG输出64维而其他方法256-512维，用户决定做维度消融实验，但最终拒绝了计划执行。\n🔄 实现GCN hidden_dim消融实验支持并在151508上启动实验 22:49:28.071 | claude_code 用户提供了详细计划来支持STAIG fusion的hidden_dim可配置化（针对151672切片模型坍塌问题）。AI修改了5个文件：run_benchmark.py添加\u0026ndash;hidden_dim参数、evaluation_planner.py添加hidden_dim字段、runner.py传递hidden_dim、pipeline_config.yaml添加4个消融实验、staig_alignment_config.yaml添加注释。所有修改通过AST解析和单元测试验证通过，随后在151508切片上启动了消融实验。\nErrorRecoveryBenchmark ✅ 策略错误检测与分类系统完整实现（v4.3） 00:37:58.141 | claude_code 用户提供了完整的策略错误检测系统规划（6个实现阶段），要求实现19个规则分类器、错误分类法、监控协调器和VLM分析层。AI系统性地实现了所有6个阶段：错误分类法（25种错误类型）、基础数据结构、19个分类器（操作/规划/进度三类）、collector集成、VLM分析层、以及32个单元测试。修复了PrematureReleaseClassifier的hold_duration计算bug（0被视为falsy）和测试期望值错误。最终73/73个测试全部通过，并更新了CLAUDE.md和项目全景总结.md。\n✅ 实现策略错误检测分类系统并补全遗漏的分类器与指标类 02:19:08.022 | claude_code 用户提供了详细的策略错误检测系统实现计划。AI探索代码库后发现大部分已实现，识别出4个具体缺口：GraspWrongPoseClassifier缺失、子包__init__.py空白、ErrorMetricsComputer未实现、相关测试缺失。逐一补全后，单元测试从73个增加到79个，全部通过，CLASSIFIER_REGISTRY达到20个分类器。\n✅ CLAUDE.md改进与项目全景总结.md事实性错误修正 04:35:19.032 | claude_code 用户执行/init命令触发文档改进。通过三个并行子任务全面核查了代码库中的实际数字，发现两份文档均有多处错误（检测器6→5、分类器19→20、错误类型25→24等）。用户要求用中文重写计划后批准执行，系统性地修正了CLAUDE.md、项目全景总结.md和error_taxonomy.py的docstring，所有79个测试通过验证。\n🔄 VLA模型集成规划与基础数据/checkpoint下载 04:45:08.678 | claude_code 用户明确了需要将错误注入Pi0/Pi0.5/Phoenix等VLA模型的真实rollout中，并用状态+视觉信息检测错误。AI探索了服务器上的模型资源（Pi0在openpi_cache、Phoenix在zhaoganlong/hf_cache），设计了Policy Server跨环境架构。同时实施了checkpoint下载计划：Pi0从HuggingFace下载进行中，BC-RNN从Stanford下载时遇到URL 404问题，MimicGen source数据集下载进行中。\n🔍 调查VLA checkpoint是否已下载 04:57:09.111 | claude_code 用户询问VLA模型checkpoint是否已下载。AI通过Explore agent发现：Pi0 LIBERO checkpoint存在（在zhaoganlong缓存），Pi0.5 MimicGen fine-tuned checkpoint不存在，Phoenix checkpoint不存在。该会话在发现问题后直接终止，未形成完整报告。\n🔄 VLA策略服务器架构实现：Pi0/Pi0.5/Phoenix跨环境推理 05:30:32.536 | claude_code 用户提供了详细的VLA模型集成计划，要求实现跨conda环境的策略服务器架构。AI实现了TCP协议的vla_server.py（支持Pi0/Pi0.5/Phoenix），PolicyServerAdapter，相机观测支持，以及rollout_generator中的自然错误捕获模式。用户在实现中途指出需要在生成计划时同步更新项目全景总结.md，并要求checkpoint统一存放在HDD_POOL/tangzijia。所有语法检查通过，但vla_server的实际模型加载接口待修复。\n✅ 修复VLA策略服务器并使端到端rollout可运行 06:51:17.308 | claude_code 用户提供了使VLA集成真正可运行的修复计划，涉及4个核心问题。AI通过阅读openpi源码发现了文档未明确的图像键名约定，修正了vla_server.py的模型加载方式和输入格式，更新了入口脚本以支持VLA和模式选择，修复了观测预处理中的格式兼容性问题，并更新了配置、Makefile和文档。语法检查全部通过，79个测试继续通过。\n✅ 集成Gemini VLM和Claude Code CLI作为错误检测器 07:14:56.791 | claude_code 用户要求基于zhaoganlong的Gemini实现为错误检测器添加VLM后端，并探索用Claude Code CLI分析错误视频。AI找到了Gemini实现（GeminiClient），确认Claude Code CLI可读取图像进行多模态分析，实现了_call_gemini()后端、配置更新、帧提取脚本、分类脚本，并在项目文档中写入完整的§12教程。用户还额外请求创建独立的tutorial.md文件。\n🔄 检查检查点下载状态并运行可视化，VLA测试被中断 16:17:30.808 | claude_code 用户询问检查点是否下载完成——AI检查发现Pi0、MimicGen核心数据集、Robomimic BC-RNN三个目录均不存在，仅有项目内的7个MimicGen源HDF5文件。随后用户要求运行端到端VLA测试（两次），均被用户主动中断。最终用户要求运行批量可视化，AI在5个GPU上并行生成了20个错误场景视频（10 baseline + 10随机化），全部成功。\n✅ VLA端到端测试：injection模式10次rollout及自然捕获50次rollout 16:23:03.791 | claude_code 执行预先批准的VLA端到端测试计划。过程中发现初始obs不含相机图像的关键bug（PolicyServerAdapter需要图像但state_extractor.extract()默认不含图像），修复后injection模式成功生成3个场景。随后运行50次natural_capture rollout，Pi0在PickPlace上完全失败，但成功捕获150个自然错误场景（总计271个），主要错误类型为joint_limit_approach和grasp_wrong_pose。多次更新项目全景总结.md反映最新进展。\n🔄 下载Pi0和MimicGen/robosuite检查点 00:37:58.420 | claude_code 用户请求下载Pi0检查点和MimicGen检查点。AI通过网络搜索和代码文件探索，确定了Pi0通过lerobot HuggingFace下载、MimicGen通过download_datasets.py下载、robomimic BC-RNN通过wget从Stanford服务器下载的方案，并将下载计划更新到计划文件中。用户在AI准备退出计划模式时中断了操作。\n❌ 初始化仓库CLAUDE.md 04:34:54.791 | claude_code 在error_recovery_benchmark项目目录下执行/init命令，尝试创建CLAUDE.md。但由于API 403错误中断，任务未能完成。\n🔄 VLA模型检查点定位与v4.5自然错误生成计划制定 04:59:17.779 | claude_code 用户询问VLA模型检查点是否已下载，AI初次搜索项目目录未找到，经用户提醒后查阅memory目录和更广泛的文件系统，最终确认Pi0（12GB）、Pi0.5等模型位于zhaoganlong目录。随后AI系统性地探索了v4.4基础设施（vla_server.py、capture_natural_errors等），确认已有完整框架但未端到端测试。制定了7步v4.5实施计划，包含VLA自然错误生成（Track A）和状态+视觉错误检测器（Track B）两个并行方向，但用户最终拒绝了退出计划模式的操作，实施未开始。\nGadgetSummarize 🔄 日报工具新增会话摘要章节（conversation_summaries）功能规划 22:51:46.447 | claude_code 用户提出需要在日报输出文件中增加一个章节来总结每段AI对话。AI读取了当前daily_summary.py的完整代码，结合Plan子代理进行设计，确定了conversation_summaries的JSON结构（含project、source、timestamp、topic、summary、outcome六个字段）。实施方案涉及修改SUMMARY_PROMPT、Anthropic工具schema、generate_markdown()渲染逻辑，并将max_tokens从4096提升至8192。计划获用户批准，准备进入实施阶段。\nToken 用量 总览 指标 数值 总 Token 98,673,131 输入 Token 68,782 输出 Token 94,167 Cache 创建 5,322,063 Cache 读取 93,188,119 Cache 命中率 94.6% 总费用 (USD) $26.3308 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 11,875 93,007 3,284,031 79,207,395 $20.4715 77.7% claude-haiku-4-5-20251001 34,279 696 1,562,326 10,921,422 $3.0828 11.7% claude-sonnet-4-5-20250929 22,628 464 475,706 3,059,302 $2.7765 10.5% 各设备用量 设备 总 Token 输入 输出 费用 DCC 25,636,195 1,799 13,314 $18.2696 tianhe 71,926,851 66,954 80,771 $5.6707 TzJsDesktop 1,110,085 29 82 $2.3905 ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-16/","summary":"在MIHD空间组学项目中完善融合方法文档并实现4种新融合策略（ElementWiseSum、AdaLN、Register Tokens、SpatialAttentionBias）完成benchmark测试；在ErrorRecoveryBenchmark项目中实现完整的策略错误检测分类系统（20个分类器）、VLA策略服务器架构、成功运行50次自然错误捕获rollout并集成Gemini VLM检测器；同时为日报工具规划并启动会话摘要章节功能开发。","title":"Bug Journal 2026-02-16"},{"content":"日报 — 2026-02-15 在 DCC 集群、Mac 和 Windows 三台设备上并行推进：执行 MIHD 多设备 benchmark（241/264 实验，Q-Former 进行中）、调研 VLA 方法在空间组学中的可迁移性、修复误差恢复 benchmark 可视化 bug（发现 OSC 控制器抵消外力的真正根因）并生成 60 个视频、完善 gadget/summarize 多设备 rclone 同步架构（原子写入、设备标识、默认行为调整）、将 test/ submodule 转为普通目录，以及将历史日报发布到 Hugo bugJournal 并部署至 GitHub Pages\n今日任务 架构与策略 🔄 MIHD benchmark 实验执行（General 环境） — 执行 286 个 benchmark 实验中的 General 环境部分。快速实验（concat/mean/attention/llava_mlp/staig_e2e 等）已完成 241 个，Q-Former（pca_uni2）8/11 sections 完成，scGPT 评估 65/77 成功（12 个因 GPU OOM 失败需重跑） ✅ 发现真正根因：demo actions 对抗外力导致机械臂不可见位移，修复为 neutral actions — 用户目视视频后发现机械臂仍无可见移动。调查发现真正根因：OSC 控制器（kp=150）将机械臂拉回轨迹完全抵消了外力。将 Phase 3 改为 neutral actions（力作用+沉降期），之后恢复 demo actions，body position 位移 90mm+64mm 可见效果验证成功 ✅ 调研 VLA 方法在 MIHD 中的可用性 — 从 tzj2006.github.io bugjournal 提取用户 VLA 理解（FAST/Unified World Models/RT-2/OpenVLA），结合 LGDiST/Nicheformer/STPath 等空间组学论文，生成完整研究报告，覆盖 Flow Matching 融合和 Register Tokens + AdaLN 两个深入方向 ✅ gadget/summarize：多设备 rclone 同步工作流 — 新增 _rclone_download_logs()，export 上传到 /logs/，merge 上传到 /reports/，merge \u0026ndash;sync 自动下载其他设备 log 后合并，config \u0026ndash;show 显示结构化子路径 🔄 规划 export 阶段按设备名跳过已 summarize 的逻辑（_merged_devices 标记） — 用户提出新需求：文件已存在时检查 _merged_devices 字段，当前设备已在列表中则跳过 \u0026ndash;summarize；summarize 完成后将设备名追加到 _merged_devices 并回写文件。AI 两次尝试 ExitPlanMode 均被用户中断，计划未获批准实施 实现与修复 ✅ 误差恢复 benchmark 批量可视化（60 个视频） — 新增 \u0026ndash;settle_steps 参数；创建 batch_visualize.py 批量脚本，为 30 个场景生成基线和随机参数（1x~2x）各一组视频，共 60 个 mp4，使用 8 GPU 并行渲染。修复 select_scenes() 边界 bug（n=30 时选出 29 个）后补生成缺失的 step75 视频 ✅ 修复可视化脚本中机械臂不移动的 bug（第一轮：force re-apply） — 在 scripts/2_visualize_scene.py Phase 3 循环中，每步重新调用 injector.apply()，模仿 drop.py 的正确实现。三个验证测试（pregrasp/proximity/force_override）全部通过 ✅ gadget/summarize：原子写入防止 JSON 损坏 — 新增 _atomic_write() 函数（tempfile + os.replace），替换 5 处 open(w) 文件写入。同时为 Anthropic 增加 tool_use 强制 JSON 输出，为 OpenAI 增加 response_format ✅ gadget/summarize：默认行为改为只 export — 无子命令时默认执行 cmd_export（不调 API），export \u0026ndash;summarize 才调 API。修复 ccusage 返回 list 时的 AttributeError bug ✅ gadget/summarize：device_summary 按设备名索引 — device_summary 从 flat dict 改为 {device_name: summary_dict}，记录哪台设备生成了 summary。_merged_devices 追踪已 summarize 设备，重复运行自动跳过 API 调用 ✅ 将 test/ git submodule 转为普通目录 — 执行 git rm \u0026ndash;cached test、删除 .gitmodules、git clone TzJ2006/test.git、rm -rf test/.git、git add test/，将 benchmark/ 包（cli.py, core.py, cpu.py, gpu.py, detect.py, report.py）作为普通文件纳入 gadget repo，更新 README.md 和 CLAUDE.md 删除 submodule 说明 ✅ 将历史日报发布到 Hugo bugJournal 并部署 — 为 2026-02-12/13/14 三份日报添加 Hugo front matter（提取 blockquote 摘要行作为 summary 字段），写入 website/content/bugJournal/，运行 update.sh 成功构建 139 页并推送到 GitHub Pages 🔄 Q-Former pca_uni2 全 11 sections 完成 — 重启 Q-Former pipeline（中途进程挂掉），当前 8/11 完成，每 section 约 55~96 分钟，预计还需 ~3 小时 ✅ export JSON 末尾添加 _source_device 字段 — 在 cmd_export() 第 942-948 行添加：未使用 \u0026ndash;summarize 时，在 export_data 末尾写入 _source_device 字符串（格式：device (platform, user@host) | summarized: false） ✅ 生成 2026-02-12/13/14 结构化日报 — 以日报分析师角色，汇总多设备（DCC + Mac + Windows）的 AI 交互记录，分别生成三天的 JSON 结构化日报 ❌ scGPT Q-Former 评估（等待 GPU 释放） — scGPT × uni2 × qformer（11 sections）因并行时 GPU OOM 失败，需等 pca Q-Former 完成后重跑 ✅ gadget/summarize：更新 README、tutorial 和 CLAUDE.md — 三份文档全面更新：默认行为、\u0026ndash;sync 工作流、rclone 子目录结构、常用命令速查等，删除单机模式描述 ✅ 清除 test/ 中对 TzJ2006/test repo 的引用 — 修改 test/benchmark/report.py:169 和 test/benchmark_report.html:171 中的 GitHub 链接，从 github.com/TzJ2006/test 改为 github.com/TzJ2006/gadget/tree/main/test ✅ 验证 export 阶段智能跳过已 summarize 设备（已有实现） — 通过 Grep 和 Read 验证了 daily_summary.py 第 930-975 行中 existing_merged_devices / device_summaries 逻辑已存在，无需额外改动 问题与解决方案 关键问题 1. 修复 force re-apply 后视频仍无可见移动（真正根因：demo actions 对抗外力） 解决方案: 将 Phase 3 从 demo actions 改为 neutral actions：力作用期间用 neutral action（控制器不追踪轨迹），沉降期继续 neutral action，之后恢复 demo actions。body position 位移 90mm+64mm 验证有效\n关键洞察: OSC 控制器 kp=150 产生的关节力矩远大于 15-20N 外力，完全抵消注入效果；neutral action 让控制器仅维持位置，外力效果得以累积。视觉验证比数值日志更可靠\n2. VLA 和空间组学是两个独立演化的领域，无法直接引用论文 解决方案: 通过技术抽象层面的映射找到对应：VLA 的 action token → 空间 embedding；VLA 的 modality tokens → 基因/图像 tokens；VLA 的 diffusion 生成 → 融合 embedding 生成\n关键洞察: 两个领域独立趋同到相同架构创新（Diffusion Transformer、对比对齐、模态 tokens），可以借鉴方法论而非直接移植代码\n3. Q-Former 每 section 约 96 分钟，阻塞了其他快速实验的调度 解决方案: 拆分 pipeline_config.yaml：新建 core_multimodal_fast（去除 qformer）和 qformer_benchmark（仅 qformer）两组，先运行快速实验（49 个，13 分钟完成），再后台运行 Q-Former\n关键洞察: 实验调度策略应按耗时分组，先跑秒级/分钟级实验确保大部分结果就绪，再让小时级实验在后台慢跑\n4. 多设备 merge 时需要从 Google Drive 下载其他设备的 log 解决方案: 新增 rclone_download_logs()，merge \u0026ndash;sync 时先下载 /logs/*.json，合并到本地后再调 API\n关键洞察: 两阶段工作流的核心瓶颈是数据汇聚，rclone 的 \u0026ndash;include 过滤器可精确下载指定日期的文件\n5. export JSON 文件在写入中途被中断（Ctrl+C、断电）会产生损坏的半截 JSON，下次 json.load 失败 解决方案: 原子写入方案：先写到 .tmp 临时文件，再 os.replace() 重命名，保证文件要么是旧的完整版，要么是新的完整版\n关键洞察: 原子性写入是防止文件损坏的标准做法，os.replace() 在同一文件系统内是原子操作\n6. 计划模式（ExitPlanMode）多次被用户中断，计划无法推进实施 解决方案: 跨会话的计划持久化（写入 .md 文件）比依赖 ExitPlanMode 工具调用链更健壮，用户可在新会话以「Implement the following plan:」格式粘贴计划文本驱动执行\n关键洞察: AI 已将计划写入 shiny-frolicking-otter.md 作为持久化备份\n一般问题 7. scGPT Q-Former 评估与 pca Q-Former 并行时 CUDA OOM（GPU 只有 32GB） 解决方案: 识别冲突根因（pca Q-Former 占 21.5GB，scGPT Q-Former 额外需要 2GB 导致 OOM），决定串行化：等 pca Q-Former 完成后再跑 scGPT Q-Former\n关键洞察: 多进程 GPU 共享时需预留缓冲，大模型（Q-Former = ~21GB）不能并行跑同类实验\n8. Q-Former 进程在 6/11 section 时无故终止（进程 0 个运行中） 解决方案: 检查日志确认非崩溃（有正常完成记录），重启 pipeline 自动检测已完成 sections（6/11）并从第 7 个继续\n关键洞察: 断点续跑机制依赖 embeddings.npz 文件存在性，进程被外部 kill 后重启不需要手动恢复\n9. 可视化视频中机械臂无可见移动（第一次诊断：force 只持续一步） 解决方案: 在 Phase 3 循环内每步 re-apply force（xfrc_applied 每物理步后自动清零），窗口结束后 clear\n关键洞察: MuJoCo 在每个物理步后自动清零 xfrc_applied，必须每步重新施加才能维持持续力\n10. batch_visualize.py 选 30 个场景时只选出 29 个（select_scenes 逻辑 bug） 解决方案: 修复 select_scenes() 函数，当 n \u0026gt;= 总场景数时直接返回全部场景；单独补生成缺失的 impulse_demo_0_step75 视频\n关键洞察: 分组选择逻辑（按 demo/力度分层采样）在 n=30 等边界条件下有截断问题，需要特判\n11. ccusage 输出为 list 而非 dict，导致 AttributeError: \u0026rsquo;list\u0026rsquo; object has no attribute \u0026lsquo;get\u0026rsquo; 解决方案: 检测 data 类型：若为 list 则包装为 {\u0026ldquo;daily\u0026rdquo;: data}，兼容两种格式\n关键洞察: 外部工具输出格式可能随版本变化，需做防御性类型检查\n12. rclone_remote 未配置时会静默不上传，用户误以为同步在运行 解决方案: 解释 _rclone_upload 在无 rclone_remote 时直接 return，config \u0026ndash;show 显示 rclone 状态方便排查\n关键洞察: 工具的静默退出策略正确（不阻断主流程），但需要在文档和 config \u0026ndash;show 中明确展示配置状态\n13. test/ 是 git submodule，转为普通目录后 test/benchmark/report.py 和 benchmark_report.html 中仍有指向旧 repo (TzJ2006/test) 的链接 解决方案: 用 Grep 全库扫描 TzJ2006/test 引用，找到两处 HTML footer 链接，修改为 TzJ2006/gadget/tree/main/test\n关键洞察: 迁移 submodule 时需同时扫描内容中的硬编码 repo URL，仅清除 .git 元数据是不够的\n人类思路 vs AI 思路 战略层面 bug 的真正根因识别 角色 思路 人类 用户目视视频后直接指出机械臂根本没有位移，驱动更深层次的调查 AI AI 第一轮仅修复了 force 只持续一步的问题，认为 body position 变化已说明修复有效，未主动看视频验证效果 差异分析: 人类通过感知输出（看视频）发现第一轮修复无效；AI 依赖日志数值（body position 变化量）得出错误结论。视觉验证比数值日志更可靠\nVLA 研究的切入角度 角色 思路 人类 从自己的 bugjournal 笔记出发（FAST/Unified World Models/SOTA VLA），要求 AI 做交叉领域可用性研究 AI 系统抓取 bugjournal 内容，交叉检索空间组学文献（LGDiST/Nicheformer/STPath），从技术抽象层面建立映射，分 Tier 1/2/3 给出可行性评级 差异分析: 人类从已有知识积累出发提问；AI 通过文献检索补全了人类未必了解的空间组学侧的对应工作，建立了双向映射\nexport 设备标识的用途设计 角色 思路 人类 用户最初说「在文件末尾加设备标识」，经过两轮澄清后细化为包含设备名和 summarized 状态，并演进为 merged \u0026lt;device1, device2\u0026gt; 格式，用于 export 阶段的幂等性判断 AI AI 首次理解为「日志末尾追加纯文本标识」，经过用户的两次纠正才理解实际需求是用 _merged_devices 字段追踪已处理设备、实现跳过逻辑 差异分析: 用户的需求从一开始就是面向行为的（跳过已处理），AI 初次理解停在展示层面（视觉标识）。用户通过多轮对话逐步澄清，体现了口语需求与实现意图之间的语义鸿沟\n误差可视化 bug 的根因分析 角色 思路 人类 提供了详细的根因假设（demo actions 被 OSC 控制器补偿）和对比（验证阶段用 neutral actions 没有此问题），直接指向解决方案 AI 实现了人类提供的修复方案（三子阶段设计），并验证了效果（body position 位移 90mm+64mm） 差异分析: 根因分析完全由人类完成，AI 负责实现；AI 在测试输出中提取了具体位移数据作为验证证据\nQ-Former 调度策略 角色 思路 人类 发现 Q-Former 阻塞后要求停掉并重新排序 AI 停掉 Q-Former，将其拆分为独立实验组，先运行所有快速实验（13 分钟完成 49 个），再后台运行 Q-Former 同时并行 scGPT 提取 差异分析: 人类发现阻塞问题并给出方向，AI 设计了具体的并行调度方案（Q-Former 后台 + scGPT 并行），最终因 GPU OOM 发现并行限制\n实现层面 rclone 目录结构设计 角色 思路 人类 用户提出每台机器应有自己的文件夹，希望多设备日志能汇聚合并 AI AI 设计了 logs//\u0026hellip; 的嵌套结构，后根据用户澄清改为 logs/_.json 的 flat 结构+按日期 filter 差异分析: 用户的直觉是按设备分目录；AI 最终提出的 flat 结构加日期过滤更简单且 rclone 原生支持\n日报发布的流程设计 角色 思路 人类 在第二个会话中直接提供完整计划文本绕过 ExitPlanMode，避免跨会话状态丢失 AI 第一个会话制定计划后调用 ExitPlanMode，但计划未能保存到下一会话 差异分析: 人类主动使用「计划文本粘贴」方式解决跨会话断点问题，这是 AI 工具链的使用技巧而非技术问题\n默认行为的期望 角色 思路 人类 用户明确说「默认不要 merge」，意思是无子命令时只 export 不调 API AI AI 最初误解为「\u0026ndash;sync 时默认不 merge」，需要再次确认才理解正确含义 差异分析: 自然语言的「默认」有歧义，AI 需要多一轮确认；用户的心智模型是 export = 轻量无 API，merge = 重量有 API\nJSON 文件损坏的修复优先级 角色 思路 人类 用户提问「哪些可以修复」，交由 AI 判断优先级 AI AI 按影响从高到低分析：原子写入（可修复）\u0026gt; LLM 输出强制 JSON（部分可改善）\u0026gt; JSONL 坏行 / ccusage 输出（不可控） 差异分析: 判断逻辑一致，但 AI 未能推进到实施阶段（被中断），展示了分析能力强于执行能力的特点\nAI 局限性 重要局限 依赖日志数值验证修复效果，未主动建议用户目视视频；第一轮 force re-apply 修复无效但 AI 误判为成功（body position 有变化但视觉不可见） 误差可视化的 neutral action 根因由用户完整分析提供，AI 未能独立从日志（body_position 有变化但视频无效果）推断出 OSC 控制器补偿机制 两次尝试 ExitPlanMode 均被用户中断，说明 AI 在判断「何时进入计划审批」上过于主动，未给用户充分时间确认需求边界就推进到计划阶段 对用户「在文件末尾加设备标识」的口语表达，初次理解停在视觉层面，未能一步到位理解「用标识实现跳过逻辑」的行为意图，需要两轮澄清才对齐 一般局限 启动 scGPT 评估时未预先计算 GPU 内存需求，导致 scGPT × Q-Former 并行时 OOM（11/77 实验失败）；应先 nvidia-smi 确认内存余量再决定是否并行 第一次会话制定日报发布计划后在 ExitPlanMode 阶段被中断，计划未能保存到下一会话，需要用户在新会话重新提供计划文本才能实施 batch_visualize.py 的 select_scenes() 在边界条件（n=30）下有 bug，选出 29 而非 30 个场景，需要用户发现后手动补生成缺失视频 误解用户「默认不要 merge」的含义，需额外一轮确认 VLA 研究中未能直接访问用户 bugjournal 中的 VLA 笔记（需要多轮 WebFetch 探测正确 URL），初次尝试通过首页/posts 均未命中，浪费了几轮工具调用 在解释 website 不是独立 git repo 的情况后，没有主动提供「是否需要为源文件建立版本管理」的后续建议 今日收获 核心收获 MuJoCo 力注入的可见性取决于控制模式：使用 OSC 控制器 + demo actions 时，控制器会主动补偿外力（kp=150 产生的力矩远大于 15-20N 外力），视觉效果完全消失；必须用 neutral actions 才能展示误差效果。同时 xfrc_applied 在每个物理步后自动清零，需要在每步循环中 re-apply force 日志数值（body position delta）不能替代视觉验证：0.117m 的 position change 在控制器补偿下完全不可见。感知输出（看视频）是验证物理仿真效果的必要手段 Benchmark 调度应按耗时分层：秒级（concat/mean）、分钟级（staig/llava）、小时级（Q-Former）分组调度，先完成快速实验确保大部分结果就绪，再后台跑慢实验 VLA 领域（机器人）和空间组学独立收敛到相同架构（Diffusion Transformer、对比对齐、模态 tokens）；Flow Matching 融合和 Register Tokens + AdaLN 是最值得在 MIHD 中尝试的两个 VLA 方法迁移方向 os.replace() 是 Python 中实现原子写入的标准方式：先写 .tmp，再 rename，保证文件状态要么旧要么新，不存在中间态 跨会话的计划持久化（写入 .md 计划文件）比依赖 ExitPlanMode 工具调用链更健壮，用户可在新会话以「Implement the following plan:」粘贴计划文本驱动执行 两阶段多设备工作流的关键设计：export=无 API 轻量操作，merge=有 API 重操作，rclone 负责数据汇聚；默认行为应匹配最常用场景（只 export） 实践收获 多设备 GPU 任务并行时需预留内存缓冲：单个 Q-Former 实例占 21.5GB/32GB，不能同时跑两个 Q-Former；应先 nvidia-smi 确认余量再决定并行度 Hugo 网站部署的标准实践：源文件不进 git，只将构建产物 public/ push 到 GitHub Pages repo；summary 字段可直接从日报的 blockquote 摘要行提取实现内容复用 git submodule 迁移为普通目录时，需额外扫描内容中的硬编码 repo URL（report.py、HTML 文件等），仅清除 .git 元数据不够彻底 外部工具输出格式需做防御性类型检查（ccusage 可能返回 list 或 dict），rclone 的脚本集成采用 early-return 模式（未配置远端 → 直接 return）是可选功能零侵入的良好实践 会话摘要 MIHD（空间转录组 Benchmark） 🔄 MIHD benchmark 全量实验执行：拆分 Q-Former、并行 scGPT 提取与评估 00:01:07.460 | claude_code DCC 集群上执行 286 个 benchmark 实验。发现 Q-Former（每 section 96 分钟）阻塞快速实验后，将其拆分为独立组，先在 13 分钟内完成 49 个快速实验，再后台运行 Q-Former（pca_uni2）并并行提取 scGPT embeddings。scGPT 评估 65/77 成功，12 个因 GPU OOM 失败（Q-Former 占 21.5GB 导致并行时内存不足）。截至会话结束，pca_uni2_qformer 完成 4/11，总计 241 个实验已完成。\n🔄 调研 VLA 方法在 MIHD 空间组学中的可迁移性 22:08:57.348 | claude_code 用户提供个人博客 VLA 笔记（FAST/Unified World Models/SOTA VLA），请 AI 调研这些方法能否用于 MIHD 多模态融合。AI 从 bugjournal 提取用户理解，检索空间组学对应论文（LGDiST/Nicheformer/STPath），发现两领域独立趋同相同架构，生成完整研究报告。随后用户追问 Flow Matching 融合和 Register Tokens + AdaLN 细节，AI 启动三个并行子任务研究后被用户中断。\nErrorRecoveryBenchmark 🔄 修复可视化脚本 force re-apply 并发现真正根因 00:05:51.186 | claude_code 实施了 force 每步 re-apply 的修复，三个验证测试通过。但用户目视视频后发现机械臂仍无位移，AI 通过两个 Explore 代理调查发现真正根因：demo actions 通过 OSC 控制器完全抵消了外力效果。制定了改用 neutral actions 的修复计划，但在 ExitPlanMode 阶段被用户中断。\n误差恢复 Benchmark（tianhe 节点） ✅ 修复可视化脚本 neutral action 问题并生成 60 个视频 03:58:45.066 | claude_code 修复 2_visualize_scene.py 中 demo action 被 OSC 控制器补偿导致力效果不可见的问题，Phase 3 改为 neutral action（力作用+沉降期）后恢复 demo。创建 batch_visualize.py 批量脚本，为 30 个场景分别生成基线和随机参数（1x~2x）两组视频，共 60 个 mp4，使用 8 GPU 并行渲染。修复 select_scenes() 边界 bug 后补生成缺失的 step75 视频。\n🔄 修复可视化视频中力注入效果不可见（xfrc_applied 单步清零） 00:00:03.445 | claude_code 视频中机械臂注入力后无可见移动，根因是 MuJoCo 每物理步后清零 xfrc_applied，导致只有 0.03 N·s 冲量。AI 设计在 Phase 3 循环中每步 re-apply force 的修复方案，但用户在 ExitPlanMode 时中断（同一问题在下一会话由修复 neutral action 方案彻底解决）。\nGadget（日报工具） ✅ CalendarPro 代码整理及 gadget 原子写入、rclone 工作流 02:10:33.706 | claude_code 实施了 CalendarPro 代码库整理（删除冗余文件、提取共享模块）和 gadget/summarize 的多个改进：原子写入防 JSON 损坏、Anthropic/OpenAI 强制 JSON 输出、多设备 rclone 同步工作流（export→logs/，merge→reports/，\u0026ndash;sync 自动下载）、默认行为改为只 export。期间解答了 rclone 目录和 rclone_remote 配置的问题。\n✅ 实施多设备 rclone 同步工作流并更新文档 04:18:30.364 | claude_code 实施了完整的 rclone 同步改进：_rclone_upload 增加 subdirectory 参数，新增 _rclone_download_logs()，merge 支持 \u0026ndash;sync 标志，config \u0026ndash;show 显示 logs/reports 子路径。同时全面更新了 README.md、summarize/README.md、tutorial.md，移除单机模式，新增 \u0026ndash;sync 工作流示例。\n✅ 实施 Export 阶段智能跳过已 summarize 的设备 02:02:44.426 | claude_code 将 device_summary 从 flat dict 改为按设备名索引的 dict，新增 _merged_devices 列表追踪已 summarize 设备，重复运行时自动跳过 API 调用。解答了用户关于 rclone 工作原理的疑问（未配置 rclone_remote 时静默跳过）。\n✅ 汇总 2026-02-13 多设备对话，生成结构化日报 00:05:25.538 | claude_code 用户调用 AI 作为日报分析师，传入来自 DCC 集群的 MIHD 7 Phase 增强实现对话记录，要求生成 JSON 格式结构化日报。AI 分析任务完成情况、问题解决过程和人机差异，生成完整日报。\n✅ 汇总 2026-02-12 DCC 集群 MIHD 调研对话，生成结构化日报 05:03:37.389 | claude_code 用户传入 DCC 集群上的 MIHD 增强设计对话（Q-Former 调研、归一化分析、QueST 集成设计），要求生成 2026-02-12 日报。AI 提取任务状态、问题与解决方案，分析人机差异，生成完整 JSON 日报。\n✅ 汇总 2026-02-14 DCC + Mac 多设备对话，生成结构化日报 05:00:32.130 | claude_code 用户传入 DCC 集群（MIHD Pipeline 架构、benchmark 测试）和 Mac（CalendarPro OAuth 修复）的对话记录，要求生成 2026-02-14 日报。AI 分析两阶段 Pipeline 架构设计、mclust fallback bug 修复等内容，生成完整 JSON 日报。\n✅ 汇总 2026-02-13/14 Windows 桌面端对话，生成结构化日报 02:09:02.066 | claude_code 用户传入 CalendarPro（代码库重构、定时循环任务功能）和 gadget/summarize（配置文件、rclone、ccusage、模型切换）的对话记录，要求生成 2026-02-14/13 结构化日报。AI 分析了任务完成情况、问题解决过程和人机协作差异，生成了完整 JSON 日报。\n✅ 汇总 2026-02-14 Windows 桌面端（CalendarPro）对话，生成结构化日报 02:04:11.775 | claude_code 用户传入 CalendarPro 的对话记录（Google Calendar 403 修复、批量删除功能实现、summarize 工具增强），AI 生成了包含 8 个任务、5 个问题解决方案的完整 JSON 日报。\nGadget（summarize 工具） 🔄 添加 export 设备标识字段 + 将 test/ submodule 转为普通目录 00:08:53.716 | claude_code 用户提出两个任务：export 未使用 \u0026ndash;summarize 时在 JSON 末尾加设备标识、将 test/ git submodule 迁移为普通文件。AI 完成了 _source_device 字段添加、submodule 移除（git rm \u0026ndash;cached + rm .gitmodules + clone + rm .git）、README/CLAUDE.md 文档更新，以及扫描修复 report.py 和 benchmark_report.html 中的旧 repo URL。用户随后提出新的 _merged_devices 跳过逻辑，AI 两次尝试 ExitPlanMode 均被中断。\n🔄 验证 export 智能跳过逻辑、解析 rclone 机制、分析 JSON 损坏风险 02:27:21.689 | claude_code 用户传入「计划已完成」指令，AI 验证了 export 阶段跳过已 summarize 设备的逻辑已存在。随后用户提问 rclone 同步目标，AI 解释了 early-return 机制和 Google Drive 配置流程。用户进一步询问哪些 JSON 解析错误可修复，AI 分析了 4 处风险并提出原子写入方案，但两次进入计划模式均被用户中断。\nGadget（日报发布工具） ✅ 实施日报发布计划并部署到 GitHub Pages 05:16:45.089 | claude_code 用户在新会话中粘贴完整计划文本要求直接实施。AI 读取三份日报（2026-02-12/13/14），为每份生成 Hugo front matter（提取 blockquote 摘要行作为 summary 字段），写入 website/content/bugJournal/，运行 update.sh 成功构建 139 页并部署到 GitHub Pages。随后确认 gadget repo 已是最新，website 不是独立 git repo，部署已通过脚本完成。\n🔄 探索日报格式与 bugJournal 格式，制定发布计划 05:05:36.473 | claude_code 用户请求将 gadget/summarize/reports 下的日报发布到 Hugo bugJournal 并部署。AI 探索两个目录的文件结构和格式，制定添加 front matter 并运行 update.sh 的计划，在 ExitPlanMode 阶段被用户中断。\nToken 用量 总览 指标 数值 总 Token 46,617,489 输入 Token 54,398 输出 Token 20,823 Cache 创建 2,919,851 Cache 读取 43,622,417 Cache 命中率 93.7% 总费用 (USD) $21.9085 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 29,028 20,061 1,517,240 26,587,094 $14.1557 64.6% claude-haiku-4-5-20251001 22,421 384 692,824 4,621,768 $1.3525 6.2% claude-sonnet-4-5-20250929 2,949 378 709,787 12,413,555 $6.4003 29.2% 各设备用量 设备 总 Token 输入 输出 费用 DCC 16,688,953 36,225 10,457 $14.1690 MacBook 871,193 191 130 $1.0724 tianhe 29,057,343 17,982 10,236 $6.6672 ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-15/","summary":"在 DCC 集群、Mac 和 Windows 三台设备上并行推进：执行 MIHD 多设备 benchmark（241/264 实验，Q-Former 进行中）、调研 VLA 方法在空间组学中的可迁移性、修复误差恢复 benchmark 可视化 bug（发现 OSC 控制器抵消外力的真正根因）并生成 60 个视频、完善 gadget/summarize 多设备 rclone 同步架构（原子写入、设备标识、默认行为调整）、将 test/ submodule 转为普通目录，以及将历史日报发布到 Hugo bugJournal 并部署至 GitHub Pages","title":"Bug Journal 2026-02-15"},{"content":"日报 — 2026-02-14 横跨三个项目的密集开发日：在HPC集群上完成MIHD多模态空间转录组两阶段Pipeline架构重写并通过端到端测试；修复机器人错误恢复基准框架中力注入机制的多层系统性Bug（EEF定位错误、持续力缺失、PreGrasp检测器五层级联Bug）；同时大幅改进gadget/summarize工具（Claude CLI后端、ccusage计费修复、rclone云盘同步、export合并、Windows兼容性），并推进CalendarPro Discord Bot的批量删除功能与代码质量修复。\n今日任务 架构与策略 ✅ 两阶段Pipeline架构设计与实现 — 将现有每次重复计算embedding的低效pipeline完全重写为两阶段架构：Phase 1提取并缓存所有encoder embedding（每个encoder×section只算一次），Phase 2从缓存加载embedding执行fusion+聚类+评估。创建了12个新文件，修改了CLAUDE.md ✅ 修复EEF body定位错误 — 发现_get_eef_body_name()方法的possible_names列表缺少\u0026rsquo;gripper0_eef\u0026rsquo;，导致fallback到bin2（垃圾箱），力被施加到错误位置 ✅ 修复PreGrasp检测器的lookahead逻辑 — _check_will_close_soon()使用全零中性动作仿真，夹爪永远不会闭合。改为通过state_info传入future_demo_actions，基于真实demo动作判断夹爪是否即将闭合 ✅ summarize工具：添加配置文件+rclone云盘同步支持 — 新增~/.config/summarize/config.json支持，含device_name/logs_dir/reports_dir/rclone_remote/rclone_path字段；实现配置读写、rclone上传工具函数；添加config \u0026ndash;show/\u0026ndash;init子命令；新增Claude CLI后端（summarize_with_claude_cli） ✅ 实现持续力注入机制 — 修改collect_rollout_stats()函数，添加error_spec和injector参数，在验证rollout过程中循环重施加力，修复duration_steps不生效的系统性bug ✅ 新建PreGrasp检测器 — 实现pre_grasp.py，检测夹爪即将闭合前的时刻（当前gripper 0.0-0.3，10步后\u0026gt;0.3），在夹爪开合前施加持续力让物体偏移 ✅ 修复get_task_phase()始终返回unknown — 将get_task_phase()改为使用已实现的get_task_completion_stages()，删除永远返回False的stub方法_check_phase_condition() ✅ 修复全局冷却时间阻断PreGrasp触发 — 将单一全局cooldown_counter改为按检测器独立追踪冷却时间，避免proximity检测器的冷却覆盖pre_grasp触发窗口 🔄 实现批量删除日历事件功能 — 扩展DeleteData模型（date_from/date_until/batch字段）、更新LLM prompt、扩展search_events时间范围参数、重写_handle_delete支持批量操作、新增BatchDeleteApprovalView Discord确认按钮。修复了LLM超时fallback（正则提取）和空消息崩溃问题。Discord回复详情优化仍在进行 ✅ summarize工具：修复ccusage Opus计费为$0的问题 — 发现\u0026ndash;offline模式下Opus 4.6定价缺失，去掉\u0026ndash;offline参数改为在线获取最新定价，Opus费用从$0恢复为正确的$19.11 ✅ summarize工具：修复Windows下npx/claude CLI调用失败 — ccusage调用加shell=True修复Windows下npx.cmd找不到问题；claude CLI调用时从环境变量中删除CLAUDECODE/CLAUDE_CODE_ENTRY避免嵌套会话检测 ✅ 修复近距离门控阻断PreGrasp — 全局max_eef_object_distance=0.05m门控阻断所有检测器，但pre_grasp工作范围是0.05-0.1m。改为让pre_grasp/grasp_precond等有内置距离检查的检测器绕过该门控 🔄 增大注入力使机械臂扰动可见 — 将力从3N增至30N，force_range改为[15,45]N，但视频中机械臂仍无可见运动。正在尝试inf大小的力以诊断根因 ✅ summarize工具：export时检测并合并已有日志 — 当同名log文件已存在时，以(source, project, timestamp)三元组去重后合并新旧会话，避免覆盖或丢失之前的记录 ✅ summarize工具：修复JSON解析健壮性 — 重写_parse_json_response：三步尝试（直接解析→提取代码块→提取第一个{}区间），修复LLM在JSON前后加解释文字导致解析失败的问题 实现与修复 ✅ 修复PreGrasp目标体名称错误 — 检测器将任务配置中的对象名（Milk）作为MuJoCo body名传给injector，导致Body not found错误。改为使用{\u0026rsquo;eef\u0026rsquo;: True}作为目标 ✅ 可视化脚本Phase 3改用demo actions — 修改2_visualize_scene.py，Phase 3注入后不再静止，而是继续执行demo轨迹，展示错误如何影响任务完成；并实现duration_steps后清除xfrc_applied ✅ Pipeline端到端测试与bug修复 — 在section 151508上测试了新pipeline，修复了两个bug：ExperimentLogger不存在save_comparison_csv方法、mclust环境缺少rpy2时未捕获ImportError。测试了concat/mean/attention/staig_fusion四种fusion策略，全部通过 🔄 RTX 5000 Ada新GPU的benchmark启动 — 切换到RTX 5000 Ada（32GB）后重新启动pipeline，Phase 1高效完成UNI2、HIPT、ResNet50、UNI-STAIG等视觉embedding提取，Phase 2完成127/190个评估实验。Q-Former因每个section约5.7h过慢，将epochs从200减至50 ✅ 清理database.py中的死代码 — 删除从未被调用的add_rejected_candidate()、get_rejected_candidates()、analyze_rejected()方法，以及DatabaseMeta中的total_rejected和rejection_stats字段 ✅ 验证pre_grasp场景生成 — 重新生成场景后确认pregrasp检测器触发（2个场景），task_phase不再是unknown ✅ 修复Google Calendar 403 insufficientPermissions — 将get_user_timezone()从calendarList().get() API改为settings().get() API，消除每次用户发消息时触发的403 WARNING日志 🔄 MIHD Benchmark 实验监控与调优 — 在DCC集群上持续运行207个benchmark实验组合（pca × uni2 × 多种fusion × 11 sections），处理了basic_contrastive超时问题（从1h调整到2h），移除了staig_fusion_e2e实验组，从264个实验减少到242个 ✅ CalendarPro代码质量修复 — dual_verify.py补充4个新intent的默认回复；executor.py裸except改为except (ValueError, TypeError)；periodic_checker.py将Scheduler实例提到循环外复用 ✅ summarize工具：切换Claude CLI为Sonnet模型 — 将claude \u0026ndash;print改为claude \u0026ndash;print \u0026ndash;model sonnet，降低调用成本 ✅ summarize工具：完善文档 — 新增requirements.txt，更新tutorial.md添加三种后端对比、rclone无sudo安装说明、配置文件字段说明，更新README.md同步信息 ✅ 更新Gadgets仓库README和CLAUDE.md — 删除Video、audio、image、papers、git、png2text.py等多个不再使用的工具，更新README.md和CLAUDE.md以反映当前只剩summarize/和test/两个活跃工具的状态 ❌ summarize工具：生成当日日报 — 尝试用daily_summary.py \u0026ndash;date 2026-02-14生成日报，但Claude CLI在嵌套Claude Code环境中返回空内容导致RuntimeError。需要在Claude Code会话外独立运行 问题与解决方案 关键问题 1. 30N力施加后视频中机械臂看不到任何运动 解决方案: 调试发现力被施加到bin2（垃圾箱）而非EEF；修复_get_eef_body_name()在possible_names中添加\u0026rsquo;gripper0_eef\u0026rsquo;等正确名称\n关键洞察: 第一个包含\u0026rsquo;ee\u0026rsquo;字符串的geom是robot0_screen_collision，属于bin2，字符串匹配fallback极其危险\n2. 现有benchmark架构低效：同一UNI2 embedding被重复计算77次（7 fusions × 11 sections） 解决方案: 完全重写pipeline为两阶段架构，Phase 1去重提取，Phase 2复用缓存\n关键洞察: 关键洞察是：缓存键不应包含fusion策略名——encoder embedding和fusion是正交的两个维度\n3. MuJoCo每步后自动清除xfrc_applied，duration_steps配置完全不生效 解决方案: 在验证rollout循环中循环重施加力：for t in range(steps): if t \u0026lt; duration_steps: injector.apply(env, spec)\n关键洞察: xfrc_applied是MuJoCo的持久字段但被每次mj_step清零，需要在外层循环持续重置\n4. PreGrasp检测器注册后从未触发——多层Bug叠加：phase始终unknown→lookahead始终False→近距离门控阻断→冷却时间覆盖→body名错误 解决方案: 逐层诊断并修复：1)用get_task_completion_stages()替换stub；2)用future_demo_actions替换零动作仿真；3)全局门控改为按检测器绕过；4)全局冷却改为按检测器独立冷却；5)target改为eef\n关键洞察: 一个功能看似\u0026rsquo;已实现\u0026rsquo;但实际不可触发，往往是多个独立bug的叠加效果。需要逐步调试而非一次性假设\n5. ccusage \u0026ndash;offline模式下Opus 4.6计费为$0，导致总费用严重偏低（$0.33 vs实际$19.44） 解决方案: 去掉\u0026ndash;offline参数，改为在线获取最新模型定价\n关键洞察: 离线模式的定价表不包含最新模型，对于经常使用新模型的场景必须在线获取定价\n6. Claude CLI在嵌套Claude Code会话中拒绝执行（CLAUDECODE环境变量触发保护） 解决方案: 在调用subprocess前从环境变量中删除CLAUDECODE和CLAUDE_CODE_ENTRY\n关键洞察: Claude Code会将会话标识注入环境变量，子进程继承后被识别为嵌套会话；清除这两个变量即可绕过\n7. impulse.py的_check_will_close_soon()用全零动作仿真，夹爪在OSC控制下对零动作响应是保持位置而非闭合 解决方案: 在rollout_generator中将future_demo_actions注入state_info，pre_grasp检测器通过检查未来动作的夹爪分量判断是否即将闭合\n关键洞察: 离线场景生成时有demo actions可以直接检查，无需仿真预测\n8. 批量删除时LLM在嵌套Claude Code环境中超时，导致intent.data为空请求被拒绝 解决方案: 将原始用户输入通过_raw_input传给handler，用正则从中提取搜索关键词、日期范围、batch标志作为fallback\n关键洞察: SR-only fallback路径需要独立的信息提取能力，不能完全依赖LLM生成结构化data；嵌套AI环境是重要运行约束\n9. Q-Former fusion极慢：每个epoch约100秒，200 epochs × 11 sections = 63小时 解决方案: 将epochs从200减至50，通过pipeline_config.yaml中的extra_config.qformer.epochs传参\n关键洞察: 训练型fusion的epoch数需要根据计算预算动态调整，配置文件要支持per-fusion超参数覆盖\n10. semantic router将「请帮我安排这两天的睡眠和吃饭」误分类为update_energy，LLM超时无法纠正 解决方案: 通过unset CLAUDECODE环境变量重启应用，让LLM可以正常运行\n关键洞察: 嵌套Claude Code环境是调试过程中的根本障碍，解决它比修复单个分类错误更重要\n11. Windows下Python subprocess调用npx报FileNotFoundError（npx实际是npx.cmd） 解决方案: ccusage调用改为shell=True，让Windows命令解析器处理.cmd文件扩展名\n关键洞察: Windows上npm全局命令是.cmd文件，在Unix-like shell(Git Bash)中可用，但Python subprocess默认不走shell解析\n12. LLM返回的JSON前后有解释文字，直接json.loads失败 解决方案: 三步解析策略：直接解析→正则提取代码块→找首尾{}区间\n关键洞察: LLM输出不可信为纯JSON，健壮解析器需要多重回退策略\n一般问题 13. force_clip参数被嵌套在injectors.impulse下，但__init__从顶层读取导致始终用默认值 解决方案: 修正配置路径，使force_clip从正确的嵌套路径读取\n关键洞察: 配置路径不匹配是静默bug的常见来源\n14. Google Calendar calendarList API返回403 insufficientPermissions，每次用户发消息都触发 解决方案: 改用settings().get(setting=\u0026lsquo;timezone\u0026rsquo;) API，该API只需calendar.events scope\n关键洞察: Google Calendar不同端点需要不同OAuth scope，settings API权限需求低于calendarList API\n15. BatchDeleteApprovalView发送确认消息后返回空字符串，主循环尝试channel.send(\u0026quot;\u0026quot;)导致Discord API报错 解决方案: 在主消息循环中添加if response:检查，空响应不发送\n关键洞察: 双重发送模式（handler直接发+主循环再发）需要明确的「已处理」信号约定\n16. basic_contrastive fusion超时：在1h限制下无法完成（平均需要77分钟/section） 解决方案: 将run_all_benchmarks.py中的超时从3600s增加到7200s（2小时）\n关键洞察: 训练型fusion（basic_contrastive、staig_fusion）比简单fusion慢10-20倍，需要区分对待\n17. mclust聚类在General环境下报ImportError（rpy2未安装），但只catch了ValueError 解决方案: 在runner.py的mclust fallback块中同时捕获ImportError，自动降级到KMeans\n关键洞察: 缺少rpy2时抛出的是ImportError而非ValueError，需要宽泛的异常处理策略\n18. SSH远程命令中无法使用cd切换工作目录，导致反复命令失败 解决方案: 使用绝对路径或在ssh命令字符串中显式拼接cd命令；最终通过写shell wrapper脚本解决。另认识到当前节点已直接连接GPU，无需SSH转跳\n关键洞察: 运行nvidia-smi先检测是否已在GPU节点上，避免不必要的SSH跳转；AI需要额外确认工具调用参数是否真的包含了计划的内容\n19. Claude CLI在嵌套Claude Code环境中生成日报时返回空内容 解决方案: 未解决（需要在Claude Code会话外运行）\n关键洞察: 日报生成脚本不能在Claude Code内部调用，需要独立终端或调度任务\n20. ExperimentLogger不存在save_comparison_csv()方法导致pipeline崩溃 解决方案: 移除错误的外部调用（ExperimentLogger在log_experiment内部自动调用_update_comparison_csv）\n关键洞察: API文档缺失时需要通过grep函数名来确认实际接口\n21. conda run输出缓冲问题：启动的pipeline进程没有实时输出 解决方案: 改用直接调用conda环境的Python二进制（/hpc/group/\u0026hellip;/bin/python -u），配合输出重定向到日志文件\n关键洞察: conda run在后台任务中会缓冲输出；直接调用Python二进制+PYTHONUNBUFFERED=1是更可靠的方案\n人类思路 vs AI 思路 战略层面 PreGrasp触发失败的根因诊断 角色 思路 人类 用户提供了完整的计划，已预先分析出get_task_phase()是stub、total_rejected是死代码两个根因 AI AI在实施过程中发现了计划未预料到的额外3个bug：lookahead使用零动作、全局冷却覆盖触发窗口、全局近距离门控阻断 差异分析: 用户的计划正确但不完整，AI在实际运行调试中发现了更深层的问题并逐步修复\n两阶段Pipeline架构的核心理念 角色 思路 人类 用户直接指出了问题的本质：encoder embedding和fusion是正交的，应该先批量提取所有embedding存文件，再复用做fusion。这是一个架构级洞察，非常简洁明确 AI AI首先进行了大量代码探索和分析，提出了三种渐进方案，然后在用户选择后设计了详细的模块化架构 差异分析: 用户直接看到了根本问题（encoder与fusion正交），AI的分析路径更长但更全面；架构核心理念来自用户，模块化实现细节来自AI\n力注入机制调试策略 角色 思路 人类 提出检验假设：先用inf力验证机制是否工作，再逐步缩小范围找问题 AI 先从理论计算所需力大小（OSC刚度kp=150），推导25-45N应足够产生3cm位移 差异分析: 人类采用工程直觉的二分法调试，AI依赖理论推导；最终调试日志揭示问题根本不是力大小，而是施力位置错误\n嵌套Claude Code环境的识别 角色 思路 人类 用户观察到LLM一直超时，提示「你能告诉我有什么办法能够优化这一块吗」 AI AI在日志中发现CLAUDECODE环境变量阻止了嵌套调用，通过unset解决 差异分析: 人类提出问题，AI找到了根本原因并给出解决方案；但AI未在一开始就提示这个环境约束\nheadless server云盘同步方案 角色 思路 人类 主动提出headless server场景的特殊性（无法装云盘app），要求调研CLI上传方案 AI 调研了rclone/gdrive/onedrive等工具，推荐rclone并说明headless认证流程 差异分析: 人类识别到了边缘场景（headless server），AI才进行针对性调研；如果不提醒，AI可能只给出桌面云盘app方案\nVision encoding耗时瓶颈分析 角色 思路 人类 用户主动提出分析vision encoding耗时，说明用户对性能瓶颈有直觉认识 AI AI通过代码探索识别出三个具体瓶颈：patch提取串行循环、每批推理后empty_cache()、模型首次加载 差异分析: 用户提出问题方向，AI提供了具体量化的瓶颈定位和修复方案\nPreGrasp触发逻辑设计 角色 思路 人类 用户指出：夹爪是跳变过程（0→1），只需检测当前开启、N步后是否变化即可，不需要复杂的\u0026rsquo;正在接近\u0026rsquo;逻辑 AI AI设计了需要连续2帧距离递减的复杂接近检测逻辑 差异分析: 人类对任务物理特性的直觉理解比AI的通用设计更简洁有效；AI的过度工程化增加了不必要的约束条件\nrejection log分析思路 角色 思路 人类 用户直接问：\u0026lsquo;如果根本没有写rejection logging，那请把这个部分删除掉\u0026rsquo;；提出问题本质是接受率vs拒绝率的问题 AI AI绕道分析各种可能的rejection原因，没有直接检验rejection logging是否真的生效 差异分析: 人类能快速识别分析框架是否有效，AI容易在无效框架下做无谓分析\n批量删除的触发条件设计 角色 思路 人类 用户明确指出问题：说「取消这些日程」时Bot一直要求「specify which event」，希望能理解批量意图 AI AI设计了batch布尔字段由LLM判断是否批量，并给出「显示列表+确认按钮」的UX方案 差异分析: 人类发现的是UX问题，AI提供了技术实现方案；但AI未预见到LLM超时的边缘情况，需要人类通过测试发现后再次纠正\nccusage计费问题定位 角色 思路 人类 直接指出手动运行npx ccusage daily \u0026ndash;json数据正常，但脚本输出不对，怀疑是\u0026ndash;offline的问题 AI 从代码逻辑出发，对比有无\u0026ndash;offline的输出差异，发现offline模式Opus的cost字段为0 差异分析: 人类凭使用经验快速定位到嫌疑点；AI通过系统性对比验证假设。两者互补\nexport日志合并需求 角色 思路 人类 提出「如果检测到已有日志，尝试合并」的需求，考虑到多次export的数据不应丢失 AI 设计了(source, project, timestamp)三元组去重+新旧会话合并的具体实现方案 差异分析: 人类提出业务需求，AI设计技术方案；去重键的选择（三元组）是AI提出的\n实现层面 Q-Former过慢的处理策略 角色 思路 人类 用户选择减少epochs（200→50），这是在质量和速度之间的工程权衡 AI AI提出了三个选项并指出了时间影响（63小时），量化分析后供用户决策 差异分析: AI提供了量化分析和选项，用户做出了最终的工程判断\nAI 局限性 重要局限 SSH命令中忘记包含cd的错误重复10次以上，无法从错误中学习调整；需要用绕道方案（shell wrapper）才解决 在调试\u0026rsquo;力无效\u0026rsquo;问题时，AI先理论推导力大小，而非先验证力是否施加到正确位置；根本问题（body定位错误）被绕过 未预见嵌套Claude Code环境会阻断LLM调用，导致批量删除实现后立即因LLM超时失效，需要人类通过实际测试才能发现 设计PreGrasp检测器时过度工程化，添加了任务阶段检查、接近度检测等多个条件；每个额外条件都增加了触发失败的可能性 计划中未预料到PreGrasp还有lookahead逻辑用零动作、全局冷却、全局门控三个额外bug，需要在实际运行后才能发现 在设计pipeline_config.yaml时，未能主动提出extra_config.qformer.epochs的配置方案，而是等用户反馈Q-Former过慢后才被动调整 调用get_task_completion_stages()等已有功能而不知情，重复造轮子；代码库中已有完整的阶段判断逻辑却没被发现 subagent探索报告了多个「确认的bug」，部分（如TimeoutError处理器不可达、ROUTE_TO_INTENT大写值问题）经AI主体手动验证属于误报 对Windows平台特性（.cmd文件、npx解析）不够敏感，需要运行失败后才发现并修复 Claude CLI嵌套会话检测的解决方案（清除CLAUDECODE环境变量）是在实际运行报错后才找到的，未能提前预判 一般局限 Discord回复信息量不足这一UX问题未被AI主动发现，需要人类明确反馈后才着手改进 盲目复制proxy设置命令，未思考当前任务是否需要网络访问 批量删除的正则日期提取（_extract_delete_query）仅支持中文格式「N月N号」，英文日期格式未覆盖 未能预见rpy2/mclust在不同conda环境下的ImportError，只处理了ValueError，导致第一次staig_fusion测试失败 对ExperimentLogger的API不熟悉，错误地调用了不存在的save_comparison_csv()方法，需要人工触发测试才能发现 conda run的输出缓冲问题需要多次尝试才能找到可靠的解决方案（\u0026ndash;no-banner不支持、直接shell缓冲等），花费了不必要的时间 Grep工具在daily_summary.py上多次超时，需要退化到Bash grep命令，说明对大文件的正则搜索存在性能问题 今日收获 核心收获 两阶段Pipeline架构（提取缓存→复用评估）是benchmark系统的关键设计模式，核心原则是：encoder embedding与fusion策略正交，缓存键不应包含fusion名称 MuJoCo的xfrc_applied在每次mj_step后被清零，持续力注入必须在外层python循环中每步重新设置 调试\u0026rsquo;已实现但不触发\u0026rsquo;的功能时，要假设存在多层bug叠加。单独测试每一层（phase→lookahead→gate→cooldown→body name），每层独立验证 Claude Code通过CLAUDECODE/CLAUDE_CODE_ENTRY环境变量检测嵌套会话，清除这两个变量即可在脚本内调用claude CLI robosuite PickPlace环境中EEF body名称是\u0026rsquo;gripper0_eef\u0026rsquo;（body_id=21），不在常见列表中；字符串匹配fallback很危险（\u0026lsquo;screen\u0026rsquo;包含\u0026rsquo;ee\u0026rsquo;会误匹配） 调试策略：先验证机制是否工作（inf力测试），再验证力施加位置（调试日志），最后才考虑参数调整；跳过任何一步都会延误定位 离线生成阶段有真实demo actions可用，不需要靠零动作仿真预测未来。将future_demo_actions注入state_info是更可靠的方法 全局门控和全局冷却是常见的架构陷阱：一个检测器的触发会阻断所有其他检测器，应设计为按检测器独立管理 双重AI验证系统（SR+LLM）中，LLM fallback不可用时，SR-only路径的intent.data为空。需要为每个intent handler设计基于原始用户输入的fallback信息提取逻辑 ccusage的\u0026ndash;offline模式定价表不会自动更新新模型，生产环境应去掉\u0026ndash;offline以获取准确计费 训练型fusion（basic_contrastive=77min/section, staig_fusion=50min, Q-Former=5.7h）与非训练型（concat/mean/attention\u0026lt;1min）速度差异极大，benchmark调度需要分类处理 框架中存在STUB方法（永远返回False的_check_phase_condition()），这类死代码会导致依赖它的检测器静默失败而难以调试 Windows上npm全局命令是.cmd文件，Python subprocess需要shell=True才能找到；Unix-like环境（Git Bash）中则无此问题 rclone是跨平台云盘CLI同步的最佳选择，支持70+云盘，headless server可通过token copy方式认证 LLM输出解析器需要多重回退策略（直接解析→提取代码块→找{}区间），不能假设输出是纯JSON 实践收获 force_clip参数被嵌套在injectors.impulse下，但__init__从顶层读取导致始终用默认值。配置路径不匹配是静默bug的常见来源 Discord确认按钮（discord.ui.View）与主消息循环之间需要明确的「已直接发送」约定。handler返回空字符串时主循环不应再次发送，需要添加if response:检查 conda环境中直接调用Python二进制（/path/to/conda/env/bin/python -u）比conda run更适合后台长时任务，避免输出缓冲问题 robosuite OSC控制器kp=150（N/m），Sawyer EEF等效质量约5-10kg，稳态位移=F/（Λ·kp），3cm位移需要约25-45N持续力 在集群上：先用nvidia-smi -L检查当前节点是否已有GPU，有则直接用；SSH到新节点时需要cd到项目目录并激活conda环境 Google Calendar不同API端点需要不同OAuth scope。calendarList需要calendar.readonly，而settings只需calendar.events。设计时应仔细核对所有使用的API端点 STAIG的mclust聚类依赖rpy2，在非STAIG环境下会抛出ImportError而非ValueError，需要宽泛的except (ImportError, ValueError)来确保KMeans fallback生效 会话摘要 MIHD Benchmark (DCC) ✅ 两阶段Pipeline架构完整实现：12个模块创建与端到端测试 20:47:13.055 | claude_code 基于用户提出的两阶段架构理念（embedding提取与fusion评估分离），AI完整实现了pipeline/模块包（cache_manager, data_preparer, extraction_planner, gene_extractor, vision_extractor, evaluation_planner, runner）及入口脚本。在section 151508上进行端到端测试，修复了两个bug（save_comparison_csv API错误、mclust ImportError未捕获），四种fusion策略（concat/mean/attention/staig_fusion）全部测试通过，ARI范围0.14-0.42。\n🔍 Vision Encoding性能分析：瓶颈识别与三项优化方案设计 00:51:22.137 | claude_code 用户请求分析vision encoding耗时瓶颈。AI识别出三个主要瓶颈：patch提取串行循环（CPU）、每批推理后empty_cache()（约116次）、encoder级无缓存导致重复计算。用户选择同时实施三项优化（添加encoder级缓存、并行化patch提取、优化GPU推理）。这直接引出了完全重写pipeline的需求，实现计划已完成但因用户操作被打断而推迟实施。\n🔄 RTX 5000 Ada新GPU上运行完整Benchmark：Pipeline监控与Q-Former调优 21:52:14.896 | claude_code 用户切换到RTX 5000 Ada（32GB GPU）后，使用新的两阶段pipeline启动完整benchmark。Phase 1高效完成所有encoder提取（UNI2/HIPT/ResNet50/UNI-STAIG），Phase 2完成127/190个实验。发现Q-Former每section需要约5.7小时（200 epochs × 100s），用户决定将epochs减至50。修复了conda run输出缓冲问题，改用直接调用Python二进制。\n🔄 MIHD Benchmark超时处理与实验恢复：basic_contrastive调优 00:07:35.904 | claude_code 在DCC集群上监控benchmark进度时发现basic_contrastive fusion超时（1h限制不足，实际需要77min/section）。将超时上限从3600s改为7200s，同时根据用户要求移除了staig_fusion_e2e实验组（删除相关代码和已有结果），最终重启benchmark调度器继续运行176个待完成实验。\nGadgets (MacBook) ✅ 删除废弃工具后更新README和CLAUDE.md 00:58:36.238 | claude_code 用户删除了Gadgets仓库中不再使用的多个工具（Video、audio、image、papers、git、png2text.py等）。AI检查了git状态确认删除范围，更新了README.md（移除废弃工具表格项）和CLAUDE.md（移除对应命令和架构说明），同时更正了test/子模块的命令（从旧的testAll.py更新为新的python -m benchmark.cli）。\nErrorRecoveryBenchmark ✅ 调试力注入机制：发现EEF body定位错误并修复 21:27:34.381 | claude_code 实施调试计划：添加\u0026ndash;force_override和\u0026ndash;duration_override参数，在impulse.py添加详细日志。关键发现：力被施加到bin2（垃圾箱）而非gripper0_eef，原因是_get_eef_body_name()的possible_names缺少正确名称导致fallback匹配到\u0026rsquo;robot0_screen_collision\u0026rsquo;（含\u0026rsquo;ee\u0026rsquo;字符串）。修复后inf力测试显示机械臂明显移动，验证机制正常。随后用户提出新需求：1秒持续力+抓取前触发。\n🔍 根因分析：get_task_phase()是STUB导致所有相位相关检测器失效 22:49:50.381 | claude_code 深入分析PreGrasp不触发的根因：get_task_phase()调用了永远返回False的_check_phase_condition()存根方法，始终返回\u0026rsquo;unknown\u0026rsquo;，而PreGrasp和GraspPrecond都要求task_phase为\u0026rsquo;reach\u0026rsquo;或\u0026rsquo;grasp\u0026rsquo;。另外total_rejected永远为0说明rejection logging存在bug。用户指出这两个问题本质相同（死代码），要求制定修复plan，但plan未被批准执行。\n✅ 修复PreGrasp检测器不触发：get_task_phase stub + 死代码清理 22:56:34.002 | claude_code 用户提供计划要求修复PreGrasp永远不触发的问题。AI读取代码后实施了两个计划内修复（get_task_phase重写、database死代码清理），并在运行场景生成验证时发现额外三个bug（lookahead零动作、全局冷却阻断、近距离门控），逐一修复后pregrasp成功触发生成2个场景。所有41个单元测试通过。\n🔄 实现持续力注入机制和PreGrasp检测器 22:05:14.381 | claude_code 实施三阶段计划：(1)修复collect_rollout_stats()支持duration_steps循环重施加力；(2)新建pre_grasp.py检测器，按用户建议简化为检测\u0026rsquo;当前开启+10步后闭合\u0026rsquo;逻辑；(3)更新配置文件。测试发现PreGrasp检测器从未触发，通过夹爪轨迹分析发现demo_1有36步满足条件但检测器仍未触发。\n🔄 增大注入力使视频中机械臂扰动可见 00:38:54.892 | claude_code 用户发现视频中机械臂没有明显运动，与AI讨论后确定原因是力太小（3N对OSC控制的Sawyer臂约产生2-4mm位移）。AI实施了力放大方案（force_range [15,45]N，force_clip 60N，\u0026ndash;force_override参数，Phase 3改用demo actions，实现duration_steps后清零xfrc_applied），但30N后视频仍无可见运动。用户提出用inf大小力进行诊断性测试，该测试在会话结束时被中断。\n🔄 可视化Phase 3改为继续执行demo actions而非静止 00:45:39.241 | claude_code 用户要求修改2_visualize_scene.py，让Phase 3注入错误后机械臂继续执行demo轨迹。AI读取文件后修改了Phase 3循环体，并在an49上成功生成视频（193帧）。通过EGL headless渲染验证，日志显示'533 remaining demo actions\u0026rsquo;确认逻辑正确。随后用户发现视频中机械臂完全没有受到扰动，开始深入调试。\nCalendarPro 🔄 实现批量删除日历事件功能与修复衍生bug 20:08:24.708 | claude_code 用户通过plan要求实现批量删除功能。AI修改了5个文件：扩展数据模型、更新LLM prompt、扩展search_events参数、重写delete handler、新增BatchDeleteApprovalView。测试时发现LLM因嵌套Claude Code环境超时导致intent.data为空，AI添加了_extract_delete_query fallback。随后发现空消息崩溃和Discord回复详情不足，分别修复。最终通过unset CLAUDECODE重启应用解决根本问题。\n🔄 修复Google Calendar 403权限错误并讨论其他代码问题 00:47:55.337 | claude_code 用户通过plan要求修复calendarList API 403错误，AI将get_user_timezone()改为使用settings API。随后AI主动通过多个subagent探索代码库中的其他问题，发现了dual_verify缺少默认回复、裸except等问题，但subagent报告的部分bug属于误报（经主体验证否定）。计划制定了3项代码质量修复，但ExitPlanMode被用户拒绝。\n✅ 代码质量修复：dual_verify默认回复、裸except、Scheduler实例化优化 01:42:23.755 | claude_code 用户通过plan要求实现3项代码质量修复。AI在dual_verify.py添加了4个新intent的默认回复，修复了executor.py的裸except，并优化了periodic_checker.py中Scheduler的重复实例化。所有82个核心测试通过。\ngadget (summarize) ✅ 添加配置文件、机器标识与rclone云盘同步 22:14:06.675 | claude_code 用户需要多设备日志自动同步到云盘而不推送GitHub。AI实现了~/.config/summarize/config.json配置体系，支持device_name、logs_dir、reports_dir、rclone_remote字段，并集成rclone自动上传功能。新增了config \u0026ndash;show和config \u0026ndash;init子命令，更新了tutorial.md和README.md。\n✅ 为summarize工具添加Claude CLI后端 21:29:22.461 | claude_code 用户请求将summarize工具从Anthropic API切换到Claude Code CLI。AI新增了summarize_with_claude_cli函数，将prompt通过stdin传给claude \u0026ndash;print，并将默认后端从anthropic改为claude_cli。同时更新了tutorial.md、requirements.txt，并记录了三种后端的对比。\n✅ 修复ccusage计费、JSON解析与rclone上传结构 22:51:43.654 | claude_code 发现三个问题：ccusage \u0026ndash;offline导致Opus计费为$0，rclone上传分了logs/reports子目录不便管理，JSON解析遇到LLM在JSON前后加文字时失败。AI修复了全部三个问题：去掉\u0026ndash;offline、统一上传目录、重写解析器为三步回退策略。\n🔍 调试Claude CLI嵌套环境下返回空内容的问题 22:31:02.666 | claude_code 用户报告summarize脚本调用Claude CLI返回空stdout导致JSON解析失败。AI检查了subprocess调用代码，发现已有unset CLAUDECODE逻辑，问题可能是Claude CLI将输出写入stderr而非stdout，或者新版CLI在嵌套会话中的行为变化。会话在诊断过程中中断，问题未完全解决。\n✅ 切换Claude CLI模型为Sonnet + 完善export合并逻辑 23:26:16.145 | claude_code 用户要求将claude CLI调用切换为Sonnet模型，AI添加了\u0026ndash;model sonnet参数。随后发现ccusage Opus计费问题并修复。用户提出export时若日志已存在应合并而非覆盖，AI实现了基于三元组去重的合并逻辑。\n🔄 summarize工具添加配置文件与rclone云盘同步支持 21:55:40.143 | claude_code 用户通过plan要求为daily_summary.py添加配置文件和rclone同步功能。AI实现了config.json读写、device_name自动检测、rclone上传和config \u0026ndash;show/\u0026ndash;init子命令。随后用户尝试实际运行生成日报，但Claude CLI在嵌套环境中返回空内容导致失败，未能成功生成。\n✅ 旧版summarize日报（含2026-02-13 CalendarPro记录） 23:24:58.955 | claude_code 这是一个日报分析会话，用户提交了2026-02-13 CalendarPro代码库整理的对话记录，AI以旧版日报格式（无level/importance字段）生成了结构化日报JSON。该会话本身不涉及代码修改。\n✅ 旧版summarize日报（含CalendarPro 2026-02-14记录） 23:51:07.975 | claude_code 旧版日报分析会话，用户提交了CalendarPro 2026-02-14的Bug修复记录，AI生成了结构化日报。该会话本身不涉及代码修改，是summarize工具的一次实际使用。\n✅ 旧版summarize日报（含2026-02-13 CalendarPro记录，第二次） 23:47:19.175 | claude_code 又一个旧版日报分析会话，内容与23:24的会话基本相同，是同一天内重复运行的产物。无代码修改。\nToken 用量 总览 指标 数值 总 Token 118,217,669 输入 Token 37,508 输出 Token 110,857 Cache 创建 5,794,068 Cache 读取 112,275,236 Cache 命中率 95.1% 总费用 (USD) $45.8358 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 19,430 108,575 3,877,057 98,860,665 $40.5460 88.5% claude-haiku-4-5-20251001 9,712 1,788 1,412,273 12,219,193 $3.0059 6.6% claude-sonnet-4-5-20250929 8,366 494 504,738 1,195,378 $2.2839 5.0% 各设备用量 设备 总 Token 输入 输出 费用 DCC 20,264,782 9,756 35,824 $14.5508 MacBook 51,452 3 2 $0.1314 tianhe 59,510,236 22,621 45,170 $3.1435 TzJsDesktop 38,391,199 5,128 29,861 $28.0100 ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-14/","summary":"横跨三个项目的密集开发日：在HPC集群上完成MIHD多模态空间转录组两阶段Pipeline架构重写并通过端到端测试；修复机器人错误恢复基准框架中力注入机制的多层系统性Bug（EEF定位错误、持续力缺失、PreGrasp检测器五层级联Bug）；同时大幅改进gadget/summarize工具（Claude CLI后端、ccusage计费修复、rclone云盘同步、export合并、Windows兼容性），并推进CalendarPro Discord Bot的批量删除功能与代码质量修复。","title":"Bug Journal 2026-02-14"},{"content":"日报 — 2026-02-13 在多台设备和多个项目上高强度推进：DCC HPC集群上实现MIHD多模态空间转录组286实验可恢复调度系统并修复subprocess cwd关键bug；天河集群上修复MuJoCo cvel速度读反bug、完成机器人错误恢复基准测试代码库重构（v4.0→v4.1）、实现GPU端到端冒烟测试并成功生成30个有效错误场景；MacBook上从零实现AI对话日报两阶段多设备工具；Windows上完成CalendarPro代码库重构整理、修复多个关键Bug并实现P0稳定性修复、P1智能提升及Random Thoughts碎片想法系统。\n今日任务 架构与策略 🔄 初始化静置 + EEF 近距离门控（ErrorRecoveryBenchmark） — 在 instability.min_step 改为 100、新增 EEF 距离门控后，发现物体在整个 demo 回放中完全不动（EEF 与物体最近距离始终 ≥0.22m）。深入排查发现根本原因是 demo 录制环境（PickPlace_D0 + OSC_POSE controller）与回放环境不匹配 ✅ 实现S5.3 GPU节点端到端冒烟测试（ErrorRecoveryBenchmark） — 修复rollout_generator.py硬编码问题（改用INJECTOR_REGISTRY/VALIDATOR_REGISTRY）、命名不一致（friction_scale→friction）、参数名不匹配、缺少injector.clear()调用等多处bug。创建GPU runner脚本，在an49节点运行冒烟测试4步全部通过 ✅ 修复demo回放环境配置不匹配（ErrorRecoveryBenchmark，天河） — 发现根本原因：1_generate_scenes.py创建的是无controller_configs的普通PickPlace环境，而demo使用PickPlace_D0+OSC_POSE控制器录制。修复方案：从HDF5 dataset元数据读取env_args（含controller_configs），同步修复2_visualize_scene.py，恢复proximity detector合理阈值 ✅ 实现可恢复 Benchmark 调度器（MIHD） — 创建 scripts/run_all_benchmarks.py，支持 286 个实验（A-F 六类）的断点续跑、进度追踪、环境过滤（General/scgpt_3）和组别过滤 ✅ 修复 MuJoCo cvel 速度读反 bug（ErrorRecoveryBenchmark） — 发现 env_wrapper.py:get_object_velocity() 中 cvel[:3] 和 cvel[3:] 读反（MuJoCo cvel 布局是 [angular, linear]，代码按 [linear, angular] 读），导致角速度被当成线速度触发 instability detector 误报。修复后重新生成场景并验证 🔄 修复cvel线速度角速度读反bug（ErrorRecoveryBenchmark，第二次记录） — 发现env_wrapper.py中get_object_velocity()错误读取MuJoCo的cvel数组：cvel布局为[angular(3), linear(3)]但代码按[linear(3), angular(3)]读取。需修复后重新生成所有scenes ✅ 代码库整理重构 v4.0→v4.1.0（ErrorRecoveryBenchmark） — 删除12个冗余.md文件和~190行死代码（ReplaySystem类、重复detect()方法），确认3个新注入器（friction/pose_perturb/gripper_bias）已注册，为11+核心模块添加中文docstring，将clumsy_franka.py移至IsaacLab/。全部41个单元测试通过，版本升至4.1.0 ✅ 修复 dual_verify.py SR/LLM分歧时使用错误intent（CalendarPro） — 发现当语义路由(SR)和LLM对用户意图判断不一致时，系统始终采用SR的结果。修复为mismatch时改用LLM的intent ✅ Random Thoughts收集与自动整理系统（CalendarPro） — 新建src/thoughts/模块：thought_store.py（JSONL存储）、idle_detector.py（基于DEFAULT_MEALS/DEFAULT_SLEEP检测空闲时段）、thought_organizer.py（后台asyncio任务，每10分钟检查一次，空闲时触发AI整理为大/中/小目标结构化文档）；集成到discord_bot；新增IntentType.CAPTURE_THOUGHT和QUERY_THOUGHTS；25个测试用例全部通过 ✅ 实现定时自检与循环日程自动安排功能（CalendarPro） — 新建 3 个文件（recurring_task_store.py、periodic_checker.py、scheduling/init.py），修改 5 个文件，并创建 18 个单元测试，全部通过。每 15 分钟检查一次，自动安排餐食/睡眠及用户自定义循环任务 🔄 启动 General 环境 286 实验运行（MIHD） — 在 GPU 节点（Tesla P100-16GB）上启动 General 环境的 207 个实验，修复 subprocess cwd bug 后重新启动，运行到 pca+uni2+mean 阶段 ✅ Phase 1-2: Config Foundation + Normalization（MIHD） — config.yaml 添加 7 个新配置块；config_manager.py 添加 8 个新 dataclass；run_benchmark.py 实现三点归一化（post-encoder/pre-fusion/post-fusion）和 extra_config 参数管道 🔄 AI 对话日报工具开发（summarize/daily_summary.py） — 从零实现支持 Claude Code/ChatGPT/通用 JSON 三种数据源的日报生成工具，初版完成后用户提出多设备场景需求，重新设计为两阶段架构（export + merge），并集成 Hugo 部署流程，最终因计划审批未通过而暂停 🔄 分析可视化视频因果链缺失问题（ErrorRecoveryBenchmark） — 发现可视化视频三阶段画面几乎一样的根本原因：collect_rollout_stats 内部 step 了20步但不返回帧，且缺少 demo actions 回放来展示触发条件满足的过程 ✅ 重构可视化脚本实现三阶段demo回放（ErrorRecoveryBenchmark） — 修改scripts/2_visualize_scene.py，从静态快照恢复改为从HDF5加载demo actions并三阶段回放：Phase 1（绿色NORMAL STATE，回放demo到trigger_step）→ Phase 2（红色ERROR INJECTED!，注入错误）→ Phase 3（橙色POST-ERROR ROLLOUT，每帧渲染）。成功生成132帧视频（11+1+120），面包tip_over角度1.60 rad ✅ 编写项目全景总结文档（ErrorRecoveryBenchmark） — 探索15+个Markdown文件，汇总定量数据（6796行代码、26个文件、3个验证场景），生成395行中文综合总结，涵盖愿景、架构、目标层级、诚实进度评估与差距分析表、P1/P2/P3优先级下一步计划及风险版本历史 ✅ 移植3个新错误注入器（ErrorRecoveryBenchmark，天河） — 从mimicgen_workspace移植并适配FrictionInjector（摩擦系数缩放）、PosePerturbInjector（物体姿态扰动）、GripperBiasInjector（夹爪偏置力），更新injectors/init.py注册表和error_framework/init.py导出，版本号升至4.1.0 ✅ 创建 src/ai/provider_selector.py - 统一 AI 提供商选择（CalendarPro） — 将 8 处重复的 AI 提供商选择 for 循环提取为 provider_selector.py 中的 create_provider()、get_first_available_provider()、generate_with_fallback() 三个函数 ✅ 创建 docs/PLANNING.md 项目规划文档（CalendarPro） — 通过深度探索CalendarPro和OpenClaw代码库，生成了330+行中文规划文档，涵盖项目定位、核心理念、模块成熟度评估（★1-5）、三层目标体系、架构决策理由（6个关键决策）、P0-P3行动路线图 ✅ 修复日历操作错误信息不传回用户（CalendarPro） — calendar_service.py中所有HttpError被print()吞掉并返回None。修复：添加CalendarServiceError异常类，discord_bot的handler catch并显示详细错误 ✅ P0-1：JSON解析容错加固（CalendarPro） — 为AIProvider.parse_response()新增_extract_json()方法，实现平衡括号提取、尾逗号修复、GENERAL意图兜底三级容错策略 ✅ Phase 4: Q-Former + LLaVA MLP 融合策略（MIHD） — 新建 models/QFormerFusion.py（Q-Former 跨模态对齐）、在 Fusion.py 添加 LLaVAMLPFusion，注册到 apply_fusion() 和 init.py ✅ Phase 5: Niche 查询功能（MIHD） — 新建 utils/niche_utils.py、models/NicheEncoder.py（GIN 编码器）、scripts/run_niche_query.py（跨样本查询脚本） ✅ Phase 6: 批次校正 + Joint 多 Section 模式（MIHD） — 新建 models/BatchCorrection.py（对抗批次校正）、test_batch_correction.py，添加 \u0026ndash;joint_sections CLI 参数支持联合分析 🔄 SSH 到 GPU 节点运行 smoke test（S7.1）（ErrorRecoveryBenchmark） — 确认 GPU 节点为 an49（A800×5），成功运行场景生成（2个场景）和拒绝分析，但在 collect_data 阶段遇到 fingerprint 不匹配问题，清理旧数据后分析可视化视频效果时发现更深层问题 🔄 P0 Bug 修复 + P1 新功能规划（CalendarPro） — 进入 Plan Mode 探索代码库，规划 P0（JSON 解析容错、错误处理加固、消息队列稳定性、AI 提供商超时）和 P1（学习数据自动采集、能量投影个性化、智能排程整合学习数据、会话上下文改进）。用户拒绝了 ExitPlanMode 步骤 ✅ 修复循环事件更新逻辑（只修改特定实例）（CalendarPro） — update_event方法在更新循环事件实例时会把recurrence字段传回去，可能修改整个系列。修复：检测event的recurringEventId字段，是循环实例时移除recurrence字段再更新 ✅ P0-3：消息队列稳定性（CalendarPro） — 为asyncio.create_task添加done_callback，新增_on_queue_task_done自动清理失败任务，新增close()方法取消所有队列任务 ✅ P1-1：学习数据自动采集（CalendarPro） — LearningService新增backfill_energy_change()回填真实能量变化；discord_bot的多个handler新增数据采集逻辑 ✅ P1-2：能量投影个性化（CalendarPro） — EnergyProjector新增personalize_rhythm()方法，将用户历史数据与默认昼夜节律曲线按比例混合（最多70%历史权重） ✅ 创建 src/core/time_utils.py - 统一时间解析（CalendarPro） — 从 energy_projection.py、intelligent_scheduler.py、essential_scheduler.py 三处提取完全相同的 _parse_time() 方法，集中到新建的 time_utils.py，消除重复代码 ✅ 统一 FIXED/FLEXIBLE 关键词常量（CalendarPro） — 将两处完全相同的关键词列表提升为 models.py 模块级常量 FIXED_KEYWORDS/FLEXIBLE_KEYWORDS ✅ P1-3：智能排程整合学习数据（CalendarPro） — IntelligentScheduler构造函数接受可选learning_service参数；新增_get_energy_change()优先从学习数据获取任务能耗，无数据时回退默认值 实现与修复 ✅ 重新生成错误场景并验证（ErrorRecoveryBenchmark，天河） — 在an49 GPU节点重新生成，成功产出30个有效场景（来自10个demo，每demo3个），触发步骤从step15（臂远离物体）修正为step52-121（EEF在物体5cm内），生成两个可视化视频验证正确性 ✅ 创建结果汇总脚本（MIHD） — 创建 scripts/summarize_benchmark.py，支持按组聚合 ARI/NMI/Silhouette、热力图、完成度报告和 LaTeX 表格输出 ✅ P0-2：AI提供商30秒超时（CalendarPro） — 对GLM、OpenAI、Gemini三个提供商的所有API调用包裹asyncio.wait_for(timeout=30)，防止网络卡住导致Bot无响应 ✅ P0-5：意图分类边界测试（CalendarPro） — 创建tests/test_intent_classification.py，覆盖56个测试用例：关键词检测、general chat判断启发式、JSON解析容错（含多JSON块、截断JSON、前置文字等边界情况）、任务灵活性分类 ✅ 更新 benchmark_config.yaml（MIHD） — 添加 A-F 六个实验组定义，添加 full_benchmark、general_benchmark、scgpt_benchmark 环境组及新融合策略的 conda_env 映射 ✅ 实现 LargeOffsetValidator（S5.1）（ErrorRecoveryBenchmark） — 实现大偏移验证器，验证物体在 xy 平面是否发生足够大的位移（≥0.10m），用于验证摩擦力注入和姿态扰动的效果，并新增 16 个单元测试 ✅ 实现 StuckValidator（S5.2）（ErrorRecoveryBenchmark） — 实现卡住验证器，验证物体是否在注入后持续静止（≥80%帧速度\u0026lt;0.002m/step），适用于高摩擦粘住和夹爪偏置场景，并新增单元测试 ✅ 可视化脚本标注增强（ErrorRecoveryBenchmark） — 为 scripts/2_visualize_scene.py 添加三阶段视频标注（绿色NORMAL STATE/红色ERROR INJECTED!/橙色POST-ERROR ROLLOUT）、左下角信息框和右下角帧计数器，同时修复 hardcode ImpulseInjector 的遗留 bug ✅ 调研instability score相关文献（ErrorRecoveryBenchmark） — 搜索并汇总了6类机器人操作稳定性度量方法：Energy Margin、Tip-Over Stability Margins（ZMP/FA/MHS/TOM）、Grasp Wrench Space度量、Discrepancy Score、Anomaly Score、Stability-to-Instability Transition Detection ✅ 更新计划文档为嵌套大/中/小目标结构（ErrorRecoveryBenchmark） — 编写包含G1→M1-M7、G2→M5、G3→M4/M6嵌套结构的综合路线图，覆盖11项剩余任务 ✅ 代码库清理——删除死代码与冗余文档（ErrorRecoveryBenchmark，天河） — 删除fingerprint.py中的ReplaySystem类、ReplayDriftError和compute_drift_metric()（共~190行死代码），删除proximity.py中重复的detect()方法（含bug版本），删除12个被项目全景总结.md取代的过时.md文件 ✅ 统一餐食时间定义（CalendarPro） — 将 energy_projection.py 中硬编码的 MEAL_TIMES 字典和 prompts.py 中的餐食时间字符串，改为从 EssentialScheduler.DEFAULT_MEALS 动态派生，实现单一数据源 ✅ 修复Windows启动脚本Conda环境检测（CalendarPro） — start.bat无法在cmd中检测到conda环境。方案：创建start.ps1（PowerShell支持conda原生激活），start.bat改为自动调用PowerShell。新增stop.ps1通过PID文件强制停止孤儿进程 ✅ P0-4：服务层错误处理加固（CalendarPro） — LearningService的多个方法全部包裹try/except并记录日志；ConversationHistory的_load加入数据类型校验；record_task增加空输入验证 ✅ P1-4：会话上下文改进（CalendarPro） — prompts.py新增_format_conversation_context()函数，格式化最近5轮对话历史，包含意图流追踪和角色标注 ✅ 修改实验日志自动更新时间戳（MIHD） — 修改 experiment_logger.py 的 _append_to_global_log 方法，每次追加实验后自动更新 experiments_log.md 文件开头的「最后更新」时间戳 ✅ 更新项目全景总结文档至 v4.1（ErrorRecoveryBenchmark） — 将项目文档中的过时数据（1个注入器/26文件/6796行）更新为实际状态（4个注入器/29文件/7328行），并同步所有相关统计数字、进度状态和下一步路线图 ✅ CalendarPro 代码库整理 - 合并测试文件 — 将 test_claude_final.py 与 diagnose_claude.py 合并为 tests/test_claude_provider.py，将 cli/test_semantic.py 移动到 tests/test_semantic_routing.py ✅ 更新 CLAUDE.md（CalendarPro） — 通过 /init 命令重写项目 CLAUDE.md，新增共享工具模块说明、三层排程系统架构说明、测试注意事项，从 188 行压缩到 140 行 ✅ 更新README.md Tutorial（CalendarPro） — 扩展README中的Discord使用示例为4个分类场景，新增Discord命令速查表，将能量系统章节扩展为完整的学习系统文档，新增3个FAQ ✅ CLAUDE.md 生成（Gadgets 仓库） — 用 /init 命令分析 gadgets 仓库结构，生成包含运行命令、架构说明和依赖说明的 CLAUDE.md 文件 ✅ 更新CLAUDE.md（ErrorRecoveryBenchmark，天河） — 将615行重复冗余的CLAUDE.md重写为101行精简版，删除与README重复的内容、详细API文档，保留服务器约束、命令、架构要点、插件扩展模式 ✅ CalendarPro 代码库整理 - 删除冗余文件 — 删除临时调试脚本、Windows 生成空文件、废弃目录。成功删除：ai_planner/ 目录、临时测试 JSON 文件、4 个已被取代的测试文件。nul 为 Windows 设备名无法删除，改为加入 .gitignore ✅ 更新 .gitignore 和 pyproject.toml（CalendarPro） — .gitignore 新增 .claude/ 目录。pyproject.toml 将 google-generativeai 替换为 google-genai\u0026gt;=1.0.0，新增 aiofiles、langdetect、semantic-router、sentence-transformers 等缺失依赖 🔄 场景可视化生成与视频标注需求探索（ErrorRecoveryBenchmark） — 在an49生成83KB 105帧演示视频（tip_over错误，tip_angle=1.60 rad）。用户提出三阶段视频标注方案，AI规划了实现方案但ExitPlanMode被拒，视频标注功能尚未实现 问题与解决方案 关键问题 1. demo 回放时机械臂完全不靠近物体（最近距离 0.22m），物体全程不动，与真实 pick-and-place 行为完全不符 解决方案: 发现根本原因是环境配置不匹配：demo 用 PickPlace_D0 + OSC_POSE controller 录制，但回放用普通 PickPlace 且无 controller_configs，导致相同 action 产生完全不同轨迹。修复：从 HDF5 dataset 元数据加载完整 env_args\n关键洞察: 人类（用户）提出「pick-and-place 肯定会碰到物品，22cm 不对」，从业务常识质疑 AI 给出的数据结论，才引导出真正的根因排查。AI 缺乏「数值是否符合物理直觉」的验证意识\n2. demo回放产生错误的轨迹：机械臂在demo中从未靠近物体，导致proximity detector无法触发 解决方案: 从HDF5 dataset元数据（f[\u0026lsquo;data\u0026rsquo;].attrs[\u0026rsquo;env_args\u0026rsquo;]）加载完整的env_kwargs包括controller_configs（OSC_POSE, kp=150, damping=1），处理PickPlace_D0→PickPlace环境名映射\n关键洞察: demo录制环境和回放环境必须完全匹配——controller配置差异会导致相同action产生完全不同的运动轨迹，这是机器人学习场景中最常见也最隐蔽的bug之一\n3. 调度器显示 58 个实验成功，但实际只有 15 个 npz 文件生成，绝大多数实验没有保存输出 解决方案: 发现根因：调度器通过相对路径调用子进程，但子进程 cwd 不是 MIHD 根目录，导致 output_dir 相对路径解析错误；修复方案：使用 file 推导绝对路径 + subprocess.run 传 cwd + 将 output_dir 转为绝对路径\n关键洞察: 进度文件记录「成功」只代表 exit code=0，不代表文件实际保存到正确位置；需要同时检查 npz 文件数量来交叉验证\n4. MuJoCo cvel 布局理解错误：代码注释写 [linear, angular]，实际是 [angular, linear]，导致速度读反 解决方案: 交换 cvel[:3] 和 cvel[3:] 的赋值，同时修正注释。修复后 instability detector 不再在 step 11 误触发\n关键洞察: MuJoCo cvel 的布局是 [angular(3), linear(3)]，与直觉相反。代码注释与实际行为不一致时要以 MuJoCo 官方文档为准\n5. rollout_generator.py硬编码ImpulseInjector，忽略已注册的3个新注入器 解决方案: 重构为使用INJECTOR_REGISTRY和VALIDATOR_REGISTRY动态加载，确保所有注入器均被调用\n关键洞察: 有注册表却不用，说明新增实现后必须立即检查所有调用点是否已切换为注册表动态加载\n6. 检测器发出\u0026rsquo;friction_scale\u0026rsquo;但INJECTOR_REGISTRY键名为\u0026rsquo;friction\u0026rsquo;，导致注入器匹配失败 解决方案: 修复instability.py和base_detector.py中的命名映射，统一使用注册表键名\u0026rsquo;friction\u0026rsquo;\n关键洞察: 跨模块的命名约定必须集中管理，注册表键名是唯一真实来源\n7. 所有3条退出路径均缺少injector.clear()调用，导致friction/gripper_bias状态在set_sim_state_flat()后仍然残留 解决方案: 在rollout_generator.py的全部3条退出路径上添加injector.clear()调用\n关键洞察: 有状态对象必须在所有退出路径清除，应用上下文管理器或finally块确保清除\n8. 语义路由(SR)和LLM分歧时总是用SR结果，导致意图误分类 解决方案: 修改dual_verify.py，mismatch时改用LLM的intent，日志记录→ using LLM intent\n关键洞察: SR只做嵌入相似度匹配，缺乏上下文理解；LLM能理解对话含义。分歧时LLM更可靠\n9. conda run 在 subprocess 中无法直接 activate 环境，导致后台任务输出为空 解决方案: 找到 General 环境的绝对 Python 路径直接调用，绕过 conda activate\n关键洞察: conda run 会缓冲 stdout，但直接用环境 Python 路径可以正常输出；子进程会继承该 Python 的环境，无需额外处理\n10. instability detector 在 demo 初始化阶段（step 10-15）因 MuJoCo 残余震动误触发，产生不真实场景 解决方案: 将 instability.min_step 从 10 改为 100（5s@20Hz），跳过初始化震动期。其他 detector 保持原值\n关键洞察: 只有 instability detector 受初始化震动影响，不应一刀切地提高所有 detector 的 min_step\n11. 可视化视频三阶段画面几乎一样，看不到面包倾倒的效果 解决方案: 根因有两个：(1)缺少demo回放上下文，只恢复静态快照就注入；(2)collect_rollout_stats()内部step了20步但不返回渲染帧。解决方案：从HDF5加载demo actions，三阶段全程捕获帧\n关键洞察: 错误效果不可见的真正原因是架构设计问题（validation阶段吞帧），而非标注或渲染问题\n12. grasp_precond.py中参数名与注入器接口不匹配（position_magnitude→random_offset+offset_range，bias→bias_force） 解决方案: 修正检测器的参数名以对齐注入器接口合同\n关键洞察: 接口合同需要在文档或类型系统中显式定义，否则跨模块参数名会发散\n13. calendar_service.py所有HttpError（含403权限错误）被吞掉，用户看不到错误原因 解决方案: 新增CalendarServiceError异常类，所有HttpError改为raise而非return None；Discord handler catch后返回详细错误信息\n关键洞察: 静默失败是调试最大的障碍。错误信息应该尽可能透传到用户界面\n14. JSON解析正则在多JSON块场景下贪婪匹配导致无效JSON 解决方案: 改用平衡括号计数算法提取第一个完整JSON对象，而非贪婪正则\n关键洞察: 正则无法处理嵌套结构，括号计数才是正确的JSON边界检测方法\n15. EEF 近距离门控阈值反复调整，仍无法生成足量场景 解决方案: 诊断后发现根本问题是 demo 回放环境不匹配，这些调整都是在错误前提下的妥协。根因修复后恢复合理阈值：distance_threshold=0.15m，confidence_threshold=0.5\n关键洞察: 在没有修复根本问题的情况下调整阈值是治标不治本。应优先验证数据质量，再调参\n16. 4_analyze_results.py将JSONL中的success字段加载为字符串\u0026rsquo;False\u0026rsquo;而非布尔值False 解决方案: 添加显式布尔转换：bool(data[\u0026lsquo;success\u0026rsquo;]) if isinstance(\u0026hellip;, bool) else data[\u0026lsquo;success\u0026rsquo;].lower() == \u0026rsquo;true\u0026rsquo;\n关键洞察: 从序列化格式加载布尔字段时永远不要假设类型自动转换\n17. 更新循环事件时可能修改整个系列而非特定实例 解决方案: 在update_event中检测recurringEventId字段，移除recurrence字段后用instance ID更新，让Google Calendar只创建单实例exception\n关键洞察: Google Calendar API中循环事件实例的ID格式为base_id_YYYYMMDDTHHMMSSZ，用instance ID更新会自动创建exception而不影响系列\n一般问题 18. run_evaluation() 函数内部无法访问 argparse 的 args 对象，导致 extra_config 参数无法传递 解决方案: 在 main() 中构建 extra_config dict 并作为参数传入 run_evaluation()，替代 getattr(args, \u0026hellip;) 的错误方案\n关键洞察: 大型函数的参数传递应通过显式 dict 而非全局变量或隐式访问外层 scope\n19. joint_sections 批次校正代码引用了不存在的 from utils.clustering_utils import cluster_embeddings 解决方案: cluster_embeddings 定义在 run_benchmark.py 本身，移除错误 import 直接调用本地函数\n关键洞察: 实现时需要先 grep 确认函数定义位置，不能假设模块结构\n20. niche embedding 保存代码引用了 spatial_coords，但该变量仅在 fusion 块内部定义，范围外不可用 解决方案: 改用 try/except 在保存块内重新加载坐标\n关键洞察: 跨代码块引用局部变量是常见的作用域陷阱，特别是在大型函数中\n21. proximity.py中存在重复的detect()方法，第二份还有bug（_generate_impulse_specs缺少closest_obj参数） 解决方案: 删除重复的第二份detect()方法，修复_generate_impulse_specs签名接受closest_obj参数\n关键洞察: 复制粘贴的代码会引入难以发现的不一致bug\n22. Windows cmd无法识别conda activate，启动脚本检测逻辑失效 解决方案: 从cmd切换到PowerShell脚本（start.ps1），conda在PowerShell中原生支持；start.bat改为桥接脚本调用PowerShell\n关键洞察: conda在Windows上对cmd和PowerShell的支持差异显著，应优先使用PowerShell\n23. 测试中 asyncio mock 路径错误导致单元测试失败 解决方案: 将 mock 从顶层 import patch 改为方法内 lazy import patch 方式，使 patch 路径与实际代码一致\n关键洞察: Python 的 patch 目标必须是实际引用的路径，而非原始定义路径；lazy import 需要 patch 方法内部的 import 位置\n24. ssh 命令中 cd 不生效，多次调用 Python 脚本时路径错误 解决方案: 通过写 shell 脚本内嵌 cd，或直接传绝对路径给 PYTHONPATH + python 解决\n关键洞察: ssh \u0026ldquo;cmd1 \u0026amp;\u0026amp; cmd2\u0026rdquo; 中，cd 在 bash 子进程中生效，但 ssh 的工作目录仍是 HOME。需要用 bash -c \u0026lsquo;cd \u0026hellip; \u0026amp;\u0026amp; cmd\u0026rsquo; 或绝对路径\n25. smoke test 中 collect_data 遇到 fingerprint 不匹配：旧场景用旧版代码生成，当前代码版本已变 解决方案: 清理 outputs/error_scenes/data/ 下的旧场景文件，重新从 Step 1 生成\n关键洞察: Fingerprint 校验机制在代码迭代时需要配套的数据清理流程\n26. 临时文件名含 Unicode 私用区字符，ls 无法找到但 Python os.listdir 可见 解决方案: 用 Python os.listdir() 遍历查找含 \u0026lsquo;Temp\u0026rsquo; 的文件名，再调用 os.remove() 删除\n关键洞察: Windows 路径中的全角冒号在 Git Bash 环境下被编码为 Unicode 私用区字符，shell 工具无法直接处理，但 Python 可以\n27. pytest 测试挂起（test_semantic_routing.py 需要下载 sentence-transformers 模型） 解决方案: 使用 \u0026ndash;ignore 排除需要外部资源的测试文件，只跑核心逻辑测试，26/26 通过\n关键洞察: 需要网络或模型下载的测试应标记 skip 条件或分组到 integration 目录，避免阻塞 CI\n28. Grep/Ripgrep 在搜索大型单文件（discord_bot.py，1800+ 行）时多次超时 解决方案: 改用 bash grep 命令直接定位行号，再用 Read 工具按行号读取具体内容\n关键洞察: 对大型单文件使用复杂正则 Grep 效率低，直接用行号定位更可靠\n29. 单元测试中 exact threshold 边界浮点精度失败 解决方案: 将边界测试值从精确阈值改为略高于阈值的值（如 0.10001 代替 0.10），避免浮点比较的不确定性\n关键洞察: 边界测试应使用「刚好超过」而非「精确等于」，除非测试的是严格相等语义\n30. SSH 到 GPU 节点时脚本路径报错，Python 总是在 HOME 目录下找脚本 解决方案: SSH 命令中必须显式 cd 到项目目录，或使用绝对路径。最终使用绝对路径解决\n关键洞察: 集群节点共享同一文件系统，路径与本地完全相同，但 SSH 登录后默认工作目录是 HOME，必须显式指定路径\n31. Windows 下 nul 设备文件无法删除（Access is denied） 解决方案: 识别为 Windows 保留设备名，不尝试删除，改为加入 .gitignore 忽略追踪\n关键洞察: Windows 的 nul 等设备名是操作系统保留名称，是命令行重定向产物，不能通过文件系统 API 删除\n32. conda activate 在 Git Bash 下不可用（CondaError: Run conda init first） 解决方案: 直接使用完整路径 C:/Users/tongt/miniconda3/envs/calendarpro/python.exe 调用 conda 环境的 Python\n关键洞察: 在非 conda-initialized shell 中使用 conda 环境时，应直接引用 Python 可执行文件绝对路径，而不是 activate\n33. MetricsWithCI类缺少__format__方法，导致f\u0026rsquo;{metric:.3f}\u0026lsquo;格式化输出失败 解决方案: 为MetricsWithCI添加__format__方法，委托给self.value的格式化\n关键洞察: 自定义数值类型应实现__format__以支持标准格式化语法\n人类思路 vs AI 思路 战略层面 EEF 与物体距离 0.22m 的合理性质疑（ErrorRecoveryBenchmark） 角色 思路 人类 用户从业务常识出发：「pick-and-place demo 里肯定会碰到物品，怎么可能有 22cm 距离」，直接质疑数据结论的合理性 AI AI 接受了 0.22m 这个数值，并在此基础上不断调整阈值，试图在错误前提下找到可行方案，没有质疑数值本身是否合理 差异分析: 人类凭借领域常识识别出数据异常，触发了正确的根因排查。AI 缺乏「数值是否符合物理直觉」的验证意识，倾向于接受测量结果并在其约束内优化\n发现instability detector误触发的根本原因（ErrorRecoveryBenchmark） 角色 思路 人类 直接观察视频：机械臂明明离面包很远，面包没动，但detector却触发了。用感知经验直接质疑检测结果的合理性 AI 查看JSON元数据，看到linvel=0.445 m/s，认为这是物理上合理的速度并尝试从仿真初始化角度解释 差异分析: 人类通过视觉直觉立刻识别出不合理（面包没动但报告高速），AI则先相信数据并尝试合理化解释。人类的观察直接引导到了cvel读反的真正bug\ndemo回放失败的根因分析（ErrorRecoveryBenchmark，天河） 角色 思路 人类 人类提供了完整的根因分析文档，明确指出是controller_configs不匹配导致轨迹偏差，并提供了正确的参考实现位置 AI AI上一轮不理解根因，选择放宽阈值参数来「凑」出可用的scene，是一种治标不治本的解决方案 差异分析: 人类通过系统性分析找到了根本原因（环境配置不匹配），而AI倾向于局部调参来绕过症状\n从mimicgen移植错误类型的决策（ErrorRecoveryBenchmark） 角色 思路 人类 用户在AI给出选项后主动选择从mimicgen移植部分错误类型，而非重写或放弃这4610行代码 AI AI发现mimicgen_workspace有17种错误类型共4610行，识别复用价值并提出选项供用户选择 差异分析: 人类做了战略决策（复用 vs 重写），AI做了发现和选项生成。方向由人确定，执行由AI完成\n视频标注的UX设计方案（ErrorRecoveryBenchmark） 角色 思路 人类 用户提出具体的三阶段标注设计：绿色NORMAL STATE→红色ERROR INJECTED→橙色POST-ERROR，并明确要求显示检测触发信息的文本 AI AI在用户提出后制定了实现方案（cv2.putText + 半透明背景矩形），但未主动提议这一功能 差异分析: 用户提出了完整的UX设计洞见，AI只是实现者。可视化沟通设计完全来自人类\nsubprocess cwd bug 的发现（MIHD） 角色 思路 人类 人类通过「实际 npz 文件数量 vs 进度文件记录数量不一致」发现问题，要求继续 benchmarking 时触发了调查 AI AI 在被告知继续后才开始对比 npz 文件数量和进度记录，找到根因 差异分析: 人类更早地察觉到了结果异常（进度显示58成功但文件不存在），AI 则需要多次探测才定位到 cwd 问题\n日报工具的多设备架构设计 角色 思路 人类 用户主动提出「需要在 Windows/Linux/Mac 上各自导出，最后汇总」这一架构需求，识别出单机版设计不满足实际使用场景 AI AI 初始设计为单机一体化流程（解析 + 总结 + 输出），在用户提出需求后才重新设计为两阶段架构（export → merge） 差异分析: 用户主动考虑了跨设备的工作流现实，AI 默认从单机视角设计。用户对自己工作模式的理解比 AI 更准确\n可视化视频问题的诊断方向（ErrorRecoveryBenchmark） 角色 思路 人类 人类直觉地提问：「为什么在这个时间点 inject？这个 error 并不会带来任何影响」——引导 AI 思考注入时机和因果关系 AI AI 最初误解为「关键帧丢失」问题（collect_rollout_stats 不返回帧），计划去掉重验证逻辑 差异分析: 人类关注的是「为什么这个时刻注入」（触发条件/因果链），AI 关注的是「渲染帧在哪里」（技术实现）。人类的视角更系统，直接指向了更深层的问题\nRandom Thoughts系统的核心设计理念（CalendarPro） 角色 思路 人类 用户明确提出将空闲时段（吃饭、睡觉）作为自动整理触发器，复用EssentialScheduler的DEFAULT_MEALS和DEFAULT_SLEEP作为单一数据源，并要求输出大/中/小目标层级结构 AI AI负责具体实现：JSONL存储格式选择、平衡括号计数算法、asyncio任务管理模式、mock测试策略 差异分析: 用户提供了完整的产品思维（何时触发、如何呈现、数据复用原则），AI专注于技术实现细节\nbenchmark 调度的可恢复性设计（MIHD） 角色 思路 人类 人类提出「通过检查 embeddings.npz 文件是否存在」来判断实验是否完成，这是利用已有 skip_cached 机制的关键洞察 AI AI 在实现上添加了两层检查（调度器层 + 底层 run_benchmark.py 的 skip_cached），以避免不必要的模型加载开销 差异分析: 人类提供了核心设计原则（幂等性检查），AI 负责具体实现细节\n代码整理计划的来源（CalendarPro） 角色 思路 人类 人类预先在 PLANNING.md 中完成了深入的代码审查，识别出 8 处重复 AI 提供商逻辑、3 处重复时间解析、2 处重复关键词等，并制定了详细的操作计划（包含具体文件路径和行号） AI AI 严格按照人类计划逐步执行，只在执行遇到障碍时（如 nul 文件、Unicode 文件名）自行找到解决方案 差异分析: 战略规划完全来自人类，AI 专注于战术执行。人类对代码库的理解已经达到足以写出精确重构计划的深度，AI 在此起代码修改自动化的作用\n循环事件只修改特定实例的需求（CalendarPro） 角色 思路 人类 用户提出业务需求：修改循环事件时只改某一个实例 AI 分析Google Calendar API的recurringEventId机制，通过移除recurrence字段实现单实例修改 差异分析: 用户提出了AI没有主动想到的边界场景。AI专注于错误处理修复，未考虑循环事件的特殊性\nP0修复的优先级排序（CalendarPro） 角色 思路 人类 用户在计划文档中已明确分级P0（稳定核心）和P1（提升智能），以及每个任务的具体修改文件和代码示例 AI AI按照计划顺序实现，但在JSON提取时主动发现了贪婪正则的边界问题并改进算法 差异分析: 计划由用户预先设计，AI在实现过程中发现并主动修复了计划中未预见的边界问题\nCalendarPro 功能实现计划执行（CalendarPro 定时自检） 角色 思路 人类 人类预先设计了完整的架构计划，包括 3 个新文件的接口规范、5 个修改文件的具体位置、核心逻辑（防重复、时间过期跳过、回调通知），以及与现有 ThoughtOrganizer 模式对齐的指导 AI AI 按照计划逐步实现，先探索现有代码库获取具体接口细节，再按依赖顺序创建文件，最后编写测试并修复问题 差异分析: 人类提供了高层设计决策和架构约束，AI 负责将其转化为具体可运行的代码\nQ-Former 逐 spot vs batch 化实现的权衡（MIHD） 角色 思路 人类 人类主动决定先用「逐 spot 循环」的简单实现，明确后续可优化为 batch 化版本 AI AI 按决策实现了逐 spot 版本 差异分析: 人类做出了「先快速实现，后优化」的工程决策；AI 只是执行\n实现层面 主动要求看可视化效果（ErrorRecoveryBenchmark） 角色 思路 人类 冒烟测试通过后用户主动询问可视化在哪里，驱动了演示视频的生成 AI AI完成冒烟测试后未主动展示可视化，专注于技术指标（SR/SPL/RP）而非直观效果 差异分析: 人类更关注具体可见的效果，AI倾向于汇报抽象指标。用户纠正了AI的优先级判断\n拒绝ExitPlanMode（多次）（ErrorRecoveryBenchmark + CalendarPro） 角色 思路 人类 用户多次拒绝AI的ExitPlanMode请求并说\u0026rsquo;Continue/Please continue\u0026rsquo;，表明在执行性任务中倾向直接执行而非反复确认 AI AI习惯性地进入规划模式并寻求审批，增加了不必要的审批环节 差异分析: 用户通过行动纠正了AI的行为模式：对范围明确的任务应直接执行并汇报结果，而非先规划再确认\nAI 局限性 重要局限 AI 未主动验证 subprocess 的 cwd 是否正确，直到人类要求继续 benchmarking 时才发现 58 个实验都没有正确保存输出文件——这是一个重大的前期疏漏（MIHD） 缺乏对测量数据的物理合理性验证。AI 测量到 EEF-物体距离 0.22m 后没有质疑这个数值是否符合 pick-and-place 任务的物理预期，而是直接在此基础上调整参数，浪费了大量时间（ErrorRecoveryBenchmark） 在不理解根因的情况下，AI选择放宽参数阈值（distance_threshold 0.15→0.30，confidence 0.5→0.2）来让测试通过，而不是深入调查为什么机械臂不靠近物体——这是一种局部优化掩盖全局问题的典型AI行为模式（ErrorRecoveryBenchmark，天河） 尽管INJECTOR_REGISTRY已存在，rollout_generator.py仍然硬编码了ImpulseInjector。AI在新增注入器时没有检查所有调用点，导致注册表形同虚设直到被人工发现（ErrorRecoveryBenchmark） 多处集成bug（命名不一致、参数不匹配、缺少clear()调用）未被41个单元测试覆盖，暴露了测试套件对跨模块集成路径覆盖不足的问题（ErrorRecoveryBenchmark） AI 在首次 dry_run 验证后就启动了实际运行，没有做单实验端到端测试（run 1 experiment + 确认 npz 文件存在）就直接批量启动，导致 ~58 个实验的计算资源浪费（MIHD） 在调整 EEF 门控阈值时，AI 没有主动追问「为什么物体在回放中完全不动」这个更根本的问题，而是接受症状并在错误前提下优化参数（ErrorRecoveryBenchmark） 面对数据（linvel=0.445 m/s）与观察（面包没动）的矛盾时，AI倾向于先相信数据并尝试合理化解释，而不是立刻质疑数据来源的正确性（ErrorRecoveryBenchmark） 多次生成 0 个场景时，AI 倾向于调参而非系统性地诊断。诊断脚本是在多次失败后才写的（ErrorRecoveryBenchmark） 理解用户问题时存在表层化倾向：用户问「为什么在这个时间点注入」时，AI 首先理解为技术实现问题（帧丢失），而非系统设计问题（因果链缺失/demo 回放缺失）（ErrorRecoveryBenchmark） 计划执行时缺乏对 collect_rollout_stats 副作用的预判：实现 Phase 3 时没有考虑到 collect_rollout_stats 会消耗掉关键的 20 步仿真而不返回帧（ErrorRecoveryBenchmark） 未主动想到循环事件修改的边界场景，需要用户提醒才去处理。在修复update_event时只关注了错误传播，忽略了Google Calendar API对循环事件实例的特殊处理逻辑（CalendarPro） JSON提取的初版实现使用贪婪正则，在多JSON块测试用例中失败，需要人类测试发现后才改为平衡括号计数算法（CalendarPro） 一般局限 AI 在编写 joint_sections 代码时错误引用了不存在的 utils.clustering_utils 模块，需要后来修正（MIHD） ssh 命令中 cd 的处理反复出错（连续约 10 次以上相同错误），需要绕过使用 shell 脚本或绝对路径才解决。AI 在重复遇到同一错误时应该更早转换策略（ErrorRecoveryBenchmark） Mock路径选择错误（动态import场景下patch位置判断有误），需要运行测试后看报错才能发现并修正（CalendarPro） conda run 的 stdout 缓冲问题导致 AI 多次尝试不同方式查看进度，浪费了多轮交互（MIHD） 对集群 SSH 环境的工作目录行为理解不牢固：SSH 登录后默认 HOME 目录，需要显式 cd，但 AI 反复忘记这一点（ErrorRecoveryBenchmark） 在大型文件中使用复杂正则模式的 Grep 工具多次超时，需要降级为 bash grep + 行号定位的方式来绕过限制（CalendarPro） 当 Background Agent 的输出文件为空时，AI 使用了 offset=1 参数导致读取失败，对 Read 工具的边界条件（offset 是 1-indexed 行号）理解不够准确（CalendarPro） AI 在执行文件删除时，对 Windows 特殊设备名 nul 和 Unicode 私用区字符文件名的行为预判不足，需要通过试错才发现正确处理方式（CalendarPro） 未主动提出视频标注的UX设计方案（三阶段横幅+信息框），该洞见完全来自用户，说明AI对可视化沟通设计的主动性不足（ErrorRecoveryBenchmark） 多次触发ExitPlanMode流程请求审批，在用户已表明倾向直接执行的情况下仍重复相同模式，缺乏对用户行为偏好的适应性（ErrorRecoveryBenchmark + CalendarPro） 冒烟测试通过后未主动提议生成可视化结果，需要用户明确询问才采取行动，缺乏展示成果的主动意识（ErrorRecoveryBenchmark） 今日收获 核心收获 MuJoCo cvel 布局是 [angular(3), linear(3)]，不是 [linear, angular]。代码注释与实际行为不一致时以官方文档为准。这是一个容易犯的错误，影响所有基于速度的物理计算 robosuite demo 回放必须使用与录制完全相同的环境配置（env_name、controller_configs、robots 等），否则相同 action 会产生完全不同的轨迹。应从 HDF5 metadata 读取并复现原始 env_args 机器人demo回放的关键原则：回放环境必须从dataset元数据（HDF5 env_args）加载完整配置，包括controller_configs、robot参数等，任何配置差异都会导致轨迹完全不同 在 HPC 环境中验证 subprocess 输出时，必须同时检查：(1) exit code (2) 实际输出文件是否存在于预期路径 (3) 文件内容是否有效——三者缺一不可，进度记录 success 不等于文件正确保存 调试复杂系统时应先验证数据质量（物体是否真的在运动？EEF 是否真的在接近？），再调整算法参数。用诊断脚本打印关键状态变量比反复调参更高效 集群可视化 pipeline 中，「恢复快照+注入」和「回放 demo 到触发点+注入」是完全不同的事情。前者只能展示注入的瞬间效果，无法展示触发条件如何满足；后者才能展示完整因果链 端到端集成冒烟测试比单元测试更能暴露跨模块的命名/参数不一致问题；新注入器注册后应立即运行集成测试 使用注册表模式时，新增实现类后必须立即检查所有调用点是否已切换为动态加载，硬编码具体类是注册表模式的反模式 Google Calendar API中，search_events使用singleEvents=True会把循环事件展开为独立实例，每个实例有格式为base_id_YYYYMMDDTHHMMSSZ的唯一ID。用instance ID调用update/delete时，API自动只操作该实例（创建exception），不影响整个系列 语义路由和LLM双重验证的正确策略：两者一致时用SR（快，已验证）；两者不一致时用LLM（有上下文理解）。SR只适合高置信度的关键词匹配，不适合处理含歧义的自然语言 批量实验前应先做单实验端到端验证（运行1个实验 + 确认所有预期产物存在），再扩展到全量，节省 HPC 资源 在robot manipulation benchmark的scene生成pipeline中，detector应区分「因」（机械臂动作）和「果」（物体状态变化），目前instability detector只看物体结果状态，可能导致在机械臂未接触时误触发 有状态注入器必须在所有退出路径清除状态（建议用finally块或上下文管理器），set_sim_state_flat()不会自动撤销物理属性修改 可视化和具体演示是里程碑的标准交付物，冒烟测试通过后应主动生成并展示可视化结果而非仅汇报数值指标 调试策略：当观测到「proximity detector从不触发」时，应首先检查EEF与物体的实际距离分布，而不是调整阈值。通过日志记录最近距离即可快速定位是轨迹本身有问题 PLANNING.md 作为「项目内置路线图」的模式：人类在文件中预先完成代码审查、识别问题、制定计划，AI 按计划执行。这种人机分工比让 AI 自主规划更可靠，因为人类对业务语义的理解更准确 错误处理架构：底层服务层（calendar_service）应raise具体异常而非返回None，上层（discord_bot、api_routes）负责catch并给用户展示人性化错误信息。静默失败会让调试极其困难 平衡括号计数是从混合文本中提取JSON的正确方法，贪婪正则无法处理多JSON块或嵌套结构 asyncio后台任务应始终添加done_callback处理异常，避免task静默死亡导致系统状态不一致 可视化benchmark中，validation rollout会「吞掉」关键帧（内部step但不渲染），导致错误效果不可见。解决方案是将验证逻辑与渲染逻辑完全分离，全程捕获post-error阶段的每一帧 错误注入器移植模式：从mimicgen的BaseError接口适配到本框架BaseInjector接口，核心是将sim.model/sim.data替换为env.get_mj_model()/env.get_mj_data() 「提取共享模块」重构的正确顺序：先创建新文件，再更新 import，最后删除旧的重复代码。每步之后立即跑 import 验证，可快速发现问题 Python mock时需要patch被测代码实际使用的命名空间（动态import在方法内部时，patch路径需指向该方法所在模块），而非原始定义位置 后台异步循环的「每日首次」检测模式（记录 last_checked_date 并与当日日期比较）是一种简洁有效的防重复触发机制，适用于需要每日一次触发但不依赖定时任务调度器的场景 JSONL 格式结合 dataclass + asdict 是 Python 中轻量级持久化存储的良好组合：无需数据库依赖，支持增量追加写入，且便于原子性替换（先写临时文件再 rename） 实践收获 conda activate 在非交互式 subshell 中不可用；应直接使用 conda env 的绝对 Python 路径，并在 subprocess.run 中设置 cwd 大型函数（如 run_evaluation）的参数应通过显式 dict 传递，避免依赖外层 scope 变量（如 args 对象） 验证器全部实现后（4个：drop/tip_over/large_offset/stuck），错误类型覆盖率从 67% 提升至 100%，为后续多类型场景生成扫清了障碍 proximity detector 的有效触发距离 = distance_threshold × (1 - confidence_threshold)，调整一个参数时要同步考虑另一个，否则有效范围与预期差距很大 在 Windows + Git Bash 环境下操作文件：shell 工具（ls/rm）对含 Unicode 私用区字符的文件名无效，应改用 Python os 模块操作；conda activate 不可用时应直接使用绝对路径调用 conda env 的 Python 需要外部资源（模型下载、Claude CLI、浏览器）的测试应与纯逻辑测试分组，避免 pytest 挂起。使用 \u0026ndash;ignore 标志或 pytest marks 是合适的处理方式 Windows上conda环境的最佳实践：PowerShell原生支持conda，cmd需要额外初始化。脚本应检测并优先使用PowerShell；关闭Terminal窗口不保证发送SIGINT，必须显式管理PID文件实现进程清理 ssh \u0026ldquo;multi \u0026amp;\u0026amp; cmd\u0026rdquo; 中 cd 不会持久，需用 bash -c \u0026lsquo;cd dir \u0026amp;\u0026amp; cmd\u0026rsquo; 或写 shell 脚本内嵌 cd，或直接用绝对路径 边界值单元测试应使用略高于/低于阈值的值，避免浮点精度导致的不稳定测试 视频标注中叠加状态信息（检测条件、注入的错误类型、当前阶段）能大幅提升调试效率和成果沟通质量，应纳入标准可视化工具链 对范围明确的执行性任务应直接执行并汇报结果，减少不必要的规划确认环节，用户工作流效率更高 会话摘要 MIHD 🔄 实现可恢复的 286 实验 Benchmark 调度系统 19:04:03.641 | claude_code 用户提供了完整的 MIHD benchmarking 计划，要求实现支持断点续跑的调度脚本。AI 创建了 run_all_benchmarks.py（支持 A-F 六类实验、环境过滤、组别过滤、进度追踪）、summarize_benchmark.py（结果汇总可视化）并更新了 benchmark_config.yaml 添加六个实验组定义。Dry run 验证显示 286 个实验矩阵正确，随后成功启动 General 环境的 209 个实验并监控运行。\n🔄 修复 subprocess cwd bug 并重启 benchmark + 实现增强 Phase（第二次） 06:41:43.000 | claude_code 发现之前 58 个实验虽然进度记录为成功但只有 15 个 npz 文件，根因是子进程 cwd 错误导致输出路径解析失败。修复调度器使用绝对路径和 cwd 参数，重新启动 207 个实验并确认文件正确保存。同时再次实现了 MIHD 7 Phase 增强，并修改 experiment_logger.py 支持日志文件自动更新时间戳。\n✅ 实现 MIHD 融合管道 7 大增强 Phase 17:39:21.063 | claude_code 用户提供了 MIHD Enhancement Plan，要求按 Phase 1-7 顺序实现归一化接入、Q-Former/LLaVA MLP 新融合策略、Niche 查询、批次校正等功能。AI 完成了所有 7 个 Phase：修改 config.yaml/config_manager.py，新建 QFormerFusion.py/NicheEncoder.py/BatchCorrection.py/niche_utils.py 等 5 个新文件，所有语法和功能测试通过。\nGadgets 🔄 实现 AI 对话日报工具：两阶段多设备架构设计 22:08:03.355 | claude_code 用户要求实现一个能读取 Claude Code/ChatGPT/通用 JSON 对话并生成日报的工具。AI 完成了初版单机实现后，用户指出需要在多台设备上分别导出再汇总。AI 通过询问确认了「手动拷贝 log + export 阶段可总结」的需求，开始设计两阶段架构并准备探索 Hugo 部署集成。最终因计划审批被用户拒绝而暂停，核心代码已完成，架构设计方向明确但未合并实现。\n🔄 生成 Gadgets 仓库的 CLAUDE.md 20:57:21.853 | claude_code 用户运行 /init 命令，AI 分析了 gadgets 仓库的目录结构、各子工具用途及关键依赖，生成了 CLAUDE.md。因仓库不在 git 目录下，提交失败，用户未进一步操作。\nErrorRecoveryBenchmark 🔄 初始化静置 + EEF 近距离门控实现与根因排查 23:00:21.128 | claude_code AI 实现了 instability.min_step=100 和 EEF 距离门控，但发现物体在整个 demo 回放中完全不动（EEF 与物体最近距离始终 ≥0.22m）。用户质疑「pick-and-place 肯定会碰到物品，22cm 不合理」，触发深入排查。最终发现根本原因是 demo 录制环境（PickPlace_D0 + OSC_POSE controller_configs）与回放环境（普通 PickPlace，无 controller_configs）不匹配，导致轨迹完全偏离。计划提交后用户再次拒绝审批，当日工作暂停。\n✅ GPU节点端到端冒烟测试：修复多处集成bug并验证通过 20:00:00.000 | claude_code 用户要求实现S5.3（GPU节点rollout）。AI发现并修复了rollout_generator.py中的硬编码注入器（改用注册表）、跨模块命名不一致（friction_scale→friction）、参数名不匹配以及缺少injector.clear()的状态污染问题，同时创建了GPU runner脚本。在an49 GPU节点首次运行失败（success字段布尔转换bug和MetricsWithCI格式化bug），修复后重跑，冒烟测试4步全部通过（生成2个场景、数据库分析、RandomPolicy数据采集、结果分析报告）。\n✅ 修复demo回放环境不匹配 + 恢复proximity阈值 + 重新生成30个场景（天河） 23:49:10.623 | claude_code 人类通过根因分析确认上一轮场景生成失败的根本原因：回放环境缺少controller_configs（OSC_POSE），导致机械臂轨迹与demo完全不同，proximity detector永远无法触发。修复方案是从HDF5 dataset元数据加载完整env_args，同步修复generate_scenes和visualize_scene脚本，恢复合理的proximity阈值（0.15m, confidence=0.5, EEF门控5cm）。修复后在an49 GPU节点成功生成30个有效场景（10个demo各3个），触发步骤从step15修正为step52-121，并生成两个可视化视频验证效果。\n✅ 修复 MuJoCo cvel 速度读反 bug 并重新生成场景 22:37:16.748 | claude_code AI 按计划修复了 env_wrapper.py 中 cvel[:3]/[3:] 读反的 bug，清理旧场景数据并在 an49 节点重新生成。修复后 instability detector 不再在 step 11 误触发，检测类型从 high_linvel 变为 high_angvel，验证符合预期。随后用户提出三个新需求：场景与 human demo 一致性、初始化静置等待、EEF 近距离门控。\n🔄 修复可视化脚本：实现三阶段demo回放展示因果链，并发现cvel读反关键bug 21:58:20.921 | claude_code 用户提供了重构2_visualize_scene.py的详细计划，AI实现了三阶段demo回放（Phase 1回放demo到trigger_step、Phase 2注入错误、Phase 3全程渲染）并成功验证（132帧，tip_over=1.60 rad）。随后用户提出增加detector触发原因的log输出需求，讨论中用户观察到机械臂距面包很远时detector就触发了，最终发现env_wrapper.py中MuJoCo cvel数组的线速度/角速度读反的关键bug，决定修复bug后重新生成所有scenes。\n✅ 代码库整理重构：删除死代码、移植注入器、添加中文注释 18:30:00.000 | claude_code 用户要求识别有用/无用/可复用代码。AI通过3个并行Agent发现死代码、存根代码和4610行高价值可复用代码，向用户询问两个关键决策（移植mimicgen错误类型、删除冗余文档）。执行5个阶段：删除12个冗余.md、确认3个新注入器已存在、为11+模块添加中文docstring、移动misplaced文件。全部41个测试通过，版本升至4.1.0。\n✅ 编写项目全景总结文档（395行中文综合文档） 18:05:16.582 | claude_code 用户要求实现编写项目全景总结.md的计划。AI调用探索Agent读取全部15+个Markdown文件并收集定量数据，生成395行综合文档，内容涵盖三层目标体系、含差距分析表的诚实进度评估（场景数1.5%/注入器25%/策略33%等）、P1/P2/P3优先级路线图。文档验证通过（wc -l=395，在300-500行目标范围内）。\n✅ 代码库整理：删除死代码、移植注入器、清理冗余文档（天河） 18:30:39.228 | claude_code 按照人类预先制定的5阶段清理计划执行：删除fingerprint.py中~190行从未使用的ReplaySystem/compute_drift_metric，修复proximity.py中重复的detect()方法（含bug），删除12个过时.md文件，从mimicgen_workspace移植并适配FrictionInjector、PosePerturbInjector、GripperBiasInjector三个新注入器，更新框架导出和版本号至4.1.0。所有阶段均完成，代码库结构更清晰。\n🔄 生成CLAUDE.md并分析项目全景计划（天河） 17:31:49.623 | claude_code 执行/init命令重写CLAUDE.md，将615行冗余文档压缩为101行精简版，保留服务器约束、Makefile命令、插件架构要点。随后用户要求整理项目计划成中文文档，AI通过三个并行子Agent探索了15+个md文件，完成了诊断分析（核心框架6800行已完成，但30个场景vs目标200+，1个注入器vs目标4个），计划文档设计阶段完成但用户拒绝了ExitPlanMode调用，文档未写入。\n🔄 场景可视化生成与视频标注需求提出（功能待实现） 22:00:00.000 | claude_code 用户询问可视化效果，AI在an49生成了83KB 105帧的演示视频（tip_over错误，tip_angle=1.60 rad，验证通过）。用户进一步要求在视频上添加三阶段标注（NORMAL STATE→ERROR INJECTED→POST-ERROR）并显示检测触发信息。AI规划了实现方案，但ExitPlanMode再次被拒，视频标注功能尚未实现。\nTianHe ✅ 天河集群登录操作 17:30:59.129 | claude_code 用户在天河集群执行了 /login 命令，登录成功。无其他操作记录。\nError Recovery Benchmark 🔄 可视化脚本标注增强、smoke test 运行及因果链问题发现 21:38:31.627 | claude_code 实现了可视化脚本的三阶段视频标注功能并修复了注入器 hardcode bug，在 an49 上成功运行生成了带标注的视频。但用户指出三阶段画面几乎一样，AI 先误解为帧丢失问题，后在用户引导下认识到真正问题：可视化缺少 demo actions 回放，无法展示 detector 触发条件满足的过程，因果链完全断裂。此问题的修复方案尚未实施。\n✅ 更新项目全景总结文档至 v4.1 并实现两个新验证器 19:12:32.829 | claude_code 用户提供了详细的 v4.1 状态快照和全目标层次计划，要求更新项目文档并实现 S5.1（LargeOffsetValidator）和 S5.2（StuckValidator）。AI 先核验了代码库实际数字（29文件/7328行/4注入器），完成了文档的10处更新，再实现了两个验证器及其 16 个单元测试，全部 41 个测试通过。最终 VALIDATOR_REGISTRY 包含 4 个验证器，所有文档同步更新。\nCalendarPro ✅ 修复Conda环境检测、服务停止、意图分类和日历错误报告 18:08:34.455 | claude_code 用户报告start.bat无法检测conda环境以及停止服务无效两个问题。AI诊断后创建了start.ps1（PowerShell原生支持conda）、stop.ps1（PID文件管理）和改造start.bat为桥接脚本。同时修复了三个更深层的Bug：dual_verify.py中SR/LLM分歧时错误地用SR结果；calendar_service.py所有HttpError被吞掉用户看不到错误；循环事件更新时可能影响整个系列而非单实例。所有修复在一次commit中完成。\n✅ 实现P0稳定核心修复与P1智能提升（9个任务全部完成） 19:04:15.684 | claude_code 用户提供完整的计划文档，AI顺序实现9个任务：JSON解析三级容错（含平衡括号算法修复）、三个AI提供商30秒超时、消息队列done_callback稳定性、服务层错误处理加固、56个意图分类边界测试、学习数据自动采集与回填、能量投影个性化混合算法、智能排程整合学习数据、会话上下文格式化。最终107/110测试通过（3个预存在的browser测试失败），lint全部修复。\n✅ 实现Random Thoughts碎片想法收集与空闲自动整理系统 20:43:47.853 | claude_code 用户要求新增随机想法捕获功能：快速记录碎片想法，利用吃饭/睡觉空闲时段自动AI整理为大/中/小目标层级文档。AI新建4个文件（ThoughtStore、IdleDetector、ThoughtOrganizer、init）、修改5个现有文件（models、config、prompts、intent_routes、discord_bot），创建25个测试用例全部通过，lint干净。中途遭遇mock路径错误和pytest-asyncio未安装两个问题，均快速解决。\n✅ 实现定时自检与循环日程自动安排功能（后台 asyncio + JSONL 存储 + Discord Bot 集成） 22:18:34.787 | claude_code 用户提供了详细的架构计划，要求为 CalendarPro 实现后台定时检查器，自动安排餐食/睡眠等必要日程，并支持用户自定义循环任务（如每天下午3点刷 LeetCode）。AI 先探索了 ThoughtOrganizer 和 Scheduler 的现有模式，然后创建了 RecurringTaskStore（JSONL 持久化）和 PeriodicChecker（asyncio 后台循环），并完成了对 5 个现有文件的集成修改。最终 18 个单元测试全部通过，lint 检查通过。\n✅ CalendarPro 代码库重构整理（删除冗余、合并测试、提取共享模块） 18:23:24.871 | claude_code 用户提交了详细的重构计划，AI 系统性执行了代码库整理：删除 ai_planner/ 和 4 个废弃测试文件，将 Claude 测试文件合并为 test_claude_provider.py，创建 time_utils.py 消除 3 处重复时间解析，将 FIXED/FLEXIBLE 关键词提升为模块级常量，创建 provider_selector.py 消除 8 处重复 AI 提供商选择逻辑，统一餐食时间到 DEFAULT_MEALS 单一来源。执行过程中遇到 Windows nul 设备名无法删除、Unicode 私用区文件名和 conda 环境调用等平台问题，均找到替代方案。最终 26/26 核心测试通过，新建文件通过 ruff lint。\n🔄 P0 Bug 修复与 P1 新功能规划（进行中）（CalendarPro） 18:23:24.871 | claude_code 用户要求按 PLANNING.md 实现 P0 稳定性修复和 P1 新功能。AI 进入 Plan Mode，启动 3 个并行探索 Agent 分析 JSON 解析容错、学习服务/能量投影、测试模式三个维度，发现了 27 个高风险问题点。完成了包含 7 个具体任务的计划文件。用户拒绝了 ExitPlanMode，会话结束时实现工作尚未开始。\n✅ 创建CalendarPro项目规划文档docs/PLANNING.md 18:08:34.455 | claude_code 用户要求为CalendarPro(Life Copilot)创建一份中文项目规划文档。AI通过并行启动多个探索Agent深度分析了CalendarPro（48+源文件）和OpenClaw生态，最终创建了330+行的PLANNING.md，包含项目定位、核心理念、模块成熟度评级（★1-5）、三层目标体系、6个架构决策理由、P0-P3行动路线图和5个维度的已知差距分析。\n✅ 初始化CLAUDE.md文档，完善项目架构说明（CalendarPro） 17:56:31.853 | claude_code 用户通过/init命令触发CLAUDE.md生成。AI深度探索代码库后，识别出原文档未记录的语义路由系统、授权系统、消息队列、对话历史持久化、优化确认服务等关键系统，并删除了冗余的重复内容，形成更精炼的项目指导文档。\n✅ 通过 /init 命令更新 CLAUDE.md 项目文档（CalendarPro） 18:23:24.871 | claude_code 用户执行 /init 命令，AI 读取现有 CLAUDE.md、README.md 和项目结构，重写了项目指导文件。新增了共享工具模块说明（time_utils、provider_selector）、三层排程系统架构和测试外部依赖注意事项，删除了可自行发现的冗余内容，整体从 188 行压缩到 140 行。\nToken 用量 总览 指标 数值 总 Token 101,386,135 输入 Token 65,053 输出 Token 68,378 Cache 创建 5,977,600 Cache 读取 95,275,104 Cache 命中率 94.1% 总费用 (USD) $72.2753 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 9,992 66,699 4,042,071 79,408,980 $66.6849 92.3% claude-haiku-4-5-20251001 54,659 1,036 1,538,261 13,263,291 $3.3090 4.6% claude-sonnet-4-5-20250929 402 643 397,268 2,602,833 $2.2815 3.2% 各设备用量 设备 总 Token 输入 输出 费用 DCC 41,018,007 38,866 26,730 $31.2547 MacBook 8,020,979 1,356 3,374 $4.7980 TzJsDesktop 52,347,149 24,831 38,274 $36.2226 ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-13/","summary":"在多台设备和多个项目上高强度推进：DCC HPC集群上实现MIHD多模态空间转录组286实验可恢复调度系统并修复subprocess cwd关键bug；天河集群上修复MuJoCo cvel速度读反bug、完成机器人错误恢复基准测试代码库重构（v4.0→v4.1）、实现GPU端到端冒烟测试并成功生成30个有效错误场景；MacBook上从零实现AI对话日报两阶段多设备工具；Windows上完成CalendarPro代码库重构整理、修复多个关键Bug并实现P0稳定性修复、P1智能提升及Random Thoughts碎片想法系统。","title":"Bug Journal 2026-02-13"},{"content":"日报 — 2026-02-12 在DCC集群上为MIHD空间转录组多模态融合框架制定并开始实施大规模增强计划，涵盖嵌入归一化、Q-Former/LLaVA MLP融合、QueST风格的niche查询与批次效应校正、以及全局超参数配置化。\n今日任务 架构与策略 ✅ MIHD增强计划制定与文档化 — 基于6个研究想法，设计了包含7个实施阶段的详细计划（Big Aim→Middle Aim→Small Aim→Tiny Aim层级），并写入docs/ENHANCEMENT_PLAN.md ✅ 创建utils/normalization.py — 实现了L2归一化、StandardScaler归一化、dispatcher函数以及PyTorch等价函数，支持三点归一化（编码器后、融合前、融合后） 🔄 Q-Former + LLaVA MLP融合实现 — 计划创建models/QFormerFusion.py，实现BLIP-2风格Q-Former和LLaVA 2层MLP连接器两种融合策略 🔄 扩展config.yaml — 开始向config.yaml添加encoder/fusion超参数配置节，实施Phase 1 Config Foundation 🔄 QueST niche查询与批次校正实现 — 计划创建utils/niche_utils.py、models/NicheEncoder.py、models/BatchCorrection.py，将QueST的niche查询和对抗式批次校正迁移到MIHD 实现与修复 ✅ 更新CLAUDE.md — 将CLAUDE.md从579行精简至188行（减少67%），消除冗余，补充缺失的GCN编码器、basic_contrastive融合、utils模块说明、测试运行说明等，并添加ASCII管道流程图 问题与解决方案 关键问题 1. MIHD当前嵌入归一化不一致：UNI2/scGPT/PCA已L2归一化，但HIPT/ResNet50/MLP/GCN未归一化，且融合前后均无统一归一化 解决方案: 设计三点归一化方案（post-encoder、pre-fusion、post-fusion），通过utils/normalization.py统一管理，L2归一化对已归一化向量幂等所以安全\n关键洞察: 归一化不一致是下游聚类质量下降的根本原因之一；L2归一化的幂等性使其可无副作用地应用于所有编码器输出\n2. 现有config.yaml和config_manager.py缺乏对大量超参数的支持，导致代码中存在大量硬编码值 解决方案: 设计完整的配置化方案，将normalization、Q-Former、LLaVA MLP、批次校正、niche查询的所有超参数纳入config.yaml和对应dataclass\n关键洞察: 配置化是超参数搜索的前提；必须先建立config foundation才能进行其他所有改进\n3. LLaVA名称歧义：用户说\u0026rsquo;Q-Former in LLaVA\u0026rsquo;，但实际上LLaVA使用的是简单2层MLP，Q-Former来自BLIP-2 解决方案: AI澄清了两种架构的区别，并向用户确认是否两者都实现；用户选择两者都实现\n关键洞察: 在实现前澄清架构名称歧义，避免了错误实现；LLaVA MLP比Q-Former更轻量适合作为基线对比\n一般问题 4. 原CLAUDE.md存在大量冗余（smart runner命令出现两次、squidpy警告出现3次、组件列表重复），且缺少关键信息 解决方案: 重写CLAUDE.md，精简67%同时补充缺失内容（GCN编码器、测试运行指令、utils模块说明等）\n关键洞察: 文档冗余会降低AI助手的工作效率；精简到关键信息同时保留架构概述最有价值\n人类思路 vs AI 思路 战略层面 6个研究想法的提出 角色 思路 人类 人类从实际研究需求出发，提出了嵌入归一化、Q-Former融合、QueST应用、批次效应校正、全局超参数配置化共6个互相关联的改进方向，并指定了QueST代码库路径 AI AI分析了依赖关系，确定了实施顺序（Config→Norm→UNI2实验→Q-Former→Niche→批次校正→完整配置），并发现了技术细节（如LLaVA实际使用MLP而非Q-Former） 差异分析: 人类提供高层次研究方向和直觉，AI负责将其转化为具体可执行的技术任务层级；AI主动澄清了\u0026rsquo;LLaVA中的Q-Former\u0026rsquo;这一名称误用\nQueST功能的使用方式 角色 思路 人类 人类想了解QueST如何处理疾病数据和批次效应，并希望在MIHD中实现类似功能 AI AI探索了QueST代码库（BatchDiscriminator、GINLayers、TLS查询逻辑），并设计了将这些功能移植到MIHD的具体方案 差异分析: 人类关注\u0026rsquo;能否做到\u0026rsquo;，AI负责\u0026rsquo;如何做到\u0026rsquo;的技术细节；AI发现QueST使用对抗训练方式进行批次校正\n实现层面 实施顺序决策 角色 思路 人类 人类在AI开始执行后主动打断，要求先写计划文件再执行 AI AI初始倾向于直接开始实施（创建了task列表并开始读取文件），未先产出可审查的计划文档 差异分析: 人类的工作流偏好是先文档化计划，这是一个AI未能预判的工作习惯；被打断后AI立即调整策略\nAI 局限性 一般局限 AI在未问清用户工作流偏好的情况下，直接开始创建任务列表和读取文件，而未先输出可供审查的计划文档，导致被用户打断 AI将\u0026rsquo;LLaVA中的Q-Former\u0026rsquo;作为需要主动澄清的歧义，而非直接按字面意思实现，这是正确的；但需要依赖用户提问环节而非自主判断最合理方案 今日收获 核心收获 空间转录组多模态融合中，嵌入归一化不一致是常见但容易忽视的问题；L2归一化的幂等性使其成为安全的默认选择，可统一应用于所有编码器输出而不损害已归一化的向量 Q-Former（BLIP-2）vs LLaVA MLP连接器是两种不同的跨模态对齐机制：Q-Former使用可学习查询向量+交叉注意力，LLaVA使用更简单的2层MLP投影；两者都值得作为融合策略尝试 QueST使用对抗式批次判别器（BatchDiscriminator + GINLayers）进行批次效应校正，通过最大化批次不可区分性来学习批次无关表示；这种方法可以作为独立的后处理模块插入到任意融合管道中 大型代码改进项目中，配置化必须先于功能实现；建立统一的配置体系（config.yaml + dataclass）是后续所有超参数搜索和实验对比的基础 实践收获 在AI辅助开发工作流中，\u0026lsquo;先写计划文档再执行\u0026rsquo;是一个重要的检查点，允许人类在大量代码生成前审查和调整方向，特别适合多阶段、多文件的大型重构任务 会话摘要 🔄 为MIHD生成CLAUDE.md并规划6大增强方向 17:12:36.312 | claude_code 用户通过/init命令触发CLAUDE.md生成，AI探索了整个MIHD代码库和QueST代码库，并对Q-Former架构进行了研究。用户随后提出6个互相关联的改进想法（嵌入归一化、UNI2+scGPT实验、Q-Former融合、QueST niche查询、批次效应校正、全局超参数配置化），AI通过澄清问题确认了实现范围（两种融合策略均实现、QueST功能移植到MIHD、完整管道配置化）。最终AI生成了详细的7阶段实施计划并写入计划文件，但用户在AI尝试退出计划模式时拒绝，会话以计划审查中状态结束。\n🔄 开始实施MIHD增强计划：config基础层和归一化工具 17:34:50.362 | claude_code AI开始按照计划实施：读取了config.yaml、config_manager.py、Fusion.py、run_benchmark.py等关键文件，读取了QueST的layers.py和model.py以了解BatchDiscriminator和GINLayers实现。成功创建了utils/normalization.py（包含L2归一化、StandardScaler归一化、dispatcher函数），并开始修改config.yaml添加超参数节，但在编辑config.yaml时被用户打断。整体实施进度约完成Phase 1-2的30%。\nToken 用量 总览 指标 数值 总 Token 5,459,892 输入 Token 45,436 输出 Token 1,231 Cache 创建 543,037 Cache 读取 4,870,188 Cache 命中率 90.0% 总费用 (USD) $4.1489 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 66 1,150 311,998 3,370,495 $3.6643 88.3% claude-haiku-4-5-20251001 45,370 81 231,039 1,499,693 $0.4845 11.7% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-12/","summary":"在DCC集群上为MIHD空间转录组多模态融合框架制定并开始实施大规模增强计划，涵盖嵌入归一化、Q-Former/LLaVA MLP融合、QueST风格的niche查询与批次效应校正、以及全局超参数配置化。","title":"Bug Journal 2026-02-12"},{"content":"日报 — 2026-02-09 在MIHD项目中为聚类可视化添加ARI/NMI指标叠加，并运行所有DLPFC切片的RM-Ideal基准测试，完成多种embedding方法的系统性评估。\n今日任务 实现与修复 🔄 运行所有DLPFC切片的RM-Ideal评估 — 对所有有embedding结果的切片（151508/151509有多种方法，151669-151675只有uni_staig_fusion）并行运行evaluate_rm_ideal.py，使用\u0026ndash;niche_label all计算所有层级的Spearman/P@K/SameLabel指标并生成可视化。5个切片已完成，151508/151509仍在运行中（因为有多个embedding方法需评估）。 🔄 运行pca+uni+staig_fusion配置的全部DLPFC切片基准测试 — 用户要求重新运行之前表现最好的配置（pca+uni+staig_fusion+STAIG对齐）在所有11个DLPFC切片上，不覆盖已有结果。151508/151509已缓存跳过，151510因缺少spatial coordinates失败，正在处理其余切片。 ✅ 为聚类可视化添加ARI/NMI/Silhouette指标叠加 — 将utils/visualization.py从scanpy的save=方式重构为显式fig/axes布局，添加metrics参数，在图底部叠加ARI/NMI/Silhouette文本框。同时修改run_benchmark.py中的调用传递metrics dict，以及evaluate_rm_ideal.py的visualize()方法添加Spearman/P@K/SameLabel指标叠加。发现run_benchmark.py内部还有一个本地copy的create_clustering_visualization函数也需同步更新。 问题与解决方案 一般问题 1. utils/visualization.py的修改未生效，因为run_benchmark.py内部有一个同名函数的本地copy覆盖了import 解决方案: 找到并同步更新run_benchmark.py:1644处的本地copy函数\n关键洞察: 代码重复（本地copy vs 模块import）导致修改无效，需要检查调用链\n2. 用户指出应参考pca_uni_staig_fusion文件夹的已有结果，而不是重新运行性能差的配置 解决方案: 检查experiments_log和summary CSV，确认pca+uni+staig_fusion是最佳配置，切换到该配置并使用\u0026ndash;skip_cached\n关键洞察: 先查看已有结果再决定运行策略，避免重复计算\n3. 三个实验并行运行时GPU内存不足，PCA+UNI2+STAIG进程被OOM Killed 解决方案: 停止并行任务，改为顺序运行（用\u0026amp;\u0026amp;链接）\n关键洞察: GPU内存不够同时加载多个STAIG模型，必须串行运行\n4. staig_fusion在151510/151672等切片上报错：requires spatial coordinates 解决方案: 这些切片数据缺少spatial CSV文件，属于数据问题，暂时跳过\n关键洞察: 部分DLPFC切片的spatial坐标文件缺失，staig_fusion无法处理\n人类思路 vs AI 思路 实现层面 选择运行配置 角色 思路 人类 用户知道pca_uni_staig_fusion文件夹里有好结果，直接指向具体目录 AI AI倾向于重新运行最新生成的uni_staig_fusion（没有pca基因编码器）配置 差异分析: AI忽略了已存在的高质量结果，用户的历史知识更准确地指向了正确配置\n并行vs串行执行策略 角色 思路 人类 用户未明确指定，但期望任务顺利完成 AI AI首先尝试并行运行三个实验，导致OOM崩溃，然后改为串行 差异分析: AI高估了GPU内存容量，需要用户环境约束来纠正策略\nAI 局限性 一般局限 在修改visualization函数后未检查是否存在同名本地copy，导致修改未生效，需要第二轮调试才发现问题 未能记住之前会话中pca_uni_staig_fusion是最佳配置，需要用户提醒才重新查看 首次计划运行时（ExitPlanMode）被用户拒绝，说明AI在实现前花费了过多时间在规划阶段 今日收获 实践收获 scanpy的sc.pl.spatial(save=)方式会自动保存到figures/目录且不返回figure对象，需要改用ax=参数+手动fig.savefig()才能添加自定义注释 DLPFC数据集中151510/151672/151674/151676等切片缺少spatial坐标，staig_fusion无法处理；需要在pipeline中做更好的数据验证 会话摘要 🔄 实现可视化指标叠加并运行全DLPFC基准测试 21:50:31.276 | claude_code 用户提供了完整的实现计划，AI依次修改了utils/visualization.py（重构为显式fig/axes，添加metrics文本框）、scripts/run_benchmark.py（传递metrics dict，同时发现并修复了本地copy函数）、scripts/evaluate_rm_ideal.py（添加Spearman/P@K叠加）。验证运行151508后图片正确显示metrics。随后用户要求运行全部DLPFC切片，AI先尝试并行运行三种配置但OOM崩溃，后改串行；用户纠正应使用pca_uni_staig_fusion配置（已有高质量结果），最终使用\u0026ndash;skip_cached避免覆盖，完成7/11切片的可视化。最后并行启动了所有可用切片的RM-Ideal评估，5个切片已完成，2个仍在运行。\n🔄 为聚类可视化添加NMI/ARI指标叠加（规划阶段） 21:46:17.845 | claude_code 用户要求在聚类可视化图上添加NMI/ARI分数，以及在RM-Ideal可视化上添加p值等基准结果。AI探索了visualization.py和evaluate_rm_ideal.py的实现，制定了四步修改计划：重构可视化函数使用显式fig/axes布局，在各调用处传递metrics参数。用户最终拒绝了ExitPlanMode执行请求，等待下一个会话实现。\nToken 用量 总览 指标 数值 总 Token 10,207,919 输入 Token 276 输出 Token 8,008 Cache 创建 510,804 Cache 读取 9,688,831 Cache 命中率 95.0% 总费用 (USD) $7.8716 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 248 7,991 453,924 9,483,634 $7.7799 98.8% claude-haiku-4-5-20251001 28 17 56,880 205,197 $0.0917 1.2% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-09/","summary":"在MIHD项目中为聚类可视化添加ARI/NMI指标叠加，并运行所有DLPFC切片的RM-Ideal基准测试，完成多种embedding方法的系统性评估。","title":"Bug Journal 2026-02-09"},{"content":"日报 — 2026-02-08 在HPC服务器上为QueST和MIHD项目实现了RM-Ideal（Region Matching Ideal）评分功能，从零开始移植WWL图核算法，并集成到两个独立的代码库中\n今日任务 架构与策略 ✅ QueST RM-Ideal算法实现（utils.py） — 在QueST的src/utils.py中新增4个函数：wwl_node_features()、_wasserstein_distance()、rm_score()和compute_rm_ideal_scores()，实现了完整的WWL图核+最优传输的RM-Ideal评分计算 ✅ MIHD RM-Ideal核心模块（rm_ideal.py） — 创建utils/rm_ideal.py，移植QueST的WWL核算法，适配MIHD框架（无squidpy/torch_geometric依赖），并创建RMIdealEvaluator类支持k-hop子图预计算和.npy缓存 ✅ MIHD CLI评估脚本（evaluate_rm_ideal.py） — 创建scripts/evaluate_rm_ideal.py，包含NicheQueryEvaluator类，支持按标签/空间/索引定义niche、计算Spearman相关、Precision@K、SameLabel@K指标，以及scanpy可视化和CSV汇总 ✅ QueST QueSTTrainer集成（trainer.py） — 在QueSTTrainer类中新增compute_rm_scores()方法，遍历所有查询niche和参考样本，将RM-Ideal分数存储到adata.obs，命名格式与Tutorial 2一致 实现与修复 ✅ MIHD空间工具扩展（spatial_utils.py） — 在MIHD的utils/spatial_utils.py中新增build_grid_connectivity()（squidpy-free空间邻接图构建）和bfs_k_hop()（纯networkx BFS k-hop子图提取）两个函数 ✅ 实际运行MIHD RM-Ideal评估（151508/Layer_3） — 在DLPFC section 151508的Layer_3 niche上运行评估，使用uni_staig_fusion embeddings，获得Spearman r=0.424, SameLabel@50=1.0的结果 ✅ 修复SameLabel@K为0的bug — 发现并修复evaluate_rm_ideal.py中同标签检索率错误为0的bug：同标签计算应在全量valid spots上进行，而非排除niche spots后的子集 问题与解决方案 关键问题 1. SameLabel@K结果全部为0，明显不合理 解决方案: 发现bug：niche定义为\u0026rsquo;Layer_3\u0026rsquo;时，所有Layer_3 spots都被排除在eval_mask之外，导致无Layer_3可检索。修复为同标签率在全量spots上计算\n关键洞察: 当niche定义覆盖某类型的所有spots时，\u0026lsquo;排除niche spots\u0026rsquo;的逻辑会使该类型完全消失，指标语义需要区分：Precision@K（检索质量vs RM-Ideal）应排除自身，SameLabel@K（类型检索能力）不应排除\n2. MIHD不依赖squidpy和torch_geometric，但QueST的RM-Ideal实现依赖这两个库 解决方案: 用sklearn NearestNeighbors重写空间邻接图构建（build_grid_connectivity），用纯networkx BFS替代PyG的k_hop_subgraph\n关键洞察: 跨项目代码移植时需要识别并替换不可用的依赖，保持算法等价性的同时适配目标框架的依赖约束\n一般问题 3. POT（Python Optimal Transport）库在任何conda环境中均未安装，且quest conda环境不存在 解决方案: 实现POT优先、scipy.optimize.linprog兜底的双路径最优传输求解器；使用General/AI环境运行\n关键洞察: 在HPC环境中不能假设特定Python包已安装，需要为关键依赖提供fallback路径\n4. QueST项目文件路径错误：初始任务指向/hpc/group/yizhanglab/zt81/MIHD/src/utils.py，但该路径不存在 解决方案: 通过glob搜索找到实际路径为/hpc/group/yizhanglab/zt81/QueST/src/utils.py\n关键洞察: 任务文档中的路径可能与实际项目结构不符，需要先验证文件是否存在再实施\n5. utils.py中导入了umap等重型库，直接import无法在没有激活相应conda环境的shell中运行 解决方案: 将核心算法函数内联到独立Python -c测试脚本中进行单元测试，或用source activate General激活环境\n关键洞察: 在HPC环境测试时需要注意conda环境激活，内联测试是快速验证算法正确性的有效方法\n人类思路 vs AI 思路 战略层面 SameLabel@K为0的问题发现 角色 思路 人类 用户立即发现SameLabel@50=0.00的结果不合理，追问\u0026rsquo;But why the SameLabel@50 has the value of 0?\u0026rsquo; AI AI完成了代码实现并输出了结果，但未主动检查SameLabel@K指标的合理性 差异分析: 用户对结果有直觉判断能力（Layer_3 spots应该能被检索到），发现了AI实现中的逻辑错误；AI需要人类提示才发现bug\nRM-Ideal集成范围的扩展 角色 思路 人类 用户主动提出将RM-Ideal不仅集成到QueST，还要移植到MIHD项目中，并要求同时支持within-section和cross-section两种查询模式 AI AI最初只考虑完成给定的QueST集成计划，未主动提出扩展到MIHD 差异分析: 用户具有跨项目整合的战略视野，主动推动评估框架的复用；AI倾向于完成当前任务范围内的工作\n实现层面 可视化展示选择 角色 思路 人类 用户在看到文字结果后主动要求\u0026rsquo;can you do the visualization of the spots you picked?' AI AI实现了完整的3-panel可视化功能，但在输出文字结果后等待用户指示才运行可视化 差异分析: 用户更倾向于空间数据的直觉图形展示；AI的默认输出是文字指标摘要\nAI 局限性 重要局限 实现了完整的指标计算框架但未主动验证结果的合理性，SameLabel@K=0这种明显异常值需要人类提示才被检测到 一般局限 在多个会话中重复尝试实现相同的RM-Ideal计划（三个会话均有Implement the following plan的记录），表明AI的实现结果没有被持久化或验证，导致重复工作 多次尝试调用不存在的conda环境\u0026rsquo;quest\u0026rsquo;，直到运行失败才发现，说明AI没有在执行前验证环境是否存在的习惯 conda run命令超时（30秒未返回），AI处理超时的策略是直接内联代码测试，跳过了环境验证步骤，这虽然有效但绕过了原始测试目标 今日收获 核心收获 WWL（Wasserstein Weisfeiler-Lehman）图核算法：通过K轮迭代邻居标签聚合+哈希构建节点特征向量，两图共享哈希表保证一致编码，再用最优传输计算Earth Mover\u0026rsquo;s Distance，最终RM=1-W。K+1维Hamming归一化确保距离在[0,1] 评估指标设计原则：Precision@K（检索与ground truth的排名一致性）应排除查询niche自身；SameLabel@K（类型检索能力）不应排除，否则当niche覆盖某类型全部spots时指标会失效 DLPFC 151508 Section上，uni_staig_fusion embeddings的Layer_3 niche查询结果：Spearman r=0.424（统计显著），SameLabel@50=1.0。说明embedding在类型级别检索准确，但无法精确复现graph-kernel级别的niche结构 实践收获 HPC多conda环境管理实践：conda env list查看可用环境，source activate [env]激活，不能假设特定环境存在。QueST在General环境下运行，quest环境不存在 会话摘要 QueST ✅ RM-Ideal算法实现、单元测试验证（8/8通过） 04:42:56.919 | claude_code 实现了QueST RM-Ideal评分系统的完整5个函数（wwl_node_features、_wasserstein_distance、rm_score、compute_rm_ideal_scores、QueSTTrainer.compute_rm_scores）。发现QueST repo实际位于/hpc/group/yizhanglab/zt81/QueST/而非MIHD目录。通过内联测试（绕过重型conda依赖）完成了8个单元测试，全部通过，包括自相似=1.0、完全不同=0.0等关键边界情况。\n🔄 QueST RM-Ideal实现方式调研及MIHD集成方案探讨 05:16:22.879 | claude_code 用户询问QueST如何使用RM-Ideal以及patch间相似度计算方式。AI探索QueST代码库发现RM-Ideal在src/utils.py已实现，查询机制基于GIN编码器+余弦相似度。随后讨论了如何将两者集成到MIHD评估框架，用户确认需要both within-section和cross-section支持，选择QueST风格的Niche query模式。会话在生成实施计划时被用户中断（未批准ExitPlanMode）。\n❌ QueST RM-Ideal二次实施尝试（与第一个会话重复） 03:13:27.888 | claude_code 收到与04:42会话相同的实施计划。探索发现QueST repo位于正确路径，读取utils.py和trainer.py，理解了现有架构。开始实施流程（写入plan文件、AskUserQuestion确认scope）时被用户中断，未实际执行代码修改。这是同一任务的另一次启动，最终在04:42会话中完成。\nMIHD ✅ MIHD RM-Ideal全套实现：spatial_utils扩展、rm_ideal模块、CLI评估脚本 05:36:39.779 | claude_code 按计划实现了MIHD的RM-Ideal评估框架：扩展spatial_utils.py添加build_grid_connectivity和bfs_k_hop，创建utils/rm_ideal.py（含RMIdealEvaluator类，支持预计算子图缓存），创建scripts/evaluate_rm_ideal.py（含NicheQueryEvaluator、Spearman/Precision@K/SameLabel@K指标、scanpy可视化）。在151508/Layer_3运行后发现SameLabel@K=0的bug，经用户指出后诊断并修复。最终结果：Spearman r=0.424，SameLabel@50=1.0。\n🔍 RM-Ideal与Niche Query集成MIHD的需求分析与规划 02:23:10.030 | claude_code 用户提出将QueST的RM-Ideal和niche查询能力集成到MIHD以评估多种embedding方法。讨论了graphcompass库（QueST论文中引用的外部库），确定了集成路径为src/utils.py。澄清了scope（utils.py+trainer.py）和OT求解器策略（POT优先+scipy兜底）。规划讨论被用户中断，计划写入plan文件后进入下一会话实施。\nToken 用量 总览 指标 数值 总 Token 6,943,631 输入 Token 3,635 输出 Token 3,815 Cache 创建 529,772 Cache 读取 6,406,409 Cache 命中率 92.4% 总费用 (USD) $4.5642 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 3,485 3,400 292,673 4,233,345 $4.0483 88.7% claude-haiku-4-5-20251001 150 415 237,099 2,173,064 $0.5159 11.3% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-08/","summary":"在HPC服务器上为QueST和MIHD项目实现了RM-Ideal（Region Matching Ideal）评分功能，从零开始移植WWL图核算法，并集成到两个独立的代码库中","title":"Bug Journal 2026-02-08"},{"content":"日报 — 2026-02-07 在DCC服务器上同时推进两个生信项目：为QueST项目创建CLAUDE.md并调研RM-Ideal评分指标，以及修复MIHD基准框架中STAIG严格对齐模式的四个关键差异，最终将151673切片ARI从~0.09提升至0.54。\n今日任务 架构与策略 ✅ MIHD STAIG严格对齐模式修复（4个差异） — 系统对比MIHD与原始STAIG实现，发现并修复4个关键差异：(1)UNI encoder跳过ImageNet归一化，(2)patch尺寸改为256×256，(3)训练后不做StandardScaler，(4)drop_feature使用CPU随机数生成器；ARI从0.09提升至0.54 ✅ MIHD mclust聚类0%ARI bug修复 — 诊断并修复mclust聚类报错\u0026rsquo;Error in svd: a dimension is zero\u0026rsquo;，根因为numpy2ri.activate()在循环中反复调用未deactivate导致转换栈损坏，以及embedding可能存在零方差列 ❌ RM-Ideal评分指标调研 — 用户询问QueST方法中RM-Ideal评分的工作原理，AI在代码库、arXiv PDF、HTML版本及网络搜索中均未找到该术语；发现论文实际使用的是RM（Region Matching）分数，基于Wasserstein Weisfeiler-Lehman图核 实现与修复 ✅ 图像patch提取性能优化 — 修复_extract_patches_from_image中np.array(pil_img)在循环内重复调用导致的严重性能问题（每个spot转换一次整张大图），将其移至循环外，并用numpy切片替代PIL crop，改为预分配输出tensor ✅ QueST项目CLAUDE.md创建 — 分析QueST（空间转录组niche查询）代码库结构，包括model.py、trainer.py、layers.py、utils.py，生成完整的CLAUDE.md指导文档 🔄 224×224 patch尺寸消融实验 — 用户要求对比patch_size=224与256的性能差异，基准测试因缓存问题未能正确运行，session结束时仍未获得结果 ✅ 更新MIHD教程文档 — 将STAIG严格对齐4项修复内容写入THOROUGH_TUTORIAL.md，说明每个修复的原理和效果 问题与解决方案 关键问题 1. STAIG严格模式的ARI远低于原始STAIG（~0.09 vs 0.45+） 解决方案: 通过逐行对比发现4个差异并逐一修复：(1)跳过ImageNet归一化，(2)256×256 patch尺寸+dynamic_img_size，(3)去除StandardScaler，(4)drop_feature强制使用CPU随机数。修复后ARI=0.54\n关键洞察: ImageNet归一化是最大的差异来源——原始STAIG只做ToTensor()不做Normalize，MIHD多了这步导致UNI embedding分布完全不同；StandardScaler对mclust聚类也有显著影响\n2. mclust报错\u0026rsquo;Error in svd: a dimension is zero\u0026rsquo;，在处理12个DLPFC切片的循环中，某些切片失败 解决方案: 根因是numpy2ri.activate()在每次调用cluster_with_mclust()时执行但从不deactivate，导致rpy2转换栈在多次调用后损坏。修复：用try/finally包裹activate/deactivate，并在传入R前过滤零方差列和NaN/Inf值\n关键洞察: 原始STAIG只调用一次mclust，因此这个bug不会触发；MIHD在循环中调用12次才暴露了潜在问题\n3. RM-Ideal评分在QueST论文和代码中均未找到 解决方案: 未解决；AI在arXiv PDF、HTML、代码库和网络搜索中均未找到该术语；论文中实际的评估指标是RM（Region Matching）分数（基于WWL图核）和Best Niche Match Accuracy\n关键洞察: 用户提到的\u0026rsquo;RM-Ideal\u0026rsquo;可能来自论文的某个补充材料或较新版本，arXiv HTML版本的访问受限影响了调研\n一般问题 4. patch提取极慢（每个spot重复将整张大图转换为numpy数组） 解决方案: 将np.array(pil_img)移至循环外只做一次，用numpy数组切片替代PIL crop，预分配输出tensor而非list.append\n关键洞察: 对于~3500个spot的样本，移除循环内的冗余大图转换可带来10-50倍加速\n人类思路 vs AI 思路 战略层面 STAIG与MIHD差异的发现方式 角色 思路 人类 人类（用户）观察到ARI结果与预期不符，提出\u0026rsquo;应该已经全部都一样了吧\u0026rsquo;的疑问，驱动了调查 AI AI通过系统性逐行对比两个代码库，运行多个并行探索agent，梳理出4个具体技术差异 差异分析: 人类的直觉判断（\u0026lsquo;应该一样\u0026rsquo;）与实际结果不符，触发了AI的系统性调查；最终发现的根因（ImageNet归一化、CPU随机数等）需要深入代码对比才能找到\nmclust bug的诊断 角色 思路 人类 用户直接提供了完整的错误traceback AI AI读取了clustering.py代码，识别出numpy2ri.activate()未配对deactivate的模式，并与原始STAIG单次调用的上下文对比，定位根因 差异分析: 人类提供了精确的错误信息和traceback，AI利用代码上下文对比（STAIG只调用一次 vs MIHD循环调用12次）找到了根因\nRM-Ideal评分的定位 角色 思路 人类 用户提供了arXiv论文链接，期望AI能从中找到RM-Ideal的定义 AI AI尝试了多种访问方式（abstract、PDF、HTML v1/v2），以及网络搜索，均未找到该术语；发现代码库中存在\u0026rsquo;RM score\u0026rsquo;（不含\u0026rsquo;ideal\u0026rsquo;），并向用户报告未找到的结论 差异分析: 人类可能记忆中混淆了术语名称，或该术语来自论文的不可访问部分；AI诚实地报告未找到而不是编造答案\nAI 局限性 重要局限 无法访问arXiv论文的HTML v2版本和部分页面，导致无法完全调研RM-Ideal评分的定义（可能在论文某些补充材料中） 一般局限 在STAIG严格对齐模式调查中，AI最初通过多个并行探索agent收集信息，但需要多轮交互才能汇总所有差异；对STAIG图像预处理链（UNI→PCA→pseudo-labels→edge weights的完整链路）需要多文件才能理解 在224×224消融实验中，AI未预见到\u0026ndash;staig_alignment参数期望YAML文件路径而非inline JSON字符串，导致实验设置失败 今日收获 核心收获 原始STAIG的UNI encoder不做ImageNet归一化（只用ToTensor），这是与大多数视觉模型benchmark不同的关键细节；将MIHD对齐这一点是ARI提升最大的单一修复 rpy2的numpy2ri.activate()在多次调用时会累积状态，必须在每次使用后配对调用deactivate()；这个问题在原始STAIG中不会触发（单次调用），但在批量benchmark循环中会导致隐蔽的维度错误 CPU vs GPU随机数生成器即使种子相同也会产生不同序列；原始STAIG中drop_feature的随机mask是在CPU生成的，复现时必须保持一致才能对齐结果 实践收获 图像patch提取的性能关键在于避免在循环内将整张高分辨率组织图像（~20000×20000）重复转换为numpy数组；移至循环外可带来10-50倍加速 会话摘要 QueST 🔄 QueST代码库分析、CLAUDE.md创建与RM-Ideal评分调研 20:12:02.765 | claude_code AI分析了QueST（空间转录组niche查询）项目的完整代码结构，包括model.py（三损失训练）、trainer.py、layers.py、utils.py，创建了CLAUDE.md指导文档。随后用户询问RM-Ideal评分机制，AI在代码库、arXiv论文（PDF/HTML）和网络搜索中均未找到该术语，发现论文实际使用RM分数（WWL图核）和Best Niche Match Accuracy作为评估指标。\nMIHD ✅ STAIG严格对齐模式与原始STAIG四项关键差异修复 20:04:02.000 | claude_code 用户发现MIHD的STAIG严格模式ARI远低于原始STAIG。AI通过系统性对比两个代码库，发现并修复了4个关键差异：UNI encoder跳过ImageNet归一化（最大差异）、patch尺寸改为256×256（加dynamic_img_size）、训练后不做StandardScaler、drop_feature使用CPU随机数。修复后在151673切片上ARI从0.09提升至0.5435，远超原始STAIG的0.45基准。\n✅ 修复mclust\u0026rsquo;维度为零\u0026rsquo;报错与图像patch提取性能优化 02:44:18.061 | claude_code 用户提供了mclust聚类的完整错误traceback。AI诊断出两个问题：numpy2ri.activate()在循环中反复调用未配对deactivate，以及patch提取中将整张大图在每个spot循环内反复转换为numpy数组。AI修复了clustering.py中的activate/deactivate配对问题并添加了零方差列检测，同时重构了_extract_patches_from_image将大图转换移至循环外。\n🔍 RM-Ideal Score集成调研与graphcompass API探索 22:13:40.661 | claude_code 用户询问如何将RM-Ideal Score集成进QueST项目，但实际指向的是MIHD项目。AI探索了MIHD代码库结构和graphcompass库的RM Score API，在代码库中发现了\u0026rsquo;region matching (RM) score\u0026rsquo;的文档说明（基于WWL图核）。会话主要停留在调研阶段，未产生具体代码修改。\nToken 用量 总览 指标 数值 总 Token 15,893,955 输入 Token 804 输出 Token 13,371 Cache 创建 1,199,406 Cache 读取 14,680,374 Cache 命中率 92.4% 总费用 (USD) $11.4595 模型明细 模型 输入 输出 Cache 创建 Cache 读取 费用 占比 claude-opus-4-6 513 12,841 784,015 10,614,015 $10.5307 91.9% claude-haiku-4-5-20251001 291 530 415,391 4,066,359 $0.9288 8.1% ","permalink":"https://tzj2006.github.io/bugjournal/2026-02-07/","summary":"在DCC服务器上同时推进两个生信项目：为QueST项目创建CLAUDE.md并调研RM-Ideal评分指标，以及修复MIHD基准框架中STAIG严格对齐模式的四个关键差异，最终将151673切片ARI从~0.09提升至0.54。","title":"Bug Journal 2026-02-07"},{"content":"今天开始测试 recovery benchmark\n和Gemini讨论了一下，Gemini建议我从issac环境开始，先做一个demo出来\n所以就开始配置issac环境\n第一个碰倒到的bug居然是网络问题\n问题如下，这个问题的报错是 No route to host, 这意味着无法连接到代理服务器\n用人话说就是计算节点是没法联网的\n要回到登录节点才行\n然后回退就work了（（\n今天做的第二件事就是配置Claude Code + GLM的配置\n为什么要用GLM呢？因为GLM一个月￥20, 但是用量可以到达100M tokens\n据说比Github Copilot 更加聪明\n所以我想试试\n具体的链接在这里\n但是呢，在加入VS code的时候遇到了方法一失败的问题，这时候可以使用方法二，即可解决。\n这个有点NB，这样在所有的服务器上都可以用claude code帮我解决问题了\n这几天试试看看好不好用\n","permalink":"https://tzj2006.github.io/bugjournal/2026-01-09/","summary":"\u003cp\u003e今天开始测试 recovery benchmark\u003c/p\u003e\n\u003cp\u003e和Gemini讨论了一下，Gemini建议我从issac环境开始，先做一个demo出来\u003c/p\u003e\n\u003cp\u003e所以就开始配置issac环境\u003c/p\u003e\n\u003cp\u003e第一个碰倒到的bug居然是网络问题\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1767989278653\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2026-01-09/1767989278653.png\"\u003e\u003c/p\u003e\n\u003cp\u003e问题如下，这个问题的报错是 \u003ccode\u003eNo route to host\u003c/code\u003e, 这意味着无法连接到代理服务器\u003c/p\u003e\n\u003cp\u003e用人话说就是计算节点是没法联网的\u003c/p\u003e\n\u003cp\u003e要回到登录节点才行\u003c/p\u003e\n\u003cp\u003e然后回退就work了（（\u003c/p\u003e\n\u003cp\u003e今天做的第二件事就是配置Claude Code + GLM的配置\u003c/p\u003e\n\u003cp\u003e为什么要用GLM呢？因为GLM一个月￥20, 但是用量可以到达100M tokens\u003c/p\u003e\n\u003cp\u003e据说比Github Copilot 更加聪明\u003c/p\u003e\n\u003cp\u003e所以我想试试\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://docs.bigmodel.cn/cn/coding-plan/tool/claude\"\u003e具体的链接在这里\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e但是呢，在加入VS code的时候遇到了方法一失败的问题，这时候可以使用方法二，即可解决。\u003c/p\u003e\n\u003cp\u003e这个有点NB，这样在所有的服务器上都可以用claude code帮我解决问题了\u003c/p\u003e\n\u003cp\u003e这几天试试看看好不好用\u003c/p\u003e","title":"Bug Journal 2026-01-09"},{"content":"Today\u0026rsquo;s problem 3432. Count Partitions with Even Sum Difference\nIntuition In this question, we need to partition the array into two parts. And the difference between these two parts are even.\nNow, both parts must have the same module to 2. That is, they are both even or both odd. So, the sum of the array needs to be even.\nNow, if the sum of the array is even, then every partition must have the same module to 2. Otherwise, the sum of the array would be odd.\nTherefore, the answer would be 0 when the sum of the array is odd, and $n-1$ when the sum of the array is even.\nApproach Return 0 when the sum of the array is odd, and $n-1$ when the sum of the array is even.\nComplexity Time complexity: $O(n)$, n is the length of the array.\nSpace complexity: $O(n)$, n is the length of the array.\nCode class Solution: def countPartitions(self, nums: List[int]) -\u0026gt; int: if sum(nums) % 2 == 1: return 0 return len(nums) - 1 ","permalink":"https://tzj2006.github.io/leetcode/2025-12-05/","summary":"\u003col start=\"3432\"\u003e\n\u003cli\u003eCount Partitions with Even Sum Difference\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-12-05"},{"content":"Question Burst Balloons\nIntuition While the ballons burst, it will not affect the score of another ballon, and the length of the list will always decrease. This means that we can use dynamic programming to solve this problem.\nApproach Consider a case, when $dp[i][j]$ means the maximum score you can get by bursting all the balloons between i and j. In this case, our answer would be $dp[0][n-1]$, if n is the length of the list.\nNow, let\u0026rsquo;s figure out how to get $dp[i][j]$. As we mentioned in part Intuition, the length of the balloon is always decreasing, which means that we might be able to get $dp[i][j]$ by first counting a shorter interval, and the increase the interval to $0 \\to n-1$.\nNow, we have our solution:\nFirst, we iterate the length of the interval.\nNext, we can break the interval $i \\to j$ into two sub intervals $i \\to k-1$ and $k+1 \\to j$.\nNow we can compute the score of interval $i \\to j$ by adding up sub intervals $i \\to k-1$, $k + 1 \\to j$, and burst the balloon k.\nBecause the balloons in sub interval $i \\to k-1$ and $k + 1 \\to j$ are already gone, so the score addition would be $nums[k] \\times nums[i-1] \\times nums[j+1]$.\nThe only thing we now need to consider is the edge of these subintervals. Knowing that the balloon we are now bursting can be the edge of the interval $i \\to j$, meaning that we need to iterate $[i, j]$ inclusively.\nFinally, you will get your result in $dp[0][n-1]$.\nComplexity Time complexity: $O(N^3)$. N is the length of the list. Space complexity: $O(N^2)$. N is the length of the list. Code class Solution: def maxCoins(self, nums: List[int]) -\u0026gt; int: n = len(nums) nums = nums + [1] dp = [[0 for _ in range(n+1)] for _ in range(n+1)] for i in range(n): dp[i][i] = nums[i] * nums[i-1] * nums[i+1] for i in range(1, n): for l in range(n): if i + l == n: break r = i + l for k in range(l, r+1): dp[l][r] = max(dp[l][r], dp[l][k-1] + dp[k+1][r] + nums[k] * nums[l-1] * nums[r + 1]) # print(dp) return dp[0][n-1] ","permalink":"https://tzj2006.github.io/leetcode/p312_burst_balloons/","summary":"Interval Dynamic Programming with clear explaination and how you can get to the solution","title":"LeetCode Question P.312 Burst Balloons"},{"content":"FAST: Efficient Action Tokenization for Vision-Language-Action Models RSS 2025 By Physical Intelligence\n当前方法局限：都是离散数据，没有办法获取高频机器人控制数据的时间相关性\n因此使用一种名字叫做DCT的压缩算法，可以把连续信号转移到频率域(一种离散域)\n(话虽如此，但是真的需要用到这么高的频率吗？)\n问题 ：传统的朴素分箱方案在处理高频连续动作时，由于动作变化小，会导致许多连续时间步的动作被离散化为相同或高度相似的标记。这意味着每个新标记带来的“边际信息”（即在已知之前标记的情况下，新标记所包含的额外信息）非常低。 朴素分箱方案独立地处理每个时间步和每个维度，完全忽略了连续动作序列中固有的时间相关性（例如，机器手臂的移动通常是平滑且连续的）。这使得模型难以学习到动作的动态模式。 Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets UW + Toyota\n读前问题：\n这个模型有什么优势？ 这个模型是如何实现“视频无标签学习\u0026quot;的？ 什么叫\u0026quot;naturally facilitate video learning\u0026quot; 觉得有意思的点：\nImitation learning do not explicitly capture temporal dynamics that are naturally present in demonstration trajectories or videos (Page 1, Introduction) 一个 Diffusion Model： UWM 使用一个单一的 Diffusion Transformer 模型来处理所有的输入和输出。它不是为每种任务（策略、正向动力学、逆向动力学、视频预测）训练一个单独的模型。 Tokenization 和输入： 就像您说的，无论是当前的观测（图片）、动作、还是未来的观测，它们都会被“tokennize”成模型能够理解的表示形式（例如，图片通过编码器转换成图像特征，动作也被编码）。这些 token 以及动作和未来观测的独立扩散时间步长 t_a 和 t_o' 会一起输入到 Diffusion Transformer 中。 灵活的输出： 通过巧妙地控制这些扩散时间步长 t_a 和 t_o'，UWM 可以在推理时灵活地得到您想要的结果。例如： 如果想得到 策略（Policy） ，就将未来观测的时间步长 t_o' 设置为最大值 T（表示未来观测完全被噪声掩盖，模型只关注动作），然后模型会输出动作。 如果想得到 正向动力学（Forward Dynamics） ，就将动作时间步长 t_a 设置为 0（表示给定明确的动作输入），然后模型会输出未来观测。 如果想得到 逆向动力学（Inverse Dynamics） ，就将未来观测时间步长 t_o' 设置为 0（表示给定明确的未来观测输入），然后模型会输出动作。 如果想得到 视频预测（Video Prediction） ，就将动作时间步长 t_a 设置为最大值 T（表示动作完全被噪声掩盖，模型只关注视频），然后模型会输出未来观测。 模态独立的扩散时间步长 (Modality-Specific Diffusion Timesteps)： 这是 UWM 最核心的设计之一。传统的多模态扩散模型可能使用一个共享的时间步长来对所有模态进行加噪和去噪。然而，UWM 引入了两个独立的扩散时间步长：t_a 用于动作 (actions)，t_o' 用于未来观测 (future observations)。 原因： 这种分离使得模型能够独立控制每种模态的噪声水平。通过在推理时灵活地设置这些时间步长（例如，将某个时间步长设为 T 表示完全加噪以“掩蔽”该模态，设为 0 表示完全去噪以“条件化”该模态），UWM 可以实现前文提到的策略、正向动力学、逆向动力学和视频预测等多种推理模式。 效果： 它使得一个单一的模型能够充当多种角色，极大地提高了模型的通用性和灵活性，同时增强了模型对模态间因果关系的理解。 统一的 Transformer 架构 (Unified Transformer Architecture)： UWM 采用单一的 Transformer 骨干网络来处理所有模态的输入（当前观测、动作、未来观测）并预测其噪声。Transformer 以编码后的图像特征、动作 token 以及独立的扩散时间步长作为输入。 原因： 这种统一的架构促进了不同模态之间的特征共享。在 Transformer 内部，动作和图像信息可以相互交流和融合，从而使模型能够学习到更丰富、更全面的表征，并捕获它们之间的潜在因果关系。 效果： 避免了为每个任务设计独立模型的复杂性，简化了模型结构，并可能提高学习效率。 寄存器令牌 (Register Tokens)： UWM 在 Transformer 的输入序列中加入了随机初始化的“寄存器令牌”。 原因： 经验性研究发现，这些冗余令牌有助于提高模型性能，尤其是在多模态数据上。研究人员推测，动作和潜在图像块是不同的模态，寄存器可以作为一个中间媒介，存储来自任一模态的信息，并在后续 Transformer 层中被检索，从而促进模态间的信息交换和更好的多模态特征共享。 效果： 尽管这些令牌最终会被丢弃，但它们在中间层起到了重要的信息桥梁作用，提升了模型的表现力。 Adaptive Layer Normalization (AdaLN) 条件化： UWM 使用 AdaLN [33] 来将扩散时间步长（以及图像特征）条件化到 Transformer 的每个块中。 原因： AdaLN 是一种有效的条件化机制，它通过调整层归一化 (Layer Normalization) 的参数（缩放和偏移）来注入条件信息。 效果： 这种方式允许模型以一种非侵入性且高效的方式将时间步长信息融入到 Transformer 的处理流程中，有助于模型更好地理解噪声水平和生成过程。 潜在扩散范式 (Latent Diffusion Paradigm)： 对于图像扩散，UWM 采用了潜在扩散范式。它使用一个冻结的预训练 VAE (Variational AutoEncoder)（例如来自 Stable Diffusion XL [34]）将原始高分辨率图像压缩到低维的潜在空间中进行加噪和去噪，最后再解码回全尺寸图像。 原因： 直接在高分辨率像素空间进行扩散通常计算成本高昂且效率低下。潜在扩散可以在更小的潜在空间中操作，显著减少计算量。 效果： 提高了图像处理的效率，同时保持了高质量的图像生成能力。 这些特殊设计共同使得 UWM 能够有效地应对多模态机器人学习的挑战，实现了一个能够从大规模异构数据中学习并灵活执行多种推理任务的强大模型。\nVILA: On Pre-training for Visual Language Models https://www.alphaxiv.org/overview/2312.07533v4\nby Nvidia \u0026amp; MIT\n更新大型语言模型 (LLM) 是必要的 ：研究发现，在预训练过程中解冻并更新 LLM 对于获得良好的上下文学习（ICL）能力至关重要。仅仅进行提示调整（prompt tuning）虽然在零样本（0-shot）准确率上表现尚可，但在上下文学习能力方面却不足。使用一个简单的线性投影层而不是 Transformer 块作为投影器，可以促使 LLM 更好地学习处理视觉输入，从而带来更好的泛化能力。 交错式视觉语言语料库有助于预训练 ：预训练时，交错式的图像-文本数据集（如 MMC4）比单纯的图像-文本对（如 COYO）更有益。交错式数据有助于保持 LLM 的文本处理能力，并提供了更准确的梯度更新。与仅使用图像-文本对相比，使用交错式数据进行预训练能显著提高视觉语言任务的准确性，并减少文本能力退化。 通过联合监督微调（SFT）恢复 LLM 性能退化 ：尽管交错式数据有助于保持文本能力，但仍存在一定的准确率下降。通过在 SFT 阶段混入文本指令数据，可以同时恢复文本处理能力的退化，并提高视觉语言任务的准确性。这表明，文本指令数据有助于提升模型的指令遵循能力。 扩大 VLM 预训练的规模 ：模型在以下几个方面进行了扩展以形成最终模型： 更高的图像分辨率 ：将图像分辨率从 224×224224×224 提高到 336×336336×336，从而能够包含更多视觉细节，这对于需要精细细节的任务（如 TextVQA）非常有帮助。 更大的 LLM ：将 LLM 主干模型从 Llama-2 7B 扩展到 Llama-2 13B，以进一步提高性能。 预训练数据 ：同时使用交错式图像-文本数据和图像-文本对进行预训练，以提高数据多样性，尽管预训练语料库的总规模为 50M 图像，小于十亿规模的数据集，但仍取得了显著的性能提升。 SFT 数据 ：包含了 LLaVA-1.5 中更高质量、更多样化的 SFT 数据，进一步提升了下游评估指标。 ","permalink":"https://tzj2006.github.io/bugjournal/2025-08-29/","summary":"\u003ch3 id=\"fast-efficient-action-tokenization-for-vision-language-action-models\"\u003eFAST: Efficient Action Tokenization for Vision-Language-Action Models\u003c/h3\u003e\n\u003cp\u003eRSS 2025 By Physical Intelligence\u003c/p\u003e\n\u003cp\u003e当前方法局限：都是离散数据，没有办法获取高频机器人控制数据的时间相关性\u003c/p\u003e\n\u003cp\u003e因此使用一种名字叫做DCT的压缩算法，可以把连续信号转移到频率域(一种离散域)\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1756516611535\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-08-29/1756516611535.png\"\u003e\u003c/p\u003e\n\u003cp\u003e(话虽如此，但是真的需要用到这么高的频率吗？)\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1756516932637\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-08-29/1756516932637.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e ：传统的朴素分箱方案在处理高频连续动作时，由于动作变化小，会导致许多连续时间步的动作被离散化为相同或高度相似的标记。这意味着每个新标记带来的“边际信息”（即在已知之前标记的情况下，新标记所包含的额外信息）非常低。\u003c/li\u003e\n\u003cli\u003e朴素分箱方案独立地处理每个时间步和每个维度，完全忽略了连续动作序列中固有的时间相关性（例如，机器手臂的移动通常是平滑且连续的）。这使得模型难以学习到动作的动态模式。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"unified-world-models-coupling-video-and-action-diffusion-for-pretraining-on-large-robotic-datasets\"\u003eUnified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets\u003c/h3\u003e\n\u003cp\u003eUW + Toyota\u003c/p\u003e\n\u003cp\u003e读前问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e这个模型有什么优势？\u003c/li\u003e\n\u003cli\u003e这个模型是如何实现“视频无标签学习\u0026quot;的？\u003c/li\u003e\n\u003cli\u003e什么叫\u0026quot;naturally facilitate video learning\u0026quot;\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e觉得有意思的点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eImitation learning do not explicitly capture temporal dynamics that are naturally present in demonstration trajectories or videos (Page 1, Introduction)\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e一个 Diffusion Model：\u003c/strong\u003e UWM 使用一个单一的 Diffusion Transformer 模型来处理所有的输入和输出。它不是为每种任务（策略、正向动力学、逆向动力学、视频预测）训练一个单独的模型。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTokenization 和输入：\u003c/strong\u003e 就像您说的，无论是当前的观测（图片）、动作、还是未来的观测，它们都会被“tokennize”成模型能够理解的表示形式（例如，图片通过编码器转换成图像特征，动作也被编码）。这些 token 以及动作和未来观测的独立扩散时间步长 \u003ccode\u003et_a\u003c/code\u003e 和 \u003ccode\u003et_o'\u003c/code\u003e 会一起输入到 Diffusion Transformer 中。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e灵活的输出：\u003c/strong\u003e 通过巧妙地控制这些扩散时间步长 \u003ccode\u003et_a\u003c/code\u003e 和 \u003ccode\u003et_o'\u003c/code\u003e，UWM 可以在推理时灵活地得到您想要的结果。例如：\n\u003cul\u003e\n\u003cli\u003e如果想得到 \u003cstrong\u003e策略（Policy）\u003c/strong\u003e ，就将未来观测的时间步长 \u003ccode\u003et_o'\u003c/code\u003e 设置为最大值 T（表示未来观测完全被噪声掩盖，模型只关注动作），然后模型会输出动作。\u003c/li\u003e\n\u003cli\u003e如果想得到 \u003cstrong\u003e正向动力学（Forward Dynamics）\u003c/strong\u003e ，就将动作时间步长 \u003ccode\u003et_a\u003c/code\u003e 设置为 0（表示给定明确的动作输入），然后模型会输出未来观测。\u003c/li\u003e\n\u003cli\u003e如果想得到 \u003cstrong\u003e逆向动力学（Inverse Dynamics）\u003c/strong\u003e ，就将未来观测时间步长 \u003ccode\u003et_o'\u003c/code\u003e 设置为 0（表示给定明确的未来观测输入），然后模型会输出动作。\u003c/li\u003e\n\u003cli\u003e如果想得到 \u003cstrong\u003e视频预测（Video Prediction）\u003c/strong\u003e ，就将动作时间步长 \u003ccode\u003et_a\u003c/code\u003e 设置为最大值 T（表示动作完全被噪声掩盖，模型只关注视频），然后模型会输出未来观测。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e模态独立的扩散时间步长 (Modality-Specific Diffusion Timesteps)：\u003c/strong\u003e 这是 UWM 最核心的设计之一。传统的多模态扩散模型可能使用一个共享的时间步长来对所有模态进行加噪和去噪。然而，UWM 引入了两个独立的扩散时间步长：\u003ccode\u003et_a\u003c/code\u003e 用于动作 (actions)，\u003ccode\u003et_o'\u003c/code\u003e 用于未来观测 (future observations)。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 这种分离使得模型能够独立控制每种模态的噪声水平。通过在推理时灵活地设置这些时间步长（例如，将某个时间步长设为 T 表示完全加噪以“掩蔽”该模态，设为 0 表示完全去噪以“条件化”该模态），UWM 可以实现前文提到的策略、正向动力学、逆向动力学和视频预测等多种推理模式。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 它使得一个单一的模型能够充当多种角色，极大地提高了模型的通用性和灵活性，同时增强了模型对模态间因果关系的理解。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e统一的 Transformer 架构 (Unified Transformer Architecture)：\u003c/strong\u003e UWM 采用单一的 Transformer 骨干网络来处理所有模态的输入（当前观测、动作、未来观测）并预测其噪声。Transformer 以编码后的图像特征、动作 token 以及独立的扩散时间步长作为输入。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 这种统一的架构促进了不同模态之间的特征共享。在 Transformer 内部，动作和图像信息可以相互交流和融合，从而使模型能够学习到更丰富、更全面的表征，并捕获它们之间的潜在因果关系。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 避免了为每个任务设计独立模型的复杂性，简化了模型结构，并可能提高学习效率。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e寄存器令牌 (Register Tokens)：\u003c/strong\u003e UWM 在 Transformer 的输入序列中加入了随机初始化的“寄存器令牌”。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 经验性研究发现，这些冗余令牌有助于提高模型性能，尤其是在多模态数据上。研究人员推测，动作和潜在图像块是不同的模态，寄存器可以作为一个中间媒介，存储来自任一模态的信息，并在后续 Transformer 层中被检索，从而促进模态间的信息交换和更好的多模态特征共享。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 尽管这些令牌最终会被丢弃，但它们在中间层起到了重要的信息桥梁作用，提升了模型的表现力。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdaptive Layer Normalization (AdaLN) 条件化：\u003c/strong\u003e UWM 使用 AdaLN [33] 来将扩散时间步长（以及图像特征）条件化到 Transformer 的每个块中。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e AdaLN 是一种有效的条件化机制，它通过调整层归一化 (Layer Normalization) 的参数（缩放和偏移）来注入条件信息。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 这种方式允许模型以一种非侵入性且高效的方式将时间步长信息融入到 Transformer 的处理流程中，有助于模型更好地理解噪声水平和生成过程。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e潜在扩散范式 (Latent Diffusion Paradigm)：\u003c/strong\u003e 对于图像扩散，UWM 采用了潜在扩散范式。它使用一个冻结的预训练 VAE (Variational AutoEncoder)（例如来自 Stable Diffusion XL [34]）将原始高分辨率图像压缩到低维的潜在空间中进行加噪和去噪，最后再解码回全尺寸图像。\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e原因：\u003c/strong\u003e 直接在高分辨率像素空间进行扩散通常计算成本高昂且效率低下。潜在扩散可以在更小的潜在空间中操作，显著减少计算量。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e效果：\u003c/strong\u003e 提高了图像处理的效率，同时保持了高质量的图像生成能力。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这些特殊设计共同使得 UWM 能够有效地应对多模态机器人学习的挑战，实现了一个能够从大规模异构数据中学习并灵活执行多种推理任务的强大模型。\u003c/p\u003e","title":"Bug Journal 2025-08-29"},{"content":"总结 目前在 Benchmark 1: SQA3D上 GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models 效果最好。达到了 Exact Maching 62.4% 的准确率。 在Benchmark 2: ScanQA上 Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (AAAI 2024) 效果最好。达到了 Exact Maching 31.29% 的准确率。\n动机 想看看VLA / 空间推理VLM 发展得怎么样，SOTA是什么\n调研方式 对于这个Task,我打算从CVPR 2025入手，看看最新的VLA都是如何实现的，又是如何比较的\nPlan 1: 首先，我会先寻找一下CVPR 2025中和VLA有关的任务，并且看看他们的表现 Paper 1: DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering 链接 From 中大 HCP\n结果： 结果一: 这张图用的是 这个benchmark (SQA3D)\n结果二：\n这张图用的是 这个benchmark (ScanQA)\nPaper 2: LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning (CVPR 2024)\n链接\nFrom Fudan, Tencent, and National University of Singapore\n结果 这张图用的是 这个benchmark (ScanQA) 如果横向对比这两个模型，其实有些数据还是这个模型高一些\nPaper 3: Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (AAAI 2024)\n链接\nFrom PKU\n结果 这张图用的是 这个benchmark (ScanQA) 和 这个benchmark (SQA3D)\n如果我们比较这一篇和第一篇CVPR2025 我们会发现，实际上这篇效果更好\nPaper 4: SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding (ECCV 2024)\n链接\nFrom BIGAI, Beijing\n结果 可以看到效果没那么理想\nPaper 5: Scene-LLM: Extending Language Model for 3D Visual Reasoning (WACV 2025) 链接\nFrom Brown University \u0026amp; Meta\n结果 可以看到这个模型在SQA3D的表现比其他模型更好，达到了54.2%\nPaper 6: Unifying 3D Vision-Language Understanding via Promptable Queries (ECCV 2024)\nFrom BIGAI, Beijing\n链接\n结果 把这个模型放在这里的原因是：这个模型的 MENTOR 和 CIDEr matrix 的表现都比之前的模型好\n注：这个模型和 Paper 4 是同一个组做的\nPaper 7: Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers (NIPS 2024)\nFrom 浙大, 上海AI lab, and 字节\n链接\n结果 这个模型比较了之前的SOTA, 结果SQA3D提高了0.4%的准确率。。。\nPaper 8: GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models (arxiv preprint March 2025)\n链接\nFrom HKU \u0026amp; 上海AI lab\n结果： 这个模型在SQA3D上做到了62.4%的准确率\nPaper 9: Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding (CVPR 2025)\n链接\nFrom The Chinese University of Hong Kong\n结果 这个模型在ScanQA的CIDEr上做到了SOTA\nPaper 10 (To be continued): 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding (arxiv preprint July 2025)\nFrom Shanghai University of Engineering Science \u0026amp; PKU\n链接\n结果： Report了非常奇怪的结果，需要进一步细看\nBenchmark 1: SQA3D SQA3D\n上图是一个询问的例子\n这个图解释了为什么有些方法会用 \u0026ldquo;Which\u0026rdquo; \u0026ldquo;How\u0026rdquo; 这些来分类。 总结来说，以这几个词开头的问句较多，且方向不同\n最后统计的是准确率(按照文章中的说法，居然是做一个706维度的分类？) 需要和Ground Truth完全一致\nBenchmark 2: ScanQA ScanQA\n上图是一个询问的例子\n同样是准确率 需要和Ground Truth完全一致\n和上面不同，这里也加入了其他metric去算答案和Ground Truth的相似性\n","permalink":"https://tzj2006.github.io/bugjournal/2025-08-25/","summary":"SOTA VLA","title":"Bug Journal 2025-08-25"},{"content":"Hi, 你好，我的读者。\n好久不见了，有没有想我呀(╹▽╹)\n上次见面的时候还是7月16号，已经隔了快一个月了呢\n这个月其实也发生了许多事情，但最后我发现，原来，有时候就是遵从自己的内心也是煎熬的。\n我现在开始相信本我、自我和超我了，本我就是最原本的“内心\u0026quot;，而超我则是最原本的”幻想\u0026quot;\n他们两个就像是两个截然不同的 loss function 一样，不断对抗着，希望我朝着某一个方向前进\n而这个 loss function 的正则项，\n","permalink":"https://tzj2006.github.io/bugjournal/2025-08-08/","summary":"\u003cp\u003eHi, 你好，我的读者。\u003c/p\u003e\n\u003cp\u003e好久不见了，有没有想我呀(\u003cem\u003e╹▽╹\u003c/em\u003e)\u003c/p\u003e\n\u003cp\u003e上次见面的时候还是7月16号，已经隔了快一个月了呢\u003c/p\u003e\n\u003cp\u003e这个月其实也发生了许多事情，但最后我发现，原来，有时候就是遵从自己的内心也是煎熬的。\u003c/p\u003e\n\u003cp\u003e我现在开始相信本我、自我和超我了，本我就是最原本的“内心\u0026quot;，而超我则是最原本的”幻想\u0026quot;\u003c/p\u003e\n\u003cp\u003e他们两个就像是两个截然不同的 loss function 一样，不断对抗着，希望我朝着某一个方向前进\u003c/p\u003e\n\u003cp\u003e而这个 loss function 的正则项，\u003c/p\u003e","title":"Bug Journal 2025-08-08"},{"content":"现在遇到了如图所示的这个问题：\n“现阶段的VLA本质上就是数据量不足，就像你说VLA让他去开车，根本不可能，神经网络没有见过这样的数据就是不理解，现阶段还是要搞数据工程，逼近scaling law”\n“确实，只要数据够多就没有out of distribution了！”\n而对于 lifelong task 来说，这种情况更是一个问题：\n“为什么不直接加数据训练？而是要通过 lifelong 持续的学习”\n记录一下我现在的想法\n首先是持续学习\n这个有什么好处呢，好处就是，这样的模型可以不用在最开始的时候就学会所有东西\n而是可以等到之前的东西学完之后然后再继续学下一个 task\n什么意思呢，就是说，我之前学习到了一个 distribution 下的所有的内容\n我让机械臂学会了这些知识\n但是呢，我现在遇到了一些 OOD (Out of distribution) 的 task\n这种情况怎么办呢？ 现在的 model 就直接束手无策，束手就擒了\n但是 lifelong learning 的 task 就可以在这种情况下，在不遗忘之前学习的 task 的情况下学习到 OOD 的 task\n这样这种模型的拓展能力就会更好\n可是。。。\n如前文所说，只要数据够多就没有out of distribution了\n而在 lifelong setting 的情境下，本来就要有数据训练，那为什么不直接 finetune 原本的模型呢\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-16/","summary":"\u003cp\u003e现在遇到了如图所示的这个问题：\u003c/p\u003e\n\u003cp\u003e“现阶段的VLA本质上就是数据量不足，就像你说VLA让他去开车，根本不可能，神经网络没有见过这样的数据就是不理解，现阶段还是要搞数据工程，逼近scaling law”\u003c/p\u003e\n\u003cp\u003e“确实，只要数据够多就没有out of distribution了！”\u003c/p\u003e\n\u003cp\u003e而对于 lifelong task 来说，这种情况更是一个问题：\u003c/p\u003e\n\u003cp\u003e“为什么不直接加数据训练？而是要通过 lifelong 持续的学习”\u003c/p\u003e\n\u003cp\u003e记录一下我现在的想法\u003c/p\u003e\n\u003cp\u003e首先是持续学习\u003c/p\u003e\n\u003cp\u003e这个有什么好处呢，好处就是，这样的模型可以不用在最开始的时候就学会所有东西\u003c/p\u003e\n\u003cp\u003e而是可以等到之前的东西学完之后然后再继续学下一个 task\u003c/p\u003e\n\u003cp\u003e什么意思呢，就是说，我之前学习到了一个 distribution 下的所有的内容\u003c/p\u003e\n\u003cp\u003e我让机械臂学会了这些知识\u003c/p\u003e\n\u003cp\u003e但是呢，我现在遇到了一些 OOD (Out of distribution) 的 task\u003c/p\u003e\n\u003cp\u003e这种情况怎么办呢？ 现在的 model 就直接束手无策，束手就擒了\u003c/p\u003e\n\u003cp\u003e但是 lifelong learning 的 task 就可以在这种情况下，在不遗忘之前学习的 task 的情况下学习到 OOD 的 task\u003c/p\u003e\n\u003cp\u003e这样这种模型的拓展能力就会更好\u003c/p\u003e\n\u003cp\u003e可是。。。\u003c/p\u003e\n\u003cp\u003e如前文所说，只要数据够多就没有out of distribution了\u003c/p\u003e\n\u003cp\u003e而在 lifelong setting 的情境下，本来就要有数据训练，那为什么不直接 finetune 原本的模型呢\u003c/p\u003e","title":"Bug Journal 2025-07-16"},{"content":"LoRA finetuning 之前一直听说 LoRA 的大名，今天来看看 LoRA 到底在做什么\n正好我看到了 huggingface 上有 LoRA tutorial 所以就过来研究了一下🧐\n注：代码顺序与 tutorial中略有出入\n省流：想看 LoRA specific 的同学请直接从[这里开始看](#Step 5: 训练前准备)\nStep1: 导入需要用到的 package import transformers import accelerate import torch import peft print(f\u0026#34;transformers version: {transformers.__version__}\u0026#34;) print(f\u0026#34;accelerate version: {accelerate.__version__}\u0026#34;) print(f\u0026#34;torch version: {torch.__version__}\u0026#34;) print(f\u0026#34;peft version: {peft.__version__}\u0026#34;) 需要导入的 package 大概就这么几个，首先是 transformers, accelerate, torch 和 peft\n今天这个 LoRA 主要就是用的 peft package.\n至于 accelerate, 就是用来加速 peft 的\nStep2: 数据集加载 # 接下来加载数据集 # 这里我们使用的是 Hugging Face 的 datasets 库来加载 Food101 数据集 from datasets import load_dataset # dataset = load_dataset(\u0026#34;food101\u0026#34;, split=\u0026#34;train[:5000]\u0026#34;) # 只加载前5000个样本以加快速度 dataset = load_dataset(\u0026#34;food101\u0026#34;, split=\u0026#34;train+validation\u0026#34;) # 加载整个数据集 from datasets import Image as DatasetsImage dataset = dataset.cast_column(\u0026#34;image\u0026#34;, DatasetsImage(decode=False)) # 然后把标签拿出来 # 这里 huggingface 要求一个 labeltoid \u0026amp; idtolabel # 所以这里要弄两个 map 互相 map labels = dataset.features[\u0026#34;label\u0026#34;].names label2id, id2label = dict(), dict() for i, label in enumerate(labels): label2id[label] = i id2label[i] = label 这里demo中用的是Food101数据集 (老演员了[doge]),\n但是这个作为 classification 的数据集其实不太好（因为食物的种类非常非常多\n但是作为LoRA刚刚好，因为LoRA就是专门针对这个事情的\n加载这个部分倒是没什么好说的\n最后这个 .cast_column 是后来发现这个数据集存的是二进制文件，所以不能 decode (by ChatGPT)\nStep3: 加载预训练模型 这个很重要！！, 不要到时候跑起来了发现没有导入预训练模型\n注：如果没有加载预训练模型，不会有任何 Warning or Error，什么都可以正常跑，但是跑到最后才会发现出问题\n# 接下来我们需要加载一个预训练的模型和图像处理器 # 这里我们使用的是 Hugging Face 的 transformers 库来加载一个 ViT 模型 # 以及一个图像处理器（image processor）来处理输入的图像 from transformers import AutoImageProcessor model_checkpoint = \u0026#34;google/vit-base-patch16-224-in21k\u0026#34; image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, use_fast=True) # 接下来我们加载一个预训练的模型 # 这里我们使用的是 Hugging Face 的 transformers 库来加载一个 ViT 模型 from transformers import AutoModelForImageClassification, TrainingArguments, Trainer model = AutoModelForImageClassification.from_pretrained( model_checkpoint, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True, # provide this in case you\u0026#39;re planning to fine-tune an already fine-tuned checkpoint ) 这里 tutorial 中输入的是 google 的 VIT 模型\n唯一要注意的是这里的 use_fast=True,\n加上之后据说图片的导入能快10x to 30x\nStep4: 加载 Dataset # 导入一些常用的图像处理函数 from torchvision.transforms import ( CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, Resize, ToTensor, ) # 接下来我们做一些预处理 # 首先是normalize normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std) # 然后是train和validation的图像变换 train_transforms = Compose( [ RandomResizedCrop(image_processor.size[\u0026#34;height\u0026#34;]), RandomHorizontalFlip(), ToTensor(), normalize, ] ) # validation也是一样的处理 val_transforms = Compose( [ Resize(image_processor.size[\u0026#34;height\u0026#34;]), CenterCrop(image_processor.size[\u0026#34;height\u0026#34;]), ToTensor(), normalize, ] ) # 然后写一个函数 process dataset def preprocess_train(example_batch): \u0026#34;\u0026#34;\u0026#34;Apply train_transforms across a batch.\u0026#34;\u0026#34;\u0026#34; example_batch[\u0026#34;pixel_values\u0026#34;] = [ train_transforms(Image.open(io.BytesIO(image[\u0026#34;bytes\u0026#34;])).convert(\u0026#34;RGB\u0026#34;)) for image in example_batch[\u0026#34;image\u0026#34;] ] return example_batch # validation也是一样的处理 def preprocess_val(example_batch): \u0026#34;\u0026#34;\u0026#34;Apply val_transforms across a batch.\u0026#34;\u0026#34;\u0026#34; example_batch[\u0026#34;pixel_values\u0026#34;] = [ val_transforms(Image.open(io.BytesIO(image[\u0026#34;bytes\u0026#34;])).convert(\u0026#34;RGB\u0026#34;)) for image in example_batch[\u0026#34;image\u0026#34;] ] return example_batch # 接下来我们对数据集进行预处理, 拆分成训练集和验证集 splits = dataset.train_test_split(test_size=0.1) train_ds = splits[\u0026#34;train\u0026#34;] val_ds = splits[\u0026#34;test\u0026#34;] train_ds = train_ds.cast_column(\u0026#34;image\u0026#34;, DatasetsImage(decode=False)) val_ds = val_ds.cast_column(\u0026#34;image\u0026#34;, DatasetsImage(decode=False)) # 然后transform一下 train_ds.set_transform(preprocess_train) val_ds.set_transform(preprocess_val) 这里和其他的模型也没有区别，都是一样的处理方式\n虽然这里看起来很长，但是其实都是 到处copy-paste 🤦‍♂️\nStep 5: 训练前准备 # 这个函数的作用是打印模型的可训练参数和总参数数量 # 这个函数是可以复用的，在这里标记一下 def print_trainable_parameters(model): trainable_params = 0 all_param = 0 for _, param in model.named_parameters(): all_param += param.numel() if param.requires_grad: trainable_params += param.numel() print( f\u0026#34;trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\u0026#34; ) # 加载config from transformers import TrainingArguments, Trainer model_name = model_checkpoint.split(\u0026#34;/\u0026#34;)[-1] batch_size = 128 args = TrainingArguments( f\u0026#34;{model_name}-finetuned-lora-food101\u0026#34;, remove_unused_columns=False, eval_strategy=\u0026#34;epoch\u0026#34;, # 注：这里在 transformers 4.46+ 版本后从 evluation_strategy 改为 eval_strategy save_strategy=\u0026#34;epoch\u0026#34;, learning_rate=5e-3, per_device_train_batch_size=batch_size, gradient_accumulation_steps=4, per_device_eval_batch_size=batch_size, fp16=True, num_train_epochs=5, logging_steps=10, load_best_model_at_end=True, metric_for_best_model=\u0026#34;accuracy\u0026#34;, push_to_hub=True, label_names=[\u0026#34;labels\u0026#34;], ) # 接下来我们设置LoRA训练参数 from peft import LoraConfig, get_peft_model config = LoraConfig( r=16, lora_alpha=16, target_modules=[\u0026#34;query\u0026#34;, \u0026#34;value\u0026#34;], # 这里的 target_modules 是指我们要对哪些模块进行 LoRA 微调 # 也就是说，我们甚至不是对整个模型的矩阵进行分解 # 而是对模型中的某些特定模块进行分解 # 这里我们选择了 query 和 value 模块 lora_dropout=0.1, bias=\u0026#34;none\u0026#34;, modules_to_save=[\u0026#34;classifier\u0026#34;], # 这里的 modules_to_save 是指我们要保存哪些模块的参数 # 也就是说，我们只保存分类器的参数 # 这样的话，需要训练的参数就会进一步减少 ) import numpy as np import evaluate metric = evaluate.load(\u0026#34;accuracy\u0026#34;) # 定义计算指标的函数 def compute_metrics(eval_pred): \u0026#34;\u0026#34;\u0026#34;Computes accuracy on a batch of predictions\u0026#34;\u0026#34;\u0026#34; predictions = np.argmax(eval_pred.predictions, axis=1) return metric.compute(predictions=predictions, references=eval_pred.label_ids) # 这个函数是huggingface transformers 中和 pytorch dataloader __getitem__ 方法对应的函数 # 它的作用是将数据集中的每个样本转换为模型可以接受的格式 # 这里我们将图像转换为 pixel_values，并将标签转换为 label def collate_fn(examples): pixel_values = torch.stack([example[\u0026#34;pixel_values\u0026#34;] for example in examples]) labels = torch.tensor([example[\u0026#34;label\u0026#34;] for example in examples]) return {\u0026#34;pixel_values\u0026#34;: pixel_values, \u0026#34;labels\u0026#34;: labels} 训练前准备包括打印参数的辅助函数，以及所有的 hyper_parameters\n还有 evaluation matrix 和 collate_fn 函数\n在这里，这些都是另外定义的，而不是像 pytorch 那样，定义在 class 里的\nStep 6: 定义模型 from transformers import AutoModelForImageClassification, TrainingArguments, Trainer # 首先定义原本的pretrain model model = AutoModelForImageClassification.from_pretrained( model_checkpoint, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True, # provide this in case you\u0026#39;re planning to fine-tune an already fine-tuned checkpoint ) # 打印模型的可训练参数和总参数数量 print_trainable_parameters(model) lora_model = get_peft_model(model, config) print_trainable_parameters(lora_model) 这一步就是把前面的config导入一下，\nTransformer is all you need\nTransformer库中的 AutoModel 会帮你完成一切的\nStep 7: 模型训练 # ok, 现在我们创建一个 Trainer 对象，然后就可以开始训练了 trainer = Trainer( lora_model, args, train_dataset=train_ds, eval_dataset=val_ds, tokenizer=image_processor, compute_metrics=compute_metrics, data_collator=collate_fn, ) # 但是先不着急，我们先看看大模型 zero-shot 的效果如何 print(\u0026#34;Evaluating the base model without LoRA...\u0026#34;) zero_shot_results = trainer.evaluate(val_ds) print(f\u0026#34;Zero-shot accuracy: \u0026#34;, zero_shot_results) with open(\u0026#34;results.txt\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(f\u0026#34;Zero-shot accuracy: {zero_shot_results}\u0026#34;) print(\u0026#34;Training the model with LoRA...\u0026#34;) train_results = trainer.train() print(\u0026#34;Evaluating the LoRA model...\u0026#34;) LoRA_results = trainer.evaluate(val_ds) print(f\u0026#34;LoRA accuracy: \u0026#34;, LoRA_results) with open(\u0026#34;results.txt\u0026#34;, \u0026#34;a\u0026#34;) as f: f.write(f\u0026#34;LoRA accuracy: {LoRA_results}\u0026#34;) 最后就是训练的部分了\n我们只需把 model, datasets, criteria_matrix 和 tokenizer 输入进去就 ok 了\n剩下的 .fit() 和 .evaluate() 会帮我们完成\n最后我们就可以输出结果啦♪\nStep 8: Results origianl model trainable params: 85876325 || all params: 85876325 || trainable%: 100.00 LoRA trainable params: 667493 || all params: 86543818 || trainable%: 0.77 可以看到这里 LoRA 只会训练 0.77% 的参数\nZero-shot accuracy: {\u0026#39;eval_loss\u0026#39;: 4.615139007568359, \u0026#39;eval_model_preparation_time\u0026#39;: 0.0094, \u0026#39;eval_accuracy\u0026#39;: 0.01287128712871287, \u0026#39;eval_runtime\u0026#39;: 68.1035, \u0026#39;eval_samples_per_second\u0026#39;: 148.304, \u0026#39;eval_steps_per_second\u0026#39;: 1.16} LoRA accuracy: {\u0026#39;eval_loss\u0026#39;: 0.4589368402957916, \u0026#39;eval_model_preparation_time\u0026#39;: 0.0094, \u0026#39;eval_accuracy\u0026#39;: 0.8781188118811881, \u0026#39;eval_runtime\u0026#39;: 60.1694, \u0026#39;eval_samples_per_second\u0026#39;: 167.859, \u0026#39;eval_steps_per_second\u0026#39;: 1.313, \u0026#39;epoch\u0026#39;: 5.0} 但是 LoRA 的效果还是挺好的，准确率从 1.3% -\u0026gt; 87.8%, 提升还是非常明显的\n我这里一共训练了一个小时左右。\npytorch lightning 版本 然后，我看 openpi_pytorch 在用 pytorch lightning, 说是懒人最爱的 AI package, 我就想试试\n省流 总结，pytorch lightning trainer 爆杀了 huggingface trainer\n那么，代价是什么呢\n你需要多写一个 module class 和 dataset class\n甚至 20 行就能搞定\nStep 9: pytorch lightning dataset class 首先，在刚才的代码的基础上，我们先定义一个 dataset:\n# 这里定义一下 Dataset class Food101DataModule(pl.LightningDataModule): def __init__(self, train_ds, val_ds, collate_fn, batch_size): super().__init__() self.train_ds = train_ds self.val_ds = val_ds # collate_fn 相当于__get_item__ self.collate_fn = collate_fn self.batch_size = batch_size def train_dataloader(self): return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn, num_workers=64, pin_memory=True, prefetch_factor=4) def val_dataloader(self): return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn, num_workers=64, pin_memory=True, prefetch_factor=4) data_module = Food101DataModule( train_ds=train_ds, val_ds=val_ds, collate_fn=collate_fn, batch_size=args.per_device_train_batch_size, ) 和之前一样，对吧♪\n只不过现在要多写一个 class 而已，其他的都没有变哦♪\nStep 10: pytorch lightning model 然后，我们只需要再写一个 pytorch lightning model 就可以了\n# 定义一下 PyTorch Lightning 模型 # 注：self.log的部分是 wandb 的内容，用于导出log的 class Food101Module(pl.LightningModule): def __init__(self, model, learning_rate, metric, collate_fn): super().__init__() self.model = model self.lr = learning_rate self.metric = metric self.collate_fn = collate_fn # PyTorch Lightning Module 的 forward 方法 def forward(self, pixel_values): # Huggingface 的模型输出的时候会返回一个字典，包含 logits 和其他信息 # 这里我们只需要 logits return self.model(pixel_values).logits def training_step(self, batch, batch_idx): pixel_values = batch[\u0026#34;pixel_values\u0026#34;] labels = batch[\u0026#34;labels\u0026#34;] logits = self(pixel_values) # 这一句话等价于 logits=self.forward(pixel_values) loss = self.model.compute_loss(labels, logits) if hasattr(self.model, \u0026#34;compute_loss\u0026#34;) else F.cross_entropy(logits, labels) # self.log(\u0026#34;train_loss\u0026#34;, loss, on_step=True, on_epoch=True) return loss def validation_step(self, batch, batch_idx): pixel_values = batch[\u0026#34;pixel_values\u0026#34;] labels = batch[\u0026#34;labels\u0026#34;] logits = self(pixel_values) # also compute and log validation loss loss = F.cross_entropy(logits, labels) # self.log(\u0026#34;val_loss\u0026#34;, loss, on_epoch=True) preds = torch.argmax(logits, dim=1) acc = self.metric.compute(predictions=preds, references=labels)[\u0026#34;accuracy\u0026#34;] # self.log(\u0026#34;val_acc\u0026#34;, acc, on_epoch=True) def configure_optimizers(self): return torch.optim.AdamW(self.parameters(), lr=self.lr) lightning_module = Food101Module( model=lora_model, learning_rate=args.learning_rate, metric=metric, collate_fn=collate_fn, ) 也和之前一样，不过是要定义一下 forward 和 optimizer 而已\n我认为唯一要注意的点就是：huggingface dataset 会返回的不止是 logits, 还有别的信息\n所以 forward return 的时候只需返回 .logits 即可\nStep 11: train and validation pl_trainer.fit(lightning_module, datamodule=data_module) res = pl_trainer.validate(lightning_module, datamodule=data_module) 没有什么好说的，就是 .fit 和 .validate\nStep 12: 结果 对此，我只能说:\n遥遥领先\n只用了 200 秒钟就完成了1个 epoch + validation\n差不多是hugging face的4.5倍速度。\n只不过呢，像 wandb这些都要自己手动加，\n而 huggingface 就会自己 push 到所有检测到的 loggers\n总结 测试了一下 pytorch lightning 和 huggingface 的 LoRA 我发现 pytorch lightning 在 CPU IO 瓶颈的情况下的表现显著比 huggingface trainer 要好\n$$ \\text{huggingface train 1 epoch} \\approx 12 min $$\n$$ \\text{pytorch lightning train 1 epoch} \\approx 2 min $$\n$$ \\text{pytorch lightning validation} \\approx 8-10 s $$\n$$ \\text{huggingface validation} \\approx 60 s $$\n最后 validation 的结果都是差不多的,但是 pytorch lightning 就是比 huggingface tutorial 要快好几倍\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-11/","summary":"LoRA tutorial","title":"Bug Journal 2025-07-11"},{"content":"Huggingface Dataset 使用说明 load_dataset 函数 详情请见：Hugging Face 页面\nSplit参数 Hugging Face 的 load_dataset(..., split=...) 参数非常强大，下面是详细说明：\nsplit=\u0026quot;train\u0026quot;\n加载整个训练集。\nsplit=[\u0026quot;train\u0026quot;,\u0026quot;test\u0026quot;]\n返回一个元组 (train_ds, test_ds)，分别为训练集和测试集。\nsplit=\u0026quot;train+test\u0026quot;\n将训练集和测试集合并为一个数据集。\n切片（slice）支持\n你可以对拆分集按行号或百分比进行切片：\n行号切片 ：\nsplit=\u0026quot;train[10:20]\u0026quot; → 加载训练集第 10 至 19 条记录（10 包含，20 不包含）。\n百分比切片 ：\nsplit=\u0026quot;train[:10%]\u0026quot; → 前 10% 数据\nsplit=\u0026quot;train[-20%:]\u0026quot; → 最后 20% 数据\nsplit=\u0026quot;train[10%:20%]\u0026quot; → 从第 10% 开始到第 20% 结束\n组合切片 ：\nsplit=\u0026quot;train[:10%]+train[-80%:]\u0026quot; → 取前 10% 和最后 80% 数据拼接\n高级用法：交叉验证切片\nsplits = [ f\u0026#34;train[{k}%:{k+10}%]\u0026#34; for k in range(0,100,10) ] datasets = load_dataset(\u0026#39;bookcorpus\u0026#39;, split=splits) 生成 10 个 10% 验证切片，方便交叉验证\n更加高级的用法：ReadInstruction API\n更结构化方式定义切片，用于程序化构建：\nsplits = [ f\u0026#34;train[{k}%:{k+10}%]\u0026#34; for k in range(0,100,10) ] datasets = load_dataset(\u0026#39;bookcorpus\u0026#39;, split=splits) 类似作用，但写法更灵活。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-09/","summary":"\u003ch2 id=\"huggingface-dataset-使用说明\"\u003eHuggingface Dataset 使用说明\u003c/h2\u003e\n\u003ch3 id=\"load_dataset-函数\"\u003eload_dataset 函数\u003c/h3\u003e\n\u003cp\u003e详情请见：\u003ca href=\"https://huggingface.co/docs/datasets/v4.0.0/loading#slice-splits\"\u003eHugging Face 页面\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"split参数\"\u003eSplit参数\u003c/h4\u003e\n\u003cp\u003eHugging Face 的 \u003ccode\u003eload_dataset(..., split=...)\u003c/code\u003e 参数非常强大，下面是详细说明：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ccode\u003esplit=\u0026quot;train\u0026quot;\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e加载整个训练集。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ccode\u003esplit=[\u0026quot;train\u0026quot;,\u0026quot;test\u0026quot;]\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e返回一个元组\u003c/strong\u003e \u003ccode\u003e(train_ds, test_ds)\u003c/code\u003e，分别为训练集和测试集。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ccode\u003esplit=\u0026quot;train+test\u0026quot;\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e将训练集和测试集合并为一个数据集。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e切片（slice）支持\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e你可以对拆分集按行号或百分比进行切片：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e行号切片\u003c/strong\u003e ：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[10:20]\u0026quot;\u003c/code\u003e → 加载训练集第 10 至 19 条记录（10 包含，20 不包含）。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e百分比切片\u003c/strong\u003e ：\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[:10%]\u0026quot;\u003c/code\u003e → 前 10% 数据\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[-20%:]\u0026quot;\u003c/code\u003e → 最后 20% 数据\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[10%:20%]\u0026quot;\u003c/code\u003e → 从第 10% 开始到第 20% 结束\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e组合切片\u003c/strong\u003e ：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esplit=\u0026quot;train[:10%]+train[-80%:]\u0026quot;\u003c/code\u003e → 取前 10% 和最后 80% 数据拼接\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003e高级用法：交叉验证切片\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esplits\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;train[{k}%:{k+10}%]\u0026#34;\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e100\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edatasets\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eload_dataset\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;bookcorpus\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003esplit\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003esplits\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e生成 10 个 10% 验证切片，方便交叉验证\u003c/strong\u003e\u003c/p\u003e","title":"Bug Journal 2025-07-09"},{"content":"Huggingface Transformers 使用说明 详情请见：Hugging Face 页面\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-10/","summary":"\u003ch2 id=\"huggingface-transformers-使用说明\"\u003eHuggingface Transformers 使用说明\u003c/h2\u003e\n\u003cp\u003e详情请见：\u003ca href=\"https://huggingface.co/docs/transformers\"\u003eHugging Face 页面\u003c/a\u003e\u003c/p\u003e","title":"Bug Journal 2025-07-10"},{"content":"Torchvision 使用说明 详情请见：Torchvision 页面\ntorchvision.transforms Compose 这个函数的作用就是把几个 transforms 连到一起，有点像 nn.Sequential 那样\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-08/","summary":"\u003ch2 id=\"torchvision-使用说明\"\u003eTorchvision 使用说明\u003c/h2\u003e\n\u003cp\u003e详情请见：\u003ca href=\"https://docs.pytorch.org/vision/main/auto_examples/index.html\"\u003eTorchvision 页面\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"torchvisiontransforms\"\u003etorchvision.transforms\u003c/h3\u003e\n\u003ch4 id=\"compose\"\u003eCompose\u003c/h4\u003e\n\u003cp\u003e这个函数的作用就是把几个 transforms 连到一起，有点像 nn.Sequential 那样\u003c/p\u003e","title":"Bug Journal 2025-07-08"},{"content":"我现在的思路是这样的： 之前我们发现了 Libero 作为 benchmark 的一些可能的不合理的点\n数据太少 任务简单 没有长程任务 以及 DMPEL 作为 lifelong task 的一些可能的不合理的点\nlifelone task 的定义可能太过狭隘 (只在训练过的 task 上能够表现好) 方法可能聚焦于如何把 LIBERO 数据集的榜刷高，但是场景变化一点点就无法解决问题 所以我现在想做的是： 用一些 数字/分数 来证明我的猜想是成立的 比如：\n在场景变换的时候 DMPEL 成功率的下降 当任务变复杂的时候 DMPEL 成功率的下降 现在我已经做的是：\n对于开柜子的任务，无论是柜子移动，旋转，还是更换语义相同但是词语不同的句子，DMPEL 的成功率都会骤降 (100% -\u0026gt; 5%) 但是，开柜子的 task 可能过难了， 所以我现在准备测试 pick and place 的 task 当物品移动，旋转，以及更换语义相同但是词语不同的句子的时候 DMPEL 的成功率会下降多少\n","permalink":"https://tzj2006.github.io/bugjournal/2025-07-07/","summary":"\u003cp\u003e我现在的思路是这样的：\n之前我们发现了 Libero 作为 benchmark 的一些可能的不合理的点\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e数据太少\u003c/li\u003e\n\u003cli\u003e任务简单\u003c/li\u003e\n\u003cli\u003e没有长程任务\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e以及 DMPEL 作为 lifelong task 的一些可能的不合理的点\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003elifelone task 的定义可能太过狭隘 (只在训练过的 task 上能够表现好)\u003c/li\u003e\n\u003cli\u003e方法可能聚焦于如何把 LIBERO 数据集的榜刷高，但是场景变化一点点就无法解决问题\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e所以我现在想做的是：\n用一些 数字/分数 来证明我的猜想是成立的\n比如：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e在场景变换的时候 DMPEL 成功率的下降\u003c/li\u003e\n\u003cli\u003e当任务变复杂的时候 DMPEL 成功率的下降\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e现在我已经做的是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e对于开柜子的任务，无论是柜子移动，旋转，还是更换语义相同但是词语不同的句子，DMPEL 的成功率都会骤降 (100% -\u0026gt; 5%)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e但是，开柜子的 task 可能过难了，\n所以我现在准备测试 pick and place 的 task\n当物品移动，旋转，以及更换语义相同但是词语不同的句子的时候 DMPEL 的成功率会下降多少\u003c/p\u003e","title":"Bug Journal 2025-07-07"},{"content":"\n这是现在的成果，总之是让环境里的物品移动了位置了\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-30/","summary":"\u003cp\u003e\u003cimg alt=\"1751276858157\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-30/1751276858157.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是现在的成果，总之是让环境里的物品移动了位置了\u003c/p\u003e","title":"Bug Journal 2025-06-30"},{"content":"DMPEL \u0026amp; Libero 环境的总结和疑惑 在 DMPEL 模型 和 Libero 模拟环境的设定下，他们是这样设定他们的训练和验证集的：\n训练 train：\n首先，他们会在一个 90 条数据的大数据集上先做预训练，得到他们的 backbone 模型 然后对于4个 各 10 条数据的小数据集，他们会在这 4 个数据集上分别做训练，作为 lifelong learning training\n验证 validation：\n之后他们会对他们的训练做验证，验证方式如下： 对于第 x 个 lifelong learning task, 他们会计算训练这个 task 的时候的 10 个 epoch 的平均成功率，作为他们的成功指标 1 （希望模型学得越快越好） 他们还会计算前 x 个 task 的平均成功率，作为他们的成功指标 2 （防止模型遗忘）\n但是在这里我就产生了一个疑问🤔： 我认为这种设置并不非常合理\n他们没有对预训练的那 90 个 task 去做遗忘测试 他们的 task 的定义似乎有点太窄了，比如：都是对于“打开盒子的第一个盖子”， 盒子放在环境的左上角和左下角就算是两个 task,感觉泛化能力也不太行 在预训练的时候那 90 个 task 在设定的时候会比 lifelong task 复杂 (训练的时候一般有 3 个步骤， 但是 lifelong task 只有 1 个，比如“把碗放进抽屉并合上” VS \u0026ldquo;打开抽屉\u0026rdquo;)， 但是他们的模型在 Zero-shot lifelong task 的时候表现也很差，很少有成功的例子 在可视化结果的时候，我发现其实模型并没有理解要做什么，而是顺着之前的 task 做。 比如，上一个 task 做的是“抓起一个碗”，下一个是“打开柜子”，在一开始，机械臂会一直想去抓碗而不是开柜子 感觉 Libero 这个数据集的数据量也不是很大，难度也不是很高，(1-2 epoch 就可以让成功率 \u0026gt; 90%) 导致了像上面那样的 overfit 的情况 测试 于是我做了更多的测试：\n现在这是一个 Libero_long 的测试，任务难度会比之前更高\n目前这个任务是：“打开炉子并且把烧水壶放在炉子上”\n您的浏览器不支持 video 标签。 但是我们看到了：\n您的浏览器不支持 video 标签。 直接拿锅的机械臂\n您的浏览器不支持 video 标签。 没开炉子的机械臂\n您的浏览器不支持 video 标签。 摔倒了之后不知所措的机械臂\n之后我们再来看看所谓成功的 task\n您的浏览器不支持 video 标签。 放上去就是成功（\n最后，是真正成功的 task\n您的浏览器不支持 video 标签。 接下来该做的方向：\n首先，我认为 Libero 的设定不太合理\n所以我们不去刷 Libero 的榜\n我猜测已有模型的泛化能力不行，benchmark设置的不合理\n所以我要通过实验依据作证清楚，然后提出更合理的设置和新的评估benchmark\n最后在此基础上研究我的新方法\n那现在有这些方向可以走：\n如果我把任务分解和lifelong结合起来，分解primitive，直接跳出现在这些paper， 那就是很好的paper\n总结：\n这是我现在要做的事情：\n首先先研究 benchmark 设置，看看怎么样的设置能够把上面说的方向结合进来\n然后再看看现有的 method 在这些 setting 下表现如何\n最后看看我的方法比起这些方案有多少进步\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-26/","summary":"DMPEL \u0026amp; Libero","title":"Bug Journal 2025-06-26"},{"content":"Libero dataset 的可视化 发现了一个讲 Libero 讲得很详细的 blog 强烈推荐阅读这篇 blog！\n一共有 9 讲，9 讲的链接全部在这一讲中写了，直接点感兴趣的讲看就 ok 了\n但是，纸上得来终觉浅，绝知此事要躬行 有一些部分可能因为环境的不同不能适应所有情况，比如 headless server (没有显示输出的 server) 所以下面我会根据我的测试写针对 headless server 的 Libero dataset 可视化\n我的 Libero 测试报告 首先，第一个要注意的点就是：.libero 文件夹在 主目录下 (我认为此处说 根目录 下更准确) 也就是在 ~/ 下, 而不是 clone libero 下来的主目录。\n剩下的就很丝滑，所有的测试都平平无奇的通过了\n直到\u0026hellip;\n真正开始运行 Robosuit Demo 的时候，我遇到了困难：\n在运行完这段代码之后 python 就会直接崩溃：\nenv = suite.make( env_name=\u0026#34;Lift\u0026#34;, # try with other tasks like \u0026#34;Stack\u0026#34; and \u0026#34;Door\u0026#34; robots=\u0026#34;Panda\u0026#34;, # try with other robots like \u0026#34;Sawyer\u0026#34; and \u0026#34;Jaco\u0026#34; has_renderer=True, has_offscreen_renderer=False, use_camera_obs=False, ) 这是因为我正在用的服务器上并没有可以 render 的设备， 没法打开一个新窗口，所以就会直接崩溃\n因此这里要改成这样：\n# %% import numpy as np import robosuite as suite # MuJoCo gets imported *inside* robosuite # Camera frames are returned in the observation dict because we pass use_camera_obs=True env = suite.make( env_name=\u0026#34;Lift\u0026#34;, robots=\u0026#34;Panda\u0026#34;, has_renderer=False, # no on-screen window has_offscreen_renderer=True, # build a single off-screen context use_camera_obs=True, # include images in obs dict camera_names=(\u0026#34;frontview\u0026#34;,), # at least one camera is required when use_camera_obs=True camera_heights=480, camera_widths=640, ) obs = env.reset() # robosuite allocates context here # The observation dict now has an RGB image rgb = obs[\u0026#34;frontview_image\u0026#34;] # shape (480, 640, 3), dtype uint8 print(\u0026#34;Initial RGB image shape:\u0026#34;, rgb.shape) 下一步就是模拟机械臂操作，一共随机 1000 步：\n# %% frames = [] for _ in range(1000): a = 0.1 * np.random.randn(env.robots[0].dof) # small random torques obs, reward, done, info = env.step(a) frames.append(obs[\u0026#34;frontview_image\u0026#34;]) frames = [np.rot90(frame, k=2) for frame in frames] print(f\u0026#34;Collected {len(frames)} frames\u0026#34;) 但是从这里采集出来的图片是上下颠倒的\n这是因为 Mujoco 和 numpy 坐标系的原点位置不一样\n所以我们还得对这个 frame 做一个 180° 的颠倒\n现在，我们就可以输出我们的结果了：\n比如这是第一帧：\nimport matplotlib.pyplot as plt plt.imshow(frames[0]) plt.axis(\u0026#34;off\u0026#34;) 那至于我们要如何把这些帧变成一个视频呢？\n我们可以通过 imageio 这个 package 来实现\nimport numpy as np import imageio # \u0026#39;frames\u0026#39; is your list of (480, 640, 3) uint8 RGB arrays collected earlier # e.g., frames = [obs[\u0026#34;frontview_image\u0026#34;] for _ in range(1000)] # Create an MP4 writer at 120 fps writer = imageio.get_writer(\u0026#34;simulation.mp4\u0026#34;, fps=120) # wraps ffmpeg :contentReference[oaicite:6]{index=6} # Append each frame for frame in frames: writer.append_data(frame) # send numpy array to ffmpeg :contentReference[oaicite:7]{index=7} # Finalize and close the file writer.close() # flushes buffers and writes trailer :contentReference[oaicite:8]{index=8} 您的浏览器不支持 video 标签。 最终，我们通过 moviepy 来实现 jupyter notebook 中播放视频的效果：\nfrom moviepy.editor import VideoFileClip # 载入前面生成的 simulation.mp4 clip = VideoFileClip(\u0026#34;simulation.mp4\u0026#34;) print(\u0026#34;原始视频时长：\u0026#34;, clip.duration, \u0026#34;秒；分辨率：\u0026#34;, clip.size) clip.ipython_display(width=640) 在 DMPEL 中的可视化 那么至此我们可以发现：其实想做一个可视化非常的简单\n只需要找到创建 env 的地方，每一次出现 env.step 的时候我们就记录一下就可以了\n比如上面是\nobs, reward, done, info = env.step(a) frames.append(obs[\u0026#34;frontview_image\u0026#34;]) 那我们就也学着这里这样做就行了\n不过呢，有一个小区别：一个是这里是一个并行的环境\n更具体地说：为了保证 CPU 和 GPU 的利用率，DMPEL 设置了 20 个环境并行运行\n那么我们就要根据环境的不同设置不同的输出路径\n周游这个区别而已。\n另外呢，因为我想看一下这个 模型到底是在什么时候成功，又是在什么时候失败\n所以对于所有成功和失败的 task,我会分开两个文件夹存储\n好的，现在我们就完成了 DMPEL 的可视化\n现在，我们获得了一些结果：\n您的浏览器不支持 video 标签。 ❌ 完全打不开柜子的1号机械臂\n您的浏览器不支持 video 标签。 ✅ 犹犹豫豫打开正确柜子的2号机械臂\n您的浏览器不支持 video 标签。 ❌ 打开错误柜子的3号机械臂\n您的浏览器不支持 video 标签。 ✅ 干脆利落打开正确柜子的4号机械臂\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-25/","summary":"\u003ch3 id=\"libero-dataset-的可视化\"\u003eLibero dataset 的可视化\u003c/h3\u003e\n\u003ch4 id=\"发现了一个讲-libero-讲得很详细的-blog\"\u003e发现了一个讲 Libero 讲得很详细的 blog\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://blog.csdn.net/weixin_53610475/article/details/136421802\"\u003e强烈推荐阅读这篇 blog！\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e一共有 9 讲，9 讲的链接全部在这一讲中写了，直接点感兴趣的讲看就 ok 了\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e但是，纸上得来终觉浅，绝知此事要躬行\n有一些部分可能因为环境的不同不能适应所有情况，比如 headless server (没有显示输出的 server)\n所以下面我会根据我的测试写针对 headless server 的 Libero dataset 可视化\u003c/p\u003e\n\u003ch4 id=\"我的-libero-测试报告\"\u003e我的 Libero 测试报告\u003c/h4\u003e\n\u003cp\u003e首先，第一个要注意的点就是：.libero 文件夹在 \u003cstrong\u003e主目录下\u003c/strong\u003e (我认为此处说 \u003cstrong\u003e根目录\u003c/strong\u003e 下更准确)\n也就是在 \u003ccode\u003e~/\u003c/code\u003e 下, 而不是 clone libero 下来的主目录。\u003c/p\u003e\n\u003cp\u003e剩下的就很丝滑，所有的测试都平平无奇的通过了\u003c/p\u003e\n\u003cp\u003e直到\u0026hellip;\u003c/p\u003e\n\u003cp\u003e真正开始运行 Robosuit Demo 的时候，我遇到了困难：\u003c/p\u003e\n\u003cp\u003e在运行完这段代码之后 python 就会直接崩溃：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eenv\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esuite\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emake\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eenv_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Lift\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e# try with other tasks like \u0026#34;Stack\u0026#34; and \u0026#34;Door\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003erobots\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Panda\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# try with other robots like \u0026#34;Sawyer\u0026#34; and \u0026#34;Jaco\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ehas_renderer\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ehas_offscreen_renderer\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003euse_camera_obs\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这是因为我正在用的服务器上并没有可以 render 的设备，\n没法打开一个新窗口，所以就会直接崩溃\u003c/p\u003e","title":"Bug Journal 2025-06-25"},{"content":"这两天做了啥： Basically, 做了这些事情：\n发了一个issue Hi DMPEL team, thanks for releasing the code! I’ve managed to run the full lifelong-learning pipeline on the libero_goal benchmark, but the success rates I obtain are lower than those reported in the paper. I’d like to confirm whether I’m missing any important tricks or recommended hyper-parameter settings.\nReproduction details\nCommand: bash exp_scripts/lifelong_scripts/dmpel.sh:\ntorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth GPU: 1 x A100 CUDA / Driver: CUDA12.0, Driver Version 525.147.05 python: 3.8.13 package: same as requirements.txt seed: 100\nMy Results\nMethod Paper’s success rate (LIBERO-Goal, 10 tasks) Re-run success rate DMPEL ≈ 28 % (Figure 5, “Lifelong” column) \u0026lt; 5 % ISCIL ≈ 23 % (same figure) \u0026lt; 5 % Additional findings\nPre-trained checkpoint sanity-check The provided multitask checkpoint matches the paper on its pre-training tasks (good success rate, most of them over 85%). Questions\nIs there a specific environment configuration (e.g., in MuJoCo or Robosuite) that significantly impacts performance? Do specific seeds, task-order curriculum strategies, or “hidden” hyperparameters (beyond what’s documented) affect performance in a meaningful way? Could you advise on recommended approaches or tools for visualizing results (e.g., exporting the robot move to .mp4 files) to diagnose where the pipeline may be underperforming? 测试了一下 DMPEL 的结果 首先，要运行他们的pipeline, 有几个地方的 code 是一定要改的：\n一个是 libero/libero/main.py 69 行 for i inrange(n_manip_tasks): 这个 for 循环里面\n要把 try 中的 get_dataset 函数的 demos parameter 删除或者注释掉\n之后在下面 90 行左右有个 manip_datasets.append(task_i_dataset) 也要移动到 try 里面去\n不然的话这个 try except 根本就没有作用，该报错还是报错\n同样，在 libero/libero/dataset.py 这个文件中的 get_dataset 函数的 demos 参数也要删除/注释掉\n因为 robomimic.utils.dataset.SequenceDataset 里面根本就没有 demos 参数。\n这样的话，这份代码就可以正常跑起来了\nP.S. 运行用的代码是这一条：\nbash exp_scripts/lifelong_scripts/dmpel.sh: i.e. torchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth 我发现，诶，怎么结果这么差？\n然后我思考了一下🤔\n认为可能是 pretrain checkpoint 有问题\n所以我就去试了一下 pretrain checkpoint\n验证方式也非常简单，就是直接拿 pretrain checkpoint 去做 Libero_90, 也就是 pretrain checkpoint 训练的时候用的数据集\n这要怎么做呢？\n很简单，只要稍微改变一下运行的命令就可以了：\ntorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_90 \\ policy=bc_foundation_dmpel_policy lifelong=dmpel \\ exp=/work/DMPEL/checkpoints/lifelong/dmpel/90 \\ pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth 没毛病吧，一下就从 Libero_goal 变回 Libero 90 了\n结果非常的 Amazing 啊，发现这个算出来还挺正常的，绝大部分的成功率都在 80% 以上\n注：其实很多点都重合在一起了，能看到的都是 outliers 🤦‍♂️\n那就说明问题一定是出在 lifelong learning 这一步了\n所以我就测试了一下 其他的 lifelong learning methods.\n结果发现，除了 ER 有一点起色之外，其他 lifelong learning methods 都是 0% 成功率\n这就表明一定是某处的设定有问题了，但是我找了一天也没发现到底是哪里有问题\n所以我就写了上面那个 issue 给 DMPEL 的团队\n最后发现竟然是 train.epoch 被设置成了 0\n所以最后的结果就是 Pretrain model 在做 zero-shot!\n在此之前，我其实做了一个实验：如果我执意让 Pretrain model 做 Zero-shot, 确实得到了一个非常相近的结果\n但是我没有怀疑到 epoch = 0 上面来，因为在测试的时候有一个点干扰了我的判断：\n在 Pretrain model Zero shot test 中, GPU 是基本没有占用的\n但是在之前的 test 中, 哪怕 train.epoch = 0 也仍然是有一个训练的 epoch 的\n只不过那个训练出来的结果并没有被拿出来 validate\n或者说就算拿出来 validate 了效果也不算好\n因此这个点干扰了我的判断。\n最终，我得到了一个和论文中相近的结果：\n在这个lifelong learning训练过程中, 显存占用不超过 20GB, 所以只要是 3090 + 级别的显卡都能运行\n最终，这两天我终于成功跑通了 DMPEL 的代码\n下一步我准备可视化他们的代码，看看哪里做得好，哪里做得不好，所以我得去研究一下 Libero 数据集了。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-24/","summary":"\u003ch3 id=\"这两天做了啥\"\u003e这两天做了啥：\u003c/h3\u003e\n\u003cp\u003eBasically, 做了这些事情：\u003c/p\u003e\n\u003ch4 id=\"发了一个issue\"\u003e发了一个issue\u003c/h4\u003e\n\u003cp\u003eHi DMPEL team, thanks for releasing the code! I’ve managed to run the full lifelong-learning pipeline on the \u003ccode\u003elibero_goal\u003c/code\u003e benchmark, but the success rates I obtain are lower than those reported in the paper. I’d like to confirm whether I’m missing any important tricks or recommended hyper-parameter settings.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eReproduction details\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCommand:\nbash exp_scripts/lifelong_scripts/dmpel.sh:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etorchrun --standalone --nproc_per_node=gpu libero/lifelong/main.py seed=100 benchmark_name=libero_goal \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        policy=bc_foundation_dmpel_policy lifelong=dmpel \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        exp=/work/DMPEL/checkpoints/lifelong/dmpel/goal \\\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        pretrain_model_path=/work/DMPEL/checkpoints/multitask_model_ep10.pth\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eGPU: 1 x A100\nCUDA / Driver: CUDA12.0, Driver Version 525.147.05\npython: 3.8.13\npackage: same as requirements.txt\nseed: 100\u003c/p\u003e","title":"Bug Journal 2025-06-24"},{"content":"WoW, 6 月 20 了呢，过得好快(✧∀✧)\n上次看到的时候还是 5 月初、5 月底，现在一下就 6 月 20 号了。\n算起来今天是来实验室的一个月了，该总结一下了。\n这个月看了很多和机器人、具身智能有关的文章，这是我目前对这个领域的总结：\n目标： 目标和做视频生成的其实差不多，都是根据一个条件(环境)生成下一部分东西(机器人 move) 这里说和视频生成差不多其实是因为视频生成的维度比文字生成要大得多，更加接近机器人 move 的维度 维度这个问题还是很恐怖的，因为机器人是在一个连续的三维空间中的，所以理论上来说有无限的自由度。 虽然说目前 FP8 的精度其实已经非常非常够用了，但你想，机器人的自由度非常高，至少有 6 - 7 DoF. 那如果每一个 DoF 都是 1000 个度量，那么一共就至少会有 $10^{18}$ 种不同的情况。 这就会让传统的 SoftMax 很难招架住了，因为这种情况得要一个 $10^{18}$ head 的 MLP 才行 所以这时候预测的值就不再是一个 softmax了，而是像 VAE 那样的一个 Distribution,只有这种 Distribution 才不会出问题 所以从目前我的角度来看，其实做机器人和做 LLM 完全就是一回事，唯二的区别是: LLM 很容易获得大量知识，许多 LLM 都能获得互联网级别的数据，而且现在的多模态技术使得几乎互联网上的所有数据都能被喂到 LLM 中。反观机器人，目前机器人在显示场景中的数据仍然非常依赖人工采集，因此数据不可能有非常多。当然，对于做学术的我来说这个不是问题，因为在学术界想要获得互联网级别的数据，和能够运行这么多数据的计算资源也是极为困难的。因此对于我们来说，其实做小数据大成果也许是一个更好的选择。 第二个区别其实就是把最后一层的 softmax 给他去掉，然后让两个 Distribution 做 loss. 学到的知识： 这是一个很大的一个板块了，但是我尽量不重不漏地把我学的全部写在这里。\n首先，目前有两个主流的机器人训练方法\n行为模仿 Behavior Cloning (下面简称 BC) 强化学习 Reinforcement Learning (下面简称 RL) 当然，也出现了这两个的有机结合，比如：先 BC 再 RL, 先 RL 再 BC, 动态分配 RL 和 BC 的比例。。。\n总之，一般来说，RL 更加难训练，BC 更加容易训练。RL 的效果上限更高，但是也更容易过拟合。\n另外，目前也有两个主流的机器人训练模型\n自回归 (Auto-regression) Diffusion policy (and/or Flow matching) 自回归理解起来比较容易，其实只要理解一个式子就行：\n$$ X_{t_i} = F(X_{t_{i-1}}, X_{t_{i-2}}, \\dots) = \\alpha_{t_{i-1}}X_{t_{i-1}} + \\alpha_{t_{i-2}}X_{t_{i-2}} + \\dots $$\n对吧，这就是自回归的意思：对于自己做回归，让所有前面的状态影响到后面的状态\n突然想起以前人们对 人生 和 DP 的总结：\n人生就像 DP, 不一定要走局部最优解，但要走全局最优解 但是我认为人生更像自回归，因为你并不能从其他状态转移过来，你只能从你之前的时刻回归到你现在的时刻。\n至于 Diffusion Policy, 他的思路也很简单：\n图片 from weijian.sun\n去马赛克。\n意思是：既然我们可以通过给一段数据不断加噪音得到一个完全噪音，那我们也可以通过一个噪音反过来还原一段数据。\n这个理念其实在很早就已经出现过了，不管是 GAN 还是 VAE 都用到了这个 Idea.\nThat is. 如果我们有一个函数从一个随机数映射到一个域中，这个域中的每一个数都是我想要的\n那每一次我就只需要随机生成一个数，经过这个函数之后就能得到我想要的东西了，有点点石成金的意思了。\n那 Diffusion 的关键是什么。其实还是数据。\n用 Diffusion 我很轻松就可以用一段数据弄出 1000 个数据，并且这 1000 个数据都是比较优质的数据：\n假设这段数据和噪声之间的差是$\\Delta$, 那每一次我只需要在这段数据上加上$\\frac{\\Delta}{1000}$不就创造了一个新的数据出来吗？\n那我现在要学的东西是：我让一个模型去预测这个$\\frac{\\Delta}{1000}$, 不就完事了吗。\n那为什么这里要设定 1000 步这么多呢，这是因为：模型在去噪的能力上其实并不是无敌的，而是有一个极限的\n这个极限在这里是马尔科夫的小方差近似。\n什么意思呢\n首先我们先回顾一下 Diffusion 在做什么\nDiffusion 的本质是在求解一个函数到另一个函数的映射\n在这里，所有的数值都是抽象的，是连续的，所以必须由一个函数来表示。\n但是计算机中的模型是离散的，所以我们要通过随机采样来近似。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-20/","summary":"Jornal","title":"Bug Journal 2025-06-20"},{"content":"今天真的是 journal 了\n因为其实没有看文章，只是在解决一些工程上的问题。\n首先是数据下载，这个下了好久才下下来\n而且下的时候还出了很多 bug, 但总之已经搞定了\n下一步就是要去把代码跑通，不过这个估计得明天才能弄了，毕竟这个数据还有 3 个小时才下完\n这个代码我看了一下他给的 pipeline,我发现：\n首先要跑一个 pretrain model 这个 model 不是 baseline,就是要 pretrain 之后让模型有个初步认知\n之后才能加 MOE 进去跑 continue learning\n我看到 hugging face 上面有他的跑好的 check point,到时候下下来看一下\n到时候有机会的话自己也跑一跑他的 pretrain 的那部分，积累一点经验。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-19/","summary":"Journal","title":"Bug Journal 2025-06-19"},{"content":"今天折腾了一天的数据下载，踩了一些坑，这里记录一下。\n这是一篇不错的笔记\n首先是从Hugging Face下载数据，目前看起来最稳定的最简单方式就是Git LFS clone.\n所以我们先来介绍一下Git LFS\n方法1：Git LFS 但是GIT LFS有时候会丢失一部分数据，不知道为什么\n比如我现在正在下载libero dataset, 但是他的 Git LFS clone 就只会下载 Libero 10, Libero Spatial, Libero Object, 和 Libero Goal, 并不会下载 Libero 100\n我认为是仓库的设置有点问题，因为 Git LFS clone 下来的数据格式和hugging face上下下来的格式有点不同。\n但是 Git LFS真的是最简单轻松的下载方式了\n只需要点击这里然后跟着操作做就完事了。\n下载的速度也比较快，基本跑满了这个服务器的代理网络。\n听说Github不能断点续传，但是实测下来几乎没有断点，真的是最稳定的那个服务了。\n方法2： hugging_face CLI huggingface-cli 是 Hugging Face 官方提供的命令行工具，自带完善的下载功能。\n但是实际用起来体验很差，经常莫名其妙就 Internet Error\n无论是使用代理还是镜像体验都不是很好。\n使用方法：\n代理：\npip install -U huggingface_hub pip install -U hf_transfer # 先下载 huggingface-cli 本体和 hf_transfer 加速插件 # hf_transfer插件真的很快，特别是在境外的服务器速度真的很快 export HF_HUB_ENABLE_HF_TRANSFER=1 # 打开 hf_transfer huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 镜像：\npip install -U huggingface_hub # 还是先安装这个 huggingface-cli export HF_ENDPOINT=https://hf-mirror.com # 这里以 hf-mirror.com 为例 # 剩下的都一样的 huggingface-cli download \u0026lt;model_name\u0026gt; --local-dir /path/to/your/folder # 下载模型请使用类似这样的命令 huggingface-cli download --repo-type dataset \u0026lt;dataset_name\u0026gt; --local-dir /path/to/your/folder # 下载数据集请用类似的命令 方法3：snapshot_download 同样是 hugging face 出品，同样的容易崩溃\n区别是这个可以在 python 中使用\n使用也很简单：\nfrom huggingface_hub import snapshot_download snapshot_download( # repo_type=\u0026#39;dataset\u0026#39;, # 这一条就看你是不是下数据的时候选择加还是不加了 repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False, # 加上这一条可以所见即所得 # 不会出现最后是个指针文件的情况 ) 方法4: hf-mirror 镜像站下载 hf-mirror镜像站 推出了 hfd, 一个 huggingface 专用下载工具，基于成熟工具 aria2，可以做到稳定高速下载不断线。\n这是 hf-mirror 网站给出的 tutorial, 方法清晰简单: 1. 下载hfd\nwget https://hf-mirror.com/hfd/hfd.sh chmod a+x hfd.sh 2. 设置环境变量\n# Linux export HF_ENDPOINT=https://hf-mirror.com or\n# Windows Powershell $env:HF_ENDPOINT = \u0026#34;https://hf-mirror.com\u0026#34; 3.1 下载模型\n./hfd.sh gpt2 3.2 下载数据集\n./hfd.sh wikitext --dataset 根据实测，速度也不赖\n就是他的这个 Copy 有点问题，要一行一行的 Copy 才可以正常运行\n或者直接从我这里copy也行\n方法5: 手动下载 当然，最后的最后还可以再网站上手动下载。\n但是对于这个分了1629个断点的数据集来说，手动下载太痛苦了。\n所以此处暂且按下不表。\nAppendix 代理开启和关闭命令：\n开启：\nexport http_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export https_proxy=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTP_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; export HTTPS_PROXY=\u0026#34;http://127.0.0.1:10808\u0026#34; 关闭：\nunset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY ","permalink":"https://tzj2006.github.io/bugjournal/2025-06-18/","summary":"Data Download","title":"Bug Journal 2025-06-18"},{"content":"对于这些任务，以下是所有任务都必须面对的问题：“灾难性遗忘” 简单来说，就是如何让模型学得又快又好？\n在这里，快是指：如何只用很少的样例 / 数据就可以让模型学到某一个任务\n好是指：如何让模型不会在学习新任务的时候忘掉旧任务\n那为什么会忘掉旧任务呢？\n其实是因为 SGD 太强了，毕竟在初始化的时候模型是一个随机化的值 而经过训练，模型就能“学会”这些动作。\n所以在学习新的任务的时候不刻意保留原本的值， 就会如同从随机模型学到一个新的动作一样，直接“遗忘掉”过去学到的知识， 变成只会新动作的机器人了\n而下面的大部分办法都是为了解决这个问题而存在的。\nLOTUs: Continual lmitation Learning for Robot Manipulation Through Unsupervised Skil Discovery 目标： 灾难性遗忘：神经网络在新的数据分布上训练时，往往会覆盖先前学习的知识，导致早期任务的性能下降。\n样本效率：真实世界的机器人学习受到数据收集高成本的限制，这使得需要大量数据的方法变得不切实际。\n任务复杂性：基于视觉的操作任务涉及复杂的感知和控制挑战，使得单片学习方法尤其困难。\n知识转移：在任务之间高效地转移知识（无论是向前还是向后）对于有效的持续学习至关重要，但难以实现。\n方法： 构建一个技能库，这个技能库是这个模型能够学习的新技能的上限 然后每次在获得一个新的数据的时候，模型会自动判断应该是更新旧的技能还是学习新的技能 最后通过模仿学习更新这个技能 那如何识别这个技能是旧的技能还是新的技能呢？\n答案是判断这个任务和之前的某个任务是否有 Sematic 上的相似\n这里是设定了一个阈值 $\\tau$, 如果这个新的数据的任务在经过一个 DinoV2 之后 和某一个技能的聚类的相似度超过 $\\tau$, 那就更新这个技能\n问题：如果多个任务相似性都超过了这个阈值，那么就更新一个相似度最高的任务\n结果 M2distill: Multi-modal distillation for lifelong imitation learning 目标 想要解决这些问题：\n潜在表征漂移：随着模型按顺序学习新任务，它们内部对先前学习任务的特征表征会逐渐变形。这意味着机器人对熟悉物体、空间关系和任务上下文的理解会随时间扭曲。\n动作分布不一致性：随着策略参数的更新以适应新技能，机器人对先前学习任务产生适当动作的能力会下降。\n方法 蒸馏\n啥是蒸馏?_?\n其实就是最小化 embedding 的差\n和谁蒸馏呢？\n其实就是和上一步做蒸馏\n啥意思呢。\n就是说：\n我保存了上一个任务训出来的参数，然后也保存了这个任务的参数 我希望在学习到这个新任务的基础上，这个参数的变化越小越好。\n结果 比上一篇Lutos好点\nFew-Shot Vision-Language Action-Incremental Policy Learning 目标 在少数据的情况下不遗忘之前学过的旧数据\n方法 引入了两个特殊的机制来解决这个问题：\n任务特定提示（TSP）：可学习的提示向量，与多模态输入数据交互，以从有限的演示中提取任务特定信息 持续演化策略（CES）：一种构建和利用任务关系图以在任务之间传递知识并减轻灾难性遗忘的机制 什么是任务特定提示呢？\n任务特定提示就是： $$ Z_p = \\text{MVTransformer}([Z_v, Z_l, P]) $$\n其中，$Z_v$表示视觉 tokens，$Z_l$表示语言 tokens，$P$表示任务 prompt。\n就是增加训练了这么一个 encoder 就可以提高 15%-17% 的成功率\n那什么是持续演化策略呢？\n持续演化策略就是： 首先，训练一个 base network, 这个 base network 在训练的时候会有 multi-task 这样的话，这个模型就可以有一个还不错的通用性能 然后，建立一个方法库，对于每一个新任务放入一个新的方法库中 对于每次训练的时候可以把之前学过的旧任务的权重加权平均，融入新策略中 这个权重是根据旧任务和新任务之间的相似性来的。 注：这里的任务库中的任务非常大 包含了一个完整的 action head.\n结果 Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning 问题背景与动机 当前的终身学习方法主要分为三类，每类都存在显著局限性：\n重放方法存储并重训练旧数据，表现良好但需要过多的内存和计算资源。对于涉及高维视觉数据的机器人操作任务，这种方法成本高得令人望而却步。\n正则化方法，如弹性权重整合（EWC），通过限制参数更新来保留旧知识，但在长任务序列中难以平衡可塑性-稳定性。\n架构方法，包括参数高效微调（PEFT）技术，创建任务特定模块，但面临两个关键问题：\n在测试时依赖预言机任务标识符，这在实际部署中不切实际 任务之间的知识隔离，阻碍了有效的前向迁移 DMPEL通过将PEFT的效率与动态专家选择和知识共享机制相结合，解决了这些局限性。\n方法 动态的专家模型: 这个保证了在只需要微调的基础上新学习到知识，同时不会忘记旧知识(因为之前学习到的参数没有变) 一个 novel 经验回放的方法: 文中的经验回放不是把之前的场景回放一遍，只回放了\u0026quot;如何选取专家\u0026quot;的权重。这样既做到了\u0026quot;回放\u0026quot;的效果，算力开销也不算大 上下文感知专家路由器：一个轻量级神经网络，它以多模态上下文（视觉、语言、本体感受）作为输入，并生成专家组合的系数，从而无需预言机任务标识符。 其他细节：\nEncoder 用的是 CLIP ViT-B/16 amazing\n对于每个新任务，DMPEL：\n使用正交初始化方式初始化一个新的低秩专家，以最大程度地减少干扰 在当前任务上训练该专家，同时冻结基础策略 任务完成后，将训练好的专家添加到专家库中 更新专家路由器，将新专家纳入动态组合中\n结果 在 FWT 上稍逊于 M2 Distill, 但是 AUC 远高于 M2 Distill, 差距 \u0026gt; 10%\nSPECl: Skill Prompts based Hierarchical Continual lmitation Learning for Robot Manipulation 目标 希望机器人能够持续学习并且适应新任务\n方法 SPECI 框架对机器人学习领域做出了几项重要贡献：\n新型分层 CIL 框架：SPECI 将多模态感知和融合、上下文感知技能推断和低级动作执行统一到一个有凝聚力的流程中。\n动态技能获取：可扩展的技能代码本支持隐式持续技能获取，无需手动技能抽象，从而促进有效的技能级别知识转移。\n增强的知识转移：模式近似通过使用特定任务和任务共享知识丰富策略来增强任务级别的知识转移。\n同样的可扩展的技能代码本：对于每个新任务，都会分配一个新的技能向量子集，而现有的技能向量则被冻结。这种方法允许持续扩展技能库。\n同样的注意力驱动的技能选择：一种机制根据技能向量与当前状态的相似性，选择前 C 个相关的技能向量，并通过加权求和将它们组合起来，从而形成合成的潜在技能。\n同样的时间 Transformer 架构：此组件旨在推断随时间的逻辑技能，从而捕获技能执行过程中的时间依赖性。\n区别是，这里不再用 LoRA,而是使用 $W = W_0 + ΔU * V^T$ 去微调\n结果 效果都比 DMPEL 差，特别是 LIBERO-GOAL 和 LIBERO-LONG\n图中其他模型效果和 DMPEL 中有些偏差\n推测是 DMPEL中的 ER 效果更好\n可惜 DMPEL 并没有测试没有 ER 的效果。\nIncremental Learning of Retrievable Skills for Efficient Continual Task Adaptation 目标 and 动机 IsCiL 的动机源于认识到终身代理，特别是家用机器人，必须在以下环境中运行：\n完整的专家演示昂贵且难以获取 任务频繁变化，没有明确的边界 隐私问题需要能够“遗忘”特定行为 不同任务之间的通用技能应被利用以提高效率 解决了三个主要挑战：\n收集全面专家演示要很高成本 在非平稳环境中进行鲁棒适应的需求 隐私问题 方法 当在新任务演示中遇到一个新技能时，做以下事情：\n原型初始化：系统在新技能数据处理时识别出最常检索到的现有技能原型。 适配器初始化：新技能的适配器参数通过复制最相似的现有技能适配器进行初始化。 适配器更新：使用模仿损失在新技能的演示上对初始化的适配器进行微调。 系统更新：新的技能原型及其适配器映射被添加到系统中。 结果 You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations 目标 双臂协作机器人训练\n该框架解决了当前双臂机器人学习方法中的根本局限。传统方法要么依赖预定义的动作分类法，限制了通用性；要么需要大量耗时且易受噪声影响的远程操作数据收集。YOTO通过从人类演示中提取丰富的时空信息，并将这个单一的教学实例快速增殖为适合现代模仿学习算法的多样化训练数据，从而弥补了这一差距。\n方法 手部运动提取与注入 提取模块通过几个复杂的处理步骤，将原始人类视频转换为机器人可执行的动作。\n3D手部轨迹处理：YOTO并未依赖可能不稳定的直接3D手部网格重建，而是采用了两阶段方法。系统首先使用WiLoR检测手部边界框并估计3D手部形状，然后将这些3D点投影到2D图像平面上，再通过立体匹配将其提升回3D。这种投影-提升策略显著提高了轨迹的稳定性和一致性。\n基于关键帧的动作表示：系统并未建模可能包含噪声和冗余的连续轨迹，而是提取对应于重要运动事件的稀疏关键帧——例如抓取器状态变化、速度极值或轨迹拐点。这使得动作空间维度从可能数百个连续帧减少到一组可管理的离散关键姿态。\n运动掩码生成：一个关键创新是运动掩码的推导，它编码了协调策略。这些二值掩码指示在每个关键帧时，每只手臂是应该移动还是保持，有效地捕获了双手任务的时间协调模式。这解决了双手操作中的一个核心挑战——学习手臂何时应该协作以及何时应该独立操作。\n演示增殖策略 增殖模块通过两种互补的方法解决了单次学习中的基本数据稀缺问题。\n自动回放验证：提取的关键帧具有高度可解释性和可编辑性，允许系统地修改物体姿态和属性。通过调整关键帧参数（例如物体的6自由度姿态或用相似类别的物体替换）并在真实机器人上执行这些修改，系统能够快速生成多样化、经过验证的演示。这种方法比传统的远程操作显著更快，同时保持了高数据质量。\n几何变换增强：系统对被操作物体的3D点云执行受控的几何变换——在机器人可达工作空间内的旋转和平移。这些变换自动生成关键帧动作的相应更新，创建了理论上无限的合成变体，而无需额外的机器人执行。\n这些策略的结合使得从单个真人演示中生成数百个多样化的训练示例成为可能，有效地弥补了单次示教与现代深度学习方法所需数据之间的差距。\n双手扩散策略 BiDP（双手扩散策略）代表了扩散模型在双手操作任务中的专门适应。\n以物体为中心的观察：BiDP并非处理完整的场景点云或RGB图像，而是专门关注任务相关物体的3D点云。这种设计选择减少了视觉噪声，加速了训练收敛，并通过移除不相关的环境变化提高了泛化能力。\n关键姿态预测：该策略预测离散的关键姿态而非连续动作序列，显著降低了扩散空间的维度并简化了学习问题。这种方法与提取阶段基于关键帧的动作表示自然契合。\n运动遮罩整合：运动遮罩在统一动作空间表示方面起着至关重要的作用。对于异步任务，遮罩能够将双手动作重组为时间有序的单臂动作序列，有效消除冗余的“保持”状态。对于同步任务，两只手臂始终保持活动。这种统一的表示允许单一的策略架构处理两种协调模式。\n底层扩散模型使用基于PointNet的修改编码器，具有SIM(3)等变性，用于处理点云观测，使其对尺度和位置变化具有鲁棒性。FiLM层提供条件机制，而卷积U-Net处理动作预测。\n结果 比较性能：BiDP在所有任务中取得了76.8%的平均成功率，显著优于包括ACT (5.7%)、标准扩散策略 (15.8%)、3D扩散策略 (19.4%) 和 EquiBot (23.4%) 在内的强大基线。这种性能差距证明了双手操作专门设计选择的有效性。\n消融研究：系统的消融实验验证了每个组件的贡献。从完整场景到仅对象观测的转变将成功率从24.1%提高到48.1%。稀疏关键帧表示进一步提高到51.9%。运动遮罩机制带来了额外的增益，而几何数据增殖显示出最显著的影响，将性能从61.1%提升至77.8%。\n泛化能力：对新颖、未见过对象的域外测试显示了BiDP卓越的泛化能力。尽管基线方法在分布外设置中表现出显著的性能下降，但BiDP保持了相对强大的性能（平均成功率为35.0%，而次优基线为8.8%），展示了强大的迁移能力。\nIn Praise of Stubbornness: The Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLM 目标 LLM是怎么出现“灾难性遗忘”的\n方法 和 结果 首先，检测大模型中是否出现了“不一致行为”\n什么是不一致行为呢\n就是说：大模型中有些行为是“新”的，有些是“收悉”的，而有些是“不一致”的。\n什么意思呢？就是说，在训练的时候我们可以从大模型的中间状态来判断：\n这个完全没学过，这个曾经学过，还是这个新学的和之前学的不一样。\n分析表明，结合激活值和梯度特征始终优于单独使用任何一种特征集。基于梯度的特征在微调模型中表现出更强的判别性，这可能是因为过拟合为熟悉信息与不熟悉信息创建了清晰的梯度信号。对于预训练模型，激活值和梯度都做出了显著贡献，这表明内部表示和学习动态对于区分信息类型都很重要。\n这篇文章指出：只要没有矛盾，LLM 就是稳定的\n否则，LLM 就会很不稳定\nJoint Flashback Adaptation for Forgetting-Resistant Instruction Tuning 目标 如何使用指令微调让LLM避免“灾难性遗忘”\n方法 核心创新在于在训练新任务期间引入有限数量的“闪回”——来自先前学习任务的提示。该框架通过几个关键机制运行：\n新任务学习：模型在新的任务数据上进行标准的监督微调，使用交叉熵损失（$L_{SFT}$），保持任务无关操作，无需显式任务识别。\n闪回整合：一小部分来自旧任务的提示作为“闪回”，无需其对应的标签即可使用。这些提示可以从验证集中采样、手动制作或由LLM合成。\n散度损失预防：一个关键组件通过散度损失（$L_{DIV}$）来防止遗忘，该损失衡量当前模型输出与参考模型在闪回提示上的输出之间的偏差。该方法采用双向KL散度来惩罚生成令牌分布的差异：\n$L_{DIV} = \\sum [KL(P_{ref} || P_{current}) + KL(P_{current} || P_{ref})]$\n组合优化：最终目标结合了两种损失：$L = L_{SFT} + \\alpha * L_{DIV}$，其中 $\\alpha$ 平衡了新任务学习与旧任务保留。PCGrad（投影冲突梯度）在优化过程中解决了竞争目标之间的冲突。\n为了解决闪回数据稀疏性并促进知识共享，JFA通过潜在任务表示整合了联合任务学习：\n潜在任务表示：系统维护潜在任务作为元组 ($e_j$, $\\Delta \\theta_j$)，其中 $e_j$ 表示任务编码，$\\Delta \\theta_j$ 包含通过LoRA实现的相应权重增量，以提高参数效率。\n动态知识检索：对于每个输入，系统使用预训练编码器（例如 RoBERTa）对其进行编码，并根据与冻结的正交潜在任务键的余弦相似度检索 k 最近邻潜在任务。\n参数重参数化：模型不是直接更新参数，而是通过添加检索到的潜在任务增量的加权平均值来动态重参数化有效参数：$\\theta\u0026rsquo;$ = $\\theta_{base}$ + $\\Delta \\theta_i$。\n联合优化：系统同时优化基础模型参数和潜在任务权重增量，从而实现跨不同输入和任务的共享知识的持续改进。\n结果 新任务性能：在SNI上，JFA在两种模型的所有指标上都取得了最高分。对于Llama3.1-8B，JFA得分43.72 (BLEU) 和51.45 (ROUGE-1)，显著优于SFT (41.50, 50.18) 和其他基线。\n旧任务保留：JFA在旧任务上始终提供顶级的性能。对于Llama3.1-8B，它在GSM-8K (83.09%) 和SVAMP (84.40%) 上取得了最高准确率，甚至超过了那些能够访问大量旧任务数据的方法。\n灾难性遗忘示例：简单的SFT在旧任务上表现出显著的性能下降（例如，Llama3.1-8B在GSM-8K上的准确率从82.79%下降到8.95%），清楚地表明了灾难性遗忘的严重性，并凸显了JFA的有效性。\nLibero: Benchmarking knowledge transfer for lifelong robot learning Please visit this file\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-17/","summary":"VLA 中的持续学习","title":"Bug Journal 2025-06-17"},{"content":"持续学习中避免灾难性遗忘：具身智能领域的研究进展综述 引言与问题背景 持续学习（Continual Learning，也称终身学习）指模型在数据分布和学习目标不断变化的情境下，能够连续学习新任务且不忘记已有知识的能力arxiv.orgar5iv.org。传统深度学习假设训练数据 i.i.d.且一次性可用，这在现实具身智能（如机器人、自主系统）中往往不成立ar5iv.orgar5iv.org。其中最大挑战之一是灾难性遗忘（catastrophic forgetting） ，即模型在顺序学习多个任务时，新知识的获取会导致旧知识被快速、大幅遗忘en.wikipedia.org。这一现象最早由 McCloskey 和 Cohen 于1989年在联结主义神经网络中发现arxiv.org，体现了所谓 稳定-可塑性权衡 ：模型既要对新信息足够可塑（plasticity），又要对既有知识保持稳定（stability）en.wikipedia.org。与生物神经系统相比，人工神经网络在顺序学习时更容易“灾难性”遗忘过去经验，而人类和动物通常表现出渐进、有选择的遗忘cell.comcell.com。如何在保持模型泛化能力的同时避免旧知识被破坏，成为持续学习研究的核心问题cell.com。为此，大量研究围绕不同技术路线展开，包括利用附加数据回放、正则化约束、动态模块化架构和外部记忆等方法来缓解遗忘arxiv.org。本文将按时间脉络梳理持续学习避免遗忘的关键进展，重点聚焦具身智能场景的应用，并比较不同方法在这些场景中的优劣差异。\n早期探索：灾难性遗忘的提出与初步对策 灾难性遗忘现象的提出（1980s–1990s）: 神经网络中的灾难性遗忘问题由 McCloskey 和 Cohen (1989)en.wikipedia.org以及 Ratcliff (1990) 首次严谨描述en.wikipedia.org。他们发现序列训练两个任务时，后学任务会显著干扰先前任务的记忆en.wikipedia.orgen.wikipedia.org。这一问题可视为Grossberg提出的稳定-可塑性两难的极端表现en.wikipedia.org。此后，研究者逐步意识到，若要让人工智能具备人类般持续学习的能力，必须解决神经网络的遗忘灾难。1990年代中期，一些学者开始探索初步对策。例如，Robins (1995) 提出了重演/伪重演 (rehearsal/pseudorehearsal) 方法，即在学习新任务时将旧任务样本或由模型生成的“伪样本”一并训练，以巩固旧知识ar5iv.org。这种方法模拟了生物大脑在睡眠中重放记忆的过程，缓解了遗忘问题，被视为后续生成式回放方法的雏形ar5iv.orgar5iv.org。与此同时，Grossberg 等人在稳定-塑性理论指导下发展**自适应共振理论 (ART)**网络，通过网络结构与记忆单元的设计减少旧记忆被覆盖的风险。这一时期的工作揭示了灾难性遗忘的严重性，并奠定了若干基本思想：通过保留过去经验（真实或模拟）或限制参数剧烈更新来保护已有知识cell.com。然而，由于当时神经网络规模和应用场景有限，这些早期方法未形成统一框架，但为后续深度学习时代的持续学习研究提供了宝贵思路。\n深度学习时代兴起前的持续学习理念: 在2000年前后，机器学习领域也出现了一些“终身学习”思想，例如 Thrun 和 Mitchell 等人在机器人领域讨论让机器人不断积累知识、自主适应新环境的算法。但受限于模型能力，这些工作多偏向理论构想或特定场景下的增量学习算法。值得一提的是，Silver et al. (2013) 等人在认知科学领域提出**“永不停歇学习”(Never-Ending Learning)的愿景，旨在构建能无限获取新知识的AI系统。这些理念上的探索进一步强调了持续学习的重要性，但真正有效的算法突破还要等待深度学习的成熟。进入2010年代，深度神经网络在图像、语音等任务上取得突破，但其遗忘现象依然明显**。Goodfellow et al. (2014) 的实证研究表明，即使现代深度网络在顺序学习多任务（如不同MNIST变换）时，仍然发生严重的性能遗忘，他们尝试用Dropout等正则策略略微缓解遗忘cs.uic.edu。这一阶段的研究重新量化了深度模型遗忘的程度，引发了学界对持续学习的关注，也为随后的关键方法发明做好了铺垫。\n深度学习时代的方法演进（2016–2020） 进入深度学习时代后，大量持续学习算法被提出。总体而言，这些方法可分为以下几类： 参数正则化 、 经验回放 （或生成回放）、 参数隔离/模块化 、以及外部记忆等arxiv.org。各类方法都有经典代表工作，我们按时间演进介绍主要方法及其贡献。\n参数正则化方法 Learning without Forgetting (LwF, 2016): Li 和 Hoiem 提出“学习不遗忘”算法arxiv.org。他们假设只能获取新任务数据，无法重温旧任务数据的现实情况，通过让模型在学习新任务时蒸馏(distillation)旧任务模型的输出分布来正则化模型参数变化arxiv.org。具体而言，在训练新任务时对旧任务模型的预测进行保持，使新模型尽可能产生与旧模型相似的输出，从而保护原有能力。LwF是知识蒸馏用于持续学习的开创性工作，实现了只用新任务数据也能较好保留旧任务性能arxiv.org。该方法在2016年ECCV发表，此后蒸馏正则化成为持续学习的重要手段之一。\nElastic Weight Consolidation (EWC, 2017): Kirkpatrick 等人（DeepMind）在PNAS 2017发表了著名的EWC算法arxiv.org。EWC通过近似计算每个参数对旧任务的重要程度（利用费舍尔信息矩阵对参数敏感度进行估计），在学习新任务的损失中添加项，惩罚对重要参数的大幅更新arxiv.org。直观来说，模型会“放慢”那些对旧任务重要参数的学习速率，以免遗忘旧知识arxiv.org。EWC在经典分类任务（如顺序MNIST）和强化学习任务（顺序Atari游戏）上验证了有效性arxiv.org。作为持续学习领域里程碑，EWC证明通过软约束参数更新，可以在一定程度上同时保持先前任务性能和新任务学习能力arxiv.org。其思路后来衍生出许多变体，例如利用更精细近似二阶信息的方法等ar5iv.org。\nSynaptic Intelligence (SI, 2017): Zenke 等人在ICML 2017提出了另一本质类似EWC的正则化方法SIarxiv.org。不同于EWC预先计算参数重要度，SI在训练过程中实时累积每个参数对损失的贡献度，结束当前任务时将其视为该参数的重要性arxiv.org。这种“智能突触”机制借鉴了生物突触强度调节的复杂性，每个突触（参数）在多个任务中累积“任务相关信息”，新任务来时利用这些信息调整学习率arxiv.org。SI在多个连续分类任务上显著降低了遗忘，同时保持了计算高效arxiv.org。与EWC相比，SI无需存储旧任务样本，同样不增加模型容量，对于资源受限的设备具有吸引力arxiv.org。但正如多数正则化方法的局限，当任务数量增多或差异较大时，单纯靠增加惩罚会导致模型学习新任务受限，需要在稳定与塑性间权衡ar5iv.orgar5iv.org。\n其他正则化进展: 除上述，2017年前后还出现了多种参数正则化方案。例如 Li et al. (2017) 的增量时刻匹配（IMM）方法通过匹配参数分布的方式融合旧新任务模型；Aljundi et al. (2018) 提出的MAS（Memory Aware Synapses）利用输出敏感度评估参数重要性。这些方法本质均为在损失函数中添加某种形式的正则项，使模型避免过度调整关键参数以保留旧知识ar5iv.orgar5iv.org。正则化方法的优点是 无需存储旧样本，内存占用低 ，易于在嵌入式/机器人等设备上实现ar5iv.org。它们的不足在于当任务间差异巨大、参数冲突严重时效果受限，而且累积过多任务后模型可能进入“过度稳定”状态难以学习新任务ar5iv.org。在具身智能场景中，由于设备算力和存储有限，正则化方法仍是常用选择之一。例如 EWC 被用于机器人连续控制任务以保护低层政策参数不被遗忘arxiv.org。但如果机器人遇到全新领域任务，正则化可能限制其适应新技能的能力，这是后续方法力图解决的问题。\n经验重放与生成式回放方法 显式经验重放 (Experience Replay): 针对持续学习，最直接的思路是 在学习新任务时重温部分旧任务的数据 ，仿佛让模型“复习”以前的知识ar5iv.org。Rebuffi 等人在CVPR 2017提出的 iCaRL 方法将这一思想与深度学习结合arxiv.org。iCaRL在每学新类别时， 保存每个旧类别少量代表样本（记忆库） ，训练时将这些旧样本与新数据一起用于更新模型，并对模型输出进行知识蒸馏以防决策边界偏移arxiv.org。通过同时学习分类新类别和回顾旧类别，iCaRL实现了深度网络在长时间增量学习许多类时，比仅新数据训练的策略遗忘显著减少arxiv.org。iCaRL开创了样本记忆回放+蒸馏结合的范式。之后许多增量学习方法沿用了“小样本记忆”思路，如 Hou et al. (2019) 的UCIR、Wu et al. (2019) 的BiC等，都在如何精选和高效利用少量旧样本上下功夫。经验重放策略也直接应用于强化学习领域——在非平稳环境中，智能体可将过去经历的轨迹保存一部分，在策略更新时混入重放，以避免策略完全偏离先前成功经验。这与深度Q网络（DQN）的经验回放缓冲理念一脉相承，只是这里目的是防遗忘而不仅是破除相关性。在机器人学习中，经验重放意味着机器人在学习新技能时定期练习已掌握的技能（通过模拟旧技能的传感器输入等），这在实践中提高了多技能机器人系统的鲁棒性。\nGradient Episodic Memory (GEM, 2017): Lopez-Paz \u0026amp; Ranzato 在 NeurIPS 2017 提出的 GEM 则进一步创新了重放的使用方式arxiv.org。与直接将旧样本混入训练不同，GEM把少量旧任务样本存入 episodic memory ，在每次参数更新时，通过约束新梯度与旧样本梯度的内积为非负，确保新任务训练不会增加旧任务损失arxiv.org。这种基于优化约束的方法保证了模型对记忆样本性能不下降，实现了一定的“向后迁移”能力：在学习新任务的同时还有可能改进旧任务表现arxiv.org。GEM开创了利用回放样本的梯度信息指导优化的思路，其后续简化版A-GEM (Chaudhry et al. 2018)降低了计算成本。对于具身智能而言，GEM这类方法的优势在于即使少量记忆也能通过优化约束起效，而且不需要明确任务边界（可以对任意过去经验施加约束）。不过其劣势是需要实时计算并存储梯度，复杂度较高，且仍需维护一个小型记忆库。\n生成式回放 (Generative Replay): 当直接存储原始旧样本受限时，另一策略是训练生成模型来产生日前学过的数据，从而实现回放。Shin et al. 在 NeurIPS 2017 提出的 Deep Generative Replay (DGR) 是该思路的里程碑arxiv.org。DGR构建了一个生成模型（如GAN或VAE）作为“记忆仿真器”，在学习新任务时利用生成模型产生日前各任务的合成样本，并与新数据混合训练Solver模型arxiv.org。在每完成一个任务后，Solver的参数固定，然后训练生成模型去拟合更新后的Solver分布，以便下次产生更新的数据分布ar5iv.org。这种双模型协同框架受到大脑“海马-新皮层”互作机制的启发，即利用快速变化的“海马体”生成回忆来训练慢更新的“皮层”网络arxiv.org。DGR实验证明，即使不保存任何真实旧样本，模型仍能通过生成模拟数据达到与有存储时相近的效果ar5iv.orgarxiv.org。生成回放的优势是 不直接占用存储真实数据 ，在隐私敏感或内存极小的设备上尤为有用arxiv.org。其缺点在于生成模型本身也面临持续学习问题（如何不忘记早期的数据分布），且训练开销较大ar5iv.org。后续不少工作改进了生成回放，如 Wu et al. (2018) 将GAN与变分特征结合（MeRGAN），Rios et al. (2018) 用生成对抗网络生成特征而非像素，提高效率等ar5iv.orgar5iv.org。生成回放还被应用到强化学习的状态生成中，例如 Caselles-Dupré et al. (2019) 提出的自触发生成回放（S-TRIGGER）用于连续学习环境状态表示ar5iv.org。总的来看，回放类方法（包括经验重放和生成回放）在各种基准上往往表现突出，被认为是目前抗遗忘最有效的范式之一。然而它们对存储或生成能力有要求，在具身智能中需权衡内存/算力和性能：对于机器人等设备，存储少量关键经验（如图像片段、关键帧）进行回放在实践中较常用，而实时训练复杂生成模型则相对少见。\n模块化架构与参数隔离方法 Progressive Neural Networks (PNN, 2016): Rusu 等人提出的渐进神经网络是持续学习的架构派代表arxiv.org。PNN在每遇到新任务时 冻结已有网络 ，并侧旁新增一组“列”网络用于学习新任务arxiv.org。同时，通过旁路连接让新任务列能够利用之前各列学到的特征（实现知识迁移）arxiv.org。这种架构确保旧任务的参数永不修改，从而彻底避免遗忘arxiv.org；而新增模块可以专门学习新任务，有充足的模型容量。PNN在Atari游戏和3D迷宫导航等一系列强化学习任务上取得优于微调的成绩，并显示出显著的前向迁移能力arxiv.org。其缺点也很明显：每增加一个任务网络规模就线性增长，在任务数很多时不切实际arxiv.org。尽管如此，PNN证明了模块隔离在避免遗忘上的有效性，许多后续方法受此启发引入可扩展或可选择激活的架构。\nPathNet (2017) 与 PackNet (2018): 为了缓解PNN网络爆炸的问题，Fernando 等人 (2017) 提出的 PathNet 利用进化算法在固定网络中为每个任务选择一条互不干扰的子网络路径，相当于在共享参数的前提下实现参数隔离。Mallya 和 Lazebnik (2018) 则提出 PackNet ，通过反复剪枝和重训练来为新任务腾出参数空间arxiv.org。具体来说，PackNet先训练初始任务模型，然后剪除一定比例不重要的参数（权重置零但保留位置），学习第二个任务时仅利用空闲参数；如此迭代，将多个任务“打包”进单个网络中arxiv.org。实验表明，在ImageNet等大型数据上，PackNet可在一个VGG模型中连续容纳多个细粒度分类任务，性能接近于单独训练arxiv.org。PackNet无需存储旧数据，也不引入新参数，因此相比PNN更高效arxiv.org。但PackNet需要预先设定剪枝比例，且剪枝过多可能损害旧任务性能，过少则限制新任务空间。后来一些变体如 “Piggyback” (Mallya, 2018) 则改为学习任务特定的掩码，更灵活地实现参数复用。总体而言，参数隔离类方法（含动态扩张和网络剪枝）通过结构上的硬约束避免了遗忘，其优势是旧知识完全保留、无干扰arxiv.org。在机器人等具身智能中，如果任务集是离散且有限的，这类方法可考虑使用。例如在多任务机器人控制中，可为每个任务分配专属网络模块或参数子集，新任务加入时扩展网络并冻结旧模块，从而保持以往技能arxiv.org。然而，在开放环境下任务可能连续涌现且无法预知数量，单纯无限扩展网络不切实际。因此近期一些工作尝试结合元学习或 条件网络 ，自动决定何时复用旧参数、何时增加新参数，以兼顾模型规模和遗忘防护。比如 Serra et al. (2018) 提出的 HAT 方法对每层参数学习可训练门控，通过门控向量的稀疏化实现在相同网络中隔离不同任务的激活区域，从而在不显著增加参数的情况下减少干扰。\n脑启发的双记忆体系: 值得注意的是，一些方法从神经科学的双重内存理论汲取灵感，将快速学习模块和稳定长时模块结合起来应对遗忘。例如 Kemker 和 Kanan (2018) 提出的 FearNet 模型采用“大脑 海马-新皮层 ”的架构arxiv.org：用一个类似海马体的小网络专门快速学习当前任务，并在适当时机（模拟睡眠）将新知识整合（consolidate）到另一个类似皮层的大网络中做长期存储arxiv.org。同时还有一个类似杏仁核的模块，根据输入判断应该用哪套记忆系统回答arxiv.org。FearNet不需存储旧样本，依赖生成式机制回忆旧类数据，达到与iCaRL相当的性能arxiv.org。这类方法实质上属于架构+回放的混合策略（因为短期网本身可看作一种内生记忆生成器）。双记忆策略对具身智能有自然的意义：机器人或代理可以配置一个“小而快”的在线学习器来及时适应新变化，同时定期将知识固化到“大而稳”的长期模型中，从而两全其美。不过如何确定巩固频率以及双网络的容量匹配仍在探索中。\n方法对比与小结 不同持续学习方法各有优劣，在具身智能场景下需要平衡选择ar5iv.org。正则化方法不需保存样本、开销低，适合嵌入式设备在线更新，但在任务变化剧烈时可能束缚新知识获取ar5iv.org。回放方法往往效果最佳，即便少量样本重放也能显著降低遗忘arxiv.org；对于机器人这种可反复与环境交互的场景，还可通过自主采样过去环境状态进行重演。然而存储真实数据可能受限于隐私或容量，而训练生成模型又对计算资源有较高要求arxiv.org。模块化/参数隔离方法彻底杜绝了遗忘，在多任务机器人系统（任务有限且可拆分）中很有价值，但在开放任务中扩展性受限arxiv.org。外部记忆和双重内存策略提供了一种折中：通过引入专门的记忆模块，模型可以在不反复调整主要网络权重的情况下查询和更新知识。例如在多人对话交互机器人中，引入一个可读写的记忆单元存储历史对话要点，有助于长期一致的对话理解。但引入记忆也增大了系统复杂度，需要设计高效的检索和写入机制。\n此外，许多先进方法不再局限于单一策略，而是混合多种机制以取长补短ar5iv.org。例如 Schwarz 等人 (2018) 提出的 Progress \u0026amp; Compress 框架将动态架构与蒸馏结合：使用Progressive Network扩展新任务列，然后通过蒸馏将新列知识压缩回主干网络，从而既避免遗忘又控制模型规模arxiv.org。再如 von Oswald et al. (2019) 的 MER 方法将元学习思想融入记忆重放，通过元训练提高模型表示对新旧任务的解耦，从而辅助减少干扰。这些综合方法在近年不断涌现，说明持续学习领域正朝着多策略融合与自动适应方向发展。\n具身智能场景中的持续学习应用 具身智能领域（如机器人、自主车辆、智能代理）为持续学习提供了最实际也最具挑战的用武之地arxiv.org。与静态数据集不同，具身智能体在物理世界中连续感知和行动，环境非平稳且任务边界往往不明确ar5iv.orgar5iv.org。以下我们重点考察持续学习方法在几个具身场景的应用进展：\n机器人视觉与物体识别： 服务机器人需要在不断变化的环境中识别新对象、适应新场景，这正是开放域的持续学习问题。为评测算法，2020年提出了OpenLORIS-Object机器人视觉数据集，包含随时间推移环境光照、视角、物距等变化的数据流sciencedirect.com。在该数据集上，Lomonaco 等人组织了持续学习挑战，促进了算法在真实机器人感知条件下的比较。一系列方法被测试：如 iCaRL 的小样本存储结合知识蒸馏策略在这种增量物体识别中取得稳健表现；又如 IROS 2020 的 Latent Replay 方法，Pellegrini et al. 提出只在特征空间保存和重放旧数据link.springer.com。具体而言，机器人摄像头图像经卷积网络得到中间表示，将这些低维激活缓存代替原始高维图像，可大幅减少存储并实现实时回放ieeexplore.ieee.org。实验表明，在OpenLORIS这种持续视觉任务中，Latent Replay比直接存图像几乎不降性能，却更高效满足机器人实时性需求ieeexplore.ieee.org。另一最新进展是 Hajizada et al. (2024) 提出的 Continually Learning Prototypes (CLP) 算法arxiv.orgarxiv.org。CLP针对机器人少样本在线学习和开放世界场景设计：它采用原型向量表征每类知识，并通过元可塑性机制动态调整每个原型的学习速率来平衡新旧知识稳定性arxiv.orgarxiv.org。同时CLP具备新类别自我检测与无监督学习能力（即机器人遇到未知物体时可判断新类别并自主创建原型学习）arxiv.orgarxiv.org。重要的是，CLP不使用任何显式回放数据且兼容神经形态芯片，实现了超低能耗下的持续学习arxiv.orgarxiv.org。这对于内存和电池有限的移动机器人具有现实意义。总的来看，在机器人视觉领域，混合使用 小样本记忆 、 特征回放 、适应性学习率等技术已取得显著效果，使机器人能逐步扩展认知能力且遗忘受控。\n人机交互与多模态学习： 具身智能体常涉及多模态感知（视觉、听觉、语言）和人机交互，这带来了持续学习的新课题。例如社交机器人需要持续学习新的对话内容、新的手势动作等。NLP领域已有针对增量学习的综述（如 Biesialska et al. , 2020link.springer.com），其中提到自然语言处理任务在持续学习中面临词汇和语义随时间演变的问题。一些方法通过动态扩充词典或嵌入空间缓解了“遗忘”早期语义的现象。对于多模态交互，Kulkarni et al. (2019) 提出在对话系统中使用弹性权重约束来保留模型早期对话技能，同时新增新领域对话意图。交互学习中一个重要方面是 用户在环（human-in-the-loop） ：机器人可通过用户反馈实时修正知识。近期有工作探索 交互式持续学习 ，如 Hazifa et al. (2022) 结合神经形态计算，利用片上在线学习快速吸收用户教授的新知识，同时通过正则保护已有知识dl.acm.org。虽然具体算法仍在早期，但这些尝试指出了方向——未来的具身智能体应能通过持续人机交互 自我进化 ，并且做到“学而不忘”。\n连续控制与强化学习： 在自主驾驶、机器人控制等连续决策场景，持续学习同样关键。例如自动驾驶车辆遇到新道路场景，需要学习新策略而不忘记基本驾驶技能。Shaheen et al. (2022) 的综述link.springer.comlink.springer.com总结了三类自主系统（无人车、无人机和移动机器人）中的持续学习挑战：模型需在在线方式从大量顺序数据中学习，且资源受限、须保障安全稳定link.springer.comlink.springer.com。一些研究采用策略蒸馏或迁移学习避免遗忘旧任务策略。如 Rusu et al. 在DeepMind的机器人实验中，用渐进网络将仿真训练的技能迁移到现实机器人上，同时保持仿真技能不丢失arxiv.org。又如 Traoré et al. (2019) 提出的 DiscoRL 框架，将旧策略压缩为策略库，再用Policy Distillation（策略蒸馏）技术在新环境中融合旧策略以加速学习，同时旧策略作为教师防止遗忘ar5iv.org。在连续控制中，策略往往以神经网络表示，类似分类任务的遗忘也会发生：新环境下调整策略网络，会导致旧环境下性能下降。为此 Rolnick et al. (2019) 提出的 CLEAR 方法，将off-policy经验重放引入强化学习的策略梯度训练，既提高新任务样本效率又维持旧任务价值函数不变。该方法在Atari游戏顺序学习中取得好结果，被视为强化学习领域对抗遗忘的有效方案之一。需要强调的是，强化学习场景中任务界限往往模糊，甚至代理可能在一个不断演变的环境中持续学习（如运营多年的家庭服务机器人，会不断遇到新任务）。这接近无任务标签 (task-agnostic)的持续学习。Aljundi et al. (2019) 针对此提出了在线持续学习方案：通过检测网络对新数据的干扰程度动态触发记忆重放（MIR）link.springer.com，以及使用梯度稀疏化挑选对旧任务干扰最大的记忆样本来更新，从而在无明确任务边界下也能抑制遗忘。此类方法在机器人持续感知与导航中具有潜力，因为现实中机器人很难知道自己何时“切换了任务”，只能根据环境变化连续调整。\n开放世界和自主适应: 具身智能体经常处于开放世界，可能遇到训练时未见过的全新情况。持续学习的终极目标是在这种开放环境中实现持续适应而不崩溃。Open-world持续学习需要综合上述技术，还涉及新知识的自主发现和 主动学习 。比如前述CLP方法引入了新类检测机制，让机器人在开放世界下识别何时需要学习新对象arxiv.orgarxiv.org。又如 Mundt et al. (2020) 探讨了结合异常检测和持续学习，使模型在检测到输入分布偏移时能触发新任务学习流程。对自主车而言，面对从未见过的道路情况（极端天气、新施工区域），如果能自动检测出“新情境”并调用持续学习模块更新模型，将大幅提高安全性。当然，这也带来安全约束下的学习稳定性问题，需要确保新学习不会在尚未充分验证时投入决策。近期一些研究主张引入 不确定性估计 （如Bayesian NN）判断模型何时需要学习新任务，以及学习后的性能变化link.springer.com。这些探索尚属前沿，但对于真正长期自主运作的智能体至关重要。\n近年新进展（2020–2025）与展望 过去五年中，持续学习领域涌现了一系列新趋势和方法，进一步提高了模型在复杂环境中的持续适应能力：\n任务无关持续学习: 越来越多工作关注在无明确任务边界、数据连续流动场景下的学习link.springer.comlink.springer.com。这更贴近现实中的机器人/代理感知流。为此，方法上强调在线更新、有限内存和即时评估。例如 2020 年的 GDumb 方法提出一种极端简单但强大的baseline：始终只训练当前模型在收到的全部数据上（存储一定量最近数据），每次新数据到来直接从头训练。这种方法虽谈不上高明，却在一些线上学习赛道表现接近更复杂的方法，提示我们需要重新审视评价指标。在可预见的将来， 线上持续学习 （Single-Pass Continual Learning）将成为研究热点，它要求算法一次遍历数据且不泄露未来信息，在边学习边推理的同时抗遗忘arxiv.orgarxiv.org。具身智能如实时视频流分析、持续语音识别都属于这种场景。 持续学习评测基准丰富化: 近年构建了许多新数据集和基准来评测持续学习算法在更复杂任务上的性能。如 CORe50、OpenLORIS 等视觉序列数据集用于评测在线物体识别link.springer.com；ACL 2021 Lifelong NLP挑战提供持续自然语言理解任务；还有不同领域的持续强化学习基准、连续无人驾驶仿真环境等。这些基准推动算法从依赖任务ID的小规模实验，走向更贴近真实的情境。评测指标也越发丰富，除了遗忘率、累计精度外，开始考虑模型的计算效率、内存开销以及在长期学习中的稳定性link.springer.comlink.springer.com。这些综合指标对于具身智能系统尤为重要，因为实际应用中资源受限且需要持续运行。 联邦持续学习与分布式学习: 在物联网和边缘计算兴起的背景下，联邦持续学习成为新方向arxiv.org。即多个分散设备（如一群机器人或智能传感器）在各自持续学习的同时，定期交流模型更新，从而在保证隐私下实现知识共享和共同进化。诸如 FedWeIT、FCL 等算法探索了如何在联邦场景下减少遗忘并高效通信arxiv.org。对具身智能来说，这意味着例如一队协作机器人的经验可以融合，使整体学习速度加快且每个体遗忘降低。该领域仍在起步，面临异步学习、设备差异等挑战，但前景值得期待。 理论分析与可解释性: 持续学习理论方面，近年有人尝试从信息论和最优化角度给出遗忘的分析框架，如用Fisher信息界定参数迁移平衡ar5iv.org。另外，对持续学习过程中的可解释性要求也在提高——在机器人应用中，理解模型为何遗忘某能力、何时需要触发新学习，对于建立用户信任很重要。一些研究利用可视化技术观察随着任务增添，网络内部表示如何演化，以寻找缓解遗忘的线索。还有工作将神经符号方法引入持续学习，以借助符号逻辑的约束保持旧知识。这些方向虽属于前沿探索，但表明社区已不仅满足于经验提升性能，也在寻求持续学习更深层的理论和可解释支撑。 综上，持续学习作为迈向真正智能系统的关键一步，近年来在算法和应用上都取得了显著进展。从最初发现问题、提出启发式对策，到如今各种融合策略在复杂环境中落地，我们离“像人一样终身学习”的AI越来越近。在具身智能领域，实现持续学习将赋予机器人和自主代理长久的自主适应能力，使其能够随着环境和任务变化不断成长，而无需频繁人工干预。展望未来，持续学习研究需要进一步结合元学习、强化学习、因果推断等范式，研发更加通用高效的算法。同时，在真实世界大规模部署持续学习系统时，还需重视 安全机制 （防止在学习过程中性能突然退化）、 伦理与隐私 （学习过程中对用户数据的处理）等问题。可以预见，随着研究的深化，持续学习将在机器人自主导航、智能助理、自动驾驶乃至通用人工智能等领域扮演日益重要的角色，推动人工智能从“静态聪明”走向“动态成长”。\n参考文献：\nMcCloskey, M. \u0026amp; Cohen, N. J. (1989). Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem . In Psychology of Learning and Motivation , vol. 24, pp. 109–165en.wikipedia.org. ( 首次揭示神经网络顺序学习遗忘问题 ) French, R. M. (1999). Catastrophic forgetting in connectionist networks . Trends in Cognitive Sciences, 3 (4):128–135cell.com. ( 灾难性遗忘综述，分析原因并讨论可能解决方案 ) Robins, A. (1995). Catastrophic forgetting, rehearsal and pseudorehearsal . Connection Science, 7 (2):123–146ar5iv.org. ( 提出伪重演方法，用随机伪样本重放旧知识 ) Goodfellow, I. et al. (2014). An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks . In ICLR 2014 . ( 实证分析深度网络遗忘现象，评估基本缓解策略 ) Li, Z. \u0026amp; Hoiem, D. (2016). Learning without Forgetting . In ECCV 2016arxiv.org. ( 知识蒸馏用于持续学习，只用新任务数据保持旧任务性能 ) Kirkpatrick, J. et al. (2017). Overcoming catastrophic forgetting in neural networks . PNAS, 114 (13):3521–3526arxiv.org. ( 提出EWC，通过弹性权重凝固保护重要参数arxiv.org ) Zenke, F. et al. (2017). Continual Learning Through Synaptic Intelligence . In ICML 2017arxiv.org. ( 提出SI算法，智能累积参数重要性减少遗忘arxiv.org ) Rebuffi, S.-A. et al. (2017). iCaRL: Incremental Classifier and Representation Learning . In CVPR 2017arxiv.org. ( 提出增量分类策略，结合样本保存和蒸馏避免遗忘arxiv.org ) Lopez-Paz, D. \u0026amp; Ranzato, M. (2017). Gradient Episodic Memory for Continual Learning . In NeurIPS 2017arxiv.org. ( 提出GEM算法，用梯度约束保证新任务不增大旧任务损失arxiv.org ) Shin, H. et al. (2017). Continual Learning with Deep Generative Replay . In NeurIPS 2017arxiv.org. ( 提出深度生成回放DGR，通过生成模型重现旧样本融合训练arxiv.org ) Rusu, A. A. et al. (2016). Progressive Neural Networks . arXiv:1606.04671arxiv.org. ( 提出渐进网络架构，扩展新列避免遗忘并实现知识迁移arxiv.org ) Mallya, A. \u0026amp; Lazebnik, S. (2018). PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning . In CVPR 2018arxiv.org. ( 通过迭代剪枝为新任务腾出容量，实现单网络多任务无遗忘arxiv.org ) Schwarz, J. et al. (2018). Progress \u0026amp; Compress: A scalable framework for continual learning . In ICML 2018arxiv.org. ( 提出进展-压缩框架，结合渐进扩展和蒸馏压缩，实现无增长持续学习arxiv.org ) Aljundi, R. et al. (2019). Online Continual Learning with Maximal Interfered Retrieval . In NeurIPS 2019 . ( 提出在线持续学习算法MIR，选择干扰最大的记忆样本回放，任务无关场景有效 ) Pellegrini, L. et al. (2020). Latent Replay for Real-Time Continual Learning . In IROS 2020link.springer.com. ( 提出在特征空间进行重放，支持机器人实时持续学习，降低存储与计算需求 ) Kemker, R. \u0026amp; Kanan, C. (2018). FearNet: Brain-Inspired Model for Incremental Learning . In ICLR 2018arxiv.org. ( 提出双内存脑启发模型，不存原始数据通过生物式记忆系统整合知识arxiv.org ) Hajizada, E. et al. (2024). Continually Learning Prototypes . arXiv:2404.00418arxiv.orgarxiv.org. ( 提出原型持续学习方法，少样本在线学习并支持开放世界新类发现，无需存储回放 ) Shaheen, K. et al. (2022). Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks . Journal of Intelligent \u0026amp; Robotic Systems, 105 (9)link.springer.comlink.springer.com. ( 面向自主系统的持续学习综述，分析算法在无人车、无人机等中的性能和挑战 ) Lesort, T. et al. (2020). Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges . Information Fusion, 58 :52–68arxiv.orgar5iv.org. ( 持续学习在机器人领域的综述，提出评测框架和跨领域方法借鉴思路 ) 引用\n[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[2312.10549] Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomyhttps://arxiv.org/abs/2312.10549Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[PDF] Continual Learning and Catastrophic Forgettinghttps://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1706.08840] Gradient Episodic Memory for Continual Learninghttps://arxiv.org/abs/1706.08840[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1805.06370] Progress \u0026amp; Compress: A scalable framework for continual learninghttps://arxiv.org/abs/1805.06370[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Towards lifelong object recognition: A dataset and benchmarkhttps://www.sciencedirect.com/science/article/abs/pii/S0031320322003004Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccLatent Replay for Real-Time Continual Learning - IEEE Xplorehttps://ieeexplore.ieee.org/document/9341460/Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccInteractive continual learning for robots: a neuromorphic approachhttps://dl.acm.org/doi/10.1145/3546790.3546791Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfcc[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfcchttps://arxiv.org/pdf/2302.00487https://arxiv.org/pdf/2302.00487Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026amp;code=2588747a-8932-4197-a391-b846298fdfccFederated Continual Learning for Edge-AI: A Comprehensive Surveyhttps://arxiv.org/html/2411.13740v1\n2024–2025 年具身智能持续学习新方法综述：缓解灾难性遗忘的创新策略 持续学习（Continual Learning）旨在让智能体能顺序学习多个任务而不遗忘已学知识。然而传统方法（如 EWC、iCaRL、GEM、PackNet、DGR 等）主要在静态数据上验证，在具身智能场景中效果有限。具身智能中的持续学习面临真实环境的挑战：任务顺序模糊、样本稀少、实时交互、高维感知等。这要求新机制在稳定-可塑性间取得更佳平衡，避免旧知识被覆盖（灾难性遗忘）同时高效习得新技能。以下我们综述 2024–2025 年出现的多篇新论文，每篇提供方法简介、核心创新、与传统方法的区别及其在具身场景中的优势分析。 Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation (Yuanqi Yao 等, 2025 年, arXiv) 方法简介：这项工作提出了PSPL（Primitive-level Skill Prompt Learning）方法，用于机器人操作的无重温持续学习。PSPL 将技能解构为可复用的“原语”（primitives），通过前缀提示（prefix prompts）来表示，并在两阶段训练方案中不断扩展技能库。首先进行多技能联合预训练，学习一组运动感知的技能提示（motion-aware skill prompts），提取不同技能间的共性语义与运动原语。随后在增量学习新技能时，为每个新技能添加新的前缀提示，通过与旧技能提示的跨注意力机制共享知识，从而加速新技能学习。这一提示学习策略使机器人能够在无需重放旧任务数据的情况下连续获取新技能。 **核心创新：PSPL 将提示学习（prompt learning）引入机器人持续学习领域，创新性地使用“技能前缀提示”作为可扩展的模块化单元来表示和存储技能知识。相比以往为每个新任务增添适配器的方式，PSPL 的提示可以在技能之间共享和重用，显式捕捉技能间的共性原语（包括语义和运动层面的共性）。此外提出的文本+光流联合提示查询（text-flow query）机制，将语言指令与视觉运动信息融合，用于检索适当的原语提示，从而关联语义截然不同但运动模式相似的技能。这一机制解决了仅靠文本嵌入查询时，不同技能间缺乏关联的问题。 与传统方法区别：相较于 EWC 等对网络权重施加正则的方式，PSPL 并不限制参数更新，而是通过前缀提示的模块化扩展来避免遗忘：旧技能的提示向量固定，新技能通过新增提示获取模型容量，从而防止旧知识被覆盖。这类似 PackNet 的“逐任务分配参数”思想，但PSPL的提示可以通过跨注意力与旧提示交互，实现旧新知识的共享迁移，弥补 PackNet 类方法模块隔离导致的无法迁移问题。与 iCaRL、GEM 等需要存储旧样本回放不同，PSPL 不需要任何经验回放——模型通过提示机制保留旧技能，因此在训练新任务时无需重温旧数据便可避免遗忘。相比于 LwF 这类基于知识蒸馏的方法，PSPL直接在模型内部保留了原技能的“软提示”，避免了仅凭日志概率约束可能出现的信息不足。 具身场景下的优势：首先，无重放需求降低了实际机器人系统的存储和隐私负担（无需记录旧任务视频或演示数据），非常适合那些无法无限存储过去经验的场景。其次，PSPL 的原语提示捕捉了跨任务的共性动作模式，使机器人能在少样本情况下将已学技能迁移到新任务上，例如仅观察少量演示也能通过提示召回相关原语来加速学习。再次，该方法对每个技能只新增少量提示向量，属于参数高效的扩展方式，不像全面微调那样消耗大量新参数，适合机器人有限算力的约束。最后，在 LIBERO 基准的模拟和真实操作实验中，PSPL 在前向迁移（FWT）和背向保留（BWT）指标上均达到当前最好水平，验证了其在长期任务序列中的遗忘防御和跨技能知识共享能力。 Preserving and Combining Knowledge in Robotic Lifelong Reinforcement Learning (Yuan Meng 等, 2025 年, Nature Machine Intelligence) 方法简介：该研究提出了名为LEGION的机器人终身强化学习框架 nature.com 。LEGION 构建了一个非参数贝叶斯知识空间（knowledge space），用以逐步积累和保存机器人的技能知识。具体而言，作者使用Dirichlet 过程混合模型（DPMM）来根据任务生成的表征不断拓展知识空间的簇，以容纳新任务知识，同时保留旧任务的簇不被覆盖。每当机器人从连续到来的一次性任务流中学习完一个任务，DPMM 就在知识空间中产生或调整相应的知识组分，实现对该任务知识的分离存储与渐进扩充。此外，LEGION 将语言嵌入引入任务表征，使智能体理解任务语义，从而在学习和推理中利用语言指导任务推断。在执行阶段，机器人能够将知识空间中多个任务的技能组合重新应用于长序列复合任务（例如依次执行多步操作完成“清理桌子”等目标）。 核心创新：LEGION 的突出创新在于引入贝叶斯非参模型管理知识：不同于传统固定架构，DPMM 知识空间可以动态增加新的知识组分，无需预先限定任务数量。这解决了过去结构模块化方法在任务数未知时难以扩展的问题 nature.com 。知识空间中的每个簇本质上代表一类技能或策略的“知识单元”，新任务到来时，如果其知识无法用现有簇解释，就会自动生成新簇存储，避免了旧知识的遗忘与冲突。同时，每个任务的知识以概率混合模型形式保存在连续的先验空间中，便于在需要时被识别和重新调用。另一个创新是融合语言描述提升任务区分和推理能力：语言嵌入提供了任务的高层语义提示，帮助知识空间进行任务归属推断和知识检索，使得在执行长视距任务时，智能体可以更准确地推理需要调用哪些已有技能。 与传统方法区别：LEGION 综合了多种持续学习理念但又有所突破。相较于 EWC 等正则化方法，LEGION 不直接作用于网络参数，而是在参数输出的高层空间保存知识，因而不存在权重被覆盖的问题。相比 PackNet 这类结构隔离方法，LEGION 的知识空间无需预设模块数且能持续增长，解决了传统结构方法难以应对未知数量任务的局限 nature.com 。同时，有别于纯经验回放策略（如 GEM）必须存储大量过往数据，LEGION 通过贝叶斯记忆高效概括了过去任务的精华，不需要完整保留旧样本即可实现类似回放的效果。虽然训练中仍使用有限缓冲作为强化学习的 replay（SAC 算法自身需要）, 但由于知识已存于非参空间，LEGION 即使在不增加缓冲容量的情况下依然能逐任务提高成功率。另外，LEGION 与以往假定任务明确分布范围的多任务学习不同，它能够适应非参数任务变化，处理现实中出现的新规则或新交互形式——这些情景下老方法（例如基于高斯混合的元学习）因需预设任务数量而难以胜任。 具身场景下的优势：LEGION 在真实机器人持续学习中展示出多项优势。首先，其知识保存与扩展机制使机器人几乎无遗忘地累积技能——实验表明，即使经过长时间训练再次回访早期任务，成功率仍接近初始掌握水平，证明旧技能通过知识空间得到有效保留。其次，LEGION 可以将已学知识用于新任务的快速推理和组合，体现了少样本快速迁移能力：作者展示了机器人在学习一系列基本任务后，能够在一次提示下将相关技能串联完成复杂长任务，例如“制作咖啡”，这源于知识空间支持对已学技能的自由组合。再次，LEGION 引入语言降低了感知与任务推断难度，使机器人在多模态指令下表现出色——通过语言描述任务，机器人无需大量探索即可定位对应知识簇，从而更快适应新任务语境 nature.com 。最后，在对比实验中，LEGION 在顺序任务中稳定提升成功率，显著优于“完美记忆”（无限重放）、“reservoir采样回放”和 A-GEM 等方法。这表明在数据受限、任务未知的真实环境中，LEGION 的知识空间机制比传统持续学习策略更适合维持和重用机器人知识。 LEAGUE++: Empowering Continual Robot Learning through Guided Skill Acquisition with Language Models (Zhaoyi Li 等, 2024 年, VLMNM 2024 研讨会) 方法简介：LEAGUE++ 提出了一种利用大语言模型（LLM）引导机器人持续学习技能的框架。该方法将任务与运动规划（TAMP）和深度强化学习（DRL）相结合：先由预训练的大语言模型将复杂长任务自动分解为一系列子任务操作序列（即生成操作符及步骤规划），并产生日志级别的稠密奖励信号指导每个子任务的强化学习训练。机器人通过 DRL 学习各子步骤的低层技能，同时维护一个符号化技能库记录已学的技能。当新任务到来时，LEAGUE++ 使用已有技能库中的技能作为Warm-Start（预热），加速学习新技能。整个系统在四个模拟任务域上进行了验证，包括家务操作等复杂任务场景。结果表明，LEAGUE++ 相比基线有更快的学习速度和更高的任务成功率，并能通过技能重用在新域中显著缩短收敛时间。 核心创新：LEAGUE++ 最大的创新在于将大模型的规划能力和传统强化学习有机结合，实现“边规划、边学习”。具体来说，它利用 LLM 强大的语义理解和分解能力，自动将高层指令分解为低层动作序列（解决了RL直接面对长远稀疏奖励任务时的困难），从而大幅降低了长视距任务的学习难度。其次，它提出了符号技能库概念：已学技能以可调用的符号模块形式存储，使机器人在新任务时可以检索并复用相关技能，而非从零学习。这种技能库机制相当于一种渐进专家网络：随着任务增多，库中专家技能增多，策略可以根据需要调用对应子政策，类似人类在新任务中调用过往经验。最后，LEAGUE++ 将现有预训练模型用于策略初始化（warm-start），例如视觉模型用于感知、行为克隆模型用于初始策略等。这减少了新任务的探索开销，使持续学习更高效稳定。 与传统方法区别：传统持续学习多半依赖网络自身调节参数来避免遗忘，而 LEAGUE++ 则更像一种“学习+规划”双轨方案。它没有采用 EWC/GEM 等直接在RL算法中增加遗忘惩罚，而是通过任务分解将复杂问题拆解，降低每阶段学习干扰。这种方式避免了以往RL持续学习中，不同任务策略存于同一网络导致的梯度干扰，从根本上缓解了遗忘。此外，与 PackNet 等为每任务固定网络模块不同，LEAGUE++ 通过符号调用机制在同一策略网络中执行不同技能，相当于软模块化——技能库中的技能相当于可切换的策略段，无需冻结参数就能隔离任务干扰。相比纯回放或迁移学习，LEAGUE++ 依赖 LLM 提供额外知识（如何解任务、设计奖励），扩充了信息来源：这类似人类导师提供提示，加速了学习而非仅靠算法克服遗忘。可以说，LEAGUE++ 将知识提炼（通过LLM）与技能留存（通过库）结合，超出了传统持续学习框架。 具身场景下的优势：LEAGUE++ 非常契合具身智能体执行复杂任务的需求。首先，LLM 提供的分层规划使机器人哪怕面对长且模糊的指令（例如“准备早餐”）也能一步步拆解执行，避免在长时间任务中遗忘目标或迷失——这在人类复杂指令场景尤为关键。其次，技能库的重用大幅提升了数据效率：当机器人进入新环境或任务，只要能从库中找到类似子任务，就无需从头练习，少样本即可适应。这特别适合真实机器人训练昂贵、无法大量试错的情况。第三，由于采用符号层指导和技能初始化，LEAGUE++ 在每个任务的学习过程都更快、更稳定，降低了资源消耗和试错成本。作者的实验表明，在多个复杂模拟环境（如家庭清洁、装配任务）中，LEAGUE++ 全面优于纯RL的持续学习基线。例如在新域任务上，因复用了旧域技能，学习速度显著快于从零开始的对照。这证明在真实机器人需要反复学习不同任务的长期部署中，引入LLM指导和技能库的框架能够实现效率和稳健性的兼顾，这是传统方法难以同时达到的。 Scalable and Efficient Continual Learning from Demonstration via a Hypernetwork-generated Stable Dynamics Model (Sayantan Auddy 等, 2024 年, arXiv) 方法简介：该工作针对机器人示教学习（LfD）场景提出了一个持续学习方法，利用超网络（Hypernetwork）和稳定动力学模型来保持多技能学习过程中的稳定性和不遗忘。核心思想是训练一个超网络，生成两个子网络：一个用于学习每项示范任务的轨迹动力学模型，另一个生成与之配套的李雅普诺夫稳定性函数。通过在每次引入新示范任务时，让超网络输出相应的轨迹模型并辅以稳定性约束，该方法确保机器人复现的轨迹是收敛稳定的，不会在插值或续航过程中发散。更重要的是，这种引入稳定性的方式还显著提升了持续学习性能：作者发现，相较不考虑稳定性的基线，引入李雅普诺夫函数后，机器人在序列学习多个技能时的遗忘显著减少。此外，为了提高效率，作者提出了分块超网络（chunked hypernetwork）和随机正则化策略，将训练多个任务的总时间从原先的 $O(N^2)$ 降低到 $O(N)$。整个方法在实时的机器人绘画轨迹学习任务（包括 LASA 数据集扩展到高维、和作者自建的 RoboTasks9 实验）上验证，结果显示无论在轨迹精度还是持续学习指标上都优于现有方法。 核心创新：本方法的创新点在于将鲁棒控制中的稳定性保障引入持续示教学习。通过让超网络同时学轨迹模型和Lyapunov 函数，每新增一项技能，系统不仅学会了模仿轨迹本身，还获得了对该技能的稳定约束，使其在整个状态空间具有吸引域。这解决了过去很多 LfD 方法关注模仿误差、却忽视了轨迹泛化稳定性的问题——即使没有回放旧示范，稳定性保障使旧技能的轨迹在训练新技能时不易被扰乱。其次，使用超网络作为核心架构，使得多任务知识统一存储在超网络的权重中，通过不同输入（如任务ID或上下文）生成相应任务的模型。相比传统逐任务微调网络，超网络天然具备参数共享和隔离的双重特性：共享是指不同任务的共性部分自动在超网络中得到整合，隔离是指超网络输出不同任务模型，相当于为每个任务保留了一套隐含的参数。这种方式巧妙地避免了不同任务参数直接冲突覆盖。再次，提出的随机正则项训练方法，通过每次训练仅对一个随机选取的旧任务施加约束，代替以往对所有旧任务都正则化的做法，大幅降低了训练复杂度。这一技巧保持性能不变却将训练开销线性化，提升了持续学习的可扩展性。 与传统方法区别：相比 EWC 等对参数层面的稳定约束，本方法将稳定性提升到行为层面：EWC关心参数不剧烈变化，而该方法直接确保每个技能的输出轨迹在引入新技能后依然收敛不发散，因而对遗忘的防御更直接有效。与 iCaRL/GEM 等需要存储和重放示范不同，该方法不需要对旧示范数据回放：旧技能知识蕴含在超网络权重中，加之稳定性函数的限制，新任务训练不会破坏旧技能轨迹，无需样本重温也能保护旧技能。同时，相较 PackNet 逐任务固定部分网络的硬隔离，超网络属于软隔离：所有任务共享同一个生成网络，但通过任务索引输出不同技能模型，相当于用函数生成参数，既避免干扰又保持容量节省。与之前一项采用超网络+神经ODE方法做 continual LfD 的工作相比（作者提及的最近方法），本方法增加了严格的稳定性保障，因此旧技能保真度和新技能学习效果都有明显提升。总的来说，新方法在理念上融合了控制理论和持续学习，提供了一种不同于以往任何单一范式的框架。 具身场景下的优势：在实际机器人示教应用中，该方法具有显著优势。首先，稳定性约束确保机器人在执行已学轨迹时不会因为学习了新技能而出现意外失稳或偏差，这对物理系统尤为重要——避免了因遗忘造成的运行危险，提高了安全性和可靠性。其次，由于无需反复重训旧示范，机器人能更高效地学习新技能：作者在真实机器人轨迹跟踪任务中展示，新技能加入后旧技能仍能零-shot 执行，表现几乎不下降，这意味着机器人随时可用, 不需要频繁校准旧技能。第三，该方法在高维技能扩展上表现良好，证明其可应用于涉及多自由度的复杂操作（例如机械臂同时控制位置和姿态的轨迹）。传统方法在高维连续控制上的持续学习往往不稳定，而本方法借助Lyapunov函数确保了每个技能在高维空间都是吸引子的，因此具备更强的鲁棒性。最后，在资源受限的机器人上，训练效率的线性提升非常有价值：每新增技能的训练开销不会随着技能数量指数增长，使得机器人可以规模化地持续学习几十上百个技能而不会因为训练时间过长变得不可行。综上，该方法为现实中机器人持续学习大量示教任务提供了安全、高效、稳健的解决方案。 Online Continual Learning for Interactive Instruction Following Agents (Byeonghwi Kim 等, 2024 年, ICLR 2024) 方法简介：这项工作定义了更贴近现实的交互式指令跟随持续学习场景，并提出了信心感知滑动平均（CAMA）算法来缓解遗忘。作者指出，过去大多假设智能体一开始就能获取所有训练任务的数据，而不符合机器人应持续探索、持续学习的现实。为此论文提出两个设置：(1) 行为增量学习（Behavior-IL）：机器人不断学习新指令行为；(2) 环境增量学习（Environment-IL）：在新环境中执行之前学过的指令。困难在于任务边界可能不明确，传统“数据先验”方法（如基于旧任务输出日志概率的蒸馏）需要明确任务边界和缓存旧任务信息。CAMA 则不需要任务边界：智能体在训练过程中对每个观察到的指令计算模型输出的置信度，并以滑动平均的方式更新其对旧任务的输出分布估计。当模型学习新行为时，如果对某类以前学过的指令置信度下降，CAMA 会根据之前维护的滑动平均分布对模型进行轻微调整，以拉回对旧知识的信心，从而达到持续无边界学习的目的。实验证明，在作者设计的新基准上（包含连续新增指令和环境的条件），CAMA 相较现有方法取得了显著更好的表现。 核心创新：CAMA 的创新在于引入任务无关的置信度追踪机制：传统方法如 LwF 或 iCaRL 需要存储每个旧任务的模型输出分布或实例样本，当新任务到来时通过知识蒸馏或重放维持旧任务性能。这实际隐含了已知的任务边界和任务身份。而 CAMA 不存储具体样本或明确任务ID，而是对模型输出信心做持续监控。它维护一个滑动平均估计，可视作模型对过去经验的“模糊记忆”，随着时间衰减地记录旧任务的输出倾向。当模型学新任务时，如果这种倾向发生显著漂移，表明遗忘在发生，那么CAMA依据滑动平均的差异，对模型参数进行微调修正（如调整输出层偏置等），无需知道具体哪个任务受影响，只基于置信度变化即可纠正。这种方法跳出了任务级别的框架，实现了任务无关（task-free）**的持续自稳训练。另外，CAMA 所需存储的信息量极小，仅为每个输出类别一个滑动均值，计算与存储开销极低，非常简洁却有效。 与传统方法区别：与 EWC 等正则方法需要在参数变化上加全局约束不同，CAMA 工作在输出空间：它不直接限制权重，而是看模型对以前输出的信心水平是否下降。这有点类似知识蒸馏（LwF）的思想，但不需要旧模型保存——蒸馏通常要保存旧模型输出作为固定目标，而CAMA持续更新的滑动平均本质上充当了“旧模型记忆”，避免了存储多个旧模型或大量旧样本。相比 iCaRL 维护每类样本代表来近似旧任务分布，CAMA 维护的是压缩的统计量（置信度均值），因而不需要样本库也没有额外模型，只需少量内存。与 GEM 这类基于梯度投影的方法相比，CAMA 算法更简单，不需要复杂的二次规划求解，只以移动平均做调整，在线即可执行。另外，CAMA 属于任务无边界的方法，解决了传统方法大多假定任务切换已知、或者需要在检测到切换后才能应用策略的问题。在现实机器人持续学习中，任务变化往往连续发生且未必有明显分界，CAMA 正是面向这种情况设计的。 具身场景下的优势：CAMA 所针对的交互式指令跟随场景极具代表性：机器人在不断遇到新指令、新环境的过程中持续学习，这对服务机器人等非常实际。CAMA 能在无监督任务切换的前提下，让机器人保持之前学会的指令技能。例如，一个家庭助理机器人在学会一系列语音指令后，被主人带到新房间教它新任务，CAMA 确保机器人在学新任务时不会忘记旧房间的指令执行方法。由于不需要存储大量过往数据，CAMA 也适合长期部署：机器人可以不断学习数十数百种指令而不必担心存储开销或隐私问题（无需录音或录像存档）。同时，CAMA 的实时性很高，它在模型训练时即可动态调整参数，无需离线阶段，适合机器人在线学习的需求。作者报告该方法在提出的 Behavior-IL 和 Environment-IL 基准上显著优于已有的持续学习算法，这表明它成功地解决了任务模糊情况下的遗忘问题，让机器人在变化的环境和任务中保持稳健性能。总之，CAMA 为具身智能体提供了一种轻量级但有效的持续学习机制，使其能像人一样一边执行指令、一边随着环境改变不断积累本领。 ICAL: In-Context Abstraction Learning for Multimodal Agents (Gabriel Sarch 等, 2024 年, arXiv) 方法简介：ICAL 提出了一种利用大模型自身的生成与内省能力，持续提升多模态智能体决策的方案。该方法关注这样的问题：大型语言或视觉-语言模型（LLM/VLM）虽具有强大的Few-shot能力，但需要高质量范例作提示。人为提供高质量示例既昂贵又不具通用性。ICAL 的解决方案是：让大模型从次优的示范轨迹和人类反馈中自动生成可用于提示的抽象示例。具体流程包括：给定一个全新领域的嘈杂演示（例如一个任务的视频示范，可能包含低效动作或错误），首先由视觉-语言模型对该轨迹进行解析抽象。模型会产出一个通用的程序化描述，修正了示范中的低效步骤，并添加认知注释，标注出任务涉及的关系、对象状态变化、时间子目标和关键细节等。接下来，让真实机器人/代理尝试执行这份抽象计划，在执行过程中人类可以提供自然语言反馈纠正模型的错误理解或补充知识。模型根据反馈交互式地细化之前产生的抽象。最终得到的这些抽象示例（带有语言注释和优化后的步骤），被存储起来作为内存范例。在后续决策中，大模型可以将这些示例作为提示的一部分，从而显著提升对于类似任务的决策能力。ICAL 在三个具有挑战的环境中验证：TEACh对话式指令跟随、VisualWeb互联网多模态代理、Ego4D第一人称视频动作预测，都取得了新的SOTA性能。 核心创新：ICAL 将持续学习转化为持续积累提示示例的过程，而非传统的持续调权过程。这是一个范式转变：与其反复梯度更新模型参数，ICAL 让模型自己“理解反思”示范并生成抽象的知识总结，逐步丰富模型的提示库。这样的内省式示教机制是首次提出。具体创新点包括：(1) 自动抽象：利用预训练VLM从视觉示范中提取高层语义——不像以往只关注低级动作序列，ICAL 提取了任务因果关系、对象状态变化、时间逻辑、任务构造等四类认知抽象，这些抽象比原始示范更精炼、更具概括性。(2) 人机交互细化：ICAL 不仅依赖模型自我生成，还引入人类在模型执行时给出自然语言反馈，让模型迭代改进其抽象。这种循环使得次优示范（含错误或低效部分）也能通过反馈纠正而变得有价值，突破了以往示范学习要求专家示范的限制。(3) 提示记忆库：将生成的抽象示例存入一个不断扩展的库，并使用检索增强的大模型决策——模型在推理新任务时，会从库中取出相关示例放入提示，提高决策准确率。ICAL 相当于在持续学习过程中渐进地提升模型的上下文学习能力：模型见到的新任务越多，就自行总结出越多可泛化的知识片段，下次遇到类似任务时即使不调权也能通过提示完成。实验显示，ICAL 随着学习示例增多，模型性能持续提升，呈现出真正的“持续改进”特性。 与传统方法区别：传统持续学习多在参数空间操作，如EWC约束权重变化、经验回放存数据样本等。而 ICAL 完全绕开了参数遗忘问题：模型主体参数基本保持不变（或只在最后有小幅微调作为增强），因此谈不上灾难性遗忘。取而代之，ICAL 通过不断加入新的知识提示，让模型具备解决更多任务的能力。这种基于大模型提示的持续学习不同于以往任何持续学习范式。与知识蒸馏类方法相比，ICAL 并非让模型去逼近旧模型输出，而是生成更好的知识为己所用；与模块化或正则方法相比，它无需设计模型结构拆分或损失项，利用的是LLM/VLM自身强大的Few-shot学习能力和生成能力来“吸收”经验。在需要多模态理解和指令执行的任务中，ICAL 运用了视觉和语言的结合，这比单纯依赖视觉记忆（如CV领域一些记忆网络）要丰富，也比仅语言的持续学习（如对LLM增量训练新知识）更直观：ICAL 让模型直接看视频学，也听取人类语言，得到的是跨模态的知识。这些知识用自然语言+视觉标记来存储，具有很强的可解释性和可移植性。传统持续学习很难在不调整参数情况下大幅提高性能，而ICAL 展示了另一种可能：通过增强模型输入（上下文）而非改变模型本身来实现持续学习。 具身场景下的优势：ICAL 针对的场景包括家庭机器人任务（TEACh）、网络操作代理以及视频预测，都与具身智能密切相关。它的优势在于：对于复杂开放环境，很难人工定义明确的任务边界或提供足够训练数据，而ICAL 利用了现有大型模型，只需较少示范和交互即可让模型适应新任务。因此在具身智能典型的少样本、多样任务情况下，ICAL 能快速扩展技能。此外，它不要求持续占用机器人反复训练（除了必要的人机对话反馈），大部分“学习”都在模型的推理过程中完成（让模型生成抽象并评估）——这意味着机器人可以在线学习新任务而不中断服务，通过对话方式边干边学，正如人类新手在导师指导下学习新技能一样。对于任务模糊或长时间任务，ICAL 的抽象总结能力尤为关键：机器人可以从冗长的示范中提炼要点，避免执行时被次要细节干扰。这提高了机器人在复杂任务下的鲁棒性和泛化。实验证实，ICAL 在 TEACh 基准上将先前SOTA的目标达成率提升了12.6%，在VisualWebArena中成功率从14.3%提高到22.7%，在Ego4D动作预测上击败了Few-shot GPT-4V。更重要的是，性能提升是持续的：每当学习更多示例，模型表现就有所提高。因此在长期来看，ICAL 提供了一个持续增长智能体能力的途径，而无须反复训练模型参数，大大降低了具身人工智能系统维护的成本和风险（例如不会出现传统持续学习不慎遗忘必须人工纠正的问题）。这表明，充分利用大模型强大的自监督和内省能力，或许是具身智能持续学习的一条有效新路。 MLLM-CL: Continual Learning for Multimodal Large Language Models (Hongbo Zhao 等, 2025 年, arXiv) 方法简介：MLLM-CL 提出了一个多模态大模型（视觉-语言模型）持续学习的基准和方法。该工作将持续学习划分为两种情形：(1) 领域持续学习（Domain CL）：模型依次学习遥感、医学、科学、自动驾驶、金融等不同视觉问答领域的知识；(2) 能力持续学习（Ability CL）：模型顺序学习OCR识别、数学逻辑、视觉感知、GUI操作代理等不同能力。作者发现，传统增量学习范式（每次在上一个模型参数基础上微调新任务）不适用于多模态大模型：直接用上一任务的权重初始化下一任务，会损害模型对新任务的可塑性，导致次优结果。为解决这一任务冲突问题，MLLM-CL提出了MR-LoRA方法。具体来说，在每个新任务到来时，并不在原模型权重上继续微调，而是为该任务新建一个LoRA低秩适配器，从零随机初始化。这样每个领域/能力都有自己独立的LoRA模块，避免了继承上个任务权重带来的冲突，同时LoRA只引入极少参数，保证高效。在推理时，为了自动选择对应任务的LoRA，作者设计了一个基于大模型的路由器：利用多模态大模型本身处理复杂输入的能力，输入待测试样本后，由模型生成一个路由指令，选择最合适的专家LoRA来回答。这个路由器通过在持续学习每阶段后收集少量任务样本，对大模型进行轻量few-shot微调得到。总体而言，MR-LoRA包含多LoRA专家 + 大模型路由两部分。实验结果显示，在上述多个领域和能力的持续学习任务序列中，该方法在所有任务上的平均性能和最终性能都超越了现有方法，如直接微调、参数隔离、基于提示的方法等。 核心创新：MLLM-CL 的创新在于针对大模型的持续学习提出了避免遗忘的新范式。其一，每任务新LoRA的策略突破了以往串行微调“一脉相承”导致性能下降的窘境。这种做法类似“进渐网络（Progressive Networks）”，但由于LoRA参数规模小，可以在不剧增参数的情况下无限扩展到更多任务。各任务的LoRA彼此独立，保证旧任务参数不受新任务影响，从根本上杜绝了遗忘。同时，所有LoRA附加在同一个大模型上，共享其通用表示能力，又实现了知识迁移（因为基础模型权重保留了多任务共性表示）。其二，引入大模型驱动的路由机制，这是此前持续学习中少有探索的。传统任务识别通常依赖样本的特征相似度或任务ID，而该方法让多模态大模型读入输入后自主生成路由决策。由于大模型本身掌握高层语义和复杂推理，它可以比简单特征距离更精准地选出对应领域的LoRA专家。这保证了在推理阶段对于复杂多模态输入也能正确地匹配到相应能力模块。其三，该方法还建立了系统化的多模态持续学习基准（MLLM-CL基准），涵盖不同类型迁移（领域和能力）并提供了评价指标和数据集构建流程。这填补了多模态大模型持续学习缺少评测标准的空白，为后续研究提供了平台。 与传统方法区别：MLLM-CL 在设计上融合了参数高效微调和专家路由思想。与 EWC 等全模型正则不同，它不直接约束原模型参数，而是固定主干模型，只对每任务引入少量新参数，这避免了原模型权重冲突累积。与 PackNet 类似的方法相比，MR-LoRA 也采用了“每任务额外参数”的思路，但PackNet通过剪枝固定网络容量，MR-LoRA 则用可增添的LoRA实现弹性容量，理论上任务越多仅线性增加参数，无需预留容量。和提示学习（L2P、ModalPrompt 等）相比，提示方法在多模态大模型上效果有限，而且往往需要固定提示长度、可能仍有干扰；MR-LoRA 通过独立LoRA完全隔离任务，比软提示更干净利落。还有，许多多头网络或专家混合模型需要已知任务ID才能选专家，MR-LoRA 则通过路由器实现任务自动判别，不需要人工指示任务类别。可以说，它将模块化与任务识别两个问题一起解决了。在多模态场景下（例如同时应对图片文本、多任务问答），这种方法比单纯视觉或单纯语言的持续学习方法更复杂但也更全面地考虑了输入多样性和任务多样性。 具身场景下的优势：虽然 MLLM-CL 的实验主要是多模态问答和 GUI 任务，但其思想对许多具身智能应用同样有益。比如，一个家庭服务机器人具备多个视觉语言能力（识物、对话、读屏、算术等），MR-LoRA 可以让其持续添加新能力而旧能力不衰减：每项新技能加一个LoRA，机器人就掌握了新本领，又不会遗忘以前学过的（因为以前的LoRA保留原样）。这样的能力库扩展非常符合具身AI逐步进化的需求。其次，由于LoRA模块小，机器人可以在资源受限设备上部署多个技能专家，而不会像扩增整个模型那样内存爆炸。例如针对移动设备，把不同领域的视觉问答能力拆成多个LoRA加载，按需调用，远比加载多个大模型高效。再次，路由器机制使机器人在遇到新感知输入时能自动判断调用哪种能力，这对于多模态交互场景很关键——现实中用户不会明确告知机器人“这是一道医学问题”或“现在开始OCR任务”，机器人必须自适应切换内部技能。MR-LoRA 的路由方案正是训练机器人根据输入自行选择专家。作者实验也表明，在跨领域问答中，MR-LoRA 能正确选择相应LoRA模块，最终在所有领域上同时取得高精度，相比其它方法在后期任务训练后前期任务精度大幅下降，MR-LoRA 几乎零遗忘且新任务精度也高。这证明了其稳定性和塑造性兼顾的能力，非常适合需要终身学习的多才多艺型机器人。总之，MLLM-CL 为具身智能体集成多模态大模型指明了一条持续进化的道路：通过低秩模块化扩展和智能路由，实现持续学习众多技能而性能不减。 Task-Unaware Lifelong Robot Learning with Retrieval-based Weighted Local Adaptation (Pengzhi Yang 等, 2024 年, arXiv) 方法简介：此工作面向机器人开放环境，提出了无任务标识的终身学习框架，结合检索式记忆和局部微调来提升持续学习效果。在真实世界中，机器人遇到的新任务往往没有清晰的边界或ID，且无法存储海量以往数据。为此作者的方法在机器人学习过程中维护一个情景记忆（Episodic Memory），存储每个任务的一小部分关键示例。当机器人在序列学习新技能时，主要模型仍采用常规的基于经验回放的训练（比如采用一部分记忆样本进行练习，以降低遗忘）。然而仅靠有限回放仍难免遗忘一些早期技能。因此在测试执行阶段，如果机器人遇到类似以前学过却已部分遗忘的情境，该方法会从记忆中检索最相关的过往示例，并对当前策略进行快速局部微调（local fine-tuning），以恢复在该情境下的性能。为了提高这种局部微调的效率，作者引入选择性加权机制：首先让机器人在当前情境下执行若干回合，记录其性能（例如哪些步骤出错），然后将这些失败轨迹与记忆中检索到的示范进行比对，自动衡量哪部分旧示范对当前情况帮助最大，给予这些片段更高权重来指导微调 arxiv.org 。简言之，该框架模仿人类温习知识的过程：在需要时重点复习遗忘的难点，从而高效恢复技能。整个方法适用于没有明确任务划分的开放式场景，作者在 LIBERO等操作任务基准上测试了该方法，结果表明即使任务无明显边界，机器人也能持续学到新技能并在需要时重新找回旧技能，大幅优于不采用检索适应的基线。 核心创新：该方法的创新之处在于提出了任务无关的回顾与适应机制。传统持续学习通常假定任务边界明确（如任务开始和结束）以便采取对应策略，比如任务后冻结部分网络或保存样本。然而现实中机器人可能连续不断遇到各种挑战，无法提前知道哪些属于同一任务。本文通过检索式记忆绕过了显式的任务划分：无论当前遇到的情况是否是以前练过的任务，机器人都可以基于当前观测从记忆库检索类似经验。这赋予了模型一种情景感知能力，让它能够自行判定何时需要参考旧经验。其次，提出的Weighted Local Adaptation将元学习思想引入了持续学习测试阶段：不像传统只在训练阶段防遗忘，这里在测试阶段也运行一次短暂微调，相当于在机器人执行时刻临时提高旧技能专门性。这种做法以往少见，因为通常假设模型定型后就执行不变，但在终身学习背景下，适度的测试微调可以极大提升旧技能复现效果。通过限制微调只针对检索到的示例且有选择地重点学习难点，既保证了微调幅度受控不会破坏模型总体性能，又有针对性地弥补遗忘。第三，任务无关意味着模型无需任务标签输入或边界信号，这对开放环境学习非常关键。作者利用视觉和语言嵌入的一致性作为检索键值 arxiv.org ，避免了因持续学习造成的表征漂移（通过预训练模型确保不同阶段embedding空间一致）。这保证了即使模型学了新东西，仍能用统一标准去查找旧记忆，提高了检索可靠性。 与传统方法区别：相比 EWC等在训练时防遗忘的方法，该框架将遗忘补救延伸到了测试时刻：传统模型一旦训练完部署，如果遗忘了旧技能往往束手无策；而此方法允许机器人在执行某任务前“温习一遍”相关经验，相当于给模型一个自行恢复的机会。与 GEM 等在训练时用小样本回放不同，这里的记忆检索主要服务于执行阶段的微调，而非整个训练过程持续混入，因而不会显著增加训练难度，同时又充分利用了记忆在关键时刻的价值。和 iCaRL 直接根据记忆做最近邻推断不同，该方法选择的是微调模型，因此能适应当前状况的细微差别，而不是简单模式匹配，效果更强。在没有任务ID的情况下，很多传统方法如多头网络就无法应用，而本方法完全不依赖任务标签或边界，通过检索+微调实现了隐式的任务处理。可以将其视作结合了经验回放和快速自适应：既保留少量记忆（回放理念），又在需要时通过微调快速适应（元学习理念），融合了二者优点。 具身场景下的优势：对于在动态未知环境中工作的机器人来说，该方法提供了极高的灵活性。机器人无需预先知道将面对哪些任务，也不用在模型结构上做固定分配，它可以不断遇新仍保持从容：每当遗忘可能影响当前任务时，就从记忆中找回相关信息补强自己。这很像人类在现场快速翻阅笔记确认知识点，从而表现出色。尤其在长期部署中，机器人可能几天甚至几月不执行某项技能，这种情况下直接执行可能失败，但如果允许机器人在执行前复习一下过去案例，就能大幅提升成功率。作者的方法正是提供了这种快速热身功能，让机器人在现实使用中更可靠。其次，由于只存储挑选的一小部分示范数据，记忆库很轻便，不会像保存所有数据那样不现实。并且由于采用预训练embedding统一表示，不同时间学习的经验可以共同检索，这意味着机器人可以在跨环境、跨时间的情况下整合经验，非常符合具身智能需要持续集成多方信息的特性。最后，实验中该方法在没有任务边界信息的情况下，其性能超出了使用明确任务边界且大量回放的策略，体现出开放场景适应性。总之，此方法赋予机器人一种人类般的学习策略：即使偶尔遗忘细节，通过快速翻阅过往经历又能想起来，从而在不断变化的任务挑战中保持整体能力不退化，为具身智能的长期自主学习提供了新的思路。 方法演化趋势与对比 综上所述，2024–2025 年关于具身智能持续学习的方法展现出一些共同的发展趋势与对过去方法的根本区别： 从参数保护到知识重组：过去方法（EWC 等）通过阻止参数改动来防遗忘，但在机器人复杂任务中效果不佳。新方法更强调对知识的表达与重组，如通过前缀提示、技能库、知识空间等将知识单元化，避免直接在同一参数上博弈。这些机制允许共享与隔离并存：共享的是不同任务的共性（如PSPL原语、LEGION语言嵌入），隔离的是各任务独特部分（如每任务LoRA、每技能提示）。因此新方法能在不牺牲塑性前提下保持稳定性。 从任务清晰到任务模糊：许多新工作针对任务边界不明的现实。比如任务无关检索、CAMA 的置信度调整都不需要预先知道任务什么时候切换。相比之下，传统方法大多假定任务切换明确并提供信号，现实中难以满足。新方法通过连续监测模型行为或环境，相当于赋予模型自适应觉察能力，真正做到持续学习“终身运行”而非分阶段训练。 从被动防御到主动利用：以前方法把旧知识当需要保护的内容，新方法则倾向主动利用旧知识来帮助新学习。例如PSPL利用旧技能提示加速新技能习得、LEGION 用旧知识簇推断长序列任务、LEAGUE++ 复用已学技能解决新问题。这种正向迁移思路解决了传统方法虽然防遗忘但也不擅长迁移的问题，新方法在防止遗忘的同时显著提高了前向迁移性能，使机器人越学技能越多，解决新任务反而越快。 减少对存储和重放的依赖：传统方法如经验重放在机器人上往往不可行（存储无限增长且隐私风险）。许多新方法都强调“不依赖回放”：PSPL 无需回放旧经验、CAMA 无需存样本只存统计、LEGION 只以有限回放辅以知识空间、Hypernetwork 方法不重训旧示范。即便需要，也以小规模记忆或生成器代替全量存储（如Task-Unaware方法的小记忆+检索、本质上非常有限）。这使持续学习更能适应机器人存储和计算受限的条件。 引入大模型与多模态：与以往专注单一模型训练不同，新方法勇于将预训练大模型纳入持续学习框架，如利用LLM规划子任务（LEAGUE++）、用VLM自我生成知识（ICAL）、大模型本身作为路由器和骨干（MLLM-CL）。多模态信息（语言、视觉、触觉）也被融入，如LEGION用语言辅助任务编码、PSPL用文本和光流提示、ICAL用视频+语言反馈。这些让机器人更好地理解任务和环境，也提供了额外手段缓解遗忘（例如语言描述可作为语义锚点，帮助模型回忆对应技能）。这是持续学习与大模型、多模态技术的交叉融合，标志着具身智能持续学习进入一个更智能更复杂的阶段。 小结：传统持续学习方法在具身智能情境下面临诸多挑战，如参数共用导致遗忘、任务未知导致方法失效、资源有限导致回放不可行等。2024–2025 年的一系列新方法通过全新的机制——提示网络、非参知识库、超网络稳定、符号技能库、任务无关调整、大模型自监督等，成功地解决或缓解了这些问题。在机器人连续学习复杂技能、跨模态理解指令的任务中，这些方法展现出显著优于旧方法的性能，有的甚至实现了遗忘几乎为零且持续正迁移的效果。可以预见，未来具身智能持续学习将沿着模块化+共享、智能检索+自适应、融合大模型知识的路线继续发展，使智能体更接近人类的终身学习水平，在不断变化的世界中保持学习新知识的同时不忘记旧本领。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-16/","summary":"Avoid Catastrophy forget","title":"Bug Journal 2025-06-16"},{"content":"一、Vision-Language-Action (VLA) 模型 论文 目标 创新 平台 Code 算力 总结 UniAct 将 28 种机器人异构动作映射到统一“通用动作空间”，提升跨形态迁移 动作离散化＋对比约束学习，0.5 B 参数模型优于 7 B 基线 Open-X Embodiment、Libero、Droid；64×A100 训练 GitHub 64 × A100，10 天训练 UniAct 针对不同机器人动作空间异构的问题，提出“Universal Action Space”，把 28 种平台的演示映射到一组离散原子行为，使跨形态学习成为可能。该框架通过语义对比与动作重构双重约束，让 0.5 B 参数模型在多任务操控上超越 7 B 基线。实验表明，在 OXE 与 LIBERO 任务上显著提高数据效率与泛化性。 MoManipVLA 把固定底座 VLA 策略快速迁移到移动底座 双退火搜索选基座位姿＋SLSQP 精细优化 OVMM benchmark；Hexman Echo Plus + RM65 TBD 4×RTX 3090（作者建议） MoManipVLA 设计“双退火 + SLSQP”策略，把固定底座 VLA 生成的末端轨迹快速迁移到移动底座，实现零样本导航-操控一体。方法不改动原 VLA，只在推理阶段搜索最优基座位姿并微调手臂解算器。仿真与 Hexman + RM65 实验展示跨房间场景的 70 %→89 % 成功率提升。 CoT-VLA 为 VLA 引入显式visual chain-of-thought时序规划 先自回归预测未来视觉帧，再输出短动作序列 PyBullet 6DoF 手臂模拟 TBD 8×A100（7 B 模型预训练） CoT-VLA 将“视觉链式思考”引入 VLA：模型先自回归预测未来图像帧，作为显式视觉目标，再生成短动作序列。这样的分两步推理显著提升复杂操作的时间规划能力，并在 RT-X benchmark 刷新成绩。其 7 B 版本在无需额外模态标签下超越同规模基线。 SOLAMI 3D 虚拟角色“看-说-动”社交交互 合成 SynMSI 多模态对话 + 双塔解耦发声/动作 Oculus Quest 3 VR 前端；2×H800 推理 GitHub 32×V100 预训，16×V100 指令微调 SOLAMI 是首个端到端“社交” VLA 框架，可同时生成语音与动作驱动 3D 角色与人沉浸式互动。作者合成 6.3 K 多轮多模态对话数据 SynMSI 并预训练双塔模型，显著提升动作-语音一致性。用户 VR 研究验证，其角色在情感贴合度上优于 GPT-4o + 动画基线。 PVM Revisit 系统评测 PVM 预训练策略在机器人任务中的效果 提出 SlotMIM：在非对象中心数据上也保留 object-centric 表征 Franka Kitchen / Meta-World / Habitat GitHub 8×A100 训练 论文系统比较 DINO、iBOT、MAE 等预训练方法对机器人任务影响，发现“对象中心”特征是关键。为解决非对象中心数据劣化，提出 SlotMIM：语义瓶颈 + 跨视图一致约束诱导对象显式槽位。在 Franka Kitchen、Habitat 等 8 项任务全面超越现有 PVM。 Think Small, Act Big (PPL) 终身学习中复用“动作基元”避免遗忘 Primitive Prompt+Motion-Aware Query与 Diffusion Transformer MimicGen + LIBERO (sim)；Franka Panda TBD 4×A100（预训），1×A40（增量学习） Primitive Prompt Learning 首先在多技能库中学习可重用“动作基元”提示，再在终身阶段冻结旧提示、增量插入新提示，缓解灾难遗忘。模型利用光流-文本查询选择最相关基元，引导条件扩散 Transformer 产出动作。仿真与 Franka 实测均较 PEFT + 经验回放提升 20 % +。 Phoenix 失败后自我反思并细粒度修正动作 LLM 生成“语义-动作”双反思 → Diffusion Policy 校正 RoboMimic 仿真 + UR5 实机 GitHub 单张 RTX 4070 即可微调 Phoenix 通过 MLLM 生成“语义反思+动作反思”文本，先粗调运动指令，再用条件扩散策略做高频微调，实现失败后的自我纠正。框架把泛化压力转移到 LLM 层，低阶策略仅需少量新数据迭代。RoboMimic \u0026amp; UR5 显示跨任务鲁棒性提升 25 %。 OmniManip 物体中心交互基元 + VLM 推理，实现开箱即用操控 定义 canonical 空间 + 方向约束；零样本泛化 Franka Panda + RealSense GitHub TBD 4×A6000 训练 OmniManip 引入 “Object-Centric Interaction Primitive” 作为中间空间约束，让 VLM 只需输出 3D 关键点与方向即可驱动精密操控。通过零样本 IK + 轨迹优化，在 Franka 上完成 12 项复杂任务，平均成功率 78 %。方法避免昂贵 robot-Finetune，兼具成本与泛化优势。 Domain Discrepancy Mitigation 减小人演示与机器人视觉差 Vision Adapter 对齐嵌入；双塔 CLIP 训练 RLBench；xArm 7 GitHub 4×A6000 论文揭示人演示视频与机器人视觉存在形态与尺度差异，导致直接迁移失效。作者在视觉编码器尾部插入可学习适配器，并用跨域对比损失同时拉近语义与几何分布。RLBench 与 xArm7 实验显示在 unseen-object 设定下成功率翻倍。 Object-Centric Prompt-Driven (CrayonRobo) 用彩线标注关键方向减少冗余 四色线标 + GPT 选择最相关 3 条 SAPIEN + Franka GitHub \u0026gt;40 GB VRAM 研究用四色线条在图像中显式标注接触点与位姿方向，减少语言与视觉冗余。GPT 自动从 32 候选线中挑 3 条最相关提示，送入 CLIP-LLM 预测 SE(3) 接触位姿后由 IK 执行。模拟与 Franka 真实机结果优于端到端 VLM 8–12 pp。 Robotic Visual Instruction 用箭头/圆圈视觉语言代替冗余文本 手工 15 K 图像标注 RoVI 语言；VLM 直接生成代码 UR5 / XArm6 + SAPIEN GitHub TBD 1×A40 RoVI 提出箭头、圆圈、颜色、数字四元素的视觉指令语言，解决纯语音交互空间精度不足与嘈杂场景受限问题。作者手工标注 15 K 图像并用 VLM 生成任务代码，机器人可直接按图索骥。UR5 /X-Arm6 实验展示在无语音环境中完成多步装配。 RoboGround 结合语言先验做抓取/推动等 统一 Vision-Language-Action Prompt SAPIEN；Franka Panda GitHub 8×A100（训练） RoboGround 以 grounding-mask 作为视觉中介，将对象定位与策略网络分离；同时自动合成大规模仿真数据扩展训练域。结果在 see/unseen 物体抓取、推拉任务上均超越 End-to-End Diffusion Policy。 二、Policy / Diffusion 控制 论文 目标 创新 平台 Code 算力 总结 KStar Diffuser 双臂协作轨迹生成 物理关节动态图 + 可微前向运动学 Bimanual Franka (sim \u0026amp; real) TBD 8×A100 KStar Diffuser 用机器人双臂物理拓扑构造动态时空图并融入可微正运动学，引导扩散过程生成协调关节动作。此设计在 bimanual 夹取与对接任务上比 baseline Policy Diffusion 提升 17 %。引入 kinematic loss 使收敛速度加快 35 %。 RoboPEPP 视觉-关节姿态预训练 时序预测任务 + 姿态编码器 Isaac Gym 7DoF GitHub 4×A6000 RoboPEPP 把“遮盖-预测”自监督移植到机器人图像，强迫编码器重建被 Mask 关节嵌入，从而学到物理结构感知。微调后在多数据集姿态估计误差降低 30 %，对遮挡鲁棒性最强且推理耗时最低。 Lift3D Policy 将 2D 基础模型迁移到 3D 抓取 深度 Lift 模块 + Domain Adapt - GitHub 8×A100 Lift3D 先用任务感知 MAE 为 2D 基础模型注入深度重建能力，再通过坐标映射“抬升”到点云编码，构建显式 3D 表征。方法在 ManiSkill 等 3D 任务大幅超过纯 2D 预训和显式 3D CNN 基线。 PDFactor 三视角扩散场统一多任务策略 Tri-Perspective 视图条件扩散 Meta-World GitHub 8×A6000 PDFactor 以“鸟瞰-第一视角-自由视角”三视图为条件，学习统一 Policy Diffusion Field，同步处理抓取、转动、插配多任务。多视角条件显著提高迁移，CVPR 实验成功率相较单视角提升 22 %。 Two by Two 跨任务配对装配 Pairwise Object Assembly + Transformer SAPIEN GitHub 4×A100 作者发布含 517 物体对、18 类日常装配任务的大型 2BY2 数据集，并提出等变 SE(3) 两步姿态估计方法。新方法在所有装配任务上刷新 SOTA，并在真实机器人验证兼容性。 FlowRAM Region-Aware Mamba + Flow Matching 局部注意 + OT 监督 RLBench TBD 4×A40 FlowRAM 结合 Region-Aware Mamba 感知器与 Flow-Matching 生成器，统一视觉编码与动作扩散，提高采样效率。测试显示对 occlusion 与 multi-object 情况鲁棒性显著提升。 G3Flow 3D 语义流生成多场景抓放 TSDF + Diffusion Flow Habitat GitHub 8×A100 G3Flow 用 TSDF 构建稠密场景，再以生成式 3D 语义流预测目标-手序列，实现跨场景抓-放一体。Pose-aware 设计让训练迭代减少 40 %，零样本任务成功率提升 18 %。 DexHandDiff 自适应灵巧手规划 接触感知 diff + 物理约束 Shadow Hand TBD 8×A100 DexHandDiff 引入手-物接触显式编码与能量约束到扩散规划中，避免“漂浮抓取”幽灵态。框架在 Shadow Hand 抓转与重定位七项基准刷新记录。 Tra-MoE 多域轨迹预测条件策略 动态专家 gate + BC 初始化 ManiSkill2 TBD 4×RTX 3090 Tra-MoE 通过稀疏门控专家网络吸收多域外数据，预测任意目标轨迹，并以可学习 2D mask 对齐视觉观测指导策略。实验证明在 DomainGap20 % 情况下仍维持高成功率。 AffordDP 利用可迁移 affordance 的泛化策略 Affordance Mask + Diffusion Policy LIBERO TBD 4×A6000 AffordDP 把 3D 接触点 + 轨迹定义为可迁移 affordance，并在扩散采样中注入 6D 变换引导，支持 unseen 类别操控。与原 Diffusion Policy 比，新架构在真实实验 unseen 物体上提升 30 % 成功率。 三、Grasp 论文 目标 创新 平台 Code 算力 总结 UniGraspTransformer 通用抓取蒸馏 Transformer-Distil 模型简化训练 Shadow Hand (sim) GitHub 4×A40 提出通用 Transformer 抓取网络及简化蒸馏流程，既减低训练成本，又扩展到海量手型。实验证实缩短 40 % 训练时间同时保持高成功率。 DexGrasp Anything 面向任意物体的物理感知抓取 物理引擎约束 Embedding Franka + Mujoco GitHub 8×A100 DexGrasp Anything 在训练和采样阶段显式加入物理约束，并发布 3.4 M 抓姿-15 K 物体数据集。方法在多 benchmark 抓取精度全线领先。 ZeroGrasp 零样本形状重建驱动抓取 SDF ↔ Partial Depth BP Isaac Gym GitHub 4×A6000 ZeroGrasp 联合实时 3D 重建与抓姿预测，实现 zero-shot 抓取；采用 SDF 先验避免碰撞。近实时推理 (\u0026lt; 50 ms) 在模拟和 Franka 验证效果稳定。 四、Humanoid 论文 目标 创新 平台 Code 算力 总结 Humanoid Hiking 复杂崎岖地形步行 多技能 Curriculum + Terrain Adapt Unitree H1 TBD 8×A100 LEGO-H 框架通过层次 Transformer 预测未来局部目标，并以特权学习将视觉导航与步态控制整合，使模拟 humanoid 能走崎岖山路。跨多机器人形态验证了迁移与鲁棒性。 MobileH2R 合成数据学手递物 Unity-based Synthetic Human→Robot 数据 Clearpath Ridgeback + UR5 GitHub 4×A40 MobileH2R 用 Unity 合成高质量人-机器人交接数据，使移动底座在大空间内可靠收物。Sim-to-Real 迁移到 Ridgeback + UR5，达到 90 % 交接成功率。 五、3D Vision \u0026amp; Perception 论文 目标 创新 平台 Code 算力 总结 3D-MVP 用多视图 MAE 预训 3D 表征 分离视图编码 + Objaverse 预训 RVT + ManiSkill TBD 8×A100 3D-MVP 把 MAE 扩展到多视图 point-cloud，利用 RVT 对齐 voxel-level 特征；在多任务上全面提升 3D 理解与操控性能。 VidBot 2D 野外视频 → 零样本 3D 轨迹 SfM + 粗-细阶段扩散 Hexman Echo Plus TBD 4×A6000 VidBot 从网络 2D 视频自动恢复手轨迹与目标点，再通过粗-细扩散生成 3D 交互轨迹，实现零样本部署；在 13 项家庭任务上显著超越 SOTA。 Touch2Shape 触觉条件 3D 形状扩散 Vision-Touch 融合扩散 GelSight + iCub 手 TBD 4×A100 Touch2Shape 将 GelSight 触觉嵌入 3D Diffusion，结合 RL 引导探索，实现高保真局部细节重建；在触觉+视觉形状重建基准取得最佳分数。 六、Planning \u0026amp; Reasoning 论文 目标 创新 平台 Code 算力 总结 RoboBrain 从抽象到具体统一规划 层级 Brain 模型 + 递归搜索 SAPIEN (多机器人) GitHub 8×A100 RoboBrain 融合机器人与通用多模态数据，多阶段训练 MLLM，能处理长视频 + 高分辨率图像并输出操控计划，在多任务刷新成绩。 PhysVLM 让 VLM 知道“够不着” S-P Reachability Map 注入视觉编码 PyBullet + UR3/XArm6 GitHub 8×A800 × 48h PhysVLM 预先计算 Reachability Map 并注入 SigLip + Qwen 语义空间，LLM 可回答“到不了哪里”并辅助 VoxPoser 规划；零样本实机验证成功率最高。 RoboSpatial 训练 VLM 空间推理三视角 1 M 图 + 5 K 扫描数据集 Kinova Jaco + cuRobo GitHub 8×H100 × 20–40h ROBOSPATIAL 构造百万图像 + 5 K 3D 扫描数据集，以三视角问答方式训练 VLM 空间理解；Kinova 抓放实验超越 GPT-4o。 Tartan IMU 轻量惯导定位 F-model Transformer-Tiny + IMU 重加权 Tartan Drive Hugging Face TBD 单 RTX 2080 提出轻量级 Transformer 模型仅用 IMU 即可做定位，统一导航与姿态估计；在 TartanDrive 数据集上达成 SOTA。 Code-as-Monitor 视觉编程自动生成约束检测 LLM-to-Code + 生命周期触发 Amazon AWS 码实验室 TBD CPU-only CaM 将开集故障判定统一为约束求解问题，用 VLM 生成 Python 代码实时监控。系统在三模拟器+真实场景把成功率提高 28.7 %。 七、Video \u0026amp; Representation 论文 目标 创新 平台 Code 算力 总结 TASTE-Rob 任务导向手-物交互视频生成 三阶段：DynamiCrafter → MDM → SD-Adapter SAPIEN (sim) GitHub 8×A100 三阶段视频生成管线（DynamiCrafter→MDM→SD-Adapter）让语言-场景-手姿一致，可直接用作模仿学习数据；SAPIEN 验证动作可复现。 GraphMimic 视频 → Graph-to-Graph 生成策略 Skeleton-Graph VAE RoboMimic TBD 4×A6000 GraphMimic 把视频抽象为时序场景-动作图，并训练 Graph-to-Graphs 生成器预训练策略网络，在少量下游数据时显著提升。 八、Sim2Real \u0026amp; 机器人模型 论文 目标 创新 平台 Code 算力 总结 Prof. Robot 无自碰 \u0026amp; 无静态穿模可微渲染 带 Signed Distance 渲染器 Blender + PyTorch3D GitHub 1×A40 在可微渲染中加入 Eikonal 正则化学习碰撞分类器，实现无静态 / 自碰的梯度优化动作；对比 Dr.Robot 成本相当但碰撞率降低一半。 AutoURDF 无监督点云→URDF 模型 Cluster Registration + IK 估计 RealSense 点云 GitHub 1×RTX 3090 AutoURDF 通过点云聚类配准，无监督推断关节拓扑与参数，自动生成符合主流模拟器的 URDF；在多机器人扫描数据集精度领先。 九、基准与数据集 论文 目标 创新 规模 Code / 数据 算力 总结 RoboTwin 双臂数字孪生基准 生成式 Digital Twin 105 场景 × 10 K 轨迹 GitHub 数据预处理需 8×CPU RoboTwin 使用 3D 生成与 LLM 产出多样专家示范，建立双臂数字孪生评测；提供 105 场景、10 K 轨迹，填补双臂基准空白。 Pixel-aligned RGB-NIR Stereo 近红外+RGB 对齐立体数据 同轴 NIR Stereo 标定 50 K 对 TBD 无 提出同轴 RGB-NIR 立体系统并发布多光照数据集，示范两种融合方法显著改进暗光下深度与语义性能。 RoboSense 拥挤非结构环境 egocentric 感知 3D 相机 + 激光混合注释 150 K 帧，900 min 视频 GitHub 无 RoboSense 搭建 360° 相机-LiDAR 采集平台，发布 133 K 帧、1.4 M 3D box 的拥挤非结构环境数据，并定义近场匹配指标，覆盖 KITTI 270× 标注量。 ","permalink":"https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/","summary":"CVPR 2025 Robotics Paper Summary","title":"Bug Journal CVPR2025-Summary"},{"content":"快速文章阅读 prompt, 用于找到一个自己喜欢的题目\n这篇文章要做什么，目标是什么 动机是什么 数据是从哪里来的 算力要求多少 公开代码吗 For Robotics: 现在有这一篇文章： \u0026lt;文章标题\u0026gt; 请用中文回答我： 这篇文章要做什么，目标是什么 动机是什么 数据是从哪里来的 算力要求多少 公开代码吗 模拟环境用的是什么平台 现实环境用的是什么平台 ","permalink":"https://tzj2006.github.io/bugjournal/2025-06-14/","summary":"Prompt for fast paper read","title":"Bug Journal 2025-06-14"},{"content":"Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction CVPR 2025\nFrom 人大 \u0026amp; 上海AI lab\n要达成的事情：\n让机器人能够自我反思到底是哪里做得不好，然后自我调整\n动机：\n人类可以很自然地反思：为什么失败了，为什么机器人不行呢？\n模型实现方式：\n首先，这是一个“语言指导”的RL方法。\n对于这个方法，首先由 LLM 生成一个文字指令：\n比如：现在我要移动一个杯子，我要怎么做\n然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。\n如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。\n现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL\n最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。\n对于修正指令这个步骤，模型一共会输出两条语句，分别是 \u0026ldquo;语义上的反思\u0026rdquo; (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)\n最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。\n那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。\n结果：\n更强的学习能力，更强的泛化能力。\n计算要求：\n仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行\n虚拟环境为 RoboMimic 模拟器\n使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs\nPhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability CVPR 2025\nFrom 北京交通大学 \u0026amp; 广东技术师范大学\n目标： 告诉机器人什么位置它到不了\n动机： 有时候机器人不知道一个位置到不到得了，结果把自己搞坏了\n模型流程：\n首先离线计算什么位置是机械臂能达到的。 形成一个点云 (S-P Map)\n然后用 SigLip-400M 提取图像和点云的特征\n然后把这个 embedding 和文字的 embedding 混合之后\n通过 Qwen-2.5-Instruct-3B，生成一个回答给 VoxPoser 做。\n实验设计：\n仿真：基于 CUDA 11.x 的 PyBullet 环境，六种机器人（UR5、FR5、CR5、FRANKA、UR3、XArm6），共 1.3K 问答，零样本评测。\n实机：在 UR3、XArm6 真实平台上各 10 次零样本任务，评估任务成功率。\n评估指标：\nEQA-phys：基于 LLM 评分（5 分正确、1 分错误）计算平均分； RoboVQA-val / OpenEQA：标准 BLEU-4 与准确率； 任务规划：成功率。\n结果：\nS-P Map 在很多 LLM 上都有用\nPhysVLM-3B 效果平均最好\n数据集： Zero-shot\n算力要求：\n\u0026lt; 48h * 8 * A800\n代码：\n开源\nObject-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation CVPR 2025\nFrom PKU Agibot Lab\n目标： 帮助机器人排除冗余信息干扰\n动机： 语言和视频中冗余信息过多\n模型流程：\n在图片上加一些标记 分别是：\n接触点（蓝色） 末端执行器在接触时的 z 轴方向（红色） y 轴方向（绿色） 接触后移动方向（黄色） 这些标记可能来自 Grounding-Dino + SAM 或者 ChatGPT-4o 或者 手动标记\n标记方式如下：\n均匀生成 N(32) 条线，让 GPT 选择 3 条标上颜色\n然后把这个“增强”过后的信息输入给 CLIP,然后再让 LLM 输出\u0026quot;应该在哪里，以什么角度接触\u0026quot;\n对于这个信息，我们可以和GT 做 train\n最后，我们得到了这个位置和姿态的信息，我们就可以用IK求解路径了。\n结果：\n数据集：\n模拟环境：SAPIEN + PartNet-Mobility •\t平台：使用 SAPIEN（一个支持刚体与关节物体的物理模拟器），其提供高保真动力学与渲染接口，适合零样本机器人操控研究 ￼ ￼。 •\t资产集：加载 PartNet-Mobility 中的 1,500 多个关节化 CAD 模型，涵盖抽屉、门、笔记本电脑盖等常见家电与家具 ￼。 •\t飞行夹持器（Flying Gripper）：采用 SAPIEN 中的“飞行”版 Franka Panda Gripper，使得末端执行器可在自由空间中无碰撞地移动，以便专注测试模型对提示的理解能力 ￼。 •\t摄像机随机化：在每个场景随机采样摄像机视角（水平±45°、俯仰30°–60°），并在单帧中记录 RGB 图像与 3D 末端执行器位姿，用于提示生成与训练 ￼。 •\t数据采集：共采集 ~10,000 条训练样本，过程耗时约 6–8 小时；测试集与训练集按照固定比例拆分，模拟了 Seen/Unseen 物体类别的泛化能力评估 ￼ ￼。 现实机器人平台 •\t硬件平台：使用真实的 Franka Emika Panda 机械臂（7 自由度，集成高精度力矩传感器），配合标准的 RGB-D 摄像头（如 Intel RealSense D415）采集视觉输入 ￼。 •\t执行流程：将 CrayonRobo 在模拟环境中输出的 SE(3) 接触位姿与移动方向，通过 ROS + MoveIt! 的 IK 求解与笛卡尔轨迹规划一键下发真实机器人执行 ￼。 •\t测试任务：与模拟中一致，包括“拉抽屉”“开门”“掀笔记本盖”等单步原子操作，每个任务在 5–10 个不同初始姿态下重复试验。 •\t评价指标：以二值成功率衡量：当末端执行器按照预测方向将物体部件移动超过阈值（如 5 cm）即记为成功；平均成功率达到 74%（Seen）与 72%（Unseen） ￼。 算力要求： 未知 建议 \u0026gt; 40 GB VRAM\n代码： 开源\nCheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation CVPR 2025\nFrom PKU Agibot lab\n目标： 让机器人读取说明书之后根据说明书做出正确操作\n动机： 阅读说明书\n电子产品有时候视觉信息不够，很多有着相似外观的东西可能有着不同的功能。\n所以要读说明书\n模型流程：\nOCR → GPT-4o 提取图文信息 → GPT‑4o 生成操作步骤 → SoM、Grounding‑DINO、SAM 实现视觉对齐 → FoundationPose 匹配 CAD\n最后输出{部件，操作} pair, 最后使用 VoxPoser 进行操作。\n实验设置 模拟平台：SAPIEN 0.8 + Franka Panda 飞行夹具 ￼\n数据集： PartNet-Mobility CAD 模型； CheckManual 合成说明书（已公开，可下载使用） ￼\n评估指标： 任务完成率\n现实验证： Franka + RealSense 摄像头，完成单个用例的实物测试\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\nCode availability: 开源\n结果：\n总之有 manual 效果更好\nROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics CVPR 2025\nFrom OSU and NVIDIA\n目标：\n引导 VLM 2D \u0026amp; 3D 视觉，理解空间结构。\n动机：\nVLM 目前无法理解空间结构。 原因并非 VLM 不行，而是数据不够\u0026quot;spatial\u0026quot;\n模型实现细节：\n首先是数据收集：\n输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”\n同时，生成一个俯视图，来看看哪里适合放置一个物体。\n最后在看看这个物体是否适合被放在这个地方。\n对于物体的方位，每一次会从三个角度问问题：\n以机器人为中心的视角 (第一视角) 以物体为中心的视角 (第三视角) 以世界为中心的视角 (fix-cam)\n自我中心：“从您的视角看，书在电脑的左边吗？” 以物体为中心：“从电脑的视角看，书在电脑的左边吗？” 以世界为中心：“从海拔高度看，书在电脑的上方吗？” 这样的好处是可以让 VLM 有更强的空间理解\n结果：\n在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o\n算力要求：\n20-40h * 8 * H100\n模拟环境：\nROBOSPATIAL 数据集，这包括 ScanNet, Matterport3D, 3RScan, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。\n真实环境：\nKinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。 机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。\nRobotic Visual Instruction: A New Paradigm for Human-Robot Communication CVPR 2025\nFrom IC + 上海 AI lab + UCSD \u0026hellip;\n目标：\n更好的人机交互\n动机：\n语言有很多冗余信息，那在图片中增加信息不就行了？\n模型实现方式：\n机器人视觉指令 (RoVI) RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：\n箭头：指示运动方向和轨迹 圆圈：标记感兴趣的对象或动作目标 颜色：区分多个指令或动作步骤 数字：指示操作顺序 这种视觉语言具有以下几个优点：\n空间精确性：视觉标记精确地指示3D空间中的位置和路径 时间清晰性：顺序步骤被清晰地划分 直观设计：这些符号易于人类理解和创建 跨文化实用性：视觉指令超越语言障碍 RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。\n靠手动标记了 15K 图片。。。 工作量爆炸💥 而且以后也要人手标。。。\n🤔： 为什么不能自动标，难道作者没有想过这一点吗\n但总之，现在在这个图像的基础上，VLM 会帮忙生成：\n任务的文字描述 可以执行这些任务的代码 代码中包含：\n路径 起点，终点，过程点\n然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取\n算力要求：\nNvidia A40\n现实实验设置：\nUFACTORY X-Arm 6和UR5 两台机械臂 两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。 两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。\n模拟环境设置：\n使用了SAPIEN 作为模拟器。 SIMPLER 作为基础环境。\nTASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation CVPR 2025\nFrom Xiaoguang Han\u0026rsquo;s Lab at 港中深\n目标： 优化对齐视频-人手数据集\n动机： 现在有这些问题：\n视角不一致 动作语义无法对齐 手部姿态稳定性不高 这个模型想要解决这些问题 模型流程：\n数据集构建（Sec. 3）： •\t100,856 条 1–8 秒单动作视频，静态 1080p 视角，语义指令一一对应。 粗视频生成（Stage I – Coarse Action Planner）： •\t基于 DynamiCrafter（512 × 512 分辨率），语言指令＋静态环境图→粗交互视频； •\t微调参数：batch=16, lr=5×10⁻⁵, 30K steps。 姿态优化（Stage II – MDM Refinement）： •\t使用 Motion Diffusion Model 以 60 帧手部关键点序列为条件，保证握姿一致性； •\t训练设置：batch=64, lr=1×10⁻⁴, 500K steps; 推理 10-step DDIM denoise。 最终生成（Stage III – Frame-wise Adapter）： •\t将优化后手部姿态图像与环境图和语言指令一起接入 Stable Diffusion 2 Adapter，生成最终视频； •\t训练设置：batch=32, lr=5×10⁻⁵, 30K steps。 实验设置 •\t仿真验证：在 SAPIEN 模拟平台上，用 DynamiCrafter＋MDM + Adapter 生成视频，并通过 Figure 17 示范在虚拟机器人上的模仿效果。\n模型流程：\n第一阶段：Coarse Action Planner（粗动作生成） •\t目标：在给定语言指令（如“拿起杯子并倒入水”）和静态环境图（512×512 分辨率）条件下，生成一段粗略的、合理的手–物体交互视频序列（通常 16 帧）。 •\t模型架构：基于 Video Diffusion Model（VDM），如 DynamiCrafter，使用文本–图像条件扩散网络。 •\t训练细节： •\tBatch size = 16，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t推理时使用 50-step DDIM 采样，平衡生成质量与速度。 •\t输出：一段低分辨率、含大致手部运动轨迹和物体交互的粗视频，用于后续姿态细化。\n⸻\n第二阶段：Motion Diffusion Model (MDM) Refinement（姿态细化） •\t目标：针对第一阶段生成的粗视频，从中提取手部的 3D 关键点序列（60 帧），并利用扩散模型细化运动轨迹，提升抓握姿态的连贯性与自然度。 •\t模型架构：1D 时序扩散网络，对帧间手部关键点做去噪与重建。 •\t训练细节： •\tBatch size = 64，学习率 1×10⁻⁴； •\t训练步数 500K steps； •\t推理时使用 10-step DDIM，快速得到精细关键点序列。 •\t输出：一组平滑、符合物理约束的 3D 手部关键点轨迹，用以指导下一阶段的视频生成。\n⸻\n第三阶段：Frame-wise Adapter（帧级最终生成） •\t目标：将姿态细化后的关键点与原环境图、语言指令结合，生成最终高质量、手部姿态稳定的交互视频。 •\t模型架构：基于 Stable Diffusion 2，通过 Adapter 模块将手部姿态（以可视化关节点或姿态图形式）作为条件，连同环境图与文本，一起输入扩散模型。 •\t训练细节： •\tBatch size = 32，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t采用与 Stage I 相似的 DDIM 采样流程，保证画质与动作一致性。 •\t输出：分辨率可达 512×512 的连续视频帧序列，手–物体交互清晰、握姿自然，可直接用于机器人模仿学习。\nMitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation CVPR 2025\nFrom 港科广\n目标：\n缩小人机之间的 Gap\n动机：\n人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap\n问题：\n现在有这些 Gap:\n视觉环境差异 人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。\n形态差异： 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。\n尺度和视角： 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。\n解决这一差距的传统方法分为两大类，每类都有显著的局限性：\n预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。\n在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。\n模型实现方式：\nbasically, 就是说希望用一个 Adaptor 来 fill in the gap.\n把从 human demo pretrain embedding 转换成 robot demo embedding.\n对于任务感知也是如此。\n用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。\n结果：\nAlign 一下效果变好了\n算力要求：\n4 * Nvidia A6000\n模拟环境为 RLBench\n真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头\nMoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation CVPR 2025\nFrom 北邮 + 南阳理工 + 清华\n目标：\n导航 + 空间操作\n动机：\n虽然静态的操作已经没问题了，但是若是平台移动就不太好办。\n实现细节：\n这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分\nMoManipVLA使用以下方法实现这些优化问题：\n双退火搜索算法用于基座位置寻找优化 序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案 该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：\nRGB相机进行视觉感知 关节编码器进行本体感知（感知机器人自身位置） (optional) 深度感知以增强障碍物避免\n结果：\n模拟环境：\n模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。\n真实环境（机械臂）：\n在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。\n算力：\n4 * RTX 3090\nThink Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation CVPR 2025\nFrom 上交，复旦，上海 AI lab\n目标：\n如何避免灾难性遗忘\n动机：\n有些动作有相似之处，比如递筷子和递镊子有相似之处 那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？\n模型实现细节：\nPPL 框架的核心组件包括：\n输入编码器： 本体感觉编码器：处理机器人的关节状态和夹爪姿势 视觉编码器：处理场景的 RGB 图像 光流编码器：处理光流信息以捕获运动模式 文本编码器：处理任务的语言指令\n基元提示： 跨任务共享的基本运动模式的学习表示 注入到多头自注意力层的键和值中\n终身提示 (Lifelong Prompts)： 在终身学习期间为新任务学习的特定于任务的提示 与原始提示连接以自定义模型的行为\n运动感知提示查询 (Motion-Aware Prompt Query)： 结合光流和文本指令信息 用于确定不同原始提示的相关性\n扩散Transformer (Diffusion Transformer)： 基于条件输入和提示生成机器人动作\n算力：\n论文中没有直接说明具体的GPU类型和训练时间。\n模拟环境：\n模拟实验是在基于 MimicGen 和 LIBERO 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。\n机械臂：\n真实世界的实验是在 Franka Panda 机械臂上进行的。\n数据集：\n论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。\n获取方式：\n这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。\nGenerating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision CVPR 2025\nFrom Kyoto University\n这篇论文的标题是“从自我中心视角下的动作描述生成6自由度物体操作轨迹”，其核心目标是根据文本描述和初始视觉输入来生成物体在3D空间中的操作轨迹序列。\n目标： 从自我中心视频中提取6自由度（6DoF）物体操作轨迹。 基于这些提取的轨迹和相关的动作描述，开发能够根据文本动作描述生成物体操作轨迹的模型。\n动机： 开发能够协助人类活动的交互式机器人，其中一个关键能力是让机器人能够按人类指令操作工具和物体。 训练生成此类操作轨迹的模型需要大量多样且详细的物体操作演示，但这在实际中很难大规模收集。 利用日常工作场景中的视频来提取人类演示中的各种物体操作，是一种很有前景的解决方案。 现有的3D物体操作轨迹预测方法受限于昂贵的3D注释成本，难以处理多样化的操作动作。 现有数据集（如HOT3D）中的人类运动轨迹数据有限，不足以开发能够生成物体操作轨迹的模型。\n数据来源： 训练数据： 论文使用Ego-Exo4D数据集 [30] 来构建大规模训练数据集，包含6DoF物体操作轨迹以及图像、深度图和动作描述。这个过程是自动化的，不依赖于预设的相机外部参数。\n评估数据： 论文使用现有的HOT3D [3] 数据集进行评估。HOT3D是一个用于3D手和物体跟踪的自我中心视角数据集，通过光学标记和多红外OptiTrack相机记录，提供了精确的手和物体6DoF信息。\n算力要求： 论文提到模型训练了30个epoch，批处理大小为8。优化器使用了AdamW，基础学习率为2e-5（对于LLMs）和2e-4（对于其他参数）。 为了处理旋转连续性问题，每个角度（roll, pitch, yaw）都用[cos(θ), sin(θ)]表示。 虽然论文没有直接给出具体的算力消耗数字（例如所需的GPU数量或训练时间），但从训练epoch和批处理大小来看，以及使用了BLIP-2、VILA、PointLLM和MiniGPT-3D等大型视觉语言模型作为骨干网络，可以推断出这需要相当大的计算资源。这些模型本身就比较大，并且训练涉及处理大规模视频数据。\n公开代码： 论文在摘要中明确提到了数据集和代码的链接：https://biscue5.github.io/egoscaler-project-page/。\n模拟环境和现实环境平台： 论文没有提到使用了特定的模拟环境平台。 在现实环境方面，论文主要使用了自我中心视频数据。HOT3D [3] 数据集是通过Project Aria眼镜 [21] 和Quest 3 [65] 记录的，这些是现实世界中的可穿戴设备，用于捕捉自我中心视角下的活动。这表明他们的研究是基于真实世界中的人类活动数据。\nUniversal Actions for Enhanced Embodied Foundation Models CVPR 2025 From 清华\n这篇文章介绍的 UniAct 框架，目标是解决具身基础模型在处理异构动作数据时面临的挑战，并构建一个能够在通用动作空间中操作的框架。\n主要目标：\n构建通用动作空间： 学习一种能够捕捉不同机器人通用原子行为的动作空间，从而消除机器人之间因物理形态和控制接口差异造成的动作异构性。\n实现跨形态泛化： 使得具身基础模型能够有效利用跨领域数据，并在不同的机器人形态之间实现更好的泛化控制和适应能力。\n提高模型效率： 训练一个相对较小（0.5B 参数）但性能优于更大（14倍）现有模型的具身基础模型，证明通用动作的优势。\n动机： 数据异构性挑战： 现有的大型基础模型在自然语言处理和计算机视觉领域取得了巨大成功，主要得益于海量的、多样化的互联网数据。然而，将同样的方法应用于具身智能体时，面临一个显著的挑战：不同机器人收集的动作数据存在显著的异构性，因为它们有不同的物理形态和控制接口。这种异构性严重阻碍了跨领域数据共享和通用具身基础模型的发展。\n现有解决方案的局限性： 大多数现有方法要么强制性地将不同动作空间视为等效，采用统一的离散化或归一化技术，但这可能导致动作编码的物理意义冲突；要么试图设计一个适用于各种机器人系统的物理可解释动作空间，但这需要大量人工工程，且未能充分利用不同具身动作空间之间的内在联系。\n对通用代理的需求： 开发能够处理跨任务、跨环境和跨形态泛化的通用具身基础模型，是构建通用具身智能体的一个有前景的途径。 数据来源：\nUniAct-0.5B 模型在训练时整合了来自多个开源机器人数据集的示范数据。这些数据集包括：\nOpen-X Embodiment (OXE) Libero Droid 这些数据被标准化，以包含第三人称视角观察和语言指令，同时保留了动作的异构性。总共使用了来自 28 种不同机器人形态的约 100 万个示范数据进行训练。\n算力要求： UniAct-0.5B 的训练是在 64 块 A100 GPU 上进行的，并使用了 DeepSpeed 进行优化，持续了 10 天。\n公开代码： 是的，文章中提到了项目的项目页面，通常这意味着代码是公开的： 项目页面\nSOLAMI: Social Vision‑Language‑Action Modeling for Immersive Interaction with 3D Autonomous Characters CVPR 2025\nSOLAMI 这篇文章旨在介绍一个端到端的社交视觉-语言-动作（VLA）建模框架，用于与 3D 自动角色进行沉浸式交互。\n文章的目标是构建能够感知、理解并与人类互动的 3D 自动角色，使其具备类似于人类的社交智能，通过多模态响应（语音和动作）驱动角色进行社交互动。\n这项研究的动机在于，目前的字符代理在与用户交互时，主要限于文本或语音交互，缺乏更丰富的模态。在社交互动中，沉浸感越深，人类体验越好。因此，研究人员希望构建具有更丰富模态的 3D 自动角色。此外，多模态交互数据非常稀缺，难以获取，这也促使他们开发了数据合成方法。\n数据主要来源于以下几个方面：\n交互式多模态数据（SynMSI）： 这是一个合成的多模态社交互动数据集，通过自动化流程生成，利用了现有的文本-动作数据集、基于文本的角色扮演模型和语音合成方法。SynMSI 数据集包含 6.3K 多轮多模态对话项。 运动数据：为了进行预训练阶段的运动与文本对齐，以及生成多模态数据用于指令微调，研究人员收集了包含丰富社交动作的现有数据集，例如 HumanML3D (24K 运动-文本对)、Inter-X (20K 运动-文本对和 10K 两人运动对)，以及 DLP-MoCap (2K 运动-文本对)。\n语音数据： 用于预训练阶段的语音-文本对齐，使用了 CommonVoice (150K 语音-文本对)、AnyInstruct (200K 语音-文本对和 100K 语音到语音项)，以及通过文本到语音方法（Azure TTS 和 XTTS_v2）生成的合成语音数据 (60K 语音-文本对)。\n算力要求： 在预训练阶段，SOLAMI 使用了 32 块 V100 GPU 来训练模型，批处理大小为 256。 在指令微调阶段，SOLAMI 使用了 16 块 V100 GPU，批处理大小为 48。推理时，所有模型都部署在 2 块 H800 GPU 上，并采用 vLLM 框架和异步机制来提高性能并保持公平性。\n代码： 项目的 GitHub 链接\n模拟环境在文章中没有明确提及，但实验中提到了使用 VR 界面进行用户研究，其中用户可以与各种 3D 角色进行沉浸式交互。\n现实环境指的是 VR 界面，研究人员开发了一个基于 Oculus Quest 3 前端和后端服务的 VR 界面。前端实现用户与 3D 自动角色的沉浸式交互，后端由 2 块 H800 GPU 提供算力支持。在实际使用中，VR 头显捕获用户的语音和身体动作，并将其发送到后端计算节点。\nA Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning CVPR 2025 from HKU\n这篇文章主要研究了预训练视觉模型（PVMs）在机器人学习任务中的应用，特别是视觉运动控制和感知任务。文章的目标是找出最优的预训练方法和数据来源，以提高PVMs在机器人学习任务中的表现。\n动机 文章的动机源于当前PVMs在机器人学习任务中的应用存在一些问题和局限性。尽管PVMs在传统视觉任务中表现出色，但它们在机器人学习任务中的最优配置仍不清楚。文章通过系统性的评估发现，虽然某些PVMs（如DINO和iBOT）在视觉运动控制和感知任务中表现出色，但它们在非对象中心（NOC）数据上的表现会显著下降。这种下降与它们学习对象中心表示的能力减弱密切相关。\n数据来源 文章中使用的数据集包括：\n对象中心数据集：ImageNet 场景中心数据集：COCO 网络爬取数据：CC12M 以自我为中心的数据：Ego4D 这些数据集被用来评估PVMs在不同类型的数据上的表现。\n算力要求 由于PVMs的训练和评估需要大量的计算资源，文章中提到使用了8个A100 GPU进行训练。对于某些任务，如导航任务，需要大约400M到500M步的训练和512到320个并行环境，这对计算资源提出了极高的要求。\n代码公开情况 文章中提到，他们的代码和模型是公开可用的，链接\n模拟环境 文章中使用了多个模拟环境平台进行评估，包括：\nFranka Kitchen Meta-World Habitat（用于导航任务，包含HM3D和Gibson环境） 现实环境 虽然文章主要关注模拟环境中的评估，但提到PVMs在现实环境中的应用潜力。现实环境中的平台并未在文章中具体提及，但提到了多个现实环境中的机器人学习任务和应用。\n总结 总的来说，这篇文章通过系统性的评估和实验，提出了SlotMIM方法，以有效地从NOC数据中学习对象中心的表示，并在多个任务中取得了优于现有方法的性能。文章的研究为PVMs在机器人学习任务中的应用提供了新的见解和方法。\nOmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints 这篇文章介绍了一个名为 OmniManip 的新方法，旨在实现通用机器人操作。\n这篇文章要做什么，目标是什么？ 这篇文章提出了 OmniManip，这是一种开放词汇的操控方法，旨在弥合视觉-语言模型 (VLM) 的高层推理能力与低层精确操控之间的差距。其核心目标是开发一个通用的机器人操控系统，能够通过物体中心交互基元作为空间约束，在非结构化环境中执行各种操控任务，并具有强大的零样本泛化能力。\n动机是什么？ 开发能够在非结构化环境中进行通用机器人操控的系统是一个重大挑战。虽然 VLM 在高层常识推理方面表现出色，但它们缺乏精确操控任务所需的精细 3D 空间理解能力。现有的解决方案，例如在机器人数据集上微调 VLM，面临数据收集成本高昂和泛化性差的问题。通过将机器人动作抽象为交互基元并利用 VLM 定义空间约束，是解决这些挑战的动机。\n数据是从哪里来的？ 文章中提到，为了评估 OmniManip 在真实世界场景中的操控能力，他们设计了 12 个任务来评估模型的操控能力，这些任务涵盖了各种对象和复杂环境。虽然没有明确说明具体的数据集来源，但实验部分提到了通过 OmniManip 自动生成演示数据，并收集了每项任务 150 条轨迹用于训练行为克隆策略。\n算力要求多少？ 文章中没有直接给出具体的算力要求，但提及了多个 VLM 调用会带来计算挑战，即使进行了并行处理也是如此。这暗示了该系统可能需要较高的计算资源。\n公开代码吗？ 文章中没有明确提到代码是否公开。\n模拟环境用的是什么平台？ 文章中没有提到具体使用了哪个模拟环境平台。\n现实环境用的是什么平台？ 现实环境实验平台是基于 Franka Emika Panda 机器人臂搭建的，并配备了 UMI 机械手。感知方面，使用了两台 Intel RealSense D415 深度相机，一台安装在机械手上提供第一人称视角，另一台则放置在机器人对面提供第三人称视角。\nVidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic CVPR 2025\nFrom Technical University of Munich\n目标： 大规模网络视频人类样本学习 训练 家务机器人 模型\n动机： 机器人依赖实例教学，但是做家务没那么多教学\n模型流程：\n模型概览\nVidBot 通过三大模块——3D 可交互性提取、粗阶段 affordance 预测、细阶段轨迹生成——实现从“野外”RGB 视频到机器人可执行动作的零样本迁移。首先，它利用 Structure-from-Motion 和度量深度模型，从单目视频中恢复一致的 3D 手部轨迹与接触/目标点；然后，因式分解成粗预测网络 πc（提取高层次接触点与目标点）与细预测网络 πf（基于扩散生成精细轨迹），并在推理时引入多目标、法线与避碰等可微成本进行采样指导；最后，将生成的 3D 交互轨迹直接部署于多种机器人平台，实现开“即用”的零样本操控能力 ￼ ￼。\n⸻\n3D 可交互性提取管道 1.1 数据准备 •\t视频与语言输入：给定原始 RGB 图像序列 {Ĩ0,…ĨT} 及指令 l。 •\tSfM 与深度预测：使用 SfM 系统估计相机内参 K、无尺度位姿 {TWC} 及稀疏地标，再调用度量深度基模型（如 ZoeDepth）生成密集深度 {D̂t}，实现时序一致且度量尺度的重建 ￼； •\t手-物体分割与填充：借助手—物体检测（如 [72]）与分割模型（如 [94]）提取手部与接触物体掩码，再通过视频修复生成无手帧，以消除动态遮挡对优化的影响 ￼。\n1.2 姿态与尺度优化 •\t全局尺度校正：优化全帧尺度 sg，使稀疏地标深度与预测深度对齐； •\t位姿细化：联合优化每帧位姿 TWCi 与局部尺度 si，补偿 SfM 在手—物体动态区域的误差，实现一致的 3D 重建 ￼。\n1.3 交互表示提取 •\t手部中心轨迹：将优化后各帧手部中心点恢复至首帧坐标系，插值形成平滑交互轨迹 τ̂； •\t接触与目标点采样：在首帧均匀下采样手部中心生成接触点 ĉ，在末帧提取目标点 ĝ，用于监督模型的中间预测； •\t表示定义：最终得到的 3D affordance a = {c, τ}，其中 c∈ℝNc×3 为接触点集，τ∈ℝH×3 为轨迹序列 ￼ ￼。\n⸻\n粗—细分级 Affordance 学习 2.1 模型结构因式分解\n将 affordance 模型 π({Ĩ, D̃},l) 分解为： 1.\t粗阶段 πc：从 RGB-D 图像 Ĩ→D̃ 及指令 l 中预测目标点 g 和接触点 c； 2.\t细阶段 πf：在粗阶段输出 {g,c} 及场景上下文指导下，通过扩散模型生成精细轨迹 τ ； 整个流程利用测试时可微成本（多目标到达、碰撞避让等）对采样进行引导，增强与新场景/新形体的适应性 ￼ ￼。\n2.2 粗阶段：目标与接触点预测 •\t输入预处理：使用开集物体检测器裁剪关注物体区域的 RGB-D 图像； •\t网络架构：πc 由两支网络 πgoalc 与 πcontc 组成，分别输出目标和接触热图及（目标点）深度。 •\t融合视觉特征、RoI 池化后得到的物体嵌入、语言嵌入（CLIP 特征）和位置编码，通过 Perceiver 与 Transformer 生成每像素概率分布； •\t3D 投影：根据相机内参与热图深度，将像素坐标提升到三维空间，得到 g∈ℝNg×3 与 c∈ℝNc×3 ￼。\n2.3 细阶段：基于扩散的轨迹生成 •\t条件扩散模型：πf 采用 1D U-Net 架构，输入包含轨迹状态 τk、TSDF 空间特征（由 3D U-Net 从 RGB-D 构建的体素化 TSDF 地图提取）、语言与物体嵌入等； •\t正向/反向过程：遵循扩散概率模型，逐步加入高斯噪声，再由网络学习去噪，直接回归未加噪轨迹 τ̄0； •\t测试时引导：在每个去噪步骤中加入可微成本函数——多目标到达、法线一致、碰撞避让——通过扰动引导采样，提高交互轨迹在新场景/新形体下的合理性与可执行性 ￼ ￼。\n⸻\n输入与输出 •\t输入：首帧的“修复”RGB 图像 Ĩ̃0、对应度量深度 D̃0、裁剪的物体图像 Ĩ̃0o 及语言指令 l ￼。 •\t输出：3D 接触点集 c 和交互轨迹 τ ，直接可用于机器人控制模块执行操作 ￼。 算力要求： 没说\nCode availability: 暂时没有 (2025-06-11)\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-13/","summary":"Paper review of CVPR 2025","title":"Bug Journal 2025-06-13"},{"content":"Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction CVPR 2025\nFrom 人大 \u0026amp; 上海AI lab\n要达成的事情：\n让机器人能够自我反思到底是哪里做得不好，然后自我调整\n动机：\n人类可以很自然地反思：为什么失败了，为什么机器人不行呢？\n模型实现方式：\n首先，这是一个“语言指导”的RL方法。\n对于这个方法，首先由 LLM 生成一个文字指令：\n比如：现在我要移动一个杯子，我要怎么做\n然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。\n如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。\n现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL\n最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。\n对于修正指令这个步骤，模型一共会输出两条语句，分别是 \u0026ldquo;语义上的反思\u0026rdquo; (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)\n最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。\n那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。\n结果：\n更强的学习能力，更强的泛化能力。\n计算要求：\n仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行\n虚拟环境为 RoboMimic 模拟器\n使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs\nRobotic Visual Instruction: A New Paradigm for Human-Robot Communication CVPR 2025\nFrom IC + 上海 AI lab + UCSD \u0026hellip;\n目标：\n更好的人机交互\n动机：\n语言有很多冗余信息，那在图片中增加信息不就行了？\n模型实现方式：\n机器人视觉指令 (RoVI) RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：\n箭头：指示运动方向和轨迹 圆圈：标记感兴趣的对象或动作目标 颜色：区分多个指令或动作步骤 数字：指示操作顺序 这种视觉语言具有以下几个优点：\n空间精确性：视觉标记精确地指示3D空间中的位置和路径 时间清晰性：顺序步骤被清晰地划分 直观设计：这些符号易于人类理解和创建 跨文化实用性：视觉指令超越语言障碍 RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。\n靠手动标记了 15K 图片。。。 工作量爆炸💥 而且以后也要人手标。。。\n🤔： 为什么不能自动标，难道作者没有想过这一点吗\n但总之，现在在这个图像的基础上，VLM 会帮忙生成：\n任务的文字描述 可以执行这些任务的代码 代码中包含：\n路径 起点，终点，过程点\n然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取\n算力要求：\nNvidia A40\n现实实验设置：\nUFACTORY X-Arm 6和UR5 两台机械臂 两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。 两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。\n模拟环境设置：\n使用了SAPIEN 作为模拟器。 SIMPLER 作为基础环境。\nMitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation CVPR 2025\nFrom 港科广\n目标：\n缩小人机之间的 Gap\n动机：\n人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap\n问题：\n现在有这些 Gap:\n视觉环境差异 人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。\n形态差异： 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。\n尺度和视角： 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。\n解决这一差距的传统方法分为两大类，每类都有显著的局限性：\n预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。\n在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。\n模型实现方式：\nbasically, 就是说希望用一个 Adaptor 来 fill in the gap.\n把从 human demo pretrain embedding 转换成 robot demo embedding.\n对于任务感知也是如此。\n用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。\n结果：\nAlign 一下效果变好了\n算力要求：\n4 * Nvidia A6000\n模拟环境为 RLBench\n真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头\nMoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation CVPR 2025\nFrom 北邮 + 南阳理工 + 清华\n目标：\n导航 + 空间操作\n动机：\n虽然静态的操作已经没问题了，但是若是平台移动就不太好办。\n实现细节：\n这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分\nMoManipVLA使用以下方法实现这些优化问题：\n双退火搜索算法用于基座位置寻找优化 序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案 该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：\nRGB相机进行视觉感知 关节编码器进行本体感知（感知机器人自身位置） (optional) 深度感知以增强障碍物避免\n结果：\n模拟环境：\n模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。\n真实环境（机械臂）：\n在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。\n算力：\n4 * RTX 3090\nROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics CVPR 2025\nFrom OSU and NVIDIA\n目标：\n引导 VLM 2D \u0026amp; 3D 视觉，理解空间结构。\n动机：\nVLM 目前无法理解空间结构。 原因并非 VLM 不行，而是数据不够\u0026quot;spatial\u0026quot;\n模型实现细节：\n首先是数据收集：\n输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”\n同时，生成一个俯视图，来看看哪里适合放置一个物体。\n最后在看看这个物体是否适合被放在这个地方。\n对于物体的方位，每一次会从三个角度问问题：\n以机器人为中心的视角 (第一视角) 以物体为中心的视角 (第三视角) 以世界为中心的视角 (fix-cam)\n自我中心：“从您的视角看，书在电脑的左边吗？” 以物体为中心：“从电脑的视角看，书在电脑的左边吗？” 以世界为中心：“从海拔高度看，书在电脑的上方吗？” 这样的好处是可以让 VLM 有更强的空间理解\n结果：\n在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o\n算力要求：\n20-40h * 8 * H100\n模拟环境：\nROBOSPATIAL 数据集，这包括 ScanNet, Matterport3D, 3RScan, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。\n真实环境：\nKinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。 机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。\nThink Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation CVPR 2025\nFrom 上交，复旦，上海 AI lab\n目标：\n如何避免灾难性遗忘\n动机：\n有些动作有相似之处，比如递筷子和递镊子有相似之处 那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？\n模型实现细节：\nPPL 框架的核心组件包括：\n输入编码器： 本体感觉编码器：处理机器人的关节状态和夹爪姿势 视觉编码器：处理场景的 RGB 图像 光流编码器：处理光流信息以捕获运动模式 文本编码器：处理任务的语言指令\n基元提示： 跨任务共享的基本运动模式的学习表示 注入到多头自注意力层的键和值中\n终身提示 (Lifelong Prompts)： 在终身学习期间为新任务学习的特定于任务的提示 与原始提示连接以自定义模型的行为\n运动感知提示查询 (Motion-Aware Prompt Query)： 结合光流和文本指令信息 用于确定不同原始提示的相关性\n扩散Transformer (Diffusion Transformer)： 基于条件输入和提示生成机器人动作\n算力：\n论文中没有直接说明具体的GPU类型和训练时间。\n模拟环境：\n模拟实验是在基于 MimicGen 和 LIBERO 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。\n机械臂：\n真实世界的实验是在 Franka Panda 机械臂上进行的。\n数据集：\n论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。\n获取方式：\n这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。\nGenerating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision CVPR 2025\nFrom Kyoto University\n这篇论文的标题是“从自我中心视角下的动作描述生成6自由度物体操作轨迹”，其核心目标是根据文本描述和初始视觉输入来生成物体在3D空间中的操作轨迹序列。\n目标： 从自我中心视频中提取6自由度（6DoF）物体操作轨迹。 基于这些提取的轨迹和相关的动作描述，开发能够根据文本动作描述生成物体操作轨迹的模型。\n动机： 开发能够协助人类活动的交互式机器人，其中一个关键能力是让机器人能够按人类指令操作工具和物体。 训练生成此类操作轨迹的模型需要大量多样且详细的物体操作演示，但这在实际中很难大规模收集。 利用日常工作场景中的视频来提取人类演示中的各种物体操作，是一种很有前景的解决方案。 现有的3D物体操作轨迹预测方法受限于昂贵的3D注释成本，难以处理多样化的操作动作。 现有数据集（如HOT3D）中的人类运动轨迹数据有限，不足以开发能够生成物体操作轨迹的模型。\n数据来源： 训练数据： 论文使用Ego-Exo4D数据集 [30] 来构建大规模训练数据集，包含6DoF物体操作轨迹以及图像、深度图和动作描述。这个过程是自动化的，不依赖于预设的相机外部参数。\n评估数据： 论文使用现有的HOT3D [3] 数据集进行评估。HOT3D是一个用于3D手和物体跟踪的自我中心视角数据集，通过光学标记和多红外OptiTrack相机记录，提供了精确的手和物体6DoF信息。\n算力要求： 论文提到模型训练了30个epoch，批处理大小为8。优化器使用了AdamW，基础学习率为2e-5（对于LLMs）和2e-4（对于其他参数）。 为了处理旋转连续性问题，每个角度（roll, pitch, yaw）都用[cos(θ), sin(θ)]表示。 虽然论文没有直接给出具体的算力消耗数字（例如所需的GPU数量或训练时间），但从训练epoch和批处理大小来看，以及使用了BLIP-2、VILA、PointLLM和MiniGPT-3D等大型视觉语言模型作为骨干网络，可以推断出这需要相当大的计算资源。这些模型本身就比较大，并且训练涉及处理大规模视频数据。\n公开代码： 论文在摘要中明确提到了数据集和代码的链接：https://biscue5.github.io/egoscaler-project-page/。\n模拟环境和现实环境平台： 论文没有提到使用了特定的模拟环境平台。 在现实环境方面，论文主要使用了自我中心视频数据。HOT3D [3] 数据集是通过Project Aria眼镜 [21] 和Quest 3 [65] 记录的，这些是现实世界中的可穿戴设备，用于捕捉自我中心视角下的活动。这表明他们的研究是基于真实世界中的人类活动数据。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-12/","summary":"CVPR 2025 Robotics summary","title":"Bug Journal 2025-06-12"},{"content":"PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability CVPR 2025\nFrom 北京交通大学 \u0026amp; 广东技术师范大学\n目标： 告诉机器人什么位置它到不了\n动机： 有时候机器人不知道一个位置到不到得了，结果把自己搞坏了\n模型流程：\n首先离线计算什么位置是机械臂能达到的。 形成一个点云 (S-P Map)\n然后用 SigLip-400M 提取图像和点云的特征\n然后把这个 embedding 和文字的 embedding 混合之后\n通过 Qwen-2.5-Instruct-3B，生成一个回答给 VoxPoser 做。\n实验设计：\n仿真：基于 CUDA 11.x 的 PyBullet 环境，六种机器人（UR5、FR5、CR5、FRANKA、UR3、XArm6），共 1.3K 问答，零样本评测。\n实机：在 UR3、XArm6 真实平台上各 10 次零样本任务，评估任务成功率。\n评估指标：\nEQA-phys：基于 LLM 评分（5 分正确、1 分错误）计算平均分； RoboVQA-val / OpenEQA：标准 BLEU-4 与准确率； 任务规划：成功率。\n结果：\nS-P Map 在很多 LLM 上都有用\nPhysVLM-3B 效果平均最好\n数据集： Zero-shot\n算力要求：\n\u0026lt; 48h * 8 * A800\n代码：\n开源\nObject-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation CVPR 2025\nFrom PKU Agibot Lab\n目标： 帮助机器人排除冗余信息干扰\n动机： 语言和视频中冗余信息过多\n模型流程：\n在图片上加一些标记 分别是：\n接触点（蓝色） 末端执行器在接触时的 z 轴方向（红色） y 轴方向（绿色） 接触后移动方向（黄色） 这些标记可能来自 Grounding-Dino + SAM 或者 ChatGPT-4o 或者 手动标记\n标记方式如下：\n均匀生成 N(32) 条线，让 GPT 选择 3 条标上颜色\n然后把这个“增强”过后的信息输入给 CLIP,然后再让 LLM 输出\u0026quot;应该在哪里，以什么角度接触\u0026quot;\n对于这个信息，我们可以和GT 做 train\n最后，我们得到了这个位置和姿态的信息，我们就可以用IK求解路径了。\n结果：\n数据集：\n模拟环境：SAPIEN + PartNet-Mobility •\t平台：使用 SAPIEN（一个支持刚体与关节物体的物理模拟器），其提供高保真动力学与渲染接口，适合零样本机器人操控研究 ￼ ￼。 •\t资产集：加载 PartNet-Mobility 中的 1,500 多个关节化 CAD 模型，涵盖抽屉、门、笔记本电脑盖等常见家电与家具 ￼。 •\t飞行夹持器（Flying Gripper）：采用 SAPIEN 中的“飞行”版 Franka Panda Gripper，使得末端执行器可在自由空间中无碰撞地移动，以便专注测试模型对提示的理解能力 ￼。 •\t摄像机随机化：在每个场景随机采样摄像机视角（水平±45°、俯仰30°–60°），并在单帧中记录 RGB 图像与 3D 末端执行器位姿，用于提示生成与训练 ￼。 •\t数据采集：共采集 ~10,000 条训练样本，过程耗时约 6–8 小时；测试集与训练集按照固定比例拆分，模拟了 Seen/Unseen 物体类别的泛化能力评估 ￼ ￼。\n现实机器人平台 •\t硬件平台：使用真实的 Franka Emika Panda 机械臂（7 自由度，集成高精度力矩传感器），配合标准的 RGB-D 摄像头（如 Intel RealSense D415）采集视觉输入 ￼。 •\t执行流程：将 CrayonRobo 在模拟环境中输出的 SE(3) 接触位姿与移动方向，通过 ROS + MoveIt! 的 IK 求解与笛卡尔轨迹规划一键下发真实机器人执行 ￼。 •\t测试任务：与模拟中一致，包括“拉抽屉”“开门”“掀笔记本盖”等单步原子操作，每个任务在 5–10 个不同初始姿态下重复试验。 •\t评价指标：以二值成功率衡量：当末端执行器按照预测方向将物体部件移动超过阈值（如 5 cm）即记为成功；平均成功率达到 74%（Seen）与 72%（Unseen） ￼。\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\n代码： 开源\nCheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation CVPR 2025\nFrom PKU Agibot lab\n目标： 让机器人读取说明书之后根据说明书做出正确操作\n动机： 阅读说明书\n电子产品有时候视觉信息不够，很多有着相似外观的东西可能有着不同的功能。\n所以要读说明书\n模型流程：\nOCR → GPT-4o 提取图文信息 → GPT‑4o 生成操作步骤 → SoM、Grounding‑DINO、SAM 实现视觉对齐 → FoundationPose 匹配 CAD\n最后输出{部件，操作} pair, 最后使用 VoxPoser 进行操作。\n实验设置 模拟平台：SAPIEN 0.8 + Franka Panda 飞行夹具 ￼\n数据集： PartNet-Mobility CAD 模型； CheckManual 合成说明书（已公开，可下载使用） ￼\n评估指标： 任务完成率\n现实验证： Franka + RealSense 摄像头，完成单个用例的实物测试\n算力要求： 未知 建议 \u0026gt; 40 GB VRAM\nCode availability: 开源\n结果：\n总之有 manual 效果更好\nTASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation CVPR 2025\nFrom Xiaoguang Han\u0026rsquo;s Lab at 港中深\n目标： 优化对齐视频-人手数据集\n动机： 现在有这些问题：\n视角不一致 动作语义无法对齐 手部姿态稳定性不高 这个模型想要解决这些问题 模型流程：\n数据集构建（Sec. 3）： •\t100,856 条 1–8 秒单动作视频，静态 1080p 视角，语义指令一一对应。 粗视频生成（Stage I – Coarse Action Planner）： •\t基于 DynamiCrafter（512 × 512 分辨率），语言指令＋静态环境图→粗交互视频； •\t微调参数：batch=16, lr=5×10⁻⁵, 30K steps。 姿态优化（Stage II – MDM Refinement）： •\t使用 Motion Diffusion Model 以 60 帧手部关键点序列为条件，保证握姿一致性； •\t训练设置：batch=64, lr=1×10⁻⁴, 500K steps; 推理 10-step DDIM denoise。 最终生成（Stage III – Frame-wise Adapter）： •\t将优化后手部姿态图像与环境图和语言指令一起接入 Stable Diffusion 2 Adapter，生成最终视频； •\t训练设置：batch=32, lr=5×10⁻⁵, 30K steps。 实验设置 •\t仿真验证：在 SAPIEN 模拟平台上，用 DynamiCrafter＋MDM + Adapter 生成视频，并通过 Figure 17 示范在虚拟机器人上的模仿效果。\n模型流程：\n第一阶段：Coarse Action Planner（粗动作生成） •\t目标：在给定语言指令（如“拿起杯子并倒入水”）和静态环境图（512×512 分辨率）条件下，生成一段粗略的、合理的手–物体交互视频序列（通常 16 帧）。 •\t模型架构：基于 Video Diffusion Model（VDM），如 DynamiCrafter，使用文本–图像条件扩散网络。 •\t训练细节： •\tBatch size = 16，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t推理时使用 50-step DDIM 采样，平衡生成质量与速度。 •\t输出：一段低分辨率、含大致手部运动轨迹和物体交互的粗视频，用于后续姿态细化。\n⸻\n第二阶段：Motion Diffusion Model (MDM) Refinement（姿态细化） •\t目标：针对第一阶段生成的粗视频，从中提取手部的 3D 关键点序列（60 帧），并利用扩散模型细化运动轨迹，提升抓握姿态的连贯性与自然度。 •\t模型架构：1D 时序扩散网络，对帧间手部关键点做去噪与重建。 •\t训练细节： •\tBatch size = 64，学习率 1×10⁻⁴； •\t训练步数 500K steps； •\t推理时使用 10-step DDIM，快速得到精细关键点序列。 •\t输出：一组平滑、符合物理约束的 3D 手部关键点轨迹，用以指导下一阶段的视频生成。\n⸻\n第三阶段：Frame-wise Adapter（帧级最终生成） •\t目标：将姿态细化后的关键点与原环境图、语言指令结合，生成最终高质量、手部姿态稳定的交互视频。 •\t模型架构：基于 Stable Diffusion 2，通过 Adapter 模块将手部姿态（以可视化关节点或姿态图形式）作为条件，连同环境图与文本，一起输入扩散模型。 •\t训练细节： •\tBatch size = 32，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t采用与 Stage I 相似的 DDIM 采样流程，保证画质与动作一致性。 •\t输出：分辨率可达 512×512 的连续视频帧序列，手–物体交互清晰、握姿自然，可直接用于机器人模仿学习。\nVidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic CVPR 2025\nFrom Technical University of Munich\n目标： 大规模网络视频人类样本学习 训练 家务机器人 模型\n动机： 机器人依赖实例教学，但是做家务没那么多教学\n模型流程：\n模型概览\nVidBot 通过三大模块——3D 可交互性提取、粗阶段 affordance 预测、细阶段轨迹生成——实现从“野外”RGB 视频到机器人可执行动作的零样本迁移。首先，它利用 Structure-from-Motion 和度量深度模型，从单目视频中恢复一致的 3D 手部轨迹与接触/目标点；然后，因式分解成粗预测网络 πc（提取高层次接触点与目标点）与细预测网络 πf（基于扩散生成精细轨迹），并在推理时引入多目标、法线与避碰等可微成本进行采样指导；最后，将生成的 3D 交互轨迹直接部署于多种机器人平台，实现开“即用”的零样本操控能力 ￼ ￼。\n⸻\n3D 可交互性提取管道 1.1 数据准备 •\t视频与语言输入：给定原始 RGB 图像序列 {Ĩ0,…ĨT} 及指令 l。 •\tSfM 与深度预测：使用 SfM 系统估计相机内参 K、无尺度位姿 {TWC} 及稀疏地标，再调用度量深度基模型（如 ZoeDepth）生成密集深度 {D̂t}，实现时序一致且度量尺度的重建 ￼； •\t手-物体分割与填充：借助手—物体检测（如 [72]）与分割模型（如 [94]）提取手部与接触物体掩码，再通过视频修复生成无手帧，以消除动态遮挡对优化的影响 ￼。\n1.2 姿态与尺度优化 •\t全局尺度校正：优化全帧尺度 sg，使稀疏地标深度与预测深度对齐； •\t位姿细化：联合优化每帧位姿 TWCi 与局部尺度 si，补偿 SfM 在手—物体动态区域的误差，实现一致的 3D 重建 ￼。\n1.3 交互表示提取 •\t手部中心轨迹：将优化后各帧手部中心点恢复至首帧坐标系，插值形成平滑交互轨迹 τ̂； •\t接触与目标点采样：在首帧均匀下采样手部中心生成接触点 ĉ，在末帧提取目标点 ĝ，用于监督模型的中间预测； •\t表示定义：最终得到的 3D affordance a = {c, τ}，其中 c∈ℝNc×3 为接触点集，τ∈ℝH×3 为轨迹序列 ￼ ￼。\n⸻\n粗—细分级 Affordance 学习 2.1 模型结构因式分解\n将 affordance 模型 π({Ĩ, D̃},l) 分解为： 1.\t粗阶段 πc：从 RGB-D 图像 Ĩ→D̃ 及指令 l 中预测目标点 g 和接触点 c； 2.\t细阶段 πf：在粗阶段输出 {g,c} 及场景上下文指导下，通过扩散模型生成精细轨迹 τ ； 整个流程利用测试时可微成本（多目标到达、碰撞避让等）对采样进行引导，增强与新场景/新形体的适应性 ￼ ￼。\n2.2 粗阶段：目标与接触点预测 •\t输入预处理：使用开集物体检测器裁剪关注物体区域的 RGB-D 图像； •\t网络架构：πc 由两支网络 πgoalc 与 πcontc 组成，分别输出目标和接触热图及（目标点）深度。 •\t融合视觉特征、RoI 池化后得到的物体嵌入、语言嵌入（CLIP 特征）和位置编码，通过 Perceiver 与 Transformer 生成每像素概率分布； •\t3D 投影：根据相机内参与热图深度，将像素坐标提升到三维空间，得到 g∈ℝNg×3 与 c∈ℝNc×3 ￼。\n2.3 细阶段：基于扩散的轨迹生成 •\t条件扩散模型：πf 采用 1D U-Net 架构，输入包含轨迹状态 τk、TSDF 空间特征（由 3D U-Net 从 RGB-D 构建的体素化 TSDF 地图提取）、语言与物体嵌入等； •\t正向/反向过程：遵循扩散概率模型，逐步加入高斯噪声，再由网络学习去噪，直接回归未加噪轨迹 τ̄0； •\t测试时引导：在每个去噪步骤中加入可微成本函数——多目标到达、法线一致、碰撞避让——通过扰动引导采样，提高交互轨迹在新场景/新形体下的合理性与可执行性 ￼ ￼。\n⸻\n输入与输出 •\t输入：首帧的“修复”RGB 图像 Ĩ̃0、对应度量深度 D̃0、裁剪的物体图像 Ĩ̃0o 及语言指令 l ￼。 •\t输出：3D 接触点集 c 和交互轨迹 τ ，直接可用于机器人控制模块执行操作 ￼。 算力要求： 没说\nCode availability: 暂时没有 (2025-06-11)\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-11/","summary":"Find Robotics in CVPR 2025","title":"Bug Journal 2025-06-11"},{"content":"CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models Currently on Arxiv.\nFrom Nvidia, Stanford, and MIT\n动机：\n为什么不能和 Language 一样，对 Vision 也做 CoT 呢\n实现方式：\n先生成实现这个目标的话图像应该怎么变 然后再生成动作\n实验设计：\n使用 LIBERO 四个任务组（Spatial, Object, Goal, Long），每组 10 个任务，各 50 条示范轨迹；评价指标为成功率（500 次试验，3 个随机种子）\n真实机器人：Bridge-V2（WidowX 机械臂，45k 语言标注轨迹）和 Franka-Tabletop（Panda 机械臂，10–150 条示范），分别测试视觉泛化、运动泛化、语义泛化和语言对齐四类场景；指标为成功率\n对比模型 •\tDiffusion Policy：从头训练的扩散策略，各场景均需单独训练； •\tOcto：通用 VLA，预训练于 OpenX，无 VLM 初始化； •\tOpenVLA：基于预训练 VLM 的开源 VLA； •\tSUSIE：两阶段图像编辑 + 目标条件策略。 在 LIBERO 中，CoT-VLA 平均成功率达 81.13%，高于 OpenVLA 的 76.5% 和 Diffusion Policy 的 72.4%\nRobotic Control via Embodied Chain-of-Thought Reasoning Currently on Arxiv\nFrom UCB, UWarsaw, Stanford\n动机：\n泛化能力\n实现方式：\n多步文本推理 + 框选关键物品位置 + LLM CoT 生成 \u0026ndash;\u0026gt; OpenVLA 完成所有步骤\n实验设计：\n两种设计：\n一样的物品/一样的指令/一样的视角 不同的物品/不同的指令/不同的视角 Robo-DM: Data Management For Large Robot Datasets ICRA 2025 BestPaper on Robot Learning\nfrom UCB \u0026amp; Google Deepmind\n做数据库的\n以前的数据都没压缩过，太大了; 存储，传输成本也高 这样的话加载也会很慢 这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\n他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\n最后把信息都通过 EBML file 存储\n为什么选择 EBML 呢?\n因为：\n支持嵌套结构 是自包含的，更方便复用 支持流处理，不需要一次性全部导入到内存中 支持自动时间同步 对于视频，主要选择了.H264 来压缩，显著降低了文件大小\n最后这个数据集又小又快\nAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview ICRA 2025 Best Paper on Robot Learning Finalist\nfrom Google Deepmind\n机器人打乒乓球\n打乒乓球要又快又准。所以是理想的机器人测试器\n对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\n首先用模仿学习做 base, 然后强化学习训练\n分层控制则是类似 MOE 的思路，每一个子网络都“学一种打球技术”\n然后让主网络来“选择一种打球技术”。\n控制频率：50HZ.\n并且主网络还会在 Validation 的时候 Continue Learning\n比喻：当机器人发现正手比反手好得分，那机器人就会偏向于打更多正手\n$H(s, a) = H_{offline}(s, a) + \\alpha * [R(s, a) - R_{expected}(s, a)]$\n在这里，$H(s,a)$ 是现在的偏好 $H_{offline}(s,a)$ 是训练时的偏好 $R(s,a)$ 是当前环境的奖励 $R_{expected}$ 是预期获得的奖励\n结果：\n此处，B 指 Beginner, I 指 Intermediate, A 指 Expert\n训练难度：\n2.4 Billion steps on 6k Parallel Simulators 训练出了正反手\n每一个操作需要训练 300 - 1200 Million Steps.\n但是推理难度很低，只需要一个 CPU 的 3ms CPU time 即可完成\n最终能实现 50Hz 的推理速度\nNo Plan but Everything Under Control: Robustly Solving Sequential Tasks with Dynamically Composed Gradient Descent ICRA 2025 Best Paper on robot learning finalist\nFrom University of Berlin\n部分现有方法用的是 planning 来做的机器人 manipultaion task.\n就是说比如会找到一个机器人的起点和终点，然后通过一些算法从起点移动到终点\n这样的算法会通过一些数据来训练\n但是人类在做这些 task 的时候并不会有一个 planning, 那如何不训练做这些 task 呢？\n既然现在 Gradient Descent 这么强，能不能考虑直接用 Gradient Desent 来解决这个问题呢？\n可以的：但是和传统的 Gradient Desnet 不一样的点在于：传统的 Gradient Desent 会把所有的 Gradient 全部加在一起，但是对于现在的 task, 不一定要找全局最优解，可以找当前“做哪个分解动作”最优。\n比如：现在可以让机械臂向某个轴的某个方向移动，或者让机械臂把物品抓起来。\n缺点：这个方法必须要对于每一个 task 都设计一个新的 Gradient 方向。\n优点：这个方法可以避免一些不必要的移动，并且可以根据当前状态来调整他的策略。\n实验设计：在“block world”模拟环境中和现实中推拉抽屉。\n在现实世界中会对于这个环境给予一个 干扰，看看这个模型的抗干扰能力如何。\n比较的模型是：ICRA 2020: Online replanning in belief space for partially observable task and motion problems\nPolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation ICRA 2025 Best Paper in Field and Service Robotics\n使用了多种 Modality 来增强模型的 Manipulation 能力\n使用了包括：视觉，听觉，触觉 \u0026hellip; 等 modality 的能力\n实验设计：\n耐用性 成功率 首先和一个其他的商业模型做对比，然后发现耐用性更高\n然后和自己做消融实验做对比，然后发现模态越多，效果越好\nHuman-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition ICRA 2025 Best Paper on Human-Robot Interaction\nFrom 上交 \u0026amp; UIUC\n提出了一种更高效的数据收集 \u0026amp; 训练的方式\n核心在于 Diffusion Policy 的应用。\n最开始的时候，会采集一些数据用于最初的训练\n然后在接下来的训练中，机器人会一步一步 take control, 这时人类只需要做一个大致的动作就可以了。\ne.g.\n以“拿起杯子并放到指定位置”（Pick-and-Place）任务为例：\n阶段一：接近杯子 人类操作：你只需要做一个简单的“向前移动”的手势。 智能体接管：智能体理解你的意图是“去拿杯子”，于是它会自主地、平滑地控制机械手移动到杯子正上方，并摆好最佳的抓取姿势。 关键节点 1 到达：机械手已经就位，悬停在杯子上方。第一个子任务“接近杯子”已完成。此时，智能体停下来，因为它不知道你接下来是想抓取，还是想调整位置，或是想取消任务。它在等待你的下一个指令。\n阶段二：抓取杯子 人类操作：你做一个“抓握”的手势。 智能体接管：智能体接收到“抓取”指令，于是它会自主执行精确的抓取动作，以最稳定的方式合拢机械手，握紧杯子。 关键节点 2 到达：杯子已经被成功拿起。第二个子任务“抓取杯子”已完成。现在，智能体又停下来了。它知道手里拿着杯子，但它不知道你想把杯子放到哪里去。\n阶段三：移动到目标位置并释放 人类操作：你做一个指向目标位置的“移动”手势。 智能体接管：智能体理解意图，自主地将拿着杯子的手移动到目标位置上方，然后等待你最后的指令。 人类操作：你做一个“松开”的手势。 智能体接管：智能体平稳地释放杯子。任务完成。\nVisual Chain-of-Thought Prompting for Knowledge-based Visual Reasoning AAAI 2024\nFrom MIT, UCLA, CMU\n可以作为以后的 baseline\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-10/","summary":"paper review","title":"Bug Journal 2025-06-10"},{"content":"Robo-DM: Data Management For Large Robot Datasets ICRA 2025 BestPaper on Robot Learning\nfrom UCB \u0026amp; Google Deepmind\n做数据库的\n以前的数据都没压缩过，太大了; 存储，传输成本也高 这样的话加载也会很慢 这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\n他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\n最后把信息都通过 EBML file 存储\n为什么选择 EBML 呢?\n因为：\n支持嵌套结构 是自包含的，更方便复用 支持流处理，不需要一次性全部导入到内存中 支持自动时间同步 对于视频，主要选择了.H264 来压缩，显著降低了文件大小\n最后这个数据集又小又快\nAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview ICRA 2025 Best Paper on Robot Learning Finalist\nfrom Google Deepmind\n机器人打乒乓球\n打乒乓球要又快又准。所以是理想的机器人测试器\n对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\n首先用模仿学习做 base, 然后强化学习训练\n分层控制则是类似 MOE 的思路，每一个子网络都“学一种打球技术”\n然后让主网络来“选择一种打球技术”。\n控制频率：50HZ.\n并且主网络还会在 Validation 的时候 Continue Learning\n比喻：当机器人发现正手比反手好得分，那机器人就会偏向于打更多正手\n$H(s, a) = H_{offline}(s, a) + \\alpha * [R(s, a) - R_{expected}(s, a)]$\n在这里，$H(s,a)$ 是现在的偏好 $H_{offline}(s,a)$ 是训练时的偏好 $R(s,a)$ 是当前环境的奖励 $R_{expected}$ 是预期获得的奖励\n结果：\n此处，B 指 Beginner, I 指 Intermediate, A 指 Expert\n训练难度：\n2.4 Billion steps on 6k Parallel Simulators 训练出了正反手\n每一个操作需要训练 300 - 1200 Million Steps.\n但是推理难度很低，只需要一个 CPU 的 3ms CPU time 即可完成\n最终能实现 50Hz 的推理速度\nNo Plan but Everything Under Control: Robustly Solving Sequential Tasks with Dynamically Composed Gradient Descent ICRA 2025 Best Paper on robot learning finalist\nFrom University of Berlin\n部分现有方法用的是 planning 来做的机器人 manipultaion task.\n就是说比如会找到一个机器人的起点和终点，然后通过一些算法从起点移动到终点\n这样的算法会通过一些数据来训练\n但是人类在做这些 task 的时候并不会有一个 planning, 那如何不训练做这些 task 呢？\n既然现在 Gradient Descent 这么强，能不能考虑直接用 Gradient Desent 来解决这个问题呢？\n可以的：但是和传统的 Gradient Desnet 不一样的点在于：传统的 Gradient Desent 会把所有的 Gradient 全部加在一起，但是对于现在的 task, 不一定要找全局最优解，可以找当前“做哪个分解动作”最优。\n比如：现在可以让机械臂向某个轴的某个方向移动，或者让机械臂把物品抓起来。\n缺点：这个方法必须要对于每一个 task 都设计一个新的 Gradient 方向。\n优点：这个方法可以避免一些不必要的移动，并且可以根据当前状态来调整他的策略。\n实验设计：在“block world”模拟环境中和现实中推拉抽屉。\n在现实世界中会对于这个环境给予一个 干扰，看看这个模型的抗干扰能力如何。\n比较的模型是：ICRA 2020: Online replanning in belief space for partially observable task and motion problems\nPolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation ICRA 2025 Best Paper in Field and Service Robotics\n使用了多种 Modality 来增强模型的 Manipulation 能力\n使用了包括：视觉，听觉，触觉 \u0026hellip; 等 modality 的能力\n实验设计：\n耐用性 成功率 首先和一个其他的商业模型做对比，然后发现耐用性更高\n然后和自己做消融实验做对比，然后发现模态越多，效果越好\nHuman-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition ICRA 2025 Best Paper on Human-Robot Interaction\nFrom 上交 \u0026amp; UIUC\n提出了一种更高效的数据收集 \u0026amp; 训练的方式\n核心在于 Diffusion Policy 的应用。\n最开始的时候，会采集一些数据用于最初的训练\n然后在接下来的训练中，机器人会一步一步 take control, 这时人类只需要做一个大致的动作就可以了。\ne.g.\n以“拿起杯子并放到指定位置”（Pick-and-Place）任务为例：\n阶段一：接近杯子 人类操作：你只需要做一个简单的“向前移动”的手势。 智能体接管：智能体理解你的意图是“去拿杯子”，于是它会自主地、平滑地控制机械手移动到杯子正上方，并摆好最佳的抓取姿势。 关键节点 1 到达：机械手已经就位，悬停在杯子上方。第一个子任务“接近杯子”已完成。此时，智能体停下来，因为它不知道你接下来是想抓取，还是想调整位置，或是想取消任务。它在等待你的下一个指令。\n阶段二：抓取杯子 人类操作：你做一个“抓握”的手势。 智能体接管：智能体接收到“抓取”指令，于是它会自主执行精确的抓取动作，以最稳定的方式合拢机械手，握紧杯子。 关键节点 2 到达：杯子已经被成功拿起。第二个子任务“抓取杯子”已完成。现在，智能体又停下来了。它知道手里拿着杯子，但它不知道你想把杯子放到哪里去。\n阶段三：移动到目标位置并释放 人类操作：你做一个指向目标位置的“移动”手势。 智能体接管：智能体理解意图，自主地将拿着杯子的手移动到目标位置上方，然后等待你最后的指令。 人类操作：你做一个“松开”的手势。 智能体接管：智能体平稳地释放杯子。任务完成。\n概念理解\nauto regressive Auto regressive 是一种生成方式。可以从前一个数生成下一个数。 更准确地说，$X_{t_i} = a_1X{t_1} + a_2X{t_2} + \\dots + a_{t_{i-1}}X{t_{i-1}}$.\nteacher forcing Teacher forcing 是指在训练的过程中，把真实信息放入训练\nPi 0\nflow matching\n连续的动作和离散的动作有什么区别\nGemini diffusion\n已加入 waitlist 正在测试 LLaDA 开源 Diffusion model. 目前该模型仍然无法加入 Chain of Thought 问题是：没有 Chain of Thought 的模型显著没有加入 Chain of Thought 的模型强 但是现在有一个叫做 Diffusion of Thought 的方法可以加入类似的东西\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-09/","summary":"\u003ch2 id=\"robo-dm-data-management-for-large-robot-datasets\"\u003eRobo-DM: Data Management For Large Robot Datasets\u003c/h2\u003e\n\u003cp\u003eICRA 2025 BestPaper on Robot Learning\u003c/p\u003e\n\u003cp\u003efrom UCB \u0026amp; Google Deepmind\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e做数据库的\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e以前的数据都没压缩过，太大了; 存储，传输成本也高\u003c/li\u003e\n\u003cli\u003e这样的话加载也会很慢\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"1749461291726\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461291726.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是新的数据库格式，在兼容当前格式的情况下尽量做到了最小\u003c/p\u003e\n\u003cp\u003e他存储成了统一的数据格式；可以通过内存访问；可以顺序/随机访问；有模块化设计\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461487133\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461487133.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后把信息都通过 EBML file 存储\u003c/p\u003e\n\u003cp\u003e为什么选择 EBML 呢?\u003c/p\u003e\n\u003cp\u003e因为：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e支持嵌套结构\u003c/li\u003e\n\u003cli\u003e是自包含的，更方便复用\u003c/li\u003e\n\u003cli\u003e支持流处理，不需要一次性全部导入到内存中\u003c/li\u003e\n\u003cli\u003e支持自动时间同步\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对于视频，主要选择了.H264 来压缩，显著降低了文件大小\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461874888\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461874888.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461827040\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461827040.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后这个数据集又小又快\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749461984813\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-09/1749461984813.png\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"achieving-human-level-competitive-robot-table-tennis-a-comprehensive-overview\"\u003eAchieving Human Level Competitive Robot Table Tennis: A Comprehensive Overview\u003c/h2\u003e\n\u003cp\u003eICRA 2025 Best Paper on Robot Learning Finalist\u003c/p\u003e\n\u003cp\u003efrom Google Deepmind\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e机器人打乒乓球\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e打乒乓球要又快又准。所以是理想的机器人测试器\u003c/p\u003e\n\u003cp\u003e对于这个机器人，用上了 模仿学习 + 强化学习 + 分层控制 + Continue Learning\u003c/p\u003e","title":"Bug Journal 2025-06-09"},{"content":"Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation 发表时间：15 Jun 2024\n动机 当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\n问题 1：学习难度高（需要从高维图像中学全局策略）； 问题 2：泛化差（模型可能过拟合于训练视角或场景）； 问题 3：sample efficiency 差（训练数据需求量大）\n作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的， 也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\n主要论点 提出 Local Policy Networks（LPN）： 将策略函数设计为一组 局部策略（local policy heads）； 每个 head 只负责“在自己 patch 上预测动作”； 用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）； 最终策略通过对多个 local head 输出聚合（weighted sum）得到。\n模型流程图 简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\n实验 setting: 使用数据集：RT-1（Robotics Transformer 1）:\nGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper） 数据来源 真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集 数据规模 ~130k 条实际机器人操作轨迹，覆盖 700+ 种任务 采样频率 每条轨迹包含约 50–100 帧关键帧（图像 + 动作） 场景 家庭式办公环境（桌面、水槽、地面） 物体 80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等） 语言指令 每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型） 对于每一条指令：\nRGB 图像 来自机器人头部相机的视角（尺寸通常为 240×320） 深度图（可选） 当前状态 如 gripper pose（位置 + 朝向） 语言指令 例如 “pick up the red apple and place it in the bowl” 动作标签 6-DoF 末端动作（位置增量、旋转、夹爪开合） 时间戳 当前帧在轨迹中的位置 成功标志 是否完成任务（某些版本包含） 文章通过 “加热”在数据集中的抓取位置 来 train heatmap.\n在 heatmap 中取出关键点\n之后通过神经网络预测每一个关键点的移动，加权得到机械臂移动的方向。\n通过模仿学习来算 Loss, 然后训练。\nCoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models 发表时间：13 Mar 2024\n动机 当前机器人 manipulation 研究大多集中在：\n高层任务规划（如使用 LLMs 推理“该做什么”）， 或低层控制（通过 imitation learning 或 RL 获取 end-effector 动作序列）。 然而，低层控制模块在真实世界中很难泛化，因为：\n缺乏对物体几何和功能部分的理解， 容易对新的任务和场景失效。 因此，本文希望结合 VLMs 的常识知识和几何建模能力，以“空间约束”的形式桥接语言与机器人执行之间的鸿沟，实现可泛化的低层 manipulation 能力。\n核心思想 Zero-shot! GPT-4V is all you need!\n模型流程图 首先分割场景，识别场景中有多少物体。\n简单来说就是：让 GPT 决定怎么操作这个物体。 比如说，要抓哪里，要怎么抓，物体的姿态是什么，\u0026hellip;\n最后使用一个传统路径规划算法来达成上述所有条件 一旦条件达成，任务也就完成了\n结果 MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment 发表时间: 24 Mar 2024\n动机 之前的模拟环境都太简单了，设计一个复杂的\n主要论点 设计了一个复杂的模拟环境:\n更多更复杂场景，更真实的物理引擎，更好的 Reward\nAny-point Trajectory Modeling for Policy Learning 发表时间：28 Dec 2023\n动机 数据不够用啦，我要从视频里学\n本文旨在从无动作标签的视频中提取出用于控制的轨迹，从而辅助策略学习\n主要论点 作者提出了 Any-point Trajectory Modeling (ATM) 框架：\n第一阶段：使用无动作标签的视频和现有视频追踪器，从视频中提取任意点的轨迹（即二维运动轨迹），训练一个轨迹预测模型（Track Transformer）。 第二阶段：使用轨迹预测模型，在给定任务语言描述和当前图像的情况下预测未来的轨迹，并用这些轨迹作为中间目标（subgoal）指导策略学习，从少量动作演示中学习有效的策略。 这不就是昨天的General Flow吗\n模型流程图 和昨天的那篇文章的区别在于:\n这篇不需要 Ground Truth 数据，只是视频就行，而那一篇需 如何做到的？答:用了一个 LLM Tracker 来跟踪 这篇文章是对\u0026quot;运动最显著\u0026quot;的 K(32) 个点算运动向量，那个是对每一个像素算运动向量 MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning 发表时间: 19 Oct 2024\n动机 做 RL, 还是得泛化\n过去的 embedding 太 General 了。\n思路是：如果 embedding 不分任务注意所有细节, 反而做不好 这里的泛化能忽略掉任务无关的信息\n总之，这里使用了一个 MoE 网络来处理输入。\n模型流程图 至于这个 Perturbation.. 流程如下:\n[输入图像 I] ↓ [Encoder 输出特征 z] ↓ [策略网络 → 计算 loss] ↓ [反向传播：计算 ∇_z L] ↓ [构造 δ = ε · normalized gradient] ↓ [扰动特征 z + δ → 再送策略网络训练] ↓ [更新 encoder + expert 参数] 总之就是训练的时候把 z 往成功的方向“推一下” “引导视觉 encoder 学会放大那些能带来任务成功的区域”\n结果 模拟环境：\nDMC (DeepMind Control Suite) → 如：Walker、Cheetah、Finger、Cartpole 等控制任务 Meta-World → 多任务机器人操作环境（push、reach、pick-place 等） RLBench → 视觉+控制为主的仿真机械臂任务，如插电源、开抽屉等\n真实世界任务：\n在完成 Task 的时候会干扰一下不让它完成这个 Task.\n但是最后还是成功完成了。\nTake away MoE 是什么呢？其实就是多个动态加权平均的网络。 什么是动态加权平均呢？ 就是权重是通过 $SoftMax(MLP)$ 算出来的 这样每次每个网络加权平均的权重就会不同。\nDemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025\n动机 Robust，还得是Robust。\n为什么不加数据(ο´･д･)??\n过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\n那怎么办呢(ο´･д･)??\n很简单，纯计算模拟不就是了。\n主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\n这样就不需要实际操作，但是可以直接获得大量数据。\n模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\n思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\n机器人一共有两段动作\n一段是碰到物体前的动作\n一段是碰到物体后的动作\n对于第一段，直接用一个变换矩阵变换\n对于第二段，直接规划一个新路径 (use RRT-Connect)\n现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\n如果可以用的话\n然后通过模拟环境生成这个路径的图像\n实验设定 虚拟环境：1 条 GroundTruth\n真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\n一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\n结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 + mask + 指令 + robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM + Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove 一个这样的力反馈手套\n可以链接到灵巧手上，然后展示反馈物体的力\n盲抓分辨物体\n盲眼抓杯子\nReactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation 发表时间：23 Apr 2025\n动机 人有触觉，为什么机器人不行？\n所以我们为机器人加上了触觉。\n并且用触觉来优化模型.\n可以是，触觉数据怎么来呢？🤔\n在机器人手上加上触觉组件就好了\n模型流程 方法很简单：\nimage -\u0026gt; 正常的 CV Encoder -\u0026gt; Touch Encoder -\u0026gt; (Action -\u0026gt; Touch Encoder) * N -\u0026gt; image2 \u0026hellip;\n实现效果 Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control 发表时间：June 23 2025\n动机 现在机器人的表情不够生动，希望能生动一点\n用一段语音输入进机器人\n为什么选语音？\n因为语音有语气, 语气中可能有细微的表情差别\n“我没事\u0026hellip;” “我没事！”和 “我?没事” 是有区别的\n这更适合模型生成表情.\n而现有机器人的表情做得不好，所以这个团队自己做了一个可以做更丰富表情的机器人脸\n实现方式 第一部分，有一个带有丰富语气的语音数据集 + 对应的 3D 人脸建模数据\n第二部分，为了训练模型 -\u0026gt; 电机； 随机生成了 5000 个 3D 建模 让电机去拟合这个表情\n↑以上过程都是在虚拟环境中训练完，然后搬到真实环境做的。\nTwo by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation 发表时间: 9 Apr 2025\n动机 机器人泛化能力和精确对齐的能力不行\n一一对应的插入 task 做不好\n模型流程图 一言以蔽之：先找插座位置，再预测插件位置\n这里的位置是真正的位置，分别是 Tran(三位坐标系中的位置) \u0026amp; Rot (三个轴旋转的角度)\n这样就可以计算 loss 了\n之后第二部分就是把刚才得到的 embedding 和插件的 embedding 乘起来 之后还是 predict Tran \u0026amp; Rot 算 loss.\n注：所有训练都是在虚拟环境完成，真实环境仅有 validation.\nGrasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping 发表时间: ICRA 2025\n动机 以前的方法都是以机器人为中心。 没有考虑到机器人和物块之间的相关性\n动机2 之前方法都太慢了\n动机 3 如果我们可以根据 observation 来推测出机械臂抓起物块的时刻的姿态 那我们就可以用 IK 等算法来计算这个抓取路径\n那我们就不需要数据中的路径信息了，只需要两帧，一帧开始，一帧结束\n模型流程图 先训练 Robot Encoder，用手部多配置点云对做 point-level contrastive learning，使手部特征具有结构一致性； 训练 CVAE：把训练好的 hand embedding 和 object embedding（经过 Transformer 融合）作为条件； 将 grasp pose 的手点云 输入到 CVAE 编码器，学习一个 latent 抓取表示； 用 CVAE 解码器重建抓取交互矩阵 D(R,O)，这是对 hand-object grasp 状态的结构表示； 用重建误差 + KL loss 训练整个模型。 结果 缺陷 有很多模型都只考虑了开始点和结束点的信息，没有经过 Motion Planning 这样的话如果遇到障碍物就容易出问题\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-06/","summary":"\u003ch2 id=\"leveraging-locality-to-boost-sample-efficiency-in-robotic-manipulation\"\u003eLeveraging Locality to Boost Sample Efficiency in Robotic Manipulation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2406.10615\"\u003e发表时间：15 Jun 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003e当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\u003c/p\u003e\n\u003cp\u003e问题 1：学习难度高（需要从高维图像中学全局策略）；\n问题 2：泛化差（模型可能过拟合于训练视角或场景）；\n问题 3：sample efficiency 差（训练数据需求量大）\u003c/p\u003e\n\u003cp\u003e作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的，\n也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e提出 Local Policy Networks（LPN）：\n将策略函数设计为一组 局部策略（local policy heads）；\n每个 head 只负责“在自己 patch 上预测动作”；\n用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）；\n最终策略通过对多个 local head 输出聚合（weighted sum）得到。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749017331301\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749017331301.png\"\u003e\u003c/p\u003e\n\u003cp\u003e简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\u003c/p\u003e\n\u003ch4 id=\"实验-setting\"\u003e实验 setting:\u003c/h4\u003e\n\u003cp\u003e使用数据集：RT-1（Robotics Transformer 1）:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-markdown\" data-lang=\"markdown\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据来源\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据规模\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e~130k 条实际机器人操作轨迹，覆盖 700+ 种任务\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e采样频率\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每条轨迹包含约 50–100 帧关键帧（图像 + 动作）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e场景\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e家庭式办公环境（桌面、水槽、地面）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e物体\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e语言指令\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型）\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e对于每一条指令：\u003c/p\u003e","title":"Bug Journal 2025-06-06"},{"content":"DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025\n动机 Robust，还得是Robust。\n为什么不加数据(ο´･д･)??\n过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\n那怎么办呢(ο´･д･)??\n很简单，纯计算模拟不就是了。\n主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\n这样就不需要实际操作，但是可以直接获得大量数据。\n模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\n思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\n机器人一共有两段动作\n一段是碰到物体前的动作\n一段是碰到物体后的动作\n对于第一段，直接用一个变换矩阵变换\n对于第二段，直接规划一个新路径 (use RRT-Connect)\n现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\n如果可以用的话\n然后通过模拟环境生成这个路径的图像\n实验设定 虚拟环境：1 条 GroundTruth\n真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\n一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\n结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 + mask + 指令 + robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM + Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove 一个这样的力反馈手套\n可以链接到灵巧手上，然后展示反馈物体的力\n盲抓分辨物体\n盲眼抓杯子\nReactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation 发表时间：23 Apr 2025\n动机 人有触觉，为什么机器人不行？\n所以我们为机器人加上了触觉。\n并且用触觉来优化模型.\n可以是，触觉数据怎么来呢？🤔\n在机器人手上加上触觉组件就好了\n模型流程 方法很简单：\nimage -\u0026gt; 正常的 CV Encoder -\u0026gt; Touch Encoder -\u0026gt; (Action -\u0026gt; Touch Encoder) * N -\u0026gt; image2 \u0026hellip;\n实现效果 Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control 发表时间：June 23 2025\n动机 现在机器人的表情不够生动，希望能生动一点\n用一段语音输入进机器人\n为什么选语音？\n因为语音有语气, 语气中可能有细微的表情差别\n“我没事\u0026hellip;” “我没事！”和 “我?没事” 是有区别的\n这更适合模型生成表情.\n而现有机器人的表情做得不好，所以这个团队自己做了一个可以做更丰富表情的机器人脸\n实现方式 第一部分，有一个带有丰富语气的语音数据集 + 对应的 3D 人脸建模数据\n第二部分，为了训练模型 -\u0026gt; 电机； 随机生成了 5000 个 3D 建模 让电机去拟合这个表情\n↑以上过程都是在虚拟环境中训练完，然后搬到真实环境做的。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-05/","summary":"\u003ch2 id=\"demogen-synthetic-demonstration-generation-for-data-efficient-visuomotor-policy-learning\"\u003eDemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2502.16932\"\u003e发表时间：24 Feb 2025\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003eRobust，还得是Robust。\u003c/p\u003e\n\u003cp\u003e为什么不加数据(ο´･д･)??\u003c/p\u003e\n\u003cp\u003e过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\u003c/p\u003e\n\u003cp\u003e那怎么办呢(ο´･д･)??\u003c/p\u003e\n\u003cp\u003e很简单，纯计算模拟不就是了。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\u003c/p\u003e\n\u003cp\u003e这样就不需要实际操作，但是可以直接获得大量数据。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749026226827\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749026226827.png\"\u003e\u003c/p\u003e\n\u003cp\u003e一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\u003c/p\u003e\n\u003cp\u003e思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\u003c/p\u003e\n\u003cp\u003e机器人一共有两段动作\u003c/p\u003e\n\u003cp\u003e一段是碰到物体前的动作\u003c/p\u003e\n\u003cp\u003e一段是碰到物体后的动作\u003c/p\u003e\n\u003cp\u003e对于第一段，直接用一个变换矩阵变换\u003c/p\u003e\n\u003cp\u003e对于第二段，直接规划一个新路径 (use RRT-Connect)\u003c/p\u003e\n\u003cp\u003e现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\u003c/p\u003e\n\u003cp\u003e如果可以用的话\u003c/p\u003e\n\u003cp\u003e然后通过模拟环境生成这个路径的图像\u003c/p\u003e\n\u003ch4 id=\"实验设定\"\u003e实验设定\u003c/h4\u003e\n\u003cp\u003e虚拟环境：1 条 GroundTruth\u003c/p\u003e\n\u003cp\u003e真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\u003c/p\u003e\n\u003cp\u003e一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\u003c/p\u003e\n\u003ch4 id=\"结果\"\u003e结果\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749093303146\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749093303146.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1749093327428\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749093327428.png\"\u003e\u003c/p\u003e\n\u003ch4 id=\"vs-roboground\"\u003eV.S. RoboGround\u003c/h4\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003cstrong\u003e方面\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eDemoGen\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eRoboGround\u003c/strong\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e目标问题\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e数据高效、空间泛化性差的视觉模仿学习\u003c/td\u003e\n          \u003ctd\u003e多任务泛化能力差、语义-空间信息连接弱\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e核心思想\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e从少量人类演示中\u003cstrong\u003e合成大量视觉演示数据\u003c/strong\u003e用于模仿学习\u003c/td\u003e\n          \u003ctd\u003egrounding mask（掩码）作为embedding增强泛化\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e数据生成方式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像\u003c/td\u003e\n          \u003ctd\u003e构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e人类演示\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e一条\u003c/td\u003e\n          \u003ctd\u003e在仿真中自动生成，无需真实 rollouts\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e任务表征形式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e(图像帧, 末端动作)\u003cstrong\u003e对\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e图像 + mask + 指令 + robot state\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e中间表示\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eNone（直接预测动作）\u003c/td\u003e\n          \u003ctd\u003e掩码（mask）作为空间引导\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e依赖模型\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eImmitation Learning\u003c/td\u003e\n          \u003ctd\u003e利用 VLM + Grounded Perceiver 构建 mask-guided policy\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e泛化方式\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e利用空间重定向与图像合成覆盖更多初始状态\u003c/td\u003e\n          \u003ctd\u003e通过 grounding masks 和多样 instruction 提升语义-空间泛化\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"doglove-dexterous-manipulation-with-a-low-cost-open-source-haptic-force-feedback-glove\"\u003eDOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"1749095773362\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-05/1749095773362.png\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-06-05"},{"content":"Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation 发表时间：15 Jun 2024\n动机 当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\n问题 1：学习难度高（需要从高维图像中学全局策略）； 问题 2：泛化差（模型可能过拟合于训练视角或场景）； 问题 3：sample efficiency 差（训练数据需求量大）\n作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的， 也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\n主要论点 提出 Local Policy Networks（LPN）： 将策略函数设计为一组 局部策略（local policy heads）； 每个 head 只负责“在自己 patch 上预测动作”； 用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）； 最终策略通过对多个 local head 输出聚合（weighted sum）得到。\n模型流程图 简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\n实验 setting: 使用数据集：RT-1（Robotics Transformer 1）:\nGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper） 数据来源 真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集 数据规模 ~130k 条实际机器人操作轨迹，覆盖 700+ 种任务 采样频率 每条轨迹包含约 50–100 帧关键帧（图像 + 动作） 场景 家庭式办公环境（桌面、水槽、地面） 物体 80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等） 语言指令 每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型） 对于每一条指令：\nRGB 图像 来自机器人头部相机的视角（尺寸通常为 240×320） 深度图（可选） 当前状态 如 gripper pose（位置 + 朝向） 语言指令 例如 “pick up the red apple and place it in the bowl” 动作标签 6-DoF 末端动作（位置增量、旋转、夹爪开合） 时间戳 当前帧在轨迹中的位置 成功标志 是否完成任务（某些版本包含） 文章通过 “加热”在数据集中的抓取位置 来 train heatmap.\n在 heatmap 中取出关键点\n之后通过神经网络预测每一个关键点的移动，加权得到机械臂移动的方向。\n通过模仿学习来算 Loss, 然后训练。\nCoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models 发表时间：13 Mar 2024\n动机 当前机器人 manipulation 研究大多集中在：\n高层任务规划（如使用 LLMs 推理“该做什么”）， 或低层控制（通过 imitation learning 或 RL 获取 end-effector 动作序列）。 然而，低层控制模块在真实世界中很难泛化，因为：\n缺乏对物体几何和功能部分的理解， 容易对新的任务和场景失效。 因此，本文希望结合 VLMs 的常识知识和几何建模能力，以“空间约束”的形式桥接语言与机器人执行之间的鸿沟，实现可泛化的低层 manipulation 能力。\n核心思想 Zero-shot! GPT-4V is all you need!\n模型流程图 首先分割场景，识别场景中有多少物体。\n简单来说就是：让 GPT 决定怎么操作这个物体。 比如说，要抓哪里，要怎么抓，物体的姿态是什么，\u0026hellip;\n最后使用一个传统路径规划算法来达成上述所有条件 一旦条件达成，任务也就完成了\n结果 MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment 发表时间: 24 Mar 2024\n动机 之前的模拟环境都太简单了，设计一个复杂的\n主要论点 设计了一个复杂的模拟环境:\n更多更复杂场景，更真实的物理引擎，更好的 Reward\nAny-point Trajectory Modeling for Policy Learning 发表时间：28 Dec 2023\n动机 数据不够用啦，我要从视频里学\n本文旨在从无动作标签的视频中提取出用于控制的轨迹，从而辅助策略学习\n主要论点 作者提出了 Any-point Trajectory Modeling (ATM) 框架：\n第一阶段：使用无动作标签的视频和现有视频追踪器，从视频中提取任意点的轨迹（即二维运动轨迹），训练一个轨迹预测模型（Track Transformer）。 第二阶段：使用轨迹预测模型，在给定任务语言描述和当前图像的情况下预测未来的轨迹，并用这些轨迹作为中间目标（subgoal）指导策略学习，从少量动作演示中学习有效的策略。 这不就是昨天的General Flow吗\n模型流程图 和昨天的那篇文章的区别在于:\n这篇不需要 Ground Truth 数据，只是视频就行，而那一篇需 如何做到的？答:用了一个 LLM Tracker 来跟踪 这篇文章是对\u0026quot;运动最显著\u0026quot;的 K(32) 个点算运动向量，那个是对每一个像素算运动向量 MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning 发表时间: 19 Oct 2024\n动机 做 RL, 还是得泛化\n过去的 embedding 太 General 了。\n思路是：如果 embedding 不分任务注意所有细节, 反而做不好 这里的泛化能忽略掉任务无关的信息\n总之，这里使用了一个 MoE 网络来处理输入。\n模型流程图 至于这个 Perturbation.. 流程如下:\n[输入图像 I] ↓ [Encoder 输出特征 z] ↓ [策略网络 → 计算 loss] ↓ [反向传播：计算 ∇_z L] ↓ [构造 δ = ε · normalized gradient] ↓ [扰动特征 z + δ → 再送策略网络训练] ↓ [更新 encoder + expert 参数] 总之就是训练的时候把 z 往成功的方向“推一下” “引导视觉 encoder 学会放大那些能带来任务成功的区域”\n结果 模拟环境：\nDMC (DeepMind Control Suite) → 如：Walker、Cheetah、Finger、Cartpole 等控制任务 Meta-World → 多任务机器人操作环境（push、reach、pick-place 等） RLBench → 视觉+控制为主的仿真机械臂任务，如插电源、开抽屉等\n真实世界任务：\n在完成 Task 的时候会干扰一下不让它完成这个 Task.\n但是最后还是成功完成了。\nTake away MoE 是什么呢？其实就是多个动态加权平均的网络。 什么是动态加权平均呢？ 就是权重是通过 $SoftMax(MLP)$ 算出来的 这样每次每个网络加权平均的权重就会不同。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-04/","summary":"\u003ch2 id=\"leveraging-locality-to-boost-sample-efficiency-in-robotic-manipulation\"\u003eLeveraging Locality to Boost Sample Efficiency in Robotic Manipulation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2406.10615\"\u003e发表时间：15 Jun 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003e当前机器人操作策略（如模仿学习、RL）在训练时通常会学习一个全局策略函数 \\pi(o_t)，即从全图像或全状态观察中直接输出动作。但：\u003c/p\u003e\n\u003cp\u003e问题 1：学习难度高（需要从高维图像中学全局策略）；\n问题 2：泛化差（模型可能过拟合于训练视角或场景）；\n问题 3：sample efficiency 差（训练数据需求量大）\u003c/p\u003e\n\u003cp\u003e作者提出一个核心假设：很多 manipulation 任务其实是“局部决策驱动”的，\n也就是说：只关注局部 patch（例如抓取点附近区域）即可决定动作。\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e提出 Local Policy Networks（LPN）：\n将策略函数设计为一组 局部策略（local policy heads）；\n每个 head 只负责“在自己 patch 上预测动作”；\n用 heatmap 表示哪些 patch 是可行操作区域（如抓取点）；\n最终策略通过对多个 local head 输出聚合（weighted sum）得到。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1749017331301\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-04/1749017331301.png\"\u003e\u003c/p\u003e\n\u003cp\u003e简单来说，就是寻找关键点，预测关键点的移动，然后整合成动作\u003c/p\u003e\n\u003ch4 id=\"实验-setting\"\u003e实验 setting:\u003c/h4\u003e\n\u003cp\u003e使用数据集：RT-1（Robotics Transformer 1）:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-markdown\" data-lang=\"markdown\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eGoogle Everyday Robot（移动底座 + 7-DoF机械臂 + gripper）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据来源\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e真实机器人操作任务，由人类远程操作（teleoperation）或脚本演示收集\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e数据规模\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e~130k 条实际机器人操作轨迹，覆盖 700+ 种任务\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e采样频率\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每条轨迹包含约 50–100 帧关键帧（图像 + 动作）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e场景\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e家庭式办公环境（桌面、水槽、地面）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e物体\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e80+ 类常见物体（杯子、水瓶、纸巾、玩具、锅等）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e语言指令\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e每个轨迹附有一条人类编写的自然语言 task 描述（用于多模态模型）\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e对于每一条指令：\u003c/p\u003e","title":"Bug Journal 2025-06-04"},{"content":"Catch It! Learning to Catch in Flight with Mobile Dexterous Hands 发表时间: 16 Sep 2024\n动机 Basically, 这是之前那篇 DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。\n模型流程图 这是真机器人的样子： 一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。\n训练的过程分为两步：\n第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。 第二步是微调灵巧手让手抓住这个物体。\n最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。\n解决的难点 部署到真机器人上 一步到位 end-to-end 效果没那么好 抓不住从未见过的物体 还需要解决的难点 从未见过的物体还是不好抓 仍然没有考虑材质之类的问题 还是无法在当物体在空中时就判断物体的形状 Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own 发布时间: 4 Oct. 2023\n动机 现在机器人训练要 $10^6$ 级别的数据，这要的时间太长了。 反观人类，人类不需要这么多数据。 这可能是因为人类在训练之前就知道什么能做什么不能做。\n那么，我们能不能输入一个 policy 给机器人让它也知道什么能做什么不能做呢？\n主要论点 训练方式：Reinforcement Learning\n并非模仿学习\n作者提出了一个新框架：\nRLFP（Reinforcement Learning with Foundation Priors），其中融合三种先验： 策略先验（Policy Prior）：告诉 agent “该怎么做”。 价值先验（Value Prior）：评估当前状态“是否更接近成功”。 成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 他们在此框架下构建了具体算法 FAC（Foundation-guided Actor-Critic），把三类先验引入到 Actor-Critic 的学习流程中，并在真实机器人与模拟任务中验证了该方法的有效性。\n模型流程图 策略先验（Policy Prior）：告诉 agent “该怎么做”。 这会生成一个策略分布。 这个策略分布会作为一个 KL 正则项。 希望 Actor 生成的策略和这个策略分布不会差太远\n价值先验（Value Prior）：评估当前状态“是否更接近成功”。 这会作为 Reward 的一部分。 告诉 Robot 它是不是更接近成功了。\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 这也会作为 Reward 的一部分。 告诉 Rotbot 它是不是成功了。\n实现细节 在现实中： 策略先验（Policy Prior）：告诉 agent “该怎么做”。 本文使用了 GPT-4V 来实现这个功能：\n对于每一个单独的任务，需要重新写一个这样的 prompt, 但是模板都是一样的。\n模板如下：\nYou are a helpful robot programming assistant. Here is an image of the current environment, which includes: \u0026lt;Task Instruction\u0026gt; Your job: \u0026lt;Task Instruction\u0026gt; You may only use the following primitive skills: \u0026lt;Primitive Skills List\u0026gt; \u0026lt;Image Input\u0026gt; Please write a Python code to solve this task, however, you can only write code in this format: \u0026lt;Code Format\u0026gt; e.g. (根据文章推测的 example prompt):\nYou are a helpful robot programming assistant. Here is an image of the current environment, which includes: - A plastic bottle with a green cap (the bottle is fixed to the table) - A pink plate nearby Your job: Help a robot arm **unscrew the bottle cap** and **place it on the pink plate**. You cannot lift the bottle. You must rotate the cap **anticlockwise** to unscrew it. You may only use the following primitive skills: # Primitive Skills: # 1. move_to x y z —— move the gripper to position (x, y, z) # 2. grasp —— close the gripper to grasp # 3. release —— open the gripper # 4. rotate_anticlockwise —— rotate the gripper anticlockwise (90°) # 5. rotate_clockwise —— rotate the gripper clockwise (90°) # 6. reset —— move back to the home position \u0026lt;input image\u0026gt; Please write a Python function `code_policy()` that returns a plan list using the above skills. Be sure to: - Estimate the coordinates from the image (roughly) - Include comments to explain each step - Output only the code block and nothing else Your format should be like this: def code_policy(): plans = [ \u0026#39;move_to 0.5 0.0 0.26\u0026#39;, \u0026#39;grasp\u0026#39;, \u0026#39;rotate_anticlockwise\u0026#39;, \u0026#39;move_to 0.75 0.0 0.06\u0026#39;, \u0026#39;release\u0026#39; ] return plans Now write the code: 价值先验（Value Prior）：评估当前状态“是否更接近成功”。 使用了一个 Pretrain LLM 来 “判断好不好” VIP: Universal Visual Reward and Representation via Value-Implicit Pretraining\nVIP 是一个使用大规模离线机器人/视频数据集，目标是 通过一个 image 得到一个方程 $V(O_{t_i})$, 越大表示越成功。\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 在现实中使用 GPT-4V 来判断这个任务是否完成。\n这有浇花的时候没判断成功 (3 success in all 4 tasks)。\n浇花和狡猾脚滑谐音，做不对是正常的\n在虚拟环境中 策略先验（Policy Prior）：告诉 agent “该怎么做”。 为了证明 Policy 不需要固定的形式\n使用了 \u0026ldquo;a diffusion-based policy prior, following the UniPi [25] pipeline\u0026rdquo;\n先用扩散模型生成一个完成任务的视频，再通过一个逆动力学模型把视频帧之间的状态变化转化为动作。\n为了效率起见，使用了 开源视频扩散模型 Seer [26] 预生成视频，然后离线训练（distill）出一个策略模型（policy network）\n然而，因为模拟环境图像质量比现实差，所以生成的视频效果也不好。\n所以用了 10个视频 fine-tune 了一下。\n价值先验（Value Prior）：评估当前状态“是否更接近成功”。 Same set up.\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 这里有 Ground Truth 了，就不用 GPT-4V 了 但是，为了模拟现实中 GPT-4V 的情况，加入了一些噪声。 加入方法如下： 训练一个模型，从状态 + label 预测是否成功 这个模型不是 100% 准确。\n结果 现实世界一个小时后： 模拟世界:\nAblation study\nTake away run on 3090 GPU\nGeneral Flow as Foundation Affordance for Scalable Robot Learning 发表时间: 21 Jan 2024\n动机 一言以蔽之：机器人如何从感知（图像）中知道：“我该操作哪儿”和“怎么操作”？\n当前机器人操作学习普遍依赖：\n大量手工收集的数据； 手动定义的 affordance； 复杂的模仿学习或强化学习流程 而现在的数据：\n泛化能力差 没有统一，可以拓展的，自动的，包含语义的 embedding 主要论点 General Flow（GF） —— 一种结构化、密集的视觉场表示，表征“像素应如何在操作中流动”。\n这个和 NVIDIA DLSS 中的 OPTICAL FLOW 很类似，只不过加入了语义信息。\n模型流程图 ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors 发表时间: 30 Apr 2025\n动机 机器人操作策略泛化能力差, 能不能用 mask 的办法让机器人操作策略的泛化能力更强。\n主要论点 构建一个自动化数据生成流水线，合成高复杂度、多样化语言指令的数据集（112K 指令，24K 演示）;\n利用 GLaMM 模型和 SAM 架构生成目标对象与放置区域的精细分割 mask; 将这些 masks 融合进策略网络\n模型流程图 Part 1 数据集 现在的数据集不够好，我要弄一个新的数据集。 我有一个虚拟环境，这个环境里面有一些物体。\n那我能不能写一个脚本来自动设计一个数据集。\n为了给测试用的模型增加难度，我要在环境中添加一些相似的物体。\n那就可以在图片中找一些相似的物体出来，最好是有一项特征(如颜色)完全一致\n这时候 GPT 可以帮忙\nGPT 对这些物体有一定理解 (3视图，材质，颜色)\nGPT 的这些理解也可以加入进来帮我挑选要放入那些物体。\n有了环境信息还不够，我还要一个 Language Instruction.\n首先，我可以根据位置信息自动生成一些 rule-based Instruction: 比如把 A 移动到 B的右边\u0026hellip;\n那如果要一些更 abstract 的 Instruction, 那我可以用 GPT 生成一个 Language Instruction.\n比如说 水果 -\u0026gt; making jam.\nPart 2 New method 左边的部分和那天的 SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation 很像，都是通过一个 LLM + SAM 获取起始点和终点的信息。\n而右边的部分就是通过 强调起始点和终点 的 attention 来增强起点和终点权重。\n更详细地：通过增加了两个 Query, 分别只和起点和终点做 attention 来增强。\n最后通过一个 transformer decoder 输出离散的 action。\n训练是通过模仿学习，最小化和样本之间的差距。\n效果 计算复杂度 8 * 4090 GPU Approx. 5 days.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-03/","summary":"\u003ch2 id=\"catch-it-learning-to-catch-in-flight-with-mobile-dexterous-hands\"\u003eCatch It! Learning to Catch in Flight with Mobile Dexterous Hands\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2409.10319\"\u003e发表时间: 16 Sep 2024\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"动机\"\u003e动机\u003c/h4\u003e\n\u003cp\u003eBasically, 这是之前那篇 \u003cem\u003e\u003ca href=\"https://arxiv.org/abs/2310.08809\"\u003eDexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands\u003c/a\u003e\u003c/em\u003e 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1748922875088\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-03/1748922875088.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是真机器人的样子：\n一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"1748922862160\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-06-03/1748922862160.png\"\u003e\u003c/p\u003e\n\u003cp\u003e训练的过程分为两步：\u003c/p\u003e\n\u003cp\u003e第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。\n第二步是微调灵巧手让手抓住这个物体。\u003c/p\u003e\n\u003cp\u003e最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。\u003c/p\u003e\n\u003ch4 id=\"解决的难点\"\u003e解决的难点\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e部署到真机器人上\u003c/li\u003e\n\u003cli\u003e一步到位 end-to-end 效果没那么好\u003c/li\u003e\n\u003cli\u003e抓不住从未见过的物体\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"还需要解决的难点\"\u003e还需要解决的难点\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e从未见过的物体还是不好抓\u003c/li\u003e\n\u003cli\u003e仍然没有考虑材质之类的问题\u003c/li\u003e\n\u003cli\u003e还是无法在当物体在空中时就判断物体的形状\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"reinforcement-learning-with-foundation-priors-let-the-embodied-agent-efficiently-learn-on-its-own\"\u003eReinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2310.02635\"\u003e发布时间: 4 Oct. 2023\u003c/a\u003e\u003c/p\u003e","title":"Bug Journal 2025-06-03"},{"content":"CLIP 这是 CLIP 的结构，主要是两个部分，一个是 text encoder, 用于得到 text embedding, 一个是 image encoder, 用于得到 image encodding.\nText encoder 是一个 text transformer, Image encoder 是一个 ResNet50 / ViT.\n输入数据是一张图片和它的 alternative text.\n训练的逻辑也不难理解：\n现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text, 现在我对这 N 个 pair 做两两配对。\n如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近 如果不属于同一个 pair, 那么我希望他们的 embedding 更远\n同时，我希望这个embedding的距离是有意义的，越相近的离得越近，越不同的离得越远。\n这时候我们就可以用 Cosine Similarity Loss 来比较两个 embedding 之间的距离。\nThat\u0026rsquo;s it.\n这里有个 assumption, 就是说，虽然我的数据质量不怎么样，但是我有很好的数量。\n对于每一个patch, 我有整整 32k 个图文 pair, 加起来一共 1B 个 True/False pair, 那我一定是可以学到一些东西的。\n但是这里也就是它的局限所在：32k 个图文 pair 要放到非常大量的 GPU 中才能 work, 这时设备之间的通信就成为了最大的效率瓶颈。\n但是虽然说这个模型 train 起来非常复杂，但是其实这个模型不算太大，单 GPU 就足以 inference.\n代码也非常简洁，一个简单的实现如下:\nimport clip import torch from PIL import Image device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;, device=device) image = preprocess(Image.open(\u0026#34;test.png\u0026#34;)).unsqueeze(0).to(device) # CLIP.png为本文中图一，即CLIP的流程图 text = clip.tokenize([\u0026#34;car\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;leaf\u0026#34;]).to(device) # 将这三句话向量化 with torch.no_grad(): logits_per_image, logits_per_text = model(image, text) probs = logits_per_image.softmax(dim=-1).cpu().numpy() print(\u0026#34;Label probs:\u0026#34;, probs) BLIP 这是 BLIP，大部分和之前一样：\n现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text, 现在我对这 N 个 pair 做两两配对。\n如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近 如果不属于同一个 pair, 那么我希望他们的 embedding 更远\n但是，现在我增加了一个部分：除了原本的 Contrastive learning 之外，我还要做一个图片和文字之间的 cross-attention.\n另外，原本的数据里有很多噪音。\n现在我已经初步 train 好一个图文匹配模型了。\n那我们默认在这个模型中图文匹配比较好的，就是数据中“高质量”的部分。\n这时我们再引入一个图生文模型，让模型自己学习这些“高质量”数据，然后覆盖“低质量数据”。\n这样就可以提高数据的整体质量。\nQ-Former 是轻量、任务相关、可控制的视觉语义提取器。\nSigLIP AlexNet AlexNet就是最开始的 CNN 网络\nResNet ResNet引入残差的概念，不再让 CNN 学习原始表示，而是让 CNN 学习不同层之间的差 同时，有些层的结果可以越过中间某些层直接去往更深的层。 这让更深的网络成为了可能。\nDenseNet DenseNet 则是再进一步，DenseNet 会让每层之间形成两两连接，使得网络效果更好。\nTake Away 为什么要用 Cosine Similarity:\n可大可小，既可以拉进，又可以推远；重要的是大小是有意义的，越大代表越不相近，越小代表越相近。\n单塔模型和双塔模型：\n单塔模型就是一个输入的模型；双塔输出就是有两个输入的模型。 或者说，如果一个 embedding 只过一遍模型，那就是双塔模型。 如果一个 embedding 可能要过多遍一个模型，那就是单塔模型。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-02/","summary":"Review of Three base VLA models and three basic CNNs.","title":"Bug Journal 2025-06-02"},{"content":"主流视觉-文本多模态模型技术分析 近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。\nCLIP (OpenAI, 2021) 整体架构设计： CLIP 采用 双编码器架构 ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到相同维度的向量空间lightly.ai。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型lightly.ai。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到共享的多模态嵌入空间lightly.ai。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。 模态对齐方式： CLIP通过对比学习实现视觉-语言对齐。训练时，模型给定一批图文对，学习预测哪张图像与哪段文本匹配lightly.ai。具体而言，CLIP使用 对称的跨模态对比损失 （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度lightly.ai。这种训练使图像和文本编码器产生的特征在共享空间中成对靠近lightly.ai。 输入 token 表达统一： CLIP并未显式统一图像和文本的输入表示格式。 图像和文本各有独立的token化和编码流程 ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征lightly.ai。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过独立编码+对齐空间的方式，规避了直接将图像作为序列token处理的不统一问题。 损失函数与训练策略： 损失采用 对比学习的InfoNCE损失 （实现为带温度系数的归一化softmax交叉熵）research.google。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了大批量训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化lightly.ai。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应lightly.ai。CLIP从随机初始化开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型lightly.ai。 数据集及伪标签： CLIP在一个超大规模的图文配对数据集上预训练，包含约4亿对图像-文本arxiv.org（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖自然语言的弱监督arxiv.org。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。 六大难点应对： 模态对齐困难： CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远lightly.ai。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。 token格式不统一： 采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对lightly.ai。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。 语义粒度不匹配： CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如Fine-Grained CLIP等正是受限于CLIP在局部语义对齐上的不足lightly.ailightly.ai。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。 多模态上下文保持： CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 多模态上下文 （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。 训练数据稀缺： CLIP通过大规模弱标注数据从根本上缓解了数据稀缺的问题arxiv.org。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征research.google。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。 计算开销高： 训练CLIP确实需要巨大的算力和显存，但相对来说，其双塔架构使训练可并行展开，推理时也可分别预编码图文后做相似度计算，具有一定的效率优势lightly.ai。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化lightly.ai。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现lightly.ai。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。 参考： CLIP 的论文arxiv.org详细描述了其对比预训练方法，OpenAI 的博客也提供了概述lightly.ai。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库huggingface.co。\nALIGN (Google, 2021) 整体架构设计： ALIGN（ A Large-scale ImaGe and Noisy-Text embedding ）延续了与CLIP相同的双编码器对比学习架构research.google。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：EfficientNet-L2卷积网络作为图像编码器，BERT-Large作为文本编码器，并均从随机初始化开始训练research.google。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。 模态对齐方式： ALIGN采用对比损失（normalized softmax）来训练，使匹配的图文对嵌入向量接近，不匹配的远离research.google。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以批为单位的跨模态对比训练，使模型学到强大的图文对齐表示。 输入 token 表达统一： ALIGN同样没有将图像直接离散为token序列，而是通过双通道处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出embedding空间实现统一表示research.google。 损失函数与训练策略： 使用 对比学习损失 （InfoNCE变体），在大批量上训练模型research.google。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 最小程度的过滤 ，通过数据规模来弥补噪声research.google。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。 数据集及伪标签： ALIGN的亮点在于使用了超过10亿对图像-Alt文本的超大规模数据集research.google。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 无需人工标注 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN 放宽过滤标准 ，只做了基于频率的简单过滤，最终得到约18亿对图文数据research.google。这些文本描述可能包含噪声甚至与图像无关，但研究表明 规模弥补质量 ：如此海量的数据使模型学到泛化的视觉语言表示research.google。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题research.google。 六大难点应对： 模态对齐困难： ALIGN证明了数据规模在对齐中的重要作用。通过十倍于CLIP的数据规模和强大的对比损失，模型学到了稳健的跨模态对齐能力research.googleresearch.google。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型research.google。 token格式不统一： 与CLIP类似，ALIGN通过双编码器架构回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可research.google。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。 语义粒度不匹配： ALIGN的训练目标依旧作用在 全局图像-句子层面 ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对细粒度语义的不匹配没有特殊解决方案。 多模态上下文保持： ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力research.google，未涉及多模态对话等情境。因此，ALIGN在多轮交互或长上下文问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。 训练数据稀缺： ALIGN的策略是极端扩增数据规模以消除数据稀缺瓶颈research.google。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型research.google。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 弱标注的大数据 。 计算开销高： 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展research.google。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源research.google。尽管计算开销高昂，ALIGN通过 冻结架构复杂性 （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，优先扩大数据规模比增加模型复杂度更有效research.googleresearch.google。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。 参考： ALIGN 的研究细节发表于 ICML 2021research.google。Google Research 官方博客提供了对ALIGN的通俗描述research.googleresearch.google。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。\nBLIP (Salesforce, 2022) 整体架构设计： BLIP（ Bootstrapping Language-Image Pre-training ）提出了一种 多模态混合编码-解码架构（MED） ，旨在同时支持视觉-语言的理解和生成任务lightly.ai。具体而言，BLIP的模型包含三个不同模式的子模型：① 单模态编码器 ：对图像或文本单独编码，用于提取各自模态的表示，并采用图文对比损失（ITC）对齐两种模态的全局特征lightly.ai；② 图像引导的文本编码器 ：在文本Transformer中引入跨模态注意力，让文本编码能够利用图像特征，训练时使用 图文匹配（ITM）损失来判断给定图文是否匹配lightly.ai；③ 图像引导的文本解码器 ：使用因果自注意（单向）以实现文本生成，能够在给定图像的条件下生成描述或回答，训练时使用语言模型（LM）损失lightly.ai。这三部分共享同一个视觉编码器（ViT）和大部分文本编码-解码参数，仅在自注意力层是否双向/单向上有所区别lightly.ai。这种设计使一个模型即可兼顾判别任务 （如检索、VQA判断）和 生成任务 （如图像描述）。 模态对齐方式： BLIP结合多种目标实现模态对齐和融合：(1) 图文对比学习（ITC）让图像和文本的全局表示对齐，获得CLIP类似的跨模态嵌入空间lightly.ai；(2) 图文匹配（ITM）通过引入交叉注意力的文本编码器，对图文对的匹配与否进行二分类训练，从而细粒度地对齐图像内容和文本语义lightly.ai；(3) 图像条件文本生成（LM损失）使模型学会在视觉条件下生成合适的语言输出。这三种损失共同训练，迫使模型不同层面地对齐：既有 全局对齐 （ITC保证embedding空间一致），又有 局部对齐 （ITM通过注意力机制关注图像局部来判断匹配），还有跨模态 生成对齐 （LM确保图像信息融入生成过程）。尤其ITM子任务，需要模型理解图像细节与文本句子细微差异，提高了模态对齐的精细程度。 输入 token 表达统一： BLIP并未将图像直接当作序列token交给文本Transformer，而是采用部分共享参数的编码器-解码器架构来统一多模态信息。图像首先由视觉Transformer编码为一组图像embedding序列，文本则通过词嵌入得到文本token表示。在图文交互阶段，图像embedding通过跨模态注意力机制供文本编码器/解码器读取，从而在Transformer中融合lightly.ai。由于文本端Transformer参数在编码和解码模式下共享（仅注意力方向不同），图像信息可以以一致的方式融入文本token处理流程lightly.ai。简言之，BLIP通过在Transformer中引入图像作为钥匙/值的跨注意力，将图像内容注入文本token序列的处理，使两种模态的信息在Transformer中统一表达和交互。 损失函数与训练策略： BLIP在预训练阶段联合优化三种损失lightly.ai：图文对比损失、图文匹配损失、语言建模损失。一次训练迭代中，通过共享的图像编码器提取视觉特征后，分别送入上述三种模式的网络计算损失lightly.ai。为提高效率，BLIP 让文本编码器和解码器共享参数 （仅自注意力模块不同），这样图像特征可一次读取，多头任务不会成倍增加参数lightly.ai。训练策略上，BLIP先在大规模图文数据上这样多任务预训练，然后可以微调到下游具体任务。由于同时优化多目标，需平衡各损失的权重，论文中选择了适当的超参使模型在理解和生成性能上均有提升。此外，为了利用噪声较大的网络数据，BLIP设计了 两阶段Bootstrapping策略 ：首先用预训练好的模型作为图像描述生成器（Captioner）为图像生成候选描述，然后用一个筛选器（Filter）剔除不匹配的图文对，再将清洗/补充后的数据用于进一步训练lightly.ai。这种“Captioner+Filter”流程有效降低了训练数据中的噪声，并引入了合成的伪标签描述，提高了数据利用率arxiv.org。 使用的数据集及伪标签： BLIP的预训练使用了数百万规模的公众图文对数据（如COCO Caption、Visual Genome等常用数据集的组合，以及从网上爬取的图文对）arxiv.org。相对于CLIP/ALIGN那样十亿级的弱标注数据，BLIP使用的数据规模较小但质量更高（经过一定清洗）。为了进一步扩充数据，BLIP引入 伪标签机制 ：利用自己模型生成图像描述（Captioner生成synthetic captions），再通过训练好的匹配模型（Filter）过滤噪声lightly.ai。结果是，原本嘈杂的网络图文数据被“自举”出较为可靠的新图文对，从而缓解了高质量标注数据不足的问题arxiv.org。这一过程中生成的图像描述相当于 合成标签 ，极大丰富了训练语料。在预训练后，BLIP在下游如Flickr30K检索、COCO描述、VQA等数据集上进行微调或直接评估，均取得领先性能arxiv.org。值得一提的是，BLIP的整个预训练不依赖外部标注工具（如不使用额外OCR或检测模型），完全通过多任务训练和自举数据来提高性能。 六大难点应对： 模态对齐困难： BLIP通过多重训练目标从不同层次对齐图像和文本。ITC损失提供全局嵌入对齐，ITM损失迫使模型关注细粒度关联来判断真伪配对，LM损失则确保图像信息能融入自然语言生成lightly.ailightly.ai。此外，BLIP在训练中让图像参与文本Transformer的注意力计算，直接在模型内部融合模态，这比单纯对比学习的对齐更深入。综合来看，BLIP有效缓解了模态对齐难的问题，使模型不仅对整体匹配敏感，也能对局部语义对齐做出正确判断。 token格式不统一： BLIP没有一刀切地将图像转为离散token，而是采用跨注意力融合策略保持各模态表征方式的优势。图像以连续向量形式存在，通过跨模态注意力供文本Transformer使用，实现类似“在Transformer中把图像当成一串记忆token”的效果。这种方法避免了人为定义图像token格式的问题，由模型自学怎样将图像特征融入文本语境。因此，BLIP在不显式统一输入格式的情况下，通过模型结构实现了功能上的token统一处理。 语义粒度不匹配： BLIP在架构上引入了细粒度语义对齐机制。ITM子任务要求模型判别图文是否匹配，这通常取决于对图像细节和文本词语是否对应的判断（例如一句描述中某个细节是否在图中存在）。模型通过跨注意力可以聚焦图像的局部区域来对应文本片段，从而解决了图像区域与文本词汇粒度不对齐的问题。在生成阶段，图像引导的解码器也能描述具体对象或属性，实现细粒度描述lightly.ai。因此BLIP比CLIP这类全局对齐模型更好地兼顾了细粒度的语义对齐。 多模态上下文保持： 虽然BLIP主要针对单幅图像与单段文本的配对任务，但其架构天然适用于 多轮交互的扩展 。因为BLIP的文本Transformer本质是一个语言模型，经过适当调整可以接受前文对话作为文本输入，然后结合图像生成回答。事实上，BLIP的设计理念已被后续多模态对话模型（如 InstructBLIP 等）继承，用于处理多轮对话。不过BLIP原始模型并未显示多轮对话训练。在单轮情景下，BLIP利用Transformer的长序列能力，一定程度上可以处理 更长的文本上下文 （如图像说明+问题一起作为前缀，然后生成回答）。因此在上下文保持上，BLIP通过Transformer结构具备了潜在优势，但需通过特定训练来充分发挥。 训练数据稀缺： BLIP以 自举（Bootstrapping）的方式缓解数据不足。面对高质量标注数据有限的问题，BLIP使用初步模型为大量未标注图像生成描述（相当于自动标注），再过滤噪声后加入训练arxiv.org。这种方式有效放大了训练集规模且成本低，因为生成伪标签比人工标注快得多。结果，BLIP无需像CLIP那样依赖上亿数据，就能取得优异表现arxiv.org。此外，多任务联合训练也提高了数据利用效率：同一数据同时为对比、匹配、生成三种任务服务，信息提取更加充分。总之，BLIP通过模型自生成数据+多任务学习 ，成功在有限数据下逼近甚至超越了依赖海量数据的方法。 计算开销高： BLIP的模型大小适中（基于ViT-B/16等视觉主干，文本部分与BERT-base级别相当），但同时优化三种目标确实增加了训练复杂度。不过，通过 参数共享 （文本编码器和解码器共享大部分参数）lightly.ai和 模块复用 （同一个视觉编码器和Transformer用于多任务），BLIP将训练开销控制在可接受范围。相较于为理解和生成训练两个模型，BLIP训练单个模型完成两类任务，实际上 节省了总体算力 。当然，多任务训练需要更长时间收敛，但Salesforce的实验表明收益是值得的arxiv.org。在推理阶段，BLIP可以根据任务切换模式，例如执行检索时只用编码器部分，做描述时用编码-解码器，全模型参数无需全部参与，从而 推理开销也相对可控 。综上，BLIP通过架构设计在性能和计算成本之间取得了平衡，使得大型多模态模型的训练变得更加高效。 参考： BLIP的论文发表在 ICML 2022arxiv.orgarxiv.org。官方代码已开源在 GitHubarxiv.org（salesforce/BLIP仓库），提供了预训练模型和下游任务的fine-tune实现，方便复现论文结果。\nBLIP-2 (Salesforce, 2023) 整体架构设计： BLIP-2的核心思想是利用现有的预训练模型来高效构建多模态模型arxiv.org。它冻结了图像编码器（如ViT系列）和大型语言模型（LLM，如OPT、Flan-T5等），通过引入一个轻量级的Query Transformer（Q-Former）将二者连接起来arxiv.org。架构上包括：冻结的视觉编码器-\u0026gt; Q-Former -\u0026gt; 冻结的文本生成模型。Q-Former本质是一个Transformer模块，接受视觉特征作为输入，输出一组固定数量的查询向量lightly.ai。这组向量经过投影后，作为虚拟的“视觉token”，嵌入到LLM的输入序列中，从而让LLM能够接收图像信息lightly.ai。由于LLM参数冻结，BLIP-2主要训练Q-Former和少量连接层。 模态对齐方式： BLIP-2将跨模态对齐的主要难点转移到Q-Former上。它采用两阶段训练lightly.ai：第一阶段，让Q-Former结合冻结视觉编码器进行 图文表示学习 （类似BLIP-1的方法，使用图文对比或匹配损失），使Q-Former学会提取与文本语义相关的视觉概念lightly.ai。第二阶段，将训练好的Q-Former输出连接到冻结的LLM输入embedding，利用图文对话/描述数据训练生成任务，使整个系统能够端到端地产生对应输入图像的文本lightly.ai。在对齐过程中，Q-Former充当中介：它一头通过跨注意力读取视觉特征，另一头输出的查询向量要能和LLM的语义空间对接。因此，通过专门设计的损失（如阶段一的对比/ITC+ITM，阶段二的语言建模），BLIP-2成功将视觉空间对齐到语言空间。直观来说， Q-Former学会生成“描述图像的语言向量” ，这些向量插入LLM提示中后，LLM即可理解并基于图像内容作出回答。 输入 token 表达统一： 在BLIP-2中，输入给LLM的是标准的文本token序列，但其中混入了由图像生成的 特殊嵌入向量 。具体实现是：LLM的词表中引入若干个保留位置，用来放置Q-Former生成的视觉查询向量（不一定真的映射为离散token，而是直接作为embedding）lightly.ailightly.ai。因此，从LLM角度看，它接收到了一串长度为N（固定）的“视觉token”嵌入，后面可能跟随文本token，例如问题或提示语。通过这种方式，图像信息被格式统一地并入LLM的输入序列，就好像视觉也被表示成了一组特殊的单词embedding。值得注意的是，这里的视觉token并非通过人工词典获得，而是Q-Former自由学习产生的向量。不过，对于LLM来说，无论是真实文字embedding还是视觉embedding，它都一视同仁地通过自注意力机制处理。这实现了在架构上的 输入统一 ：图像被转换成等价于文本embedding的形式，与文本共同作用于下游生成。 损失函数与训练策略： BLIP-2采用 分阶段训练策略 。阶段一使用与BLIP类似的目标（ITC对比学习、ITM匹配等）训练Q-Former，使其能够对齐视觉和文本表示lightly.ai。阶段二则固定视觉编码器和Q-Former不变（或仅微调Q-Former），仅训练将Q-Former输出喂入LLM后的生成能力lightly.ai。阶段二通常采用 语言模型损失 ：给定图像和（可选的）文本提示，让LLM输出描述或答案，与GT文本计算交叉熵损失，从而调整Q-Former和连接层使LLM的输出正确。在这一阶段，LLM本身参数冻结，所以训练信号主要作用于Q-Former，使其输出的视觉查询能被LLM高效利用。此外，BLIP-2可能使用了混合数据训练策略：既包含纯图文对话数据，也包含传统图文描述数据，以增强模型泛化能力。总结来说，第一阶段注重表示对齐，第二阶段注重生成对接arxiv.org。两个阶段结合，使模型以较低的训练成本达到对大语言模型“喂图”的效果。 使用的数据集及伪标签： BLIP-2所使用的数据包括现成的大规模图文对以及 对话式多模态数据 。阶段一使用的数据类似BLIP-1，例如COCO Caption、Visual Genome Caption以及LAION-400M等开放图文集，用于学习跨模态表示。阶段二则需要图像输入/文本输出的监督数据，如VQA问答、图像描述，以及自制的指令数据集等。由于BLIP-2本身是在2023年提出，可能利用了当时兴起的多模态指令数据（例如由GPT生成的对话）来增强模型的对话能力。关于伪标签，BLIP-2相比BLIP-1更少需要合成描述，原因是它直接利用预训练的LLM已经具备生成流畅文本的能力。相反，BLIP-2更关注 如何高效利用预训练资源 。它没有从零开始生成伪标签，而是通过降低训练需求（冻结大模型）来避免需要海量新标注数据arxiv.org。因此，除非为了特定任务，BLIP-2通常不依赖额外的伪标注数据。不过，在一些研究和开源实现中，会将BLIP-2作为基础，再用GPT-4生成的指令数据进行微调（如InstructBLIP），那属于后续fine-tuning阶段。总的来说，BLIP-2本身强调 利用已有数据与模型 ，而非采集新数据，这是一种不同于以往“大规模爬取”的范式。 六大难点应对： 模态对齐困难： BLIP-2的巧妙之处在于借助预训练模型降低对齐门槛：视觉编码器（如CLIP的ViT）本身已具有与文本对齐的表示能力，LLM则有强大的语言理解和生成能力。Q-Former经过专门训练，学习如何从图像提取出能解释文本的关键视觉概念lightly.ai。它将视觉信号压缩成几十个查询向量，使之恰好能被LLM理解。通过两阶段训练，BLIP-2成功将视觉信息嵌入LLM的上下文中，实现 模态隐式对齐 。尤其第二阶段训练，让模型生成正确描述，确保了视觉表示和语言表示在语义空间上对齐，以至于LLM可以将来自图像的embedding视作自身词汇的一部分。这解决了LLM未看过图像的难题，将跨模态对齐转换为一个中等规模Transformer训练就完成了arxiv.org。因此，BLIP-2在对齐上绕过了直接训练巨型多模态模型的难关，以更低成本达到对齐效果。 token格式不统一： BLIP-2通过 Query Transformer输出固定长度视觉token向量 ，使得图像信息以接近文本token的形式输入LLMlightly.ai。这些视觉token不是离散符号，但在Transformer中发挥的作用与普通词嵌入相同。LLM在位置嵌入上也不区分它们，这样视觉和文本序列实际上融合为一个统一的序列处理lightly.ai。因此，虽然没有显式定义图像的词汇表，BLIP-2达成了功能上的token格式统一：模型把连续视觉特征转换为离散的若干embedding插入序列。LLM可以像处理句子一样处理“图像句子”。这一设计继承了Flamingo等模型的思路，但更轻量（因为只有Q-Former承担额外计算）。 语义粒度不匹配： BLIP-2输出的视觉token向量本质上可以被视为图像的 语义摘要 。Q-Former通过训练，会针对文本任务提取图像中与语义相关的细粒度信息。例如，若任务是描述图像，Q-Former会聚焦显著对象和属性；若任务是回答问题，Q-Former会提取与问题相关的视觉线索。这种机制使图像的大量低层次像素信息被压缩，仅保留语义层面的关键内容，从而匹配LLM处理的语言粒度（概念级别）lightly.ai。因此，语义粒度的鸿沟通过Q-Former的提炼得到弥合——图像的细节被提升到语义概念后才提供给LLM。实践证明，BLIP-2能够让LLM正确识别图中具体对象并生成相应描述，说明语义层次基本匹配了语言空间arxiv.org。当然，如果图像中有非常细微的局部信息，固定数量的查询可能略有不足，但总体上BLIP-2在保持主要语义同时过滤冗余细节方面是成功的。 多模态上下文保持： BLIP-2本身不直接处理多轮对话，但由于它的输出接口是对接LLM，而LLM天然支持长上下文对话，因此BLIP-2具备扩展为多模态对话的潜力。事实上，将BLIP-2生成的视觉token视为对话的一部分，就可以实现 ChatGPT+图像 的效果。BLIP-2的论文主要评估的是单轮任务（如VQA回答、图像描述），但把它用于对话时，可以每次在提示中加入视觉token并配合已有的聊天上下文，LLM即可持续参考视觉信息进行对话。这意味着BLIP-2间接实现了 视觉上下文在多轮对话中的保持 ：视觉token可以在对话prompt中重复出现或被引用，使LLM记住之前提到的图像要点。不过，在一个会话过程中，BLIP-2通常针对每张新图像各自运行一次，不会像Flamingo那样显式处理多张图共同存在的情况。因此严格来说，BLIP-2原生支持 单图上下文保持 ，多图或连续对话需借助LLM的记忆机制来维系。 训练数据稀缺： BLIP-2的策略是 以预训练模型替代海量数据 。因为直接训练一个看图的LLM需要海量图文数据，但BLIP-2通过使用预先训练好的ViT和LLM，将主要学习任务转为训练Q-Former。Q-Former的参数规模（约1.9亿）远小于LLM，所需训练数据也相对少lightly.ai。实验表明，在已有的大模型基础上，只需在相对有限的图文数据上微调，就能达到甚至超过训练80亿参数模型（如Flamingo）的效果arxiv.org。这等于用模型知识弥补了数据量不足。此外，BLIP-2本身利用了BLIP-1时期的图文数据清洗经验，挑选高质量数据进行两阶段训练，以较小的数据量取得高性能arxiv.org。因此，对研究者而言，BLIP-2降低了训练多模态模型的数据门槛——无需爬取上亿样本，有几百万高质量样本配合预训练模型就够用。 计算开销高： 相比从头训练一个多模态Transformer（参数往往数十亿），BLIP-2的训练开销显著降低。冻结LLM和视觉编码器意味着大部分参数不需要反向传播更新，只训练Q-Former等少部分参数，使内存和算力需求下降。作者报告，BLIP-2仅有少量可训练参数却超越了一些体量大几十倍的模型arxiv.org。同时，由于分阶段训练，第一阶段可在相对小模型上完成，第二阶段虽然用LLM但只进行embedding层和Q-Former的调优，计算效率高。综合来看，BLIP-2通过迁移学习和 参数高效微调 ，极大缓和了算力需求。这也体现在推理阶段：因为LLM冻结且对话时只需将视觉token拼接输入，不增加额外推理步骤，实时性有保障。当然，BLIP-2依赖的LLM本身推理开销不低（如果LLM很大），但相较于训练一个同等大小的多模态模型，BLIP-2的总计算代价小得多。因此，在算力有限的环境下，BLIP-2提供了一种实用可行的多模态方案。 参考： BLIP-2的论文在 arXiv 发布arxiv.orgarxiv.org（ICLR 2023），详细介绍了其两阶段训练方法和在零样本VQA等任务上的性能。代码已开源在 GitHub（salesforce/LAVIS库中提供了BLIP-2实现）。BLIP-2的效果也推动了许多衍生工作（如开放对话系统 MiniGPT-4 等），这些都建立在BLIP-2提供的视觉-语言接口之上。\nGIT (Microsoft, 2022) 整体架构设计： GIT（ Generative Image-to-text Transformer ）尝试将视觉-语言任务完全统一到一个生成式Transformer框架下ar5iv.labs.arxiv.org。其架构极为简洁： 一个图像编码器 + 一个文本解码器 ，二者共同组成一个端到端的序列到序列模型ar5iv.labs.arxiv.org。图像编码器提取图像特征（采用预训练的CLIP视觉Transformer或自训练的ViT等，输出二维特征序列ar5iv.labs.arxiv.org），然后通过线性层投影并加上位置嵌入，作为文本解码器的跨注意力键值输入ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。文本解码器是标准的Transformer解码架构（多层自注意力+交叉注意力），以语言模型方式生成文本ar5iv.labs.arxiv.org。不同于许多早期方法，GIT不使用任何物体检测器或OCR模型来预处理图像，也不引入额外的多模态编码器，一切融合在Transformer解码器中完成ar5iv.labs.arxiv.org。这种纯粹的“图像到文本”架构使模型在预训练和微调阶段的结构完全一致，能够方便地泛化到各种以文本为输出的视觉任务。 模态对齐方式： GIT没有采用显式的对比对齐或ITM损失，而是通过单一的语言建模任务隐式地实现模态对齐ar5iv.labs.arxiv.org。在预训练时，模型接收图像并 直接生成整段描述文本 （或回答），训练目标是最小化生成文本与真实文本之间的交叉熵损失ar5iv.labs.arxiv.org。这种方式迫使图像编码器提取的特征必须包含生成正确文本所需的所有信息，同时解码器的交叉注意力会学习将文本词汇与相应的图像区域关联，以便正确生成。这意味着图像和文本的对齐并不是通过拉近embedding距离实现的，而是在Transformer解码过程中，通过注意力权重对齐：模型只有在正确对齐图像内容与生成词语时才能取得低损失。例如，当解码器生成单词“狗”时，跨模态注意力会自然地关注图像中狗所在的特征区域，从而将视觉语义与该单词绑定。经过大规模训练后，这种注意力驱动的软对齐形成模型内隐的模态对齐机制。值得一提的是，作者在预训练时 扩充了任务种类 ，不仅包括图像描述，还有图像问答等，这些任务都要求正确关联图像和文本才能解答，从而进一步强化了模态对齐ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。 输入 token 表达统一： GIT通过 将图像特征序列拼接进Transformer解码器的输入 ，实现了一种隐式的token统一表示ar5iv.labs.arxiv.org。具体而言，图像编码器输出经过投影变换后，作为一组“图像token”（连续向量）排列在Transformer解码器的输入序列最前ar5iv.labs.arxiv.org。紧随其后的是文本的\u0026lt;BOS\u0026gt;标记和需要生成的文本token（初始化为待预测状态）。在Transformer内部，采用一个特别的序列到序列注意力掩码ar5iv.labs.arxiv.org：文本token可以看见所有图像token和之前的文本token，而图像token之间也可以相互看到（便于图像特征全局建模）ar5iv.labs.arxiv.org。这样，Transformer解码器实际上同时处理了图像token和文本token的序列。对模型而言，图像token与普通文本embedding在同一计算图中，只是通过mask控制了注意力方向。通过这种机制，GIT无需修改Transformer结构，就实现了 图像+文本统一序列建模 ：图像被视作解码开始时的一段前缀序列。这保证了图像信息能够像前文一样参与生成过程，从而让图像上下文与文本自然融合。另外，这种方法也不需要离散化图像，只要提供足够的图像token分辨率，模型就能以连续表示处理视觉信息。 损失函数与训练策略： GIT采用 纯粹的自回归语言模型损失 。给定图像（以及可选的提示文本），让模型生成目标文本序列，计算标准的交叉熵损失来训练ar5iv.labs.arxiv.org。在预训练期间，为了让模型适应多样任务，训练数据中包含了各种形式：图像标题生成、图像问答（在这种情况下，会在图像token后加入问题文本作为前缀，然后生成答案）等ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。例如，对于VQA，输入序列是「\u0026lt;img\u0026gt;\u0026hellip;\u0026lt;img\u0026gt; 问题：\u0026hellip; 答案：」，模型学习在看到“问题”后生成正确“答案”ar5iv.labs.arxiv.org。这种统一的语言模型策略使预训练和下游任务能够共享同一套参数和目标，不需要为不同任务切换架构或损失函数。此外，作者强调扩大预训练数据和模型规模对性能至关重要ar5iv.labs.arxiv.org。他们使用了比以往更大规模的图文数据，以及训练了不同尺寸的模型（从Base到巨型）进行对比，在多个任务上取得新的SOTAar5iv.labs.arxiv.org。训练策略上没有使用教师模型或多阶段训练，而是一阶段大一统模型学尽可能多的任务。这种“无技巧（no bells and whistles）”的方法充分依赖海量数据和模型容量来获得性能ar5iv.labs.arxiv.org。 使用的数据集及合成数据： GIT的预训练数据非常广泛和庞大。微软在论文中没有公布确切的数据量，但提到**“扩大了预训练数据规模”ar5iv.labs.arxiv.org。推测他们使用了公共的大型图文数据集合集（如COYO、LAION等），以及内部收集的数据，包括图片描述和问答标注。此外，他们还将视频字幕数据扩充到模型中，使模型能处理视频（选帧作为序列的一部分）ar5iv.labs.arxiv.org。在下游微调时，GIT在12个具有挑战性的基准上测试，包括COCO、nocaps（开放词汇描述）、VizWiz（盲人拍照求助）、TextCaps（需要OCR的图片描述）、多种VQA和视频caption等arxiv.org。令人瞩目的是，GIT在TextCaps数据集上首次超越了人类表现arxiv.org，说明模型学会了相当程度的场景文本识别和理解——这归功于预训练涵盖了带文字的图像以及无需OCR模块的端到端学习。GIT并未借助合成的伪标签数据；相反，它直接在真实任务数据上大规模训练**。例如，为了让模型具备OCR能力，他们可能在预训练中加入了带文字的图像及其文字描述（如OCR-VQA等），让模型自己去学习文字区域的特征提取ar5iv.labs.arxiv.org。因此，GIT更多是通过多任务训练覆盖各种模态难点，而不是通过额外生成数据来弥补。当然，训练这样一个模型本身需要巨量的数据，但微软具备这样的资源优势。 六大难点应对： 模态对齐困难： GIT选择了端到端生成作为对齐手段。由于模型只能通过正确生成文本来降低损失，它被迫在内部对齐图像与文本。例如，Transformer解码器的交叉注意力会在训练中自动调整，使得每个生成的词与相应的图像内容关联。这种隐式对齐不需要额外的对比损失，却在模型Attention权重中形成了图像区域-文本词汇的映射关系。再加上GIT预训练涵盖问答等任务，模型学会在回答问题时关注相关图像部分，在描述时依照图像内容组织语言——这些都属于模态对齐的体现。可见，尽管没有显式对齐Loss，GIT通过任务驱动对齐实现了高质量的模态对齐ar5iv.labs.arxiv.org。模型的成功表明，只要任务设计合理，生成式训练本身就能让模型学会跨模态对齐。 token格式不统一： GIT通过序列到序列Transformer架构，巧妙地让图像和文本“同列于一个序列”。图像编码器输出一系列向量，这些向量在解码器里被视作一段上下文序列ar5iv.labs.arxiv.org。这样，虽然图像不是离散单词，但在Transformer看来，它们只是前若干个特殊的输入embedding。后续文本token可以自然地参考这些图像embedding，就如同参考句首提供的提示一样。这个设计避免了需要定义图像词典或修改模型输入结构，使 格式统一的问题迎刃而解 。换言之，Transformer模型对图像和文本一视同仁，只是通过mask控制依赖关系ar5iv.labs.arxiv.org。因此，GIT内部已经实现了对不同模态信息的格式融合，不存在单独处理再对齐的问题。 语义粒度不匹配： GIT直接使用CNN/ViT提取图像特征，并通过Transformer将其转换为语言。没有显式区域级别的对齐机制，但Transformer的交叉注意力可以细粒度地处理图像patch与词的关系。例如，模型在生成某个名词时，会极大地注意对应物体的那些视觉token，实现类似局部对齐的效果。这相当于让细粒度对齐在注意力机制中自发完成。此外，作者使用了一个trick：他们用对比学习预训练好的图像编码器ar5iv.labs.arxiv.org，保证图像特征本身具有较高级的语义表示能力（对比预训练会让相同类别/语义的图像特征聚类）。这意味着图像特征一开始就带有一定的语义概括性，减少了视觉低层细节与语言高级概念的不匹配ar5iv.labs.arxiv.org。因此，在语义粒度上，GIT通过预训练的视觉语义特征+解码器注意力两方面，较好地解决了粒度差异问题。模型的OCR能力说明它可以从小区域拼写出单词，说明精细粒度也能捕获；而在描述整图时又能抓大放小，生成整体语义，这体现了粒度上的灵活性。 多模态上下文保持： GIT的设计初衷不在对话，而在统一各种 静态视觉任务 。因此原版GIT不具备多轮对话记忆。然而，它提供了 统一的生成框架 ，理论上可以扩展对话：只需在输入序列中加入之前对话的文本，即可将历史作为上下文。而图像如果需要在对话中反复参考，可以在每轮答复时都把同样的图像token放入输入。但这会受到模型最大序列长度限制。微软没有在论文中报道对话实验，但在VQA任务里，GIT通过将“问题”作为前缀文本与图像共同输入ar5iv.labs.arxiv.org来回答，已经体现了处理图文混合上下文的能力。对于多张图像，GIT可以一次编码多张图的特征串联作为更长的图像token序列，只是论文未深入探索。这种架构天然支持多模态上下文的扩展，但需要注意计算成本会随序列长度增长。在视频场景中，作者已经验证了能处理多帧（通过给每帧加上时间嵌入再串联）ar5iv.labs.arxiv.org。所以GIT显示出一定的上下文扩展性，但要真正保持多轮对话语境，可能还需在生成策略上做些改动（如引入特殊标记区分说话人等）。总的说来，GIT为多模态上下文提供了一个统一容器，但对话管理不在其预训练范围内，需要额外设计。 训练数据稀缺： GIT依赖大规模多样化数据取得成功。它的理念是与其设计复杂模型，不如用简单模型配合巨量数据ar5iv.labs.arxiv.org。虽然作者未公开数据细节，但可以推测其使用近十亿级别的图文对进行训练（极可能包括微软内部的ALIGN-类数据或JFT系列）。通过大量数据，GIT在各任务上都达到新的高度arxiv.org。对于普通研究者而言，如此数据难以获得。但GIT证明，大模型+大数据可以在无需额外标注和复杂技巧的情况下解决很多问题。因此，GIT没有使用伪标签，它体现的是另一种思路： 以规模取胜 。这在一定程度上回避了数据稀缺，因为一旦数据够多，很多小数据集的问题都变得可以零样本解决ar5iv.labs.arxiv.org。此外，统一模型能跨任务共享知识：例如在描述任务学到的知识对VQA有帮助，这其实提高了每条数据的利用率。这种多任务迁移也缓和了单任务数据不足的情况。因此，虽然GIT本身消耗了巨大数据，但相对于分别训练多个任务专用模型，其综合效率反而更高。 计算开销高： 训练一个像GIT这样的模型（尤其是大尺寸版本）需要相当高的计算投入。微软通过大规模并行和分布式训练完成了这一过程。幸运的是，GIT架构简单统一，没有多分支，这使并行效率较高。模型参数虽多，但Transformer易于在GPU/TPU上加速。而且作者在论文中提供了不同模型规模的对比如Base、Large、Huge等ar5iv.labs.arxiv.org。在实际应用中，可根据算力选择较小的模型进行fine-tune。推理方面，由于没有双塔或额外模块，GIT生成一次回复需要完整地跑Transformer，对于长序列仍较耗时。但没有交叉模块切换开销。值得注意的是，GIT证明了 统一模型减少了重复计算 ：比起每个任务训练不同模型，一个预训练模型fine-tune各任务总计算量更小ar5iv.labs.arxiv.org。同时，它也展示了Transformer在CV任务中的威力，使GPU上的Transformer算力得以充分利用，不像以前CNN+RNN需要异构处理。所以总体看，GIT的 训练成本虽然高，但回报是一个通用模型 。随着算力的提升，这种“大一统预训练”将变得越来越现实。 参考： GIT论文发表于 2023 年CVPRarxiv.org（OpenReview提供了审稿意见）。论文附带的代码已在GitHub开源arxiv.org（microsoft/GenerativeImage2Text），方便社区使用。有关GIT的更深入讲解，可参考微软研究博客和OpenAI笔记等资源对比GIT与同类模型的设计理念。\nFlamingo (DeepMind, 2022) 整体架构设计： Flamingo是DeepMind提出的一种 少样本视觉语言模型 ，它将预训练的视觉编码器和预训练的大型语言模型结合，通过插入跨模态注意力层实现图文融合lilianweng.github.iolilianweng.github.io。具体来说，Flamingo采用了CLIP的ViT作为图像编码器（提取每张图像的一组视觉特征），采用类似GPT-3风格的大型Transformer作为文本生成模型（如Chinchilla 70B）lilianweng.github.io。在两者之间，Flamingo引入一个 Perceiver Resampler模块 ，将任意长度的视觉特征压缩成固定数量的 视觉tokens （如每张图像压缩成N≈64个token）lilianweng.github.io。然后，在语言模型的每层若干位置，插入“门控跨注意力层”，让文本流在生成过程中可以多次访问这些视觉tokenslilianweng.github.io。这些跨注意力层在语言模型层之间交织，使模型在生成每个词时，都能参考图像信息。值得强调的是，Flamingo在训练时 冻结了原有的语言模型和视觉编码器权重 ，只训练中间的新组分（包括Perceiver和跨模态层）lilianweng.github.io。这种设计确保了预训练模型的语言和视觉知识被最大程度保留，同时通过新组件实现模态融合。 模态对齐方式： Flamingo的模态对齐依赖于 预训练模型的知识+少量新的连接参数 。图像编码器CLIP本身已提供高质量的视觉表示，语言模型也有丰富的语言常识。Flamingo只训练连接部分，通过自回归语言模型目标来让视觉信息对接语言输出lilianweng.github.io。训练过程中，模型读取一串交织的图像和文本（例如一个网页内容，其中有文字和插入的图片），试图按照出现顺序预测下一个文本tokenlilianweng.github.io。这隐含地要求模型学会对齐：当遇到需要描述图像的地方，就必须利用视觉tokens提供的信息来正确地产生文字。因此，Flamingo没有明确的对比或匹配损失，而是在 序列建模过程中完成对齐 。尤其得益于CLIP提供的视觉特征空间和语言模型的语义空间都非常成熟，跨注意力层只需学会将二者关联即可。例如，Flamingo使用一个门控机制控制每个跨注意力头对视觉的依赖程度，这保证了模型不会过度依赖或忽略视觉信息，而是渐进式地融合lilianweng.github.io。经过训练，Flamingo实现了图文对齐，以至于在推理时，可以在看到图像后正确地继续对话生成相关文本。这种对齐能力在它的few-shot学习中表现突出：只需给出少量图文示例，模型就能对新图像输出合理描述或回答，表明模态对齐已经内化在模型中了lilianweng.github.io。 输入 token 表达统一： Flamingo通过 对文本序列进行特殊标记和掩码 ，实现了对图像和文本交替输入的统一处理lilianweng.github.io。他们在训练语料的文本中插入特殊标记 \u0026lt;image\u0026gt;代表图像占位符，当遇到该标记时，模型会取下一张图像的视觉tokens作为输入lilianweng.github.io。在Transformer内部，通过设计注意力mask，使得文本token只能看见最近一次出现的图像tokens以前的文本，而不能看见更早图像，以此处理多图场景lilianweng.github.io。同时，由于视觉tokens长度固定，每当有图像时，就把那N个视觉tokens嵌入序列。这样，整个输入序列可能形如：“文本段1 \u0026lt;image\u0026gt; 文本段2 \u0026lt;image\u0026gt; 文本段3\u0026hellip;”。对于Transformer来说，\u0026lt;image\u0026gt;标记只是一种指示，它实际会被替换为N个视觉embedding。最终，模型看到的是一个混合序列，其中既有文本token embedding也有视觉token embedding。Flamingo的跨注意力层保证文本可以从视觉embedding汲取信息lilianweng.github.io。总之，Flamingo实现了 在同一序列中交织图像和文本 ：在位置编码上，文本和图像embedding各据其位，模型通过mask确保因果关系正确lilianweng.github.io。这种方式处理输入使得模型能够自然地接受任意交替的多模态输入，而不需要显式地将图像转成离散标签或one-hot表示。 损失函数与训练策略： Flamingo以自回归下一个词预测作为唯一的训练目标lilianweng.github.io。训练数据是精心构造的 多模态序列 ：DeepMind构建了一个名为“M3W”（MassiveWeb）的大型数据集，从网络抓取包含图像和文字的网页片段共4300万条lilianweng.github.io。这些数据被处理成长度为256的token序列（其中可能包含最多5张图像）lilianweng.github.io。此外，Flamingo还混合了传统的 图文对数据 （如ALIGN的1.8亿图文对）和 视频-文本数据 （如从视频中抽帧及对应描述）进行训练lilianweng.github.iolilianweng.github.io。整个训练在不同数据源上采用 分布式多任务训练 ：每个batch随机抽取来自网页、多图文对、视频的样本分别计算NLL损失，再按设定权重求和优化lilianweng.github.io。这样的策略使模型同时适应多种输入形式。训练中需要注意各数据集的权重分配，作者采用均衡采样避免小数据集被忽略，同时也调整过不同任务损失的比重lilianweng.github.io。最后，通过大量算力（语言模型80B参数，加上新插入层）训练，Flamingo可以在不微调的情况下实现few-shot学习，即给定少数示例即可在16个下游任务中取得接近或超过有监督SOTA的成绩lilianweng.github.io。模型也支持进一步微调，但由于参数量巨大且新的门控层较敏感，微调需要小心调参。不过，一旦训练完成，Flamingo在多模态few-shot方面展示了卓越的能力。 使用的数据集及伪标签： 如上所述，Flamingo的主要预训练数据包括三个部分：网页多模态语料M3Wlilianweng.github.io、 图片-文本对数据 （如ALIGN 1.8B对、LAION等）lilianweng.github.io、 视频-文本数据 （如Instagram/Twitter短视频说明等，文中代号LTIP和VTP）lilianweng.github.io。M3W的构建无需人工标注，纯粹爬取网页，这可以视为引入了 大量弱监督数据 。那些网页上的文本并非专门描述图像，但模型会通过上下文学习其中关联。这有点类似伪标签，因为并非每句话都准确描述对应图像，但模型会自己找关系。另外，Flamingo并未使用生成模型来自行生产描述，它依赖真实世界的数据多样性。值得注意的是，Flamingo训练所需的监督非常少，几乎全是弱标注或无标注数据。few-shot能力使它在下游不需要大规模微调数据。因此，Flamingo充分体现了用海量弱标注数据替代高质量标注的理念。没有迹象表明Flamingo使用了由其他模型生成的伪标签数据；它更像是把互联网当作最大的标注来源，在文本和图像并存的自然场景中学习。 六大难点应对： 模态对齐困难： Flamingo借助冻结的CLIP提供良好的图文先验表示，并通过少量参数训练将其输出嵌入语言模型上下文。这意味着视觉和语言模态的大体对齐已经由CLIP和预训练LM保证，Flamingo只需学习 在具体上下文中关联 。通过跨注意力层，Flamingo学会在需要时提取视觉token信息用于生成下一个词，从而实现对齐。其few-shot性能表明，训练后模型能够快速对齐新任务的图文语义，这得益于大量多样化训练让对齐泛化良好lilianweng.github.io。换言之，Flamingo用数据多样性+强大基础模型平稳地度过了模态对齐难关。 token格式不统一： Flamingo直接在Transformer中处理交替的多模态序列，将图像表示为固定长度token插入序列，这相当于统一了输入格式lilianweng.github.io。虽然图像token不是离散符号，但它们像文本token一样有自己的位置，与前后文本共同组成序列输入Transformer。同时，引入 \u0026lt;image\u0026gt;标记作为占位符，使文本流认识到何处有图像lilianweng.github.io。这种方案无需对图像进行离散化编码，而是用连续向量表示并通过mask和标记融入序列，实现了格式统一。实验证明，这样模型可以灵活处理任意交替顺序的图文输入，这正是统一输入格式带来的好处。 语义粒度不匹配： Flamingo利用CLIP的高层视觉特征（ViT-L/14等）作为输入，这些特征本身具有较丰富的语义信息（CLIP已对齐过标签文本）。再通过Perceiver压缩，Flamingo获得一组紧凑的视觉tokens，每个可能聚合了图像若干部分信息lilianweng.github.io。这会损失一些低层细节，但保留主要语义，匹配语言模型处理的概念粒度。对于非常细的细节，如图像中的文字或小目标，Flamingo如果训练数据涵盖这类任务也能捕捉（但Flamingo主要没专门练OCR类任务，表现可能一般）。总体而言，Flamingo的设计旨在 抓主要语义 ：用几百个视觉token代表整张图lilianweng.github.io。语言模型生成注重全局语义和上下文，微观细节在few-shot场景下可能需要提示引导才能关注。不过，通过web数据训练，Flamingo也学习了不少细节（如定位照片里的物体等）。因此，它在语义粒度上采取以语义为主，细节为辅的策略，符合few-shot应用的需求。 多模态上下文保持： 这是Flamingo最大的强项之一。模型专门设计来处理 任意长度的交互式多模态上下文 。通过mask策略，Flamingo可以应对多张图和多段文本交替：保证每段文本只能看最近的图像，从而按顺序关联图像和文字lilianweng.github.io。这使模型在一个序列中可以包含多轮图文对话——实际上Flamingo天生就是支持图文混合对话的。训练中它看过网页内容的多次图文交替，因此对多模态上下文延续性有经验lilianweng.github.io。few-shot推理时，可以先给几个示例（图+问+答），模型就能在持续的多模态对话下发挥作用lilianweng.github.io。这种能力是一般模型不具备的。因此Flamingo很适合多轮对话、讲故事等需要保持上下文的场景。需要注意长序列涉及的内存和计算成本，但Flamingo通过稀疏注意力等优化应对。总的来说，Flamingo在多模态上下文保持方面达到了当时的新高度，真正实现了在Transformer中融合长上下文的多模态信息。 训练数据稀缺： Flamingo通过大规模弱标注数据和 多数据源混合 ，在没有显式人工标注的情况下取得了卓越性能lilianweng.github.io。它所需的只是网络上已有的大量图文并茂内容，而不需要额外的人工作答或描述数据（除了验证集）。这证明了利用海量的非结构化数据也能训练出强大的多模态模型。few-shot学习的优势在于，模型可以适应新任务而不需要对每个任务都有成千上万标注数据。Flamingo在16个任务上的结果显示，即使这些任务的数据对模型来说是新的，它依然靠few-shot提示达到不错效果lilianweng.github.io。这极大缓解了对监督数据的需求。因此，Flamingo的方案是 用预训练+提示学习替代下游数据 。当然，预训练本身用了43M网页和十亿级对，耗资巨大，但都是低成本获取的数据。可以说，它把收集标注的钱换成了算力钱。一旦模型训练完毕，同样权重可以few-shot解决多个任务，再也不需要逐个任务大量标注了。 计算开销高： Flamingo包含一个80B规模的语言模型（如Chinchilla 70B）和一系列新插入的层，总参数量非常高，训练消耗巨大的TPU/GPU资源。这显然是非常高的计算开销。然而，Flamingo通过 冻结大模型 ，大幅减少了需要更新的参数量lilianweng.github.io。仅训练新加的几千万参数，使得训练收敛更快、更稳定，同时避免灾难性遗忘。此外，相比从零训练80B多模态模型，这种“夹心”微调的成本要低得多。推理阶段，Flamingo的计算与一个同等大小的LM相当，外加一些跨注意力计算，可以在多卡并行生成。在few-shot时，不需要反复fine-tune，从而节省了针对每个任务微调的算力。因此，对于拥有训练超大模型能力的团队来说，Flamingo的 性价比反而不错 ：用额外\u0026lt;1B参数的代价，把一个纯语言模型变成了多模态模型。总之，Flamingo依然属于算力投入极高的模型，但在架构上做了取舍，通过参数冻结和高并行设计，把这笔开销控制在可能范围，并用其泛化能力回收了在多个任务上的成本。 参考： Flamingo的论文（Alayrac et al. 2022）可在arXiv获取lilianweng.github.iolilianweng.github.io。其中详述了模型架构和训练数据构成。DeepMind未公开Flamingo的代码，但有社区复现项目（如lucidrains的PyTorch实现）。Lilian Weng的博客对Flamingo进行了通俗讲解lilianweng.github.io。Flamingo在Few-shot VQA等任务上的表现促使后续多模态聊天模型（如OpenAI的GPT-4V）采用类似思想。\nGPT-4V (OpenAI, 2023) 整体架构设计： GPT-4V是GPT-4模型的视觉增强版本，能够接受图像和文本输入，输出文本en.wikipedia.org。虽然OpenAI并未公开GPT-4V的具体架构和参数en.wikipedia.org，“V”版的实现大致可推测为在GPT-4大型Transformer架构中融合了视觉处理模块。很可能GPT-4V采用了单一Transformer模型来同时处理图像和文本：图像通过一个卷积或ViT编码器提取特征，然后以某种形式馈入Transformer。例如，有推测称GPT-4V使用类似Flamingo的方法——一个 预训练的ViT作为图像编码器 ，将其输出作为额外的输入embedding，通过新添的跨注意力机制注入到原GPT-4的Transformer中lilianweng.github.io。也有可能GPT-4V将图像编码为若干“视觉token”直接拼接到文本token序列中处理（类似BLIP-2/GIT那样）。不管实现细节如何，GPT-4V的架构原则应是 在不大幅改变GPT-4语言能力的前提下，赋予其视觉输入通路 。因此，它很可能保留了GPT-4的大部分层和参数，仅在输入嵌入层或中间插入层增加视觉接口，使模型能够在Self-Attention中同时考虑图像和文本信息。作为一个多模态LLM，GPT-4V仍以Transformer为核心en.wikipedia.org。 模态对齐方式： GPT-4V在开发过程中应该经历了 大量多模态预训练和对齐调优 。预训练阶段，模型接受图文混合数据，学习以生成下一个token为目标（无论下一个是文字还是需要根据图像产生的文字）。这种训练会驱动模型自动建立图像与文本语义的映射关系。由于GPT-4本身非常强大，GPT-4V可能仅需较少的额外数据就能学会模态对齐。然而OpenAI可能使用了多种辅助手段：包括 对比损失 （确保图像相关的文本embedding靠近）或者多模态一致性约束等，但具体未知。可以肯定的是，GPT-4V经过了 强化学习人类反馈（RLHF）的对齐环节en.wikipedia.org：人工反馈不仅针对文本回答质量，也包括对视觉理解正确性的评价。这种人工调教确保模型在视觉问答中对齐人类期望。例如，人类监督会奖赏模型正确描述图像、严惩胡编乱造，从而促使模型更好地学习视觉-语言对齐关系。总的说来，GPT-4V的模态对齐来自两部分 ：一是模型大规模多模态训练的自我监督对齐（让模型预测正确的多模态输出而被迫对齐），二是 对抗性和人类反馈微调 （纠正不准确的对齐，如图像内容误解）以达成人类满意的对齐度。最终结果是GPT-4V在各种视觉描述、问答任务上表现出强大的理解力和对齐度，甚至可以准确解释复杂图片、阅读图中文字并将之融入答案——这说明其视觉语义已与语言很好地结合。 输入 token 表达统一： 从用户接口看，GPT-4V接受的输入是图像（像素形式）和文本，自然语言以token形式进入，图像则以文件上传形式进入API。但在模型内部，必须将图像转化为与文本token可交互的表示。根据业界经验，GPT-4V可能采用两种方式之一：其一， 离线视觉编码+前缀嵌入 。即通过一个CNN/ViT将图像转成一串embedding，然后在Transformer输入端用特殊标记占位，将这些embedding作为“视觉前缀”插入。这类似BLIP-2和GIT的策略，让视觉embedding在Transformer序列中，与后续文本共同处理。其二， 中途插入跨模态层 ，即模型运行过程中，当需要处理图像时，调用一个微型视觉Transformer将图像转成键值供专门的跨注意力层使用（类似Flamingo做法）。无论哪种，最终效果是 模型看到了一系列向量表示，部分来自图像，部分来自文本 ，并通过统一的注意力机制处理它们。因此，GPT-4V实现了 视觉信息向等价文本向量的转换 ：这些向量可能没有离散token对应，但Transformer无差别对待它们，把它们当作上下文的一部分。OpenAI也定义了GPT-4V的token计费方式：图像按一定像素大小折算成若干token成本platform.openai.com, 这暗示他们内部将图像信息映射为了固定数量的embedding，相当于一些token。这与输入统一表示的思路一致。此外，GPT-4V支持在对话中多次输入图像，模型通过聊天记忆可以连续参照多幅图像。这种灵活性也表明输入的图像已经嵌入Transformer上下文，模型可以在内部“记住”它，就像记住前文一样。因此可以说，GPT-4V在实现上做到了图像和文本输入的格式统一，至少从Transformer的视角来看是一致的序列信息流。 损失函数与训练策略： GPT-4V的训练包括两个阶段： 预训练（Self-Supervised）和对齐微调（Supervised + RLHF） en.wikipedia.org。预训练损失是标准的 因果语言建模损失 ，扩展到多模态场景，即给定之前的文本token和图像embedding，预测下一个文本tokenen.wikipedia.org。这一步可能使用了大量图文对数据和合成任务数据，让模型具备基础视觉理解和描述能力。接下来，OpenAI对GPT-4V进行了 监督微调 ，包括让模型跟随指令、可靠回答问题、避免不当输出等。这一步使用有人类标注答案的图像问答数据和对话数据，损失为交叉熵对标参考答案。最后还有 RLHF阶段 ，通过人类反馈训练一个奖励模型，对模型回答质量评分，再用策略梯度或近端策略优化调整模型参数，使之产生更符合人类期望的回答en.wikipedia.org。在RLHF中，人类会比较两版对同一图像问题的回答优劣，以训练奖励模型。这确保GPT-4V不仅正确，还要解释清楚、详尽并遵守安全守则。训练策略方面，GPT-4V很可能采用了 混合训练 ：例如让模型在大约80%时间学习纯文本任务（以不损害其语言能力），20%时间学习带图像的任务，以逐渐融合视觉能力而不遗忘语言能力。这符合OpenAI对GPT-4统一多模态模型的描述，称其在巨量算力下进行预测性能的平稳扩展arxiv.org。因此，GPT-4V训练过程相当复杂，但核心损失仍是让模型预测正确的输出序列（文本），只是过程中施加了各种人类知识和偏好约束。 使用的数据集及伪标签： OpenAI未公开GPT-4V使用的数据细节en.wikipedia.org。推测其预训练数据包含 互联网爬取的大规模图文对 （如可能使用LAION、ALIGN数据，或者自建的10亿级别数据集），涵盖多样领域。还可能有 OCR场景数据 （扫描文档及文本）、 图表数据 、网页截图和说明等，因为GPT-4V表现出识别文档、读表格、看图编程等广泛能力。监督微调阶段，他们可能编纂了一个多模态指令数据集，类似InstructGPT，但带图像：比如让标注员提供图像并提问，写出高质量参考答案。这部分数据可能较小（数万到数十万对），但涵盖不同任务（描述、定位、分类、推理等）。此外，社区猜测OpenAI可能利用GPT-4自身生成了一部分训练数据（即“判师”策略），但官方未证实。相较于开源做法（如LLaVA用GPT-4生成对COCO的问答作为训练集），OpenAI有资源直接人工标注，所以GPT-4V的关键数据更可能是人类精标而非伪标签。唯一确定的是，GPT-4V 融合了多源数据 ：文本数据（与GPT-4共享）、图像+文本数据，以及人类反馈数据en.wikipedia.org。这种多阶段、多样本训练使模型具有极其广泛的视觉语言知识。基于效果推断，GPT-4V肯定见过各种真实世界图像场景，包括照片、插画、截图、漫画等，也了解了不少常见视觉任务的问答格式。这正是其在未知图片上一样游刃有余的原因。 六大难点应对： 模态对齐困难： GPT-4V可被视为目前模态对齐最成功的例子之一。OpenAI通过 统一模型训练+精细对齐调优 ，使得GPT-4V在视觉和语言之间建立了深度联系。模型能将图像中的元素转换成文字描述或用于推理，说明跨模态概念高度统一。例如，它可以看图进行幽默理解、数学分析，这意味着不仅低层语义对齐，高层推理也对齐了。相比CLIP等需下游配对的模型，GPT-4V内部产生了 端到端的对齐 ：一幅图像输入，其内部生成的表征能直接触发与之对应的知识和词汇。RLHF过程中，人类引导模型关注正确区域、忠实描述，进一步强化了 精准对齐 （比如不编造不存在的物体）。因此，对以前悬而未决的模态对齐难题，GPT-4V以超级规模训练+人类校正的方法给出了答案：几乎可以对任意复杂图文实现正确对齐。 token格式不统一： GPT-4V在接口上依然区分图像上传和文本输入，但在模型内部已经实现了格式统一。如上推测，图像被编码成embedding插入Transformer，相当于模型看到的是统一的向量序列，其中没有本质区别区分来源。OpenAI甚至提供了一个token折算方法来计价图像，这暗示他们定义了一个统一本质的token空间包含图像platform.openai.com。GPT-4V也许没有明确的视觉词表，但通过扩展embedding层，模型接受了一批额外的向量（视觉patch的embedding或者Resampler输出）作为“视觉token”。Transformer处理自注意力时，对这些向量和普通文字embedding执行相同的矩阵计算。因此可以说，GPT-4V在实现上 消除了模态输入格式差异 ，达到了真正的多模态Transformer形态。这也是为什么用户能对它自由提问“图中有什么字”或“这个人是谁”，模型像读文字一样“读”图。这种统一在OpenAI的技术报告中虽未明说，但从其行为特征和架构趋势能推断出来。 语义粒度不匹配： GPT-4V展现出处理各种粒度语义的能力，从辨认具体细节（如图中小字、微小物品）到理解抽象场景（如人物关系、场景氛围）。这表明模型采用了高分辨率的视觉表征和 强大的分层理解 。一种可能方式是多级特征：基础ViT提供细粒度patch特征，然后Transformer多层逐步汇总，像人类视觉系统一样先看细节再理解整体。此外，OpenAI可能特意在训练集中加入了一些需要细粒度识别的任务（OCR、细分类），迫使模型关注局部细节。同时，大语言模型部分拥有强大的上下文推理能力，能从细节推导整体意义。这两方面结合，使GPT-4V能较好地弥合视觉像素级信息与语言概念级信息之间的鸿沟。例如，对一张复杂的漫画，模型既能识别面部表情这样的细节，又能归纳出搞笑之处这样的高层语义。可以认为，GPT-4V通过多尺度注意力解决了语义粒度不匹配：低层注意力抓取细节，高层Self-Attention整合语义，并在输出时选择恰当的语言粒度表述。 多模态上下文保持： GPT-4V本质是ChatGPT的扩展版，因而天然具备对话上下文记忆能力。用户可以在一次对话中连续上传多张相关图像并配以提问，模型能够参考对话历史和所有已提供的图像信息来回答。比如，用户先上传一家谱照片问“这是谁？”，再上传另一张照片问“他和前面那人是什么关系？”，GPT-4V可以基于前文记忆，将两图人物联系起来回答。这说明模型内部对多轮图像和文本都建立了表示，并通过对话状态维持了跨轮次的多模态上下文。OpenAI很可能在微调阶段加入了这类多轮、多图对话的数据，使模型学会使用 \u0026lt;image_n\u0026gt;引用之前的图像。在推理实现上，ChatGPT系统会给每张图一个编号，将其embedding保存在对话状态，后续提问如果引用，模型就会重新利用。这种机制虽未明示，但从体验上看GPT-4V确实支持相当长的多模态对话。因此，它在多模态上下文保持上达到了目前最强水平：既能处理长文本对话，又能记忆多张图像的内容并综合推理。这一能力是之前模型（如Flamingo）few-shot模拟的更高级形式，因为GPT-4V经过明确的对话格式训练和强化，对话管理更加可靠。 训练数据稀缺： 对于普通研究者来说，高质量大规模多模态数据稀缺是难点，但OpenAI通过自身积累和合作，可能获取了十分丰富的数据。GPT-4V可以被视为以数据和算力硬碰硬解决问题的典型。它用规模（模型、数据）换性能，不太依赖小技巧。值得注意的是，虽然OpenAI未公布数据，但推测很大一部分来自现有开放数据（LAION、COCO、Visual Genome等）以及定制采集的数据（比如购买版权图片、内部生成的数据等）。此外，人类标注在对齐阶段起了决定性作用，这是另一种形式的数据： 专家知识数据 。OpenAI投入了大量人力去微调模型的行为，使得最终模型的能力远超仅靠原始数据训练的版本。这相当于通过人类反馈来弥补数据集不足之处——对于一些模型自己难以领会的任务，人类示范和偏好指导提供了额外信息。这种做法开创了用少量高质量人工数据引导海量机器学习的范式。简而言之，GPT-4V应对数据稀缺的方案在于： 一手抓“大”（扩展预训练数据广度），一手抓“精”（收集人类高质量指令/反馈数据） 。两者结合，使模型既见多识广，又合乎人意。 计算开销高： GPT-4V毫无疑问是在极其庞大的算力支持下训练的。传闻GPT-4基础模型参数在数千亿以上，训练消耗数千万美元级别GPU成本。加入视觉模态后，训练复杂度进一步提高。不过OpenAI通过一些工程手段控制了成本：据报道，他们使用了训练性能预测方法，在较小模型上估计大模型表现，从而少走弯路arxiv.org。另外采用混合精度、模型并行、流水线并行等技术提高效率。模型结构上，使用统一Transformer而非多分支，可以充分利用成熟的Transformer优化器和加速器。这些都帮助缓解了计算压力。在推理阶段，GPT-4V同样需要强大算力支持，但OpenAI通过托管API方式，用优化过的推理服务器提供服务，单次调用成本对于终端用户来说隐藏在付费中。可以说，GPT-4V目前的计算开销不是一般机构能承担的，但它也展示了高投入带来高性能的路线。随着硬件进步和可能的压缩蒸馏技术，未来GPT-4V的成本有望下降。就当前而言，OpenAI通过自身资源攻克了这一难题，对外提供一个无需本地计算就能调用的强大多模态模型，这在客观上绕过了许多用户对算力的需求。 参考： GPT-4 的技术报告en.wikipedia.orgen.wikipedia.org提到其多模态能力和训练方法，但未披露细节。维基百科也指出OpenAI未公布GPT-4的架构和数据en.wikipedia.org。尽管如此，我们可以参考类似的研究（如Google PaLM-E、DeepMind Flamingolilianweng.github.io）来推测GPT-4V的设计思路。OpenAI的GPT-4发布博客openai.com和官方FAQ也提供了一些线索（如图像计费折算）。目前没有公开的GPT-4V代码或模型，但已有一些开源项目（如MiniGPT-4、LLaVA等）尝试复现其部分功能，可供了解实现原理。总的来说，GPT-4V代表了当前多模态模型技术的前沿，将视觉和语言能力融合达到了前所未有的高度。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-06-01/","summary":"\u003ch1 id=\"主流视觉-文本多模态模型技术分析\"\u003e主流视觉-文本多模态模型技术分析\u003c/h1\u003e\n\u003cp\u003e近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。\u003c/p\u003e\n\u003ch2 id=\"clip-openai-2021\"\u003eCLIP (OpenAI, 2021)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e整体架构设计：\u003c/strong\u003e CLIP 采用 \u003cstrong\u003e双编码器架构\u003c/strong\u003e ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到\u003cstrong\u003e相同维度\u003c/strong\u003e的向量空间\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到\u003cstrong\u003e共享的多模态嵌入空间\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模态对齐方式：\u003c/strong\u003e CLIP通过\u003cstrong\u003e对比学习\u003c/strong\u003e实现视觉-语言对齐。训练时，模型给定一批图文对，学习\u003cstrong\u003e预测哪张图像与哪段文本匹配\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。具体而言，CLIP使用 \u003cstrong\u003e对称的跨模态对比损失\u003c/strong\u003e （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。这种训练使图像和文本编码器产生的特征在共享空间中\u003cstrong\u003e成对靠近\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e输入 token 表达统一：\u003c/strong\u003e CLIP并未显式统一图像和文本的输入表示格式。 \u003cstrong\u003e图像和文本各有独立的token化和编码流程\u003c/strong\u003e ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过\u003cstrong\u003e独立编码+对齐空间\u003c/strong\u003e的方式，规避了直接将图像作为序列token处理的不统一问题。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e损失函数与训练策略：\u003c/strong\u003e 损失采用 \u003cstrong\u003e对比学习的InfoNCE损失\u003c/strong\u003e （实现为带温度系数的归一化softmax交叉熵）\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了\u003cstrong\u003e大批量\u003c/strong\u003e训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,2023%2C%20in%20their\"\u003elightly.ai\u003c/a\u003e。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=insensitive%20to%20the%20capacity%20of,the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。CLIP从\u003cstrong\u003e随机初始化\u003c/strong\u003e开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据集及伪标签：\u003c/strong\u003e CLIP在一个超大规模的图文配对数据集上预训练，包含约\u003cstrong\u003e4亿对图像-文本\u003c/strong\u003e\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖\u003cstrong\u003e自然语言的弱监督\u003c/strong\u003e\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e六大难点应对：\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003e模态对齐困难：\u003c/em\u003e  CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003etoken格式不统一：\u003c/em\u003e  采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e语义粒度不匹配：\u003c/em\u003e CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如\u003cstrong\u003eFine-Grained CLIP\u003c/strong\u003e等正是受限于CLIP在局部语义对齐上的不足\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder\"\u003elightly.ai\u003c/a\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=SigLIP%3A%20Optimising%20the%20loss%20function,for%20better%20scaling\"\u003elightly.ai\u003c/a\u003e。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e多模态上下文保持：\u003c/em\u003e CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 \u003cstrong\u003e多模态上下文\u003c/strong\u003e （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e训练数据稀缺：\u003c/em\u003e CLIP通过\u003cstrong\u003e大规模弱标注数据\u003c/strong\u003e从根本上缓解了数据稀缺的问题\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e计算开销高：\u003c/em\u003e 训练CLIP确实需要巨大的算力和显存，但相对来说，其\u003cstrong\u003e双塔架构\u003c/strong\u003e使训练可并行展开，推理时也可分别预编码图文后做相似度计算，\u003cstrong\u003e具有一定的效率优势\u003c/strong\u003e\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,Training%E2%80%9D%2C%20propose%20to\"\u003elightly.ai\u003c/a\u003e。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=implementation%20is%20numerically%20unstable%2C%20and,additional%20bias%20terms%2C%20and%20calculations\"\u003elightly.ai\u003c/a\u003e。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e参考：\u003c/strong\u003e CLIP 的论文\u003ca href=\"https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model\"\u003earxiv.org\u003c/a\u003e详细描述了其对比预训练方法，OpenAI 的博客也提供了概述\u003ca href=\"https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations\"\u003elightly.ai\u003c/a\u003e。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库\u003ca href=\"https://huggingface.co/docs/transformers/en/model_doc/clip#:~:text=CLIP%20uses%20an%20image%20encoder,the%20same%20number%20of\"\u003ehuggingface.co\u003c/a\u003e。\u003c/p\u003e\n\u003ch2 id=\"align-google-2021\"\u003eALIGN (Google, 2021)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e整体架构设计：\u003c/strong\u003e ALIGN（ \u003cstrong\u003eA Large-scale ImaGe and Noisy-Text embedding\u003c/strong\u003e ）延续了与CLIP相同的\u003cstrong\u003e双编码器对比学习架构\u003c/strong\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：\u003cstrong\u003eEfficientNet-L2卷积网络\u003c/strong\u003e作为图像编码器，\u003cstrong\u003eBERT-Large\u003c/strong\u003e作为文本编码器，并均从随机初始化开始训练\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模态对齐方式：\u003c/strong\u003e ALIGN采用\u003cstrong\u003e对比损失（normalized softmax）\u003cstrong\u003e来训练，使匹配的图文对嵌入向量接近，不匹配的远离\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以\u003c/strong\u003e批为单位的跨模态对比\u003c/strong\u003e训练，使模型学到强大的图文对齐表示。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e输入 token 表达统一：\u003c/strong\u003e ALIGN同样没有将图像直接离散为token序列，而是通过\u003cstrong\u003e双通道\u003c/strong\u003e处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出\u003cstrong\u003eembedding空间\u003c/strong\u003e实现统一表示\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e损失函数与训练策略：\u003c/strong\u003e 使用 \u003cstrong\u003e对比学习损失\u003c/strong\u003e （InfoNCE变体），在\u003cstrong\u003e大批量\u003c/strong\u003e上训练模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 \u003cstrong\u003e最小程度的过滤\u003c/strong\u003e ，通过\u003cstrong\u003e数据规模\u003c/strong\u003e来弥补噪声\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据集及伪标签：\u003c/strong\u003e ALIGN的亮点在于使用了\u003cstrong\u003e超过10亿对图像-Alt文本\u003c/strong\u003e的超大规模数据集\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 \u003cstrong\u003e无需人工标注\u003c/strong\u003e 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN \u003cstrong\u003e放宽过滤标准\u003c/strong\u003e ，只做了基于频率的简单过滤，最终得到约\u003cstrong\u003e18亿对\u003c/strong\u003e图文数据\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20this%20work%2C%20we%20follow,text%20pairs\"\u003eresearch.google\u003c/a\u003e。这些文本描述可能包含噪声甚至与图像无关，但研究表明 \u003cstrong\u003e规模弥补质量\u003c/strong\u003e ：如此海量的数据使模型学到泛化的视觉语言表示\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e六大难点应对：\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003e模态对齐困难：\u003c/em\u003e ALIGN证明了\u003cstrong\u003e数据规模\u003c/strong\u003e在对齐中的重要作用。通过\u003cstrong\u003e十倍于CLIP的数据规模\u003c/strong\u003e和强大的对比损失，模型学到了稳健的跨模态对齐能力\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,L2\"\u003eresearch.google\u003c/a\u003e。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003etoken格式不统一：\u003c/em\u003e 与CLIP类似，ALIGN通过\u003cstrong\u003e双编码器架构\u003c/strong\u003e回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e语义粒度不匹配：\u003c/em\u003e ALIGN的训练目标依旧作用在 \u003cstrong\u003e全局图像-句子层面\u003c/strong\u003e ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对\u003cstrong\u003e细粒度语义\u003c/strong\u003e的不匹配没有特殊解决方案。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e多模态上下文保持：\u003c/em\u003e ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e，未涉及多模态对话等情境。因此，ALIGN在\u003cstrong\u003e多轮交互\u003c/strong\u003e或\u003cstrong\u003e长上下文\u003c/strong\u003e问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e训练数据稀缺：\u003c/em\u003e ALIGN的策略是\u003cstrong\u003e极端扩增数据规模\u003c/strong\u003e以消除数据稀缺瓶颈\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 \u003cstrong\u003e弱标注的大数据\u003c/strong\u003e 。\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e计算开销高：\u003c/em\u003e 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and\"\u003eresearch.google\u003c/a\u003e。尽管计算开销高昂，ALIGN通过 \u003cstrong\u003e冻结架构复杂性\u003c/strong\u003e （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，\u003cstrong\u003e优先扩大数据规模\u003c/strong\u003e比增加模型复杂度更有效\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e参考：\u003c/strong\u003e ALIGN 的研究细节发表于 ICML 2021\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20,We\"\u003eresearch.google\u003c/a\u003e。Google Research 官方博客提供了对ALIGN的通俗描述\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable\"\u003eresearch.google\u003c/a\u003e\u003ca href=\"https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained\"\u003eresearch.google\u003c/a\u003e。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。\u003c/p\u003e","title":"Bug Journal 2025-06-01"},{"content":"主要动机 目前有三大挑战：数据稀缺、泛化能力不足，以及在复杂物理环境中效果不好 作者希望通过引入大规模语言及VLA模型在自然语言处理和计算机视觉中的成功经验解决这个问题.\n主要论点 在预训练的视-语言模型（VLM，本文采用 PaliGemma）基础上，增加一个“动作专家”（action expert），通过条件流匹配（diffusion policy）生成高频、连续的动作序列（每秒可达50 Hz）\n模型流程图 收集数据 -\u0026gt; 训练$\\phi_0$ -\u0026gt; Zero-Shot/微调/Fine-tune\n数据来自7种不同的机器人，68个不同的任务，总计 10k小时。\n每一个单独机械臂的自由度在 6-7 (有些机器人有多机械臂)\n输入有 3 块，分别是：Image, Language, and State\nImage 用 400M Pretrained VIT SigLip 得到 embedding, 一共会有up to 3 个 Image， 所以最会有 3 个 embedding (没有就填 0)\nLanguage 用 2.6B Pretrained LLM Gemma, 得到 embedding\n最后是关节信息，最多会有 18 个(没有就填 0)。\n之后运用 Diffusion Policy 来生成每一步的动作\n生成的是 $p(A_t|O_t)$, 即，在给定条件(环境下)，每一个动作的概率。\n这时给一个 chunk 内的真实动作加噪音，然后让模型学习如何去噪。\n方式如下：\n随机采一个噪声级别 $\\tau$，采一个高斯噪声 $\\varepsilon$； 构造带噪动作块 $A_t^\\tau = \\tau A_t + (1 - \\tau)\\varepsilon$； 用观测 $o_t$ 与 $A_t^\\tau$ 输入网络，预测去噪速度场 $v_\\theta(A_t^\\tau, o_t) \u0026lt;-\u0026gt; \\varepsilon - A_t$； 以 $\\left| v_\\theta - (\\varepsilon - A_t) \\right|^2$ 作为监督信号； 推理的时候就会把这个过程分成 10-20步，每一步去噪一点点，最终得到一个动作 注：\u0026ldquo;这个过程\u0026quot;指的是随机生成一个噪声，然后从这个噪声去噪的过程\nCode Inference def create_trained_policy( train_config: _config.TrainConfig, # 训练配置，包含模型定义、数据配置等 checkpoint_dir: pathlib.Path | str, # 检查点目录：存放已训练模型参数和归一化统计信息的路径 *, repack_transforms: transforms.Group | None = None, # 可选的“重打包”预处理组——在所有其他 transform 之前应用 sample_kwargs: dict[str, Any] | None = None, # 传递给 policy.sample_actions 的参数字典 default_prompt: str | None = None, # 默认提示词，如果输入数据中没有 prompt，则注入该默认值 norm_stats: dict[str, transforms.NormStats] | None = None, # 归一化统计信息（均值、方差或分位数），若未提供则从 checkpoint 中加载 ) -\u0026gt; _policy.Policy: \u0026quot;\u0026rdquo;\u0026quot; 从训练好的检查点创建并返回一个可交互的 Policy 对象。\nArgs: train_config: 用于创建模型和数据流水线的训练配置。 checkpoint_dir: 存储模型参数和归一化信息的目录路径。 repack_transforms: （可选）在所有其他数据变换之前应用的变换组。 sample_kwargs: （可选）调用 sample_actions 方法时使用的关键字参数。 default_prompt: （可选）注入到输入数据中的默认提示词。 norm_stats: （可选）归一化统计信息；如果未提供，会尝试从 checkpoint 加载。 \u0026quot;\u0026quot;\u0026quot; # 如果外部没有传入 repack_transforms，则使用一个空的 transforms.Group repack_transforms = repack_transforms or transforms.Group() # 下载 checkpoint_dir = download.maybe_download(str(checkpoint_dir)) # 怀疑是这个地方卡住了，正在测试 logging.info(\u0026quot;Loading model...\u0026quot;) # 从 checkpoint 的 params 文件中恢复模型参数，并用 jnp.bfloat16 精度加载到模型中 model = train_config.model.load( _model.restore_params(checkpoint_dir / \u0026quot;params\u0026quot;, dtype=jnp.bfloat16) ) # 使用训练配置中的 data 部分构建数据流水线（包括 asset 路径、transform 定义等） data_config = train_config.data.create(train_config.assets_dirs, train_config.model) # 如果调用方未提供归一化统计信息，则从 checkpoint 中加载 if norm_stats is None: # 确保 data_config 中配置了 asset_id，否则无法定位归一化文件 if data_config.asset_id is None: raise ValueError(\u0026quot;Asset id is required to load norm stats.\u0026quot;) # 从 checkpoint_dir/assets/\u0026lt;asset_id\u0026gt; 文件夹加载归一化统计信息 norm_stats = _checkpoints.load_norm_stats(checkpoint_dir / \u0026quot;assets\u0026quot;, data_config.asset_id) # 构造并返回 Policy 对象 return _policy.Policy( model, # 定义输入端的 transform 流水线 transforms=[ *repack_transforms.inputs, # 首先应用重打包变换 transforms.InjectDefaultPrompt(default_prompt), # 注入默认 prompt（如有） *data_config.data_transforms.inputs, # 然后是数据阶段的预处理（如裁剪、编码） transforms.Normalize(norm_stats, # 使用加载的统计信息做归一化 use_quantiles=data_config.use_quantile_norm), *data_config.model_transforms.inputs, # 最后是模型期望的输入 transform（如维度调整、拼接） ], # 定义输出端的 transform 流水线，用于将模型输出反向映射回原始数据格式 output_transforms=[ *data_config.model_transforms.outputs, # 模型输出后先做反向 transform（如反维度调整） transforms.Unnormalize(norm_stats, # 反归一化 use_quantiles=data_config.use_quantile_norm), *data_config.data_transforms.outputs, # 数据阶段的后处理（如解码、去补齐） *repack_transforms.outputs, # 最后应用重打包的输出变换 ], sample_kwargs=sample_kwargs, # 传给 sample_actions 的运行时参数 metadata=train_config.policy_metadata, # 附带的元数据 ) 创新点 VLM+流匹配的融合：首次将预训练视觉-语言骨干与流匹配（flow matching）动作生成相结合，实现高频连续动作预测。 跨平台预训练：采用跨样本（cross-embodiment）训练，将来自单臂、双臂及移动操纵器的多样化数据统一到同一模型中。 两阶段训练配方：借鉴大规模语言模型的“预训练–后训练”流程，预训练学会恢复与泛化行为，后训练习得高效、精炼策略。 动作专家模块：在 Transformer 上增设专门处理机器人状态与动作的子网络，相当于一种混合专家（mixture-of-experts）设计，提高对连续动作的建模能力 解决的难点 数据稀缺：以往专用策略仅依赖于任务特定的少量数据，难以涵盖错误恢复或未见场景；π0 通过多任务多平台数据缓解了此问题。 泛化与鲁棒性：先前的自回归离散动作方法（如 OpenVLA）不支持高频动作分块，难以处理精细操控；π0 的流匹配架构可生成连续、高精度动作，提升了对复杂任务的适应能力。 多阶段任务：传统方法往往针对单一任务设计，难以扩展到折叠衣物、装箱等涉及多步骤和语义推理的场景；π0 可直接通过语言提示或与高层策略结合，完成复杂多阶段流程 还需要解决的难点 预训练数据组成与加权：如何选择和加权最有助于下游任务的数据仍然未知。 任务可靠性：部分下游任务（尤其与预训练差异大者）仍存在不稳定性，需要更多高质量后训练数据。 跨领域通用性：尚不清楚该框架能否推广到更异质的机器人领域（如自主驾驶、步态运动等）。 资源需求：大规模预训练对算力和示教数据的需求极高，实际部署成本仍是瓶颈 Take away tmux CUDA_VISIBLE_DEVICES=num pdb\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-31/","summary":"\u003ch4 id=\"主要动机\"\u003e主要动机\u003c/h4\u003e\n\u003cp\u003e目前有三大挑战：数据稀缺、泛化能力不足，以及在复杂物理环境中效果不好\n作者希望通过引入大规模语言及VLA模型在自然语言处理和计算机视觉中的成功经验解决这个问题.\u003c/p\u003e\n\u003ch4 id=\"主要论点\"\u003e主要论点\u003c/h4\u003e\n\u003cp\u003e在预训练的视-语言模型（VLM，本文采用 PaliGemma）基础上，增加一个“动作专家”（action expert），通过条件流匹配（diffusion policy）生成高频、连续的动作序列（每秒可达50 Hz）\u003c/p\u003e\n\u003ch4 id=\"模型流程图\"\u003e模型流程图\u003c/h4\u003e\n\u003cp\u003e\u003cimg alt=\"1748596153733\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-31/1748596153733.png\"\u003e\u003c/p\u003e\n\u003cp\u003e收集数据 -\u0026gt; 训练$\\phi_0$ -\u0026gt; Zero-Shot/微调/Fine-tune\u003c/p\u003e\n\u003cp\u003e数据来自7种不同的机器人，68个不同的任务，总计 10k小时。\u003c/p\u003e\n\u003cp\u003e每一个单独机械臂的自由度在 6-7 (有些机器人有多机械臂)\u003c/p\u003e\n\u003cp\u003e输入有 3 块，分别是：Image, Language, and State\u003c/p\u003e\n\u003cp\u003eImage 用 400M Pretrained VIT SigLip 得到 embedding, 一共会有up to 3 个 Image， 所以最会有 3 个 embedding (没有就填 0)\u003c/p\u003e\n\u003cp\u003eLanguage 用 2.6B Pretrained LLM Gemma, 得到 embedding\u003c/p\u003e\n\u003cp\u003e最后是关节信息，最多会有 18 个(没有就填 0)。\u003c/p\u003e\n\u003cp\u003e之后运用 Diffusion Policy 来生成每一步的动作\u003c/p\u003e\n\u003cp\u003e生成的是 $p(A_t|O_t)$, 即，在给定条件(环境下)，每一个动作的概率。\u003c/p\u003e\n\u003cp\u003e这时给一个 chunk 内的真实动作加噪音，然后让模型学习如何去噪。\u003c/p\u003e\n\u003cp\u003e方式如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e随机采一个噪声级别 $\\tau$，采一个高斯噪声 $\\varepsilon$；\u003c/li\u003e\n\u003cli\u003e构造带噪动作块 $A_t^\\tau = \\tau A_t + (1 - \\tau)\\varepsilon$；\u003c/li\u003e\n\u003cli\u003e用观测 $o_t$ 与 $A_t^\\tau$ 输入网络，预测去噪速度场 $v_\\theta(A_t^\\tau, o_t) \u0026lt;-\u0026gt; \\varepsilon - A_t$；\u003c/li\u003e\n\u003cli\u003e以 $\\left| v_\\theta - (\\varepsilon - A_t) \\right|^2$ 作为监督信号；\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e推理的时候就会把这个过程分成 10-20步，每一步去噪一点点，最终得到一个动作\n\u003cem\u003e注：\u0026ldquo;这个过程\u0026quot;指的是随机生成一个噪声，然后从这个噪声去噪的过程\u003c/em\u003e\u003c/p\u003e","title":"Bug Journal 2025-05-31"},{"content":"Docker 的安装和调试 Docker相当于一台虚拟机。安装之后就可以在这台虚拟机上跑代码了。\n安装方式：\n首先上 Dockerhub 挑选一个心仪的 docker, 下面以 nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 为例：\n然后运行以下代码：\ndocker run -it -rm\\ --name \u0026lt;your-instance-name\u0026gt; \\ --network host \\ nvidia/cuda:12.0.0-cudnn8-devel-ubuntu22.04 \\ /bin/bash 注：这里的 -it 指的是打开一个可交互界面，-rm 指的是用后删除\n这时候就会自动下载 docker 并打开一个 bash 来用。\n现在你会发现这个虚拟机里面什么都没有，所以就需要 apt-get install\n另外，如果你的宿主机器的根目录比较小，想要挂载一个硬盘的话，就在 docker run 中间加上：\n-v /path/to/large/storage:/somepath \\ 这样就可以在 somepath 下挂载这个硬盘了。\n注：不能挂载在根目录下，必须挂载在一个文件夹下\n这里在测试的时候建议加上 -rm,这样不会产生很多个休眠中的 docker 但是在要频繁使用的时候不建议使用 -rm, 而是就让 docker休眠就好。\n一些比较常用的 docker 指令： # 启动 docker docker start \u0026lt;容器ID或名字\u0026gt; # 关闭 docker docker stop \u0026lt;容器ID或名字\u0026gt; # 重启 docker docker restart \u0026lt;容器ID或名字\u0026gt; # 删除容器 docker rm \u0026lt;容器ID或名字\u0026gt; # 进入容器 docker exec -it \u0026lt;容器ID或名字\u0026gt; bash # 查看正在运行的容器 docker ps # 查看所有容器（包括停止的） docker ps -a # 列出本地镜像 docker images # 删除镜像 docker rmi \u0026lt;镜像ID或名字\u0026gt; # 挂载目录 docker run -v /host/path:/container/path # 增加环境变量 docker run -e HTTP_PROXY=http://localhost:10086 代理的使用 这里使用的是 xray。\nxray 是这样运行的：\n./xray run -c config.json 运行之后你就可以看到哪个端口放开了，就可以在哪个端口上使用代理,比如 port: 10086。\n这时候如果你想使用代理就需要：\nexport HTTP_PROXY=http://localhost:10086 export HTTP_PROXY=http://localhost:10086 这样你的下载就会走代理辣。\n非常重要 (大坑) apt-get install 对代理的要求较高，没那么稳定的代理会很稳定的挂，报 Error 503 Service Unavailable. 这时候就直接换清华源就行了，别使用代理了，等之后下别的再用。\n代理的使用 之后就是装 git, 装conda, 装 python \u0026hellip;\n装 git:\napt update \u0026amp;\u0026amp; apt-get install git 装 conda:\napt update \u0026amp;\u0026amp; apt install -y curl cd /Path curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p /Path/miniconda3 /Path/miniconda3/bin/conda init source ~/.bashrc 装 python:\nconda --version conda create -n \u0026lt;yourname\u0026gt; python=\u0026lt;yourversion\u0026gt; -y conda activate \u0026lt;yourname\u0026gt; 注意：不要在~/.bashrc 中添加 source ~/.bashrc，而是就运行一遍 source ~/.bashrc 就可以了\nPDF压缩 gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/default -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf input.pdf ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-30/","summary":"Docker 的安装和调试","title":"Bug Journal 2025-05-30"},{"content":"DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 动机 现有机器人在静态任务（比如说开门，拿方块，玩魔方）上已经做得很好了，但是在动态任务(比如接住一支笔)上做得不行。 所以现在希望能够解决：\u0026ldquo;灵巧手动态抛接物体\u0026rdquo; 这个问题。\n主要论点 作者提出一个新的强化学习框架 LTC (Learning-based Throwing-Catching) 来操控灵巧手完成抛接动作\n利用压缩后的点云特征感知物体； 基于PPO算法的Actor-Critic策略学习； 引入Lyapunov稳定性准则 和 Intrinsic Advantage 提高捕捉稳定性与学习效率； 模型流程图 PointCloud V2 获取物体点云 -\u0026gt; PCA 压缩点云信息 后面K-Means 优化的线索\n输入观察得到的信息concat 上点云输入\n然后使用 PPO 算法优化\n简单来说，Actor 负责做一个动作，Critic 负责判断这个动作好不好，PPO则会让策略和策略之间的连贯性更强。\n另外，为了增加系统的稳定性，作者引入了一个Lyapunov 函数（经典控制理论中用来衡量系统稳定性）来让这个系统更加稳定。\n这时候有 3 个值：第一个是 原本 PPO 算法中算出来的值，第二个是 Critic 对于动作价值的预测值，第三个是Lyapunov 函数的值\n最后通过加权平均，得到最后的A_all 用于优化 actor.\n创新点 首个实现任意物体灵巧抛接的学习方法，尤其在手部侧握极不稳定条件下成功； 引入Lyapunov稳定性引导的优势估计，显著提升捕捉的稳定性； 点云 + PCA 压缩特征用于泛化物体类型，训练期间加入物理属性随机扰动； 提出混合优势估计，结合 PPO 优势、Lyapunov 稳定优势、Intrinsic 优势； 通过仿真-现实迁移设计，包括扰动鲁棒性验证和实际机器人平台部署规划。 解决的难点 引入系统稳定性约束，避免学习出高回报但不稳定的动作策略； 物体点云感知 + 训练过程中的域随机化增强泛化能力； 之前方法的缺点 大多为静态任务（如抓握、拼积木、开门）； RL方法在动态任务中效果极差（之前成功率几乎为0）； 缺乏对动态稳定性的建模或对多物体、多姿态的泛化能力。 还需要解决的难点 对复杂形状和姿态扰动的鲁棒性仍有限； 仅仅在模拟环境中验证； 复杂场景（如多机器人配合抛接）。 Take away 说到动机，想问一下学长为什么想做具身智能呢？ Mojuco 引擎是一个可以运行的虚拟环境\n对于这一段的 reinforcement learning, 这里的意思是：S 是机器人的状态，A 是机器人可以做的动作的集合，p 是环境，R 是奖励函数，$\\rho_0$是一个随机的初始状态分布，$\\gamma$是奖励因子：最终的奖励类似：$R = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} \\dots$ 而现在我们要做的是:选择一些动作让 $E[R]$ 尽可能大。\n在这里，PPO 看起来很复杂，但是其实不难：\n简单来说，Actor 负责做一个动作，Critic 负责判断这个动作好不好，PPO则会让策略和策略之间的连贯性更强。\n这里的“连贯性更强指的是”：新旧策略在面对同一个 state 的情况下做某一个 action 的概率之间的比值不会超过$(1 - \\theta, 1 + \\theta)$\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-29/","summary":"Paper review 2025-05-29","title":"Bug Journal 2025-05-29"},{"content":"SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation 发表时间：24 Jan 2025\n主要论点 这篇文章提出了一种新的模仿学习框架 SKIL（Semantic Keypoint Imitation Learning），通过结合视觉基础模型自动提取“语义关键点”（semantic keypoints），使得机器人在仅有少量示范的条件下，依然能完成具有泛化能力和复杂步骤的操作任务（如挂毛巾、折叠布料等）。该方法显著降低了训练所需的数据量，并且在未见过的物体与场景中也表现出优越的泛化能力。\nHow can we reduce sample complexity to enable robots to learn data-efficient and generalizable manipulation tasks?\n模型流程图 创新点 自动语义关键点提取： 借助如 DiFT 等视觉基础模型与 SAM，对参考图像中的目标区域进行聚类，自动生成语义关键点，不需要人工标注或专门训练。 语义关键点描述符（descriptor）设计： 结合相似度向量（cosine similarity）与 3D 坐标构建表示，每个关键点携带语义和空间信息。 结合 Diffusion Policy 输出动作序列： 使用 Transformer 编码器和扩散模型作为动作头，实现连续动作输出。 支持跨主体学习（Cross-embodiment）： 提出的 SKIL-H 模块允许利用人类视频（无动作标签）进行辅助训练，提高数据效率和泛化性。 Ensemble 推理策略： 在推理阶段进行关键点子集 dropout 与多次采样求中位数，从而降低视觉匹配误差带来的动作抖动。\nbb*SKIL-H: ** 可以把第三人称视角人的视频转换为辅助训练的数据集\n使用一个 frozen 的通用关键点检测器（如 SAM-Track 或 VIT tracker），输出每一帧的人体相关关键点（如手、手指等）。 画出这些关键点之间的轨迹 将人类演示中关键点的表示映射到机器人操作空间中的目标关键点 直接用 transformer 编码 解决的难点 利用视觉大模型提取对任务有语义意义的关键点，极大降低了状态空间维度； 通过高质量、稀疏但语义强的关键点建模，提升少样本学习能力； 使用 Transformer + Diffusion Policy 构建策略网络，强化连续动作输出的能力； 提供了一种利用人类演示辅助学习的方法，不依赖标注动作。 之前方法的缺点 慢：比如ACT要收集上万个数据; GenGP 用的是完整的语意场，有太多信息了 只预测关键帧，而不是连续的动作 (注：我认为这里说不定关键帧可能好一点) 预测连续动作的模型没有 generalizability 还需要解决的难点 关键点提取质量依赖于视觉基础模型能力： 如在 “Bulb Assembly” 等精度要求极高的任务中，DiFT 模型提取的关键点不够精确，导致失败。 忽略环境障碍与场景信息： 当前关键点只从目标对象上提取，无法感知障碍物等环境元素，可能导致安全问题。 固定视角与姿态： 当前工作主要依赖固定的第三视角 RGBD 摄像头，泛化到第一视角或动态视角仍有挑战。 动作表示维度有限： 尚未充分拓展到高自由度控制（如仿人手）、复杂轨迹规划等更广泛的应用场景。 泛化问题：这个模型并不是 zero-shot,而是每个 task 都单独 train 了一个 model\nTake away semantic keypoints: 语义关键点 指的是一个物体和操作相关的哪一个部分。比如杯子上的把手\ncosine similarity 可以获得不同关键点之间的相似度，相当于某种位置信息；之后可以把真正的位置信息也嵌入进去。\nDiffusion Policy: 这种方法使得 multi-model 变得可行\noff-the-shelf tracking models: 约等于已经开源的动作跟踪实现\n数据集 Meta World\nDATA SCALING LAWS IN IMITATION LEARNING FOR ROBOTIC MANIPULATION 发表时间：24 Oct 2024 修改于：12 Feb 2025\n主要论点 这篇文章研究了模仿学习中的数据规模定律(data scaling laws) 在机器人操作任务中的适用性，核心问题是：\n在不同环境和不同物体下增加演示数据量，是否能提升策略的泛化能力，进而使单任务策略在新环境和新物体上零样本部署成为可能？\n作者通过在真实世界收集超 4 万条人类演示、1.5 万次机器人 rollout，基于 Diffusion Policy 训练策略，在多个任务上发现：\n泛化性能随着训练环境/物体/环境-物体组合数量近似呈幂律增长，即：$y = \\alpha x ^ \\beta$, 其中，y 是用 normalized score 来衡量的泛化性能，x 是样本的多样性。 同一个物体或环境上多收集数据的效果远远不如增加多样性； 实证证明仅需 32 个不同环境-物体组合、每个 50 个演示，就能训练出成功率超 90% 的策略。 模型流程图 创新点 首次系统性提出并验证模仿学习中机器人操控的“scaling law” 测试了 环境泛化 + 物体泛化； 提出了一个高效数据采集策略：以环境和物体的多样性优先，不盲目增加演示数量； 在多项任务上（Pour Water、Mouse Arrangement、Fold Towels、Unplug Charger）进行大规模真实机器人验证； 对视觉模型和动作模型的扩展进行了 ablation study，验证视觉编码器更关键。 解决的难点 数据量大 还需要解决的难点 只验证了四个任务，任务种类仍有限，无法保证适用于所有任务 只使用了 Diffusion Policy，未评估不同学习算法对 scaling law 的依赖性差异 未来还将验证 Reinforcement Learning 的 scaling law. Take away 为了获得更好的泛化能力，采集更多不同的环境 ($# \u0026gt; 16$) \u0026gt; 在同一个环境中堆砌多个物体样本\nLearning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving 发表时间：22 Jan 2025\n主要论点 这篇文章提出了一种新颖的自我训练方法 LEPA（Learning to Plan before Answering），训练 LLM 在生成具体解答前，先生成高层次的抽象“计划”。这些“计划”是通用的元知识（meta-knowledge），可以指导模型更有效地推理和解题。\n具体流程如下： 1.\t模型先生成anticipatory plan（预测性计划）：对问题的大致解题策略。 2.\t再基于这个计划生成具体解答。 3.\t如果解答错误，模型会进行 自我反思（self-reflection），调整 plan 并重新解题，直到成功或达到最大尝试次数。 4.\t最终用 plan + solution 对模型进行监督微调（Supervised Fine-Tuning）。\n模型流程图 创新点 引入 anticipatory plan：首次在自我训练中加入计划作为元知识，帮助 LLM 更清晰地组织解题路径。 计划 + 解答联合训练：不仅学习答案，还学习如何规划解题路径，提高泛化性。 自我反思机制：错误时能分析原因并优化 plan，不再依赖外部标签。 信息隔离设计：计划不能包含具体答案细节，避免 LLM 投机取巧。 可与 RL 兼容扩展：初步展示了 LEPA 与 RL（REINFORCE）结合后的性能提升。 之前方法的缺点 没有引导模型形成通用的解题策略 → 泛化能力差； 错误的修改方式（如修改最终答案而非解题路径）容易产生假阳性答案； 缺少系统性规划与反思机制 。 解决的难点 自我训练生成的数据质量不足、泛化能力差，尤其是在复杂的数学和推理任务上表现不佳。\n之前的方法无法抽象出问题：比如，在做物理题时先抽象出公式，然后再代数运算。\n还需要解决的难点 计划与答案不一致问题：计划可能无法完全约束模型生成的推理步骤； 复杂度与推理选择权平衡：对于简单问题，可能不需要计划，LEPA 可能浪费计算； 计划的质量仍依赖 LLM 自身能力 ，弱模型难以产生有用计划； RL 优化仍是初步探索，与更强 RL 算法结合仍是未来方向； Take away Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving 主要论点 本文提出一种针对四足机器人“难以在仿真中准确建模”的目标（如总电池功耗）进行优化的数据驱动微调方法。核心思想是：\n先用预训练策略在现实中采集数据； 训练一个“测量模型”来预测那些仿真环境中缺失或误差较大的目标（如总电流）； 将这个模型集成到仿真中，作为奖励函数优化新策略； 通过仿真+现实评估的迭代更新，逐步提升策略在真实世界中的表现。 他们以“降低四足机器人总功耗”为目标，在 Unitree Go1 上进行实证研究，结果显示电池功耗减少了 24-28%。\n模型流程图 创新点 数据驱动的测量模型作为奖励代理：直接从现实数据学习一个预测真实目标的模型（如总电流），替代传统的机械功或热功率等代理指标。 目标不可建模时的微调框架：将测量模型集成进仿真中，使得原本难以仿真的目标也可以用于训练优化。 层级策略选择机制：候选策略先在仿真中初选，再在现实中精评，保留最优用于下次迭代。 迭代式收集数据 + 微调：不断积累现实数据、优化测量模型、提升策略性能，达到持续优化的效果。 解决的难点 传统 sim-to-real 方法依赖物理仿真器（如 MuJoCo），但这些仿真器通常无法准确建模：\n电池电流/电压（尤其是 PMSM 电机复杂控制） 步态噪声 电机过热等非刚体物理特性 之前方法的缺点 使用代理目标 (如机械功) 不够准确，甚至误导优化过程； 现实直接训练 RL 效率低，只适用于简单任务； 以往方法不支持针对新目标进行微调（例如预训练时没有考虑节能目标，无法直接迁移）。 还需要解决的难点 需大量真实数据才能收敛（约 38 万个样本）； 测量模型存在分布偏移（Out-of-Distribution）问题； 实验仅在 Unitree Go1 和平地环境下进行，缺乏跨机器人、复杂地形验证； 没有测试对其他 hard-to-simulate 目标（如声音、热量）的迁移适应性。 Take away Revisit when doing a robot dog related task.\n关注文章的 Motivation\n关注文章的 Motivation 是怎么和方法联系起来的\n关注文章是如何卖出去的\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-28/","summary":"2025-05-28 论文阅读笔记","title":"Bug Journal 2025-05-28"},{"content":"Today\u0026rsquo;s problem 2894. Divisible and Non-divisible Sums Difference\nIntuition We are given an integer n and a divisor m, and we want to compute the difference between:\nThe sum of numbers from 1 to n that are not divisible by m. The sum of numbers from 1 to n that are divisible by m. This means we want to partition the numbers 1 to n into two groups based on divisibility by m, sum each group, and return the difference.\nApproach We can solve this problem using two methods:\nMethod 1: Formula-Based Use the formula for the sum of the first n natural numbers: n * (n + 1) // 2 to get the total sum. Count how many numbers from 1 to n are divisible by m: k = n // m. The divisible numbers are: m, 2m, ..., km, and their sum is m * (1 + 2 + ... + k) = m * (k * (k + 1) // 2). Subtract the divisible sum from the total to get the sum of non-divisible numbers, then subtract. Method 2: Brute-Force Iteration Iterate from 1 to n. If the number is divisible by m, add it to num2. Otherwise, add it to num1. Return the difference num1 - num2. Complexity Time complexity:\nMethod 1: $O(1)$ (constant time using formulas) Method 2: $O(n)$ (linear time iteration) Space complexity:\nBoth methods: $O(1)$ (only a few variables used) Code class Solution: def differenceOfSums(self, n: int, m: int) -\u0026gt; int: # Method 1: Formula-Based total_sum = n * (n + 1) // 2 k = n // m divisible_sum = m * (k * (k + 1) // 2) return total_sum - divisible_sum class Solution: def differenceOfSums(self, n: int, m: int) -\u0026gt; int: # Method 2: Brute-Force Iteration num1, num2 = 0, 0 for i in range(1, n + 1): if i % m == 0: num2 += i else: num1 += i return num1 - num2 Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-27/","summary":"Traverse and Mathmatics!","title":"LeetCode Daily Question 2025-05-27"},{"content":"模板 请仔细阅读这篇文章，并告诉我： 1. 这篇文章的动机是什么，要解决什么问题 2. 这篇文章大概讲了什么 3. 这篇文章的创新点是什么 4. 这篇文章解决了什么问题，之前的人为什么不能解决 5. 这篇文章还有什么问题没解决 6. 这篇文章有什么需要我注意的点 7. 这篇文章是如何做实验的，setting 是什么 8. 这篇文章的算力要求是多少，多少卡运行了多久，用了什么数据集，是不是可以公开获取的，模型代码呢，能不能公开获取 如果这篇文章提出了一个模型，那请告诉我： 1. 这个模型的输入是什么 2. 输出是什么 3. 输入和输出数据经过了什么处理 4. 这个模型是如何处理输入和输出数据的 动机 主要论点 模型流程图 创新点 解决的难点 之前方法的缺点 还需要解决的难点 Take away ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-27/","summary":"A template for reading AI papers","title":"Bug Journal 2025-05-27"},{"content":"Today\u0026rsquo;s problem 2131. Longest Palindrome by Concatenating Two Letter Words\nIntuition There are in total two ways to form a palindrome.\na string that has an inverse string in the list a string that is a palindrome itself. In this case, the string that is palindrome can only exisit in the middle of the palindrome. Approach Therefore, we can use a hash to solve this problem. Note that we will first run test 1 before test 2. If there is an inverse string in the list, then put that string and the current string into the list.\nThen test whether this string is a palindrome itself.\nComplexity Time complexity: $O(N)$, N is the length of words.\nSpace complexity: $O(N)$, N is the length of words.\nCode class Solution: def longestPalindrome(self, words: List[str]) -\u0026gt; int: cnt = Counter(words) ans = 0 sp = 0 for word, t in cnt.items(): # print(word, t) if word[0] == word[1]: ans += (t - t % 2) sp |= (t % 2) else: ans += min(t, cnt[word[::-1]]) return (ans + sp) * 2 Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-25/","summary":"Hash!","title":"LeetCode Daily Question 2025-05-25"},{"content":"Today\u0026rsquo;s problem 2942. Find Words Containing Character\nIntuition Do what the question ask.\nApproach Do what the question ask, find the string in every word in words array.\nComplexity Time complexity: $O(N \\times M)$, N is the length of words array, M is the length of each word. Space complexity: $O(N \\times M)$, N is the length of words array, M is the length of each word. Code class Solution: def findWordsContaining(self, words: List[str], x: str) -\u0026gt; List[int]: ans = [] for i, word in enumerate(words): if x in word: ans.append(i) return ans Advertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-24/","summary":"Do what the question ask","title":"LeetCode Daily Question 2025-05-24"},{"content":"Today\u0026rsquo;s problem 3068. Find the Maximum Sum of Node Values\nImportant: all the methods below are based on this fact: xor even times equals xor zero times. Method 1: Tree DP Intuition and Approach In this problem, if we only consider one direction, e.g., from root to leaf, then the process will not have after effect (later decisions will not affect previous ones). Therefore, we can use DP to solve this problem.\nThe hardest part is the definition of the dp. As we have a prerequisite of a direction, a better way to define the dp formula is to exclude the effect of current node. Also, for each node, there are two status, as described above, each node can either xor odd times or even times.\nTherefore, we have our DP definition. $dp[x][0/1]$ means the largest value the children of x can achieve when the node x is changed (1) or unchanged (0).\nNow, for each child c of node x, we can do two operations: either do xor for both node x and c, or do not do xor for neither x nor c.\nThe dp formula of these two operations will be: (Note: the priority of $\\oplus$ is lower than $+$, so it is very important to add a parentheses.)\nDo the xor operation $dp[x][0] = max(dp[x][0] + dp[c][0] + nums[c], dp[x][0] + dp[c][1] + (nums[c] \\oplus k))$. $dp[x][1] = max(dp[x][1] + dp[c][0] + nums[c], dp[x][1] + dp[c][1] + (nums[c] \\oplus k))$. NOT do the xor operation $dp[x][0] = max(dp[x][1] + dp[c][1] + nums[c], dp[x][1] + dp[c][0] + (nums[c] \\oplus k))$. $dp[x][1] = max(dp[x][0] + dp[c][0] + nums[c], dp[x][0] + dp[c][0] + (nums[c] \\oplus k))$. Note that the dp[x][0] and dp[x][1] should be renewed at the same time.\nMoreover, another important thing is the initialization of the dp array. For all $dp[x][1]$, we will give it a value of $-inf$, so that we can avoid the case when c is a leaf node and the number is $\\oplus$ with k contributes to the $dp[x]$ array.\nThe final result will be $max((dp[0][0] + nums[0]), (dp[0][1] + (nums[0] ^ k)))$\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp = [[0 for _ in range(2)] for _ in range(n)] for i in range(n): dp[i][1] = -10_000_000_000 edge = [[] for _ in range(n)] for x,y in edges: edge[x].append(y) edge[y].append(x) def dfs(x, fa): for to in edge[x]: if to == fa: continue dfs(to, x) c0 = max(dp[to][0] + nums[to], dp[to][1] + (nums[to] ^ k)) c1 = max(dp[to][0] + (nums[to] ^ k), dp[to][1] + nums[to]) dp[x][0], dp[x][1] = max(dp[x][0] + c0, dp[x][1] + c1), max(dp[x][1] + c0, dp[x][0] + c1) dfs(0,-1) return max((dp[0][0] + nums[0]), (dp[0][1] + (nums[0] ^ k))) Method 2: Tree DP with better memory Intuition and Approach In the previous code, we find that the $dp[x]$ will only use two times. Once in calculating the result of $dp[x]$, once in calculating the result of $dp[fa]$.\nTherefore, we can return the value of $dp[x][0]$ and $dp[x][1]$ to avoid the extra space of the dp array.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(1)$. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) edge = [[] for _ in range(n)] for x,y in edges: edge[x].append(y) edge[y].append(x) def dfs(x, fa): dp0,dp1 = 0,-1e9 for to in edge[x]: if to == fa: continue c0, c1 = dfs(to, x) dp0, dp1 = max(dp0 + c0, dp1 + c1), max(dp0 + c1, dp1 + c0) return max(dp0 + nums[x], dp1 + (nums[x] ^ k)), max(dp0 + (nums[x] ^ k), dp1 + nums[x]) return dfs(0,-1)[0] Important: all the methods below are based on this fact: there are always a path between two nodes on a tree. Therefore, we can $\\oplus$ all the nodes on this path, resulting the $\\oplus$ of any two nodes on the tree. Method 3: DP without tree Intuition and Approach For each node, we have two status, whether to $\\oplus$ k or not. Therefore, the definition of the DP array will be: $dp[i][0/1]$ means whether there are odd (1) or even (0) $\\oplus$ k operations when traversing to the ith node.\nWe then have the formular:\nWhen this node $\\oplus$ with k: $dp[i][0] = max(dp[i-1][0] + nums[i], dp[i-1][1] + (nums[i] ^ k))$ When this node do not $\\oplus$ with k: $dp[i][1] = max(dp[i-1][1] + nums[i], dp[i-1][0] + (nums[i] ^ k))$ Note that there are always even $\\oplus$ operations, so the answer would be $dp[n-1][0]$.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp = [[0 for _ in range(2)] for _ in range(n)] dp[0][0] = nums[0] dp[0][1] = (nums[0] ^ k) for i in range(1, n): dp[i][0] = max(dp[i-1][0] + nums[i], dp[i-1][1] + (nums[i] ^ k)) dp[i][1] = max(dp[i-1][0] + (nums[i] ^ k), dp[i-1][1] + nums[i]) return dp[-1][0] Method 4: DP without tree with better memory Intuition and Approach Same as Method 2, we also find out that the dp[i] formular only use twice. In this case, we can use two variables instead of the whold array to have a better memory usage.\nAlso, the $max$ operations is too slow in python, so a better way is to use if else equations instead of max.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(1)$. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: n = len(nums) dp0, dp1 = 0, -10_000_000_000 for i in range(n): a = nums[i] b = a ^ k new_dp0 = dp0 + a if dp0 + a \u0026gt; dp1 + b else dp1 + b new_dp1 = dp0 + b if dp0 + b \u0026gt; dp1 + a else dp1 + a dp0, dp1 = new_dp0, new_dp1 return dp0 Method 5: Greedy algorithm Intuition and Approach Another way to look at this method without of tree is using greedy algorithm. Because we know that we can $\\oplus$ k as long as we can find a pair of nodes, we can use greedy algorithm to find the pairs that has the most differences after $\\oplus$ k.\nThat is, we can first $\\oplus$ every element with k, calculating the difference between the new array and the previous array, then find all the pairs that has a difference that is larger than zero, then we get our answer.\nComplexity Time complexity: $O(N)$, N is the length of nums. Space complexity: $O(N)$, N is the length of nums. Code class Solution: def maximumValueSum(self, nums: List[int], k: int, edges: List[List[int]]) -\u0026gt; int: ans = sum(nums) diff = [(x ^ k) - x for x in nums] cnt,l,r = 0,inf,-inf for x in diff: if x \u0026gt; 0: cnt += 1 if x \u0026lt; l: l = x ans += x else: if r \u0026lt; x: r = x if cnt % 2 == 1: ans += max(-l, r) return ans I don\u0026rsquo;t know why using sort to do greedy algorithm is so neat and fast. Just as the one in the official solution.\nAdvertisement For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-23/","summary":"5 Solutions in one question!","title":"LeetCode Daily Question 2025-05-23"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/zero-array-transformation-iii/\nIntuition This question requires us to find the number of sections that is \u0026ldquo;useful\u0026rdquo;, or in other words, the smallest number of sections that is enough to make the array a zero array.\nThe key to solve this question is to change a view of how we look at this problem: if we take each element in the array seperately, then we can use greedy algorithm to solve this problem.\nThe thing is, if we look at each element seperately, i.e., to make the entire array a zero array, we must make each element zero.\nIn this case, for each element, the best way is to find a section that contains this elemnt, while it has a further tail. This is because a furtuer tail means to cover more elements in the future, which will be always better compared with the sections that has a shorter tail.\nTherefore, now we need a data structure to store the current \u0026ldquo;farest tail\u0026rdquo;. This data structure need to add element dynamically and delete the largest item, when a heap will be the best way to store the \u0026ldquo;tail\u0026rdquo;.\nApproach Therefore, we can form our algorithm.\nFirst, we need to sort the array using the left end of the query as keyword. In this case, we can find which queries has a left end that is to the left of our current index. Then we need to create a difference array to deal with the section add operation; a heap to store the right end of the queries; and an index to show where we are currently at when we traverse all the queries. The next step is to traverse the number array: for each element in the number array, we first need to push all the queries that has the left end that is less the current index. This makes all the elements in the heap potentially available to use to decrease the current element. Then we will deal with the current element, finding all the available queries for the current element, then deal with the section decrease operation. (In this case, we ensure every operation is valid by checking that the endpoint of the heap top is larger or equal to the index, so that the left end of the array will be less or equal to the current index, and the right end of the array will be larger or equal to the current index. Therefore, we guarentee that the operation is valid). Finally, if the number is still larger than 0, we will return -1; otherwise, we will return the remaining element in h, which is all the unused elements. Complexity Time complexity: $O(N \\times log(m))$\nSpace complexity: $O(N + M)$\nCode class Solution: def maxRemoval(self, nums: List[int], queries: List[List[int]]) -\u0026gt; int: queries.sort(key = lambda x: x[0]) diff = [0] * (len(nums) + 1) h = [] idx, now = 0,0 for i in range(len(nums)): while idx \u0026lt; len(queries) and queries[idx][0] \u0026lt;= i: heappush(h, -queries[idx][1]) idx += 1 now += diff[i] while h and now \u0026lt; nums[i] and -h[0] \u0026gt;= i: now += 1 diff[-heappop(h) + 1] -= 1 if now \u0026lt; nums[i]: return -1 return len(h) For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-22/","summary":"Greedy Algorithm, look at each element seperately!","title":"LeetCode Daily Question 2025-05-22"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-smallest-number-from-di-string/\nIntuition This problem let us find a sequence of numbers under some constrains.\nFirst we observe that the N, the length of the sequence, is very small. Thereofre, we can use a dfs to traverse all possible situations and get the result.\nAnother way to do that is using a stack. When the array is increasing, the smallest way to get a valid array is to traverse all the numbers, filling in the smallest number possible. When the array is decreasing, becuase we still want the smallest array, we need to put the smallest number available, which should be also larger to the next number. In this case, the smallest number we can put at current place is the position + the number of consequtive \u0026lsquo;D\u0026rsquo;s afterward. Therefore, we can use a stack to temporarily sotre the number we are traversing, and add it back to the current array when we meet a \u0026lsquo;I\u0026rsquo; or reach to the end of the array.\nApproach For Solution 1, the dfs solution, all we need to do is to traverse all the solutions and get the first one that fullfills the requirement.\nFor Solution 2, the stack solution, when we meet \u0026lsquo;I\u0026rsquo;, we can put \u0026ldquo;idx + 1\u0026rdquo; to our current array, then fill all the elements in a stack into our current array in reverse order, then flush the stack. When we meet \u0026lsquo;D\u0026rsquo;, we can put \u0026ldquo;idx + 1\u0026rdquo; to our stack for our future use.\nTrick DFS: Because we are required to find the smallest valid sequence, so the first sequence that is not None is our target. This means that we can return this answer as soon as we get a valid result.\nStack: Here I intentionally add a \u0026ldquo;D\u0026rdquo; to the end of the sequence. Intuitively speaking, the last element is the largest element in the sequence, so to put it into our current sequence, it requires a \u0026ldquo;D\u0026rdquo; operation. In this case, if the original last character is \u0026lsquo;I\u0026rsquo;, then we can directlly put the largest number to the end of our original sequence, which is the same as a \u0026lsquo;D\u0026rsquo; operation. This is because the \u0026lsquo;I\u0026rsquo; operation will flush the stack, so there will only be one element in the stack, making it the same whether adding as a normal sequence or a inverted sequence. If the original last character is \u0026lsquo;D\u0026rsquo;, then the largest character should be at the position whether the consequtive sequence of \u0026lsquo;D\u0026rsquo; starts. In this case, this means that there should be a \u0026lsquo;D\u0026rsquo; operation to put this number into the right position. Though this trick makes the code more tidy and elegant, it sacrifices readability, which is not encouraged.\nComplexity Time complexity for dfs solution: $O(N!)$, N is the length of the sequence.\nTime complexity for stack solution: $O(N)$, N is the length of the sequence.\nSpace complexity for dfs solution: $O(N)$, N is the length of the sequence.\nSpace complexity for stack solution: $O(N)$, N is the length of the sequence.\nCode class Solution: def smallestNumber(self, pattern: str) -\u0026gt; str: arr = [] n = len(pattern) + 2 def dfs(arr): if len(arr) == n - 1: return arr now = len(arr) - 1 res = None if pattern[now] == \u0026#39;I\u0026#39;: for i in range(arr[now] + 1, n): if i not in arr: res = dfs(arr + [i]) if res is not None: return res else: for i in range(1, arr[now]): if i not in arr: res = dfs(arr + [i]) if res is not None: return res return res for i in range(1, n): ans = dfs([i]) if ans is not None: return \u0026#39;\u0026#39;.join(map(str, ans)) # return ans return None class Solution: def smallestNumber(self, pattern: str) -\u0026gt; str: arr = [] n = len(pattern) + 2 def dfs(arr): if len(arr) == n - 1: return arr now = len(arr) - 1 res = None if pattern[now] == \u0026#39;I\u0026#39;: for i in range(arr[now] + 1, n): if i not in arr: res = dfs(arr + [i]) if res is not None: return res else: for i in range(1, arr[now]): if i not in arr: res = dfs(arr + [i]) if res is not None: return res return res for i in range(1, n): ans = dfs([i]) if ans is not None: return \u0026#39;\u0026#39;.join(map(str, ans)) # return ans return None For more solutions, please visit My blog.\n","permalink":"https://tzj2006.github.io/leetcode/2025-02-18/","summary":"DFS beats 100% and O(N) stack with trick","title":"LeetCode Daily Question 2025-02-18"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/set-matrix-zeroes/\nIntuition This question asks us to change the rows and columns to 0 if there exists an 0 in the row or column. Therefore, we can store the rows and columns and then change all these rows and colums to zero.\nApproach Store all the columns and rows that contains 0 Change all these columns and rows Complexity Time complexity: $O(N \\times M)$, N is the length of the array, M is the width of the array.\nSpace complexity: $O(N \\times M)$, N is the length of the array, M is the width of the array.\nCode class Solution: def setZeroes(self, matrix: List[List[int]]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify matrix in-place instead. \u0026#34;\u0026#34;\u0026#34; change_row_idx = set([]) change_col_idx = set([]) # Note that here I use set to avoid recording the same row or column multiple times. for i in range(len(matrix)): for j in range(len(matrix[0])): if matrix[i][j] == 0: change_row_idx.add(i) change_col_idx.add(j) for i in range(len(matrix)): for j in range(len(matrix[0])): if i in change_row_idx or j in change_col_idx: matrix[i][j] = 0 For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-21/","summary":"Do what the question asks!","title":"LeetCode Daily Question 2025-05-21"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/zero-array-transformation-i/\nIntuition This problem means to decrease one in a range (l, r) for each query. To deal with the change of a range, we can consider prefix sum. Note that the question says \u0026ldquo;Select a subset of indices\u0026rdquo;, this means that we do not necessarily need to minus 1 for all indices in the range. In this case, because we only care whether the final array is a zero array or not, so instead of testing whether the final array is zero or not, we can test whether the final array is less or equal to zero or not becuase of the subset mentioned in the question.\nApproach For each query, we can add 1 to the difference array at l and add -1 to the difference array at r + 1. Then when we calculate the final answer, we can use the prefix sum to add them up and get the change of the array. Finally, when we want to know whether the final array is zero array or not, we can add the difference array to the original array and test whether each index is less or equal to zero or not to get the answer.\nComplexity Time complexity: $O(N + M)$, N is the length of the original array, M is the length of the query.\nSpace complexity: $O(N)$.\nCode class Solution: def isZeroArray(self, nums: List[int], queries: List[List[int]]) -\u0026gt; bool: diff = [0] * (len(nums) + 1) for l, r in queries: diff[l] -= 1 diff[r + 1] += 1 for i in range(len(nums)): if i \u0026gt; 0: diff[i] += diff[i-1] if nums[i] + diff[i] \u0026gt; 0: # print(i, nums[i], diff[i]) return False return True For more solutions, please visit My blog\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-20/","summary":"Use Chafen!","title":"LeetCode Daily Question 2025-05-20"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/type-of-triangle\nIntuition Do what the question asks.\nApproach Do what the question asks.\nComplexity Time complexity: $O(1)$\nSpace complexity: $O(1)$\nCode class Solution: def triangleType(self, nums: List[int]) -\u0026gt; str: nums.sort() if nums[0] + nums[1] \u0026lt;= nums[2]: return \u0026#34;none\u0026#34; elif nums[0] == nums[1] == nums[2]: return \u0026#34;equilateral\u0026#34; elif nums[0] == nums[1] or nums[1] == nums[2]: return \u0026#34;isosceles\u0026#34; return \u0026#34;scalene\u0026#34; For more Solutions, please visit my website.\n","permalink":"https://tzj2006.github.io/leetcode/2025-05-19/","summary":"Do what the question ask!","title":"LeetCode Daily Question 2025-05-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/painting-a-grid-with-three-different-colors/\nIntuition In this question, we find that m is relatively small compared with n. As $m \\le 5$, and $n \\le 1000$. Then we may consider to enumerate all solutions for a column, then elaborate it to the whole matrix. When elaborating it to the whole matrix, we figure out a thing: whatever a column is painted, the only influence is its next column, while future columns will not influence previous columns. This makes DP possible.\nApproach Therefore, here is our approach:\nFirst, we need to find out how many valid patterns are there in a column. Therefore, we can perform a dfs to search for all possible combinations. Second, we need to know which two patterns can be in adjcent columns, so we enumerate through each pair of patterns, and then test whether they can be in adjcent rows or not. Third, we use DP to elaborate from one column to the next. In this case, the DP formular will be: $DP[col][case_x] = \\sum DP[col-1][case_y]. \\forall \\text{casex and casey can be in two adjcent columns}$. Finally, all we need to do is to add up all the cases of the final column of DP to get our answer. Complexity Time complexity: $O(3^{2m} \\times n)$\nSpace complexity: $O(3^{2m})$\nCode class Solution: def colorTheGrid(self, m: int, n: int) -\u0026gt; int: pat = [] col = [0, 1, 2] def dfs(x, s): if x == m: pat.append(s) return for i in col: if x == 0 or s[x - 1] != i: dfs(x + 1, s + [i]) dfs(0, []) # till this step, we find all valid patterns for a column and store it in the pattern list. l = len(pat) valid = [[True for _ in range(l)] for _ in range(l)] for i in range(l): for j in range(i + 1, l): for k in range(m): if pat[i][k] == pat[j][k]: valid[i][j] = False break # till this step, we find all the pattern pairs that is valid. dp = [[0 for _ in range(l)] for _ in range(n)] mod = 1_000_000_007 for i in range(l): dp[0][i] = 1 # for column 0, each pattern is valid. for i in range(1, n): for x in range(l): for y in range(x + 1, l): if valid[x][y]: dp[i][x] = (dp[i][x] + dp[i-1][y]) % mod dp[i][y] = (dp[i][y] + dp[i-1][x]) % mod # we elaborate to the next column according to the DP formula. ans = 0 for i in range(l): ans = (ans + dp[-1][i]) % mod # finally, we add up all the answers. return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-05-18/","summary":"First DFS then DP!","title":"LeetCode Daily Question 2025-05-18"},{"content":"Mac 监控： 磁盘信息：\nbrew install smartmontools smartctl -a disk0 效果： CPU GPU占用信息：\nbrew install macmon macmon 效果展示： ","permalink":"https://tzj2006.github.io/bugjournal/2025-05-17/","summary":"\u003cp\u003eMac 监控：\n磁盘信息：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ebrew\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esmartmontools\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esmartctl\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e \u003cspan class=\"n\"\u003edisk0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e效果：\n\u003cimg alt=\"1\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-17/1.png\"\u003e\u003c/p\u003e\n\u003cp\u003eCPU GPU占用信息：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ebrew install macmon\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emacmon\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e效果展示：\n\u003cimg alt=\"2\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-05-17/2.png\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-05-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/letter-tile-possibilities/description/\nIntuition In this question, we want to know how many different tiles we can generate.\nApproach Therefore, we can use backtracking to enumerate all the solutions.\nComplexity Time complexity: $O(2^N)$, N is the length of the sequence.\nSpace complexity: $O(N)$, N is the length of the sequence.\nCode class Solution: def numTilePossibilities(self, tiles: str) -\u0026gt; int: counter = defaultdict(int) for ch in tiles: counter[ch] += 1 def dfs(counter): total = 0 for ch in counter: if counter[ch] == 0: continue # Choose character total += 1 counter[ch] -= 1 total += dfs(counter) counter[ch] += 1 # backtracking return total return dfs(counter) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-17/","summary":"\u003col start=\"1079\"\u003e\n\u003cli\u003eLetter Tile Possibilities\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-the-lexicographically-largest-valid-sequence/description/\nIntuition This question requires you to find a solution according to the requirements.\nApproach Note that $N \\le 20$, so we can use a brute search to find the answer.\nComplexity Time complexity: $O(N^N)$, N is the same definition as the question.\nSpace complexity: $O(N)$, N is the same definition as the question.\nCode Normal dfs solution class Solution: def dfs(self, pos, n, vis, pls): if pos == 2 * n - 1: return pls # if we enumerate to the end of the sequence, then we can return the answer. if pls[pos] != 0: return self.dfs(pos + 1, n, vis, pls) # if we place a number in the current position, then we can move to the next position. for i in range(n, 1, -1): # we enumerate from large to small so that we can get the largest sequence. if vis[i]: continue # if we use this number, then pass. if pos + i \u0026lt; 2 * n - 1 and pls[pos + i] == 0: pls[pos] = i pls[pos + i] = i vis[i] = True # put the number into the slot if it is available. ret = self.dfs(pos + 1, n, vis, pls) if ret is not None: return ret vis[i] = False pls[pos] = 0 pls[pos + i] = 0 if vis[1]: return None vis[1] = True pls[pos] = 1 ret = self.dfs(pos + 1, n, vis, pls) if ret is not None: return ret vis[1] = False pls[pos] = 0 # special check for 1 becuase 1 only puts into one slot. return None def constructDistancedSequence(self, n: int) -\u0026gt; List[int]: pls = [0] * (2 * n - 1) # pls is the sequence that we place numbers vis = [0] * (n+1) # visit is the sequence we test whether a number exists in the current sequence or not. return self.dfs(0, n, vis, pls) Faster solution for future use Note that the solution will not change when we input the same number, therefore, we can just store the answer we get and output it for every query.\nclass Solution: def constructDistancedSequence(self, n: int) -\u0026gt; List[int]: ans = [ [1], [2,1,2], [3,1,2,3,2], [4,2,3,2,4,3,1], [5,3,1,4,3,5,2,4,2], [6,4,2,5,2,4,6,3,5,1,3], [7,5,3,6,4,3,5,7,4,6,2,1,2], [8,6,4,2,7,2,4,6,8,5,3,7,1,3,5], [9,7,5,3,8,6,3,5,7,9,4,6,8,2,4,2,1], [10,8,6,9,3,1,7,3,6,8,10,5,9,7,4,2,5,2,4], [11,9,10,6,4,1,7,8,4,6,9,11,10,7,5,8,2,3,2,5,3], [12,10,11,7,5,3,8,9,3,5,7,10,12,11,8,6,9,2,4,2,1,6,4], [13,11,12,8,6,4,9,10,1,4,6,8,11,13,12,9,7,10,3,5,2,3,2,7,5], [14,12,13,9,7,11,4,1,10,8,4,7,9,12,14,13,11,8,10,6,3,5,2,3,2,6,5], [15,13,14,10,8,12,5,3,11,9,3,5,8,10,13,15,14,12,9,11,7,4,6,1,2,4,2,7,6], [16,14,15,11,9,13,6,4,12,10,1,4,6,9,11,14,16,15,13,10,12,8,5,7,2,3,2,5,3,8,7], [17,15,16,12,10,14,7,5,3,13,11,3,5,7,10,12,15,17,16,14,9,11,13,8,6,2,1,2,4,9,6,8,4], [18,16,17,13,11,15,8,14,4,2,12,2,4,10,8,11,13,16,18,17,15,14,12,10,9,7,5,3,6,1,3,5,7,9,6], [19,17,18,14,12,16,9,15,6,3,13,1,3,11,6,9,12,14,17,19,18,16,15,13,11,10,8,4,5,7,2,4,2,5,8,10,7], [20,18,19,15,13,17,10,16,7,5,3,14,12,3,5,7,10,13,15,18,20,19,17,16,12,14,11,9,4,6,8,2,4,2,1,6,9,11,8] ] return ans[n - 1] ","permalink":"https://tzj2006.github.io/leetcode/2025-02-16/","summary":"This is an NP Complete question","title":"LeetCode Daily Question 2025-02-16"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/sort-colors/\nIntuition In this case, we know that there are only three elements in the list, so we can use bucket sort to solve this problem.\nApproach All we need is to use a bucket to calculate the number of times each number exists, then we put these numbers into the array.\nComplexity Time complexity: $O(N)$, N is the length of the array. Space complexity: $O(Num)$, Num is the number of different numbers. Code class Solution: def sortColors(self, nums: List[int]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; # nums.sort() cnt = [0,0,0] for num in nums: cnt[num] += 1 cnt[1] += cnt[0] cnt[2] += cnt[1] cur = 0 for i in range(len(nums)): while i \u0026gt;= cnt[cur]: cur += 1 nums[i] = cur ","permalink":"https://tzj2006.github.io/leetcode/2025-05-17/","summary":"\u003col start=\"75\"\u003e\n\u003cli\u003eSort Colors\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-05-17"},{"content":"Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (ACT Algorithm) 主要论点 ALOHA 是一个开放源代码的低成本双臂远程操作硬件系统，整体成本低于 20,000 美元，主要由现成的机器人组件和少量 3D 打印部件组成。该系统支持精细、动态和接触丰富的任务，如穿拉链、乒乓球颠球和链条组装等。\n为了应对模仿学习中政策误差累积的问题，研究人员提出了 ACT 算法。该算法基于 Transformer 架构，采用条件变分自编码器（CVAE）框架，通过预测动作序列（即“动作块”）而非单步动作，减少了有效的预测范围，从而提高了学习效率和稳定性。\n在六个现实世界的精细操作任务中，如打开透明调味杯盖和插入电池等，ALOHA 系统仅通过约 10 分钟的示范数据（约 50 次演示）就实现了 80% 至 90% 的成功率，展示了其在低成本硬件上的高效学习能力。\n创新点 低成本高性能：ALOHA 系统在成本控制的同时，仍能执行复杂的双臂操作任务，降低了高精度机器人研究的门槛。\n动作块预测 ：ACT 算法通过预测动作序列，减少了政策误差的累积，提高了模仿学习的稳定性和效率。\n快速学习能力：系统仅需少量的演示数据即可学习复杂任务，展示了高效的学习能力。\n解决的难点 高精度双臂系统价格昂贵，限制研究和数据获取 。\n模仿学习中长序列预测误差累积问题严重 。\n设计低于 $20,000 的双臂系统 ALOHA + 高效 Transformer 模仿学习方法 ACT。\n通过动作 chunking 预测提升长序列动作稳定性。\n还需要解决的难点 硬件精度限制 ：尽管系统成本低廉，但硬件精度的限制可能影响在更复杂任务中的表现。\n泛化能力：系统在面对未见过的任务或环境时的泛化能力仍需进一步提升。\n实时性能 ：在实际应用中，如何确保系统的实时响应能力和稳定性是一个挑战。\nAutoregressive Action Sequence Learning for Robotic Manipulation 主要论点 论文的核心思想是将机器人动作表示为序列数据，并通过自回归序列建模生成动作序列。为此，作者提出了两项关键技术：\nChunking Causal Transformer (CCT)：该模型扩展了传统因果变换器的单步预测能力，支持在一个步骤中预测多个动作“块”。这种方法提高了对不同控制频率任务的适应性，并通过减少自回归步骤提高了效率 Autoregressive Policy (ARP)：基于CCT，作者设计了ARP架构，用于生成混合动作序列，解决多种机器人操作任务。该架构在Push-T、ALOHA和RLBench等多种机器人操作环境中进行了评估，结果显示ARP作为通用架构，在所有测试基准中匹配或超越了特定环境下的最新技术，同时在计算和参数规模上更为高效 简而言之，CCT 可以预测未来的多个动作 创新点 多动作块预测机制：CCT模型引入了预测多个动作块的能力，使其能够处理不同类型和频率的动作数据，提高了模型的灵活性和效率\n混合动作序列设计：通过将不同类型的动作（如关节位置、2D像素坐标和末端执行器姿态）混合在一个序列中，并为每种动作类型使用不同的块大小，增强了模型对复杂任务的适应能力\n通用策略架构：ARP架构作为一个通用的策略架构，在多个不同的机器人操作环境中表现出色，显示出其广泛的适用性和高效性\n解决的难点 自回归策略效率低: 传统每次只预测一个动作的自回归方法效率低，难以适配高频任务。\n动作混合表示困难：连续值与离散值混合表示在序列学习中不易统一建模。\n还需要解决的难点 动作数据的异质性 ：机器人动作数据通常包括连续值和离散值，如何有效地将这些异质数据表示为序列，并进行建模，是一个挑战。\n高频控制任务的建模 ：现有的自回归架构在处理高频控制任务时存在限制，如何扩展模型以支持高频控制任务，需要进一步研究\n混合动作序列的生成与优化 ：在生成包含多种动作类型的混合序列时，如何确保各动作类型之间的协调性和整体序列的最优性，是一个需要解决的问题\nπ0: A Vision-Language-Action Flow Model for General Robot Control 主要论点 π₀模型的核心是将预训练的视觉-语言模型（VLM）与流匹配架构相结合，形成一个统一的视觉-语言-动作（VLA）模型，用于通用机器人控制。该模型通过在多个灵巧机器人平台（包括单臂、双臂和移动操纵器）的大型多样化数据集上进行训练，学习从视觉和语言输入到动作输出的映射关系。\n模型的训练分为两个阶段：\n预训练阶段 ：在大规模多样化的数据集上进行训练，学习通用的感知和语言理解能力。 微调阶段：在特定任务的高质量数据上进行微调，以提高在特定任务上的性能。 该模型在多个任务上进行了评估，包括折叠衣物、清洁桌子和组装盒子等，展示了其在零样本学习、语言指令遵循和新技能获取方面的能力。\n创新点 流匹配架构 ：引入流匹配技术生成连续的动作分布，适用于高频率和灵巧的任务。\n跨机器人平台训练 ：结合多种机器人类型的数据进行训练，使模型能够适应不同的机器人配置和动作表示。\n高效推理机制 ：模型设计允许高效的推理过程，通过缓存和重用注意力键值对来减少计算量，适应实时控制的需求。\n解决的难点 VLA模型对高频、连续动作建模困难。\n泛化能力差，多机器人平台适应性弱。\n引入流匹配（flow matching）机制，生成连续动作分布，替代离散token生成方式。\n在多机器人、多任务、多平台上训练实现跨平台泛化。\n还需要解决的难点 数据的多样性和质量：虽然模型在多个平台和任务上进行了训练，但如何进一步提高数据的多样性和质量，以增强模型的泛化能力，仍是一个挑战。\n高频动作控制的稳定性 ：在高频率控制任务中，如何确保模型生成的动作序列的稳定性和准确性，需要进一步研究。\n模型的可扩展性和部署 ：如何将该模型部署到实际的机器人系统中，并确保其在不同硬件平台上的性能和效率，是实现其实际应用的关键。\nFAST: Efficient Action Tokenization for Vision-Language-Action Models (Chunking) 主要论点 传统的VLA模型在处理连续的机器人动作信号时，通常采用逐维、逐时间步的简单分箱（binning）策略进行离散化。然而，这种方法在面对高频率、精细操作任务时表现不佳，主要原因在于连续动作之间的强相关性导致模型难以有效学习。\n为解决这一问题，作者提出了FAST（Frequency-space Action Sequence Tokenization）方法，其核心思想包括：\n离散余弦变换（DCT） ：将连续的动作序列转换到频域，捕捉动作信号的主要频率成分，从而减少时间上的冗余信息。（关键帧技术） 量化与字节对编码（BPE） ：对DCT系数进行量化，并采用BPE进行压缩，生成信息密度更高的离散动作标记序列。 此外，作者还推出了 FAST+ ，一个在100万个真实机器人动作轨迹上训练的通用动作标记器，能够适用于多种机器人类型和控制频率的动作序列\n创新点 频域压缩的动作离散化：首次将DCT应用于机器人动作序列的离散化，有效减少了时间上的冗余信息，提高了模型对高频率动作的学习能力。\n通用动作标记器FAST+ ：通过在大规模、多样化的机器人动作数据上训练，FAST+实现了对不同机器人平台和任务的广泛适应性，减少了对特定任务手工设计标记器的需求。\n显著提升训练效率**** ：与传统的扩散模型相比，采用FAST的自回归VLA模型在训练时间上减少了多达5倍，同时在多个任务上达到了相当甚至更优的性能。\n解决的难点 传统动作token推理慢 。\n无法处理高频控制任务和长序列建模 。\n提出 DCT + BPE 的动作频域压缩方法（FAST），大幅压缩动作序列长度，提升效率。\n预训练通用tokenizer（FAST+）跨机器人平台迁移能力强。\n还需要解决的难点 高频动作的精确重建 ：虽然FAST在压缩动作序列方面表现出色，但在某些需要高精度控制的任务中，如何确保压缩后的动作序列能够准确还原原始动作，仍需进一步研究。\n（可不可以设置不同的专家模型，交由模型来判断应该使用原始序列还是压缩后的序列）\n与其他模型架构的兼容性 ：FAST主要与自回归VLA模型结合使用，其在其他类型的模型架构（如非自回归模型）中的表现和适应性尚待探索。\n实时控制的延迟问题 ：在实际机器人控制中，动作的生成和执行需要满足实时性要求，FAST在实际部署中可能面临延迟带来的挑战。\nπ0.5: a Vision-Language-Action Model with Open-World Generalization (More data, multimodel) Basically, 更强的π0。\n更强的 model, 更多的数据\n更强的开放世界泛化能力\nOpenVLA: An Open-Source Vision-Language-Action Model (远程连接 + Chunking + Dino) 主要论点 OpenVLA 是一个拥有 70 亿参数的开源 VLA 模型，基于 Llama 2 语言模型，并结合了 DINOv2 和 SigLIP 的预训练视觉特征。该模型在 Open X-Embodiment 数据集中的 97 万个真实机器人操作轨迹上进行了训练，涵盖了多种机器人形态、任务和场景。OpenVLA 能够直接控制多种机器人，并通过参数高效的微调方法快速适应新的机器人配置\n创新点 开源性与可访问性 ：OpenVLA 是首个完全开源的 VLA 模型，提供了模型检查点、微调笔记本和 PyTorch 训练代码，支持在 Open X-Embodiment 数据集上进行大规模训练。\n融合视觉编码器：模型采用了融合 DINOv2 和 SigLIP 特征的视觉编码器，结合了空间和语义信息，以增强模型对视觉输入的理解能力。\n高效的动作离散化方法：OpenVLA 通过将连续的机器人动作映射到离散的标记上，并使用语言模型的分词器进行处理，提高了模型的训练和推理效率。\n强大的泛化能力 ：在 29 个任务和多种机器人形态上，OpenVLA 的任务成功率比封闭模型 RT-2-X（55B 参数）高出 16.5%，同时参数数量减少了 7 倍。\n解决的难点 缺乏可公开访问、端到端训练的开源VLA模型 。\n跨机器人统一控制策略训练成本高 。\n基于 Llama2 + DINOv2 构建的70亿参数模型开源并附完整工具链。\n使用统一token表示动作（通过BPE），促进模块化、可复用性和可迁移性。\n还需要解决的难点 对未见机器人形态的泛化能力有限 ：OpenVLA 在预训练数据中未包含的机器人形态上，零样本泛化能力有限，需要通过微调适应新的机器人配置。\n动作离散化的精度问题 ：尽管采用了高效的动作离散化方法，但在某些需要高精度控制的任务中，如何确保离散化后的动作能够准确还原原始动作，仍需进一步研究。\n实时控制的计算需求：在实际机器人控制中，动作的生成和执行需要满足实时性要求，OpenVLA 在实际部署中可能面临计算资源和延迟的挑战。\nHi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models (LLM分层) 主要论点 Hi Robot 系统采用了两层策略结构，分别对应心理学中丹尼尔·卡尼曼提出的“系统1”（快速、直觉反应）和“系统2”（慢速、深度推理）模型：\n高层策略（System 2） ：利用预训练的视觉-语言模型（VLM），对复杂的自然语言指令进行解析，结合视觉观察，生成一系列中间步骤的低层语言命令。这一层具备推理能力，能够处理多阶段任务，并根据用户的实时反馈进行调整。 低层策略（System 1） ：基于 π₀ 模型，执行高层策略生成的原子级命令。该层专注于具体动作的执行，如“抓取杯子”，并能根据实时的视觉和状态信息进行快速反应。 该系统在三种不同的机器人平台上进行了测试，包括单臂、双臂和双臂移动机器人，任务涵盖清理杂乱的桌面、制作三明治和杂货购物等，展示了其在处理复杂任务和适应用户反馈方面的能力。\n创新点 分层架构设计：通过将任务解析与动作执行分离，Hi Robot 能够更有效地处理复杂指令和动态反馈，提高了系统的灵活性和适应性。\n“自言自语”机制 ：高层策略在生成低层命令前，会进行内部的语言推理过程，类似于人类在执行复杂任务前的思考过程，增强了系统的推理能力。\n实时反馈整合：系统能够在任务执行过程中接受并理解用户的实时语言反馈，如“那不是垃圾”，并据此调整当前的行为策略。\n广泛的任务适应性 ：通过在多种机器人平台和任务上的测试，验证了该系统在处理多样化任务和环境中的泛化能力。\n解决的难点 单一层次VLA模型无法处理长时序、开放式自然语言指令 。\n缺乏对人类实时反馈的理解和响应能力 。\n引入分层策略（System 1/2）模拟人类推理流程，实现任务分解与动作控制解耦。\n支持语言实时中断和调整（如用户打断、重说等）。\n还需要解决的难点 高层与低层策略的协同：确保高层生成的命令能够被低层准确执行，尤其是在面对未见过的任务或环境时，仍需进一步优化两层之间的接口和协同机制。\n实时性与计算资源的平衡：高层策略的推理过程可能带来计算延迟，如何在保证系统实时响应的同时，维持高层策略的推理深度，是一个需要权衡的问题。\n数据多样性与泛化能力 ：尽管系统在多个任务上表现良好，但在面对极端或未见过的指令和环境时，其泛化能力仍需通过更多样化的数据训练进行提升。\nDiffusion Policy: Visuomotor Policy Learning via Action Diffusion (Diffusion Policy) 主要论点 传统的机器人策略学习方法在处理多模态动作分布和高维动作空间时常面临挑战。为此，作者提出了 Diffusion Policy ，将机器人视觉-动作策略表示为条件去噪扩散过程（Conditional Denoising Diffusion Process）\n该方法的核心思想是：\n训练阶段：在动作空间中添加噪声，并训练模型预测如何从噪声中恢复出原始动作序列。 推理阶段：从随机噪声开始，逐步去噪，生成符合当前视觉观察的动作序列。 通过这种方式，Diffusion Policy 能够有效建模多模态动作分布，适应高维动作空间，并在多个机器人操作任务中表现出色。\n创新点 多模态动作建模能力 ：Diffusion Policy 能够自然地处理多种可能的动作路径，适应复杂任务中的多样性。 高维动作空间适应性：通过在整个动作序列上进行建模，方法在处理高维动作空间时表现稳定，避免了传统方法在高维空间中的不稳定性。 稳定的训练过程：相比于传统的能量模型（Energy-Based Models），Diffusion Policy 避免了对归一化常数的估计，提升了训练的稳定性。 闭环控制机制：结合递归视野控制（Receding Horizon Control），实现了在任务执行过程中的动态重规划，提高了系统的鲁棒性。 解决的难点 能量模型训练不稳定、归一化常数难估计 。\n无法处理多模态动作空间（例如多个解法、连续空间）。\n用扩散模型建模动作序列的分布，天然适配多模态分布。\n在多个真实世界任务中验证其高鲁棒性与学习稳定性。\n还需要解决的难点 推理速度 ：由于扩散过程需要多步去噪，推理速度较慢，可能限制了在实时控制任务中的应用。\n与其他模型的集成 ：如何将 Diffusion Policy 与其他感知或语言模型有效集成，以处理更复杂的任务，仍需进一步研究。\nRT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE 主要论点 RT-1 模型的核心目标是实现一个通用的机器人控制策略，能够处理多种任务、对象和环境。为此，研究团队收集了一个包含 13 台机器人、历时 17 个月、涵盖 700 多个任务、共计 13 万个操作示例的大规模数据集。这些数据包括机器人在实际环境中执行任务的图像序列、自然语言指令和对应的动作序列。\nRT-1 的架构包括以下关键组件：\n图像编码器 ：使用预训练的 EfficientNet-B3 模型处理输入图像，并通过 FiLM 层融合语言指令，提取与任务相关的视觉特征。 TokenLearner 模块 ：对图像特征进行压缩，生成一组紧凑的 token，提高模型的推理效率。 Transformer 模型 ：接收图像和语言的 token 输入，输出离散化的动作 token，控制机器人的执行。 在训练过程中，RT-1 通过模仿学习（Imitation Learning）方法，从收集的示例中学习任务的执行策略。模型能够以每秒 3 次的频率进行闭环控制，直到任务完成或达到预设的时间步数。\n创新点 大规模多任务学习 ：RT-1 在一个包含 13 万个示例、700 多个任务的大规模数据集上进行训练，展示了 Transformer 架构在机器人控制中的强大能力。\n统一的输入输出表示 ：将图像、语言指令和动作统一表示为 token 序列，使得模型能够处理多模态输入，并生成相应的动作输出。\n高效的推理机制：通过 TokenLearner 模块对图像特征进行压缩，显著提高了模型的推理速度，满足实时控制的需求。\n强大的泛化能力 ：RT-1 在未见过的任务、环境和对象上表现出色，展示了其在零样本学习和迁移学习方面的潜力。\n解决的难点 大规模跨任务学习难以整合语言、视觉、动作三模态数据 。\n模型在未见任务和环境上的泛化能力弱 。\n在 130,000+ 真实机器人操作轨迹上训练的 Transformer 模型。\nTokenLearner + FiLM结构实现多模态融合，提升跨任务泛化。\n还需要解决的难点 对新任务的泛化能力有限：尽管 RT-1 在多种任务上表现良好，但在面对完全未见过的任务时，其泛化能力仍有待提升。\n对复杂操作的适应性：当前模型主要针对相对简单的操作任务，对于需要高精度和复杂操作的任务，其性能尚未验证。\n实时性与计算资源的平衡：虽然模型在推理速度上有所优化，但在资源受限的实际部署环境中，如何进一步提高实时性仍是一个挑战。\nOpen-TeleVision: Teleoperation with Immersive Active Visual Feedback 主要论点 Open-TeleVision 系统结合了 VR 设备与机器人控制，允许操作者通过 VR 头显实时感知机器人的立体视觉环境，并将自身的手臂和手部动作映射到机器人上，实现如同“身临其境”的操作体验。\n系统的核心组件包括：\n主动视觉反馈 ：机器人头部配备可动的立体 RGB 摄像头，能够根据操作者的头部运动调整视角，提供实时的第一人称3D 观察。 动作映射机制 ：通过逆运动学（IK）算法和 dex-retargeting 技术，将操作者的手部关键点转换为机器人关节角度，实现精确的动作控制。 远程操作能力 ：系统支持通过互联网进行远程控制，操作者无需与机器人处于同一地点。 在实验中，研究团队使用该系统在两个不同的人形机器人（Unitree H1 和 Fourier GR-1）上完成了包括罐头分类、罐头插入、毛巾折叠和物品卸载等四项长时序、精细操作任务，并成功部署了模仿学习策略。\n创新点 沉浸式第一人称视角：通过主动立体视觉技术，操作者能够以第一人称视角直观感知机器人周围环境，增强了空间感知能力和操作**直觉。\n高精度动作映射：结合逆运动学和 dex-retargeting 技术，实现了操作者动作到机器人动作的高精度映射，支持多指灵巧手的控制。\n远程操作与数据采集 ：系统支持远程操作，操作者可以跨地域控制机器人，并收集高质量的操作数据，促进模仿学习的发展。\n解决的难点 传统远程遥操作缺乏空间沉浸感与操作精度 。\n低质量遥操作数据不利于模仿学习 。\n主动式立体视觉反馈 + VR 映射手部运动实现高保真远程控制。\n为高质量模仿学习数据采集（精细双臂、多指操作）提供有效工具。\n还需要解决的难点 my Questions 如何评估一个机械臂的能力呢？ 高频动作，怎么样算高频呢？\n为什么说机器的精度很重要，到底有多重要呢？\n输出给机器臂的值可以是连续的而不是离散的吗？对应 pi 0\n在不同机器人上训练的难点是？\n为什么需要端到端模型呢？\n机器人上的设备能支持多大的模型运行呢？\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-14/","summary":"Summary of Robotics papers","title":"Bug Journal 2025-05-25"},{"content":" 如何控制机器人的位置？机器人控制算法的输出是什么？是机械臂的位置，速度，加速度的值吗。另外，动作分布 50Hz 的意思是？机械臂的移动是一个分布吗 多模态数据对齐的时候有什么难点，需要网络有什么样的特性呢？ 一般来说泛化能力是如何实现的，加入噪声吗？ 想法：可不可以用视频数据做一个增强：比如用 LLM 总结视频里的手都做了什么，然后以这个总结为 prompt 告诉机械臂要做什么\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-09/","summary":"遇到的问题","title":"Bug Journal 2025-05-09"},{"content":"Citation Content mainly from here: 具身智能基础技术路线.\nPart1: 场景理解 这一部分主要是机器人对环境输入的理解\n希望让机器人识别出环境中比较重要的部分， 比如：要抓起的物件\n检测分割 检测分割的算法如：SAM, Open-VOC Detection 可以比较好地检测和分割输入中需要的部分和其他部分\n多模态分割 多模态大模型可以理解更加复杂的语言，比如“汽车旁的穿蓝色衣服的男人”， 并且可以做到像素级别。\nprompt 的多样化可以让分割更有针对性，模型可以更好地理解和定位\nPart2: 数据收集和引导 视频学习 优点：不需要遥控，数据量更广\n缺点：需要让机器学习视频中的动作，训练难度更高\nVR遥控 优点：可以让人手模拟，并且可以“手把手”教\n缺点：数据量少，需要每一种新情况都需要手动模拟\n环境模拟 优点：环境搭建完成之后就有无限的数据集\n缺点：环境搭建复杂，并且无法模拟特殊情况\n动作执行 生成式模仿学习 把每一个时间点的信息丢进去学习\nDiffusion policy 在每一个时间点之间做 diffusion\nAffordance 输入每一个 region可以被如何操作：比如输入一个瓶子可以被夹起来\n这样的话就不再是low level joint, 而是控制器实现的目标\nQ\u0026amp;A from LLM 将 prompt 拆解，并且可以在文本中提取出机器人最后要做的事情\nLanguage Correction 在人类观察的时候用语言/语音帮助机器人更好地完成任务\n世界模型 预测下一步会发生什么\n我对具身智能的理解 实际上是一个 Agent, 输入是用户的指令以及整个环境。\n输出是对环境做出一些改变，比如让机器人举起一个杯子。\n所以有这些需要做的：识别输入，做出反应，强化学习\nRandom Thoughts 新的LLM生成方式：token -\u0026gt; 大纲 -\u0026gt; 句子 -\u0026gt; 文章 pyramid\n","permalink":"https://tzj2006.github.io/bugjournal/2025-05-08/","summary":"初识具身智能","title":"Bug Journal 20250508"},{"content":"Talk 6: scDesign3: Semi-synthetic Negative \u0026amp; Positive Control 数据不够的时候需要一些 simulated data\n那这时候 simlator 就需要 interpretable \u0026amp; realistic (real data characteristic \u0026amp; contains ground truth data)\ne.g. single cell RNA simulator -\u0026gt; 考虑 gene gene correlation -\u0026gt; 考虑别的 cell types and omics -\u0026gt; RNA count to RNA read\n这个可以用来给数据预处理+降噪\nTalk 7: spacial omics data 理解 low-rank property of Hi-C chromatin contact maps.\n数据上有 3 维（question what type of data / or do you have ground truth of the location of the spatial data? are the model predicting the location or the gene expression or both of them? ）\n但是问题是这个数据很 sparce. e.g. 5kb 的数据就已经是丢失了 99.5%的数据了\n所以就用了一些技术来降噪\u0026amp;还原\n之前的 AI模型：训练完了之后就没有用生物相关的信息来预测我们想要的\n现在的 AI 模型，可以自己思考来相处一个 hypothsis，并且验证他\ne.g. 用 interatctions\n要解决的问题：\nblack box ML models, overwhelming hypothesis, false positive\nML model -\u0026gt; intreatcion 可能的值\n现在提出一个预测的可能的可行性\n这个值是 false positive / total accept 值越低越好\n那如何预测 false positive 呢？这样，对于每一个 feature, 分开预测\n然后再liangliang 预测，然后看结合了之后是好了还是坏了\n还有一个办法，就是用元数据 + 假数据\n如果元数据和假数据得出来的差距大，那就说明这个元数据没什么用\n否则就说明这个有用\nIntegration of histology and multi-plex for understanding pancreatic cancer.\nimage are usually 2D, that is not connected to other tissues.=m telling us there is not spatial relationship.\nquestion is every slice a slice near another.\nSpacial transcriptomics platforms (PASTA))\n太稀疏啦\n怎么办呢？用 model concentrate 喽\n但是这些 model 并不一定准（你想，要用 500gene去补 20k,太难了）\n并且这些 model 没有用 cell type / pathway 信息\npathway loss + gene similarity + pathway expression + location (这不就是 STHD吗)\n但是这里预测的不是 cell type,是 gene expression\nTalk 8: AI in medication AI 可以很好的数据，但是要求很高\n所以要用很聪明的方法使用这些 AI\n另外，预测出了结果之后要做什么呢？\nAI 还可以用来提供数据\n生成式 AI VAE \u0026amp; Transformer (需要有 meaningful order/sequence)\nDiffusion model: forward (加噪音) and backward (去噪音) （不需要 data 是 ordered, 但是 sampling 需要很多计算资源）\n如果 black box prediction model 不正确，我应该如何 make a valid inference?\nEnd to end scalable integrative analysis ","permalink":"https://tzj2006.github.io/bugjournal/2025-03-28/","summary":"\u003ch1 id=\"talk-6-scdesign3-semi-synthetic-negative--positive-control\"\u003eTalk 6: scDesign3: Semi-synthetic Negative \u0026amp; Positive Control\u003c/h1\u003e\n\u003cp\u003e数据不够的时候需要一些 simulated data\u003c/p\u003e\n\u003cp\u003e那这时候 simlator 就需要 interpretable \u0026amp; realistic (real data characteristic \u0026amp; contains ground truth data)\u003c/p\u003e\n\u003cp\u003ee.g. single cell RNA simulator -\u0026gt; 考虑 gene gene correlation -\u0026gt; 考虑别的 cell types and omics -\u0026gt; RNA count to RNA read\u003c/p\u003e\n\u003cp\u003e这个可以用来给数据预处理+降噪\u003c/p\u003e\n\u003ch1 id=\"talk-7-spacial-omics-data\"\u003eTalk 7: spacial omics data\u003c/h1\u003e\n\u003cp\u003e理解 low-rank property of Hi-C chromatin contact maps.\u003c/p\u003e\n\u003cp\u003e数据上有 3 维（question what type of data / or do you have ground truth of the location of the spatial data? are the model predicting the location or the gene expression or both of them? ）\u003c/p\u003e","title":"Bug Journal 2025-03-28"},{"content":"MCBIOS Conference Day 1:\nTalk 1: sharing data: https://datacommons.cancer.gov/ 介绍了很多数据：特点是比较多，比较新，并且user-friendly. 还有 NIH founding 可以 use start-up server.\nhttps://computational.cancer.gov/ 介绍了很多模型：都是用来预处理数据的\n比如有一个 AI-based toolbox (类似scanpy) 可以预处理所有数据\n还有 Automated Data Collection\nlink TBD\nLLM 翻译诊断结果\nTalk 2: Write code with Github Copilot. 你能用这个干嘛 当然是写代码啦，还能干嘛（\n好处是可以直接在你的 IDE 里面生成(虽然现在 ChatGPT.app 也可以了，(反正都是一家的[doge])\n比如说你可以先写一段注释来让 Copilot 生成你想要的代码\n然后选中这段代码并且点击旁边的小星星来让 Copilot 更改这段代码\n用于重复的项目效果更佳。比如分离数据集\n修bug还挺好用的(虽然有时候越修越多[doge])\n修改代码的语言：比如把 R code 换成 python code.\n还可以 explain what the code is doing (by using /explain).\n用 /doc 来写注释\n要不要用这个 想用就用，只是这个效果不一定好罢了[doge]\nTalk 3: Graph is a link between spatial omics applications and pixel graph | cell graph | spot graph\n有 graph 就有 matrix\nfrequency 的大小决定了是否 pattern（？）\nspatial variable gene thereshold\nTalk 4: Some random ideas 结合两个 LLM, e.g. Gemini \u0026amp; ChatGPT\n在小程序中内置一个 LLM 让它当 agent.\nTalk 5: scPerb style vector 可以说是一个 noise, 然后我们希望一个 neural network 能学习到这个 noise. 并且这个学习的过程是 cell type specific 的。\n","permalink":"https://tzj2006.github.io/bugjournal/2025-03-27/","summary":"\u003cp\u003eMCBIOS Conference Day 1:\u003c/p\u003e\n\u003ch1 id=\"talk-1-sharing-data\"\u003eTalk 1: sharing data:\u003c/h1\u003e\n\u003ch2 id=\"httpsdatacommonscancergov\"\u003e\u003ca href=\"https://datacommons.cancer.gov/\"\u003ehttps://datacommons.cancer.gov/\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e介绍了很多数据：特点是比较多，比较新，并且user-friendly. 还有 NIH founding 可以 use start-up server.\u003c/p\u003e\n\u003ch2 id=\"httpscomputationalcancergov\"\u003e\u003ca href=\"https://computational.cancer.gov/\"\u003ehttps://computational.cancer.gov/\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e介绍了很多模型：都是用来预处理数据的\u003c/p\u003e\n\u003cp\u003e比如有一个 AI-based toolbox (类似scanpy) 可以预处理所有数据\u003c/p\u003e\n\u003cp\u003e还有 Automated Data Collection\u003c/p\u003e\n\u003cp\u003elink TBD\u003c/p\u003e\n\u003cp\u003eLLM 翻译诊断结果\u003c/p\u003e\n\u003ch1 id=\"talk-2-write-code-with-github-copilot\"\u003eTalk 2: Write code with Github Copilot.\u003c/h1\u003e\n\u003ch2 id=\"你能用这个干嘛\"\u003e你能用这个干嘛\u003c/h2\u003e\n\u003cp\u003e当然是写代码啦，还能干嘛（\u003c/p\u003e\n\u003cp\u003e好处是可以直接在你的 IDE 里面生成(虽然现在 ChatGPT.app 也可以了，(反正都是一家的[doge])\u003c/p\u003e\n\u003cp\u003e比如说你可以先写一段注释来让 Copilot 生成你想要的代码\u003c/p\u003e\n\u003cp\u003e然后选中这段代码并且点击旁边的小星星来让 Copilot 更改这段代码\u003c/p\u003e\n\u003cp\u003e用于重复的项目效果更佳。比如分离数据集\u003c/p\u003e\n\u003cp\u003e修bug还挺好用的(虽然有时候越修越多[doge])\u003c/p\u003e\n\u003cp\u003e修改代码的语言：比如把 R code 换成 python code.\u003c/p\u003e\n\u003cp\u003e还可以 explain what the code is doing (by using /explain).\u003c/p\u003e","title":"Bug Journal 2025-03-27"},{"content":"Today\u0026rsquo;s Problem https://leetcode.com/problems/check-if-grid-can-be-cut-into-sections\nIntuition This question asks about merging sections. In this case, if we smash it into 1D array, it just means \u0026ldquo;Is there more than two gaps inside the section?\u0026rdquo;\nApproach Therefore, we can sort the list, and then iterate the whole list and see whether there is a gap between the section we already iterated and the new section. If there is, then add 1, else, merge this new section to our old section. The thing is that you need to do it twice.\nComplexity Time complexity: $O(N)$, N is the length of rectangles. Space complexity: $O(1)$. Code class Solution: def checkValidCuts(self, n: int, rectangles: List[List[int]]) -\u0026gt; bool: N = len(rectangles) def get_res(a,b): rectangles.sort(key = lambda x: (x[a], x[b])) gapCnt,maxPos,l = 0,1,0 while(l \u0026lt; N): while(l \u0026lt; N and rectangles[l][a] \u0026lt; maxPos): maxPos = max(maxPos, rectangles[l][b]) l += 1 if l == N: break else: gapCnt += 1 maxPos = rectangles[l][b] # print(a,l) if gapCnt \u0026gt; 1: return True return False return get_res(0,2) or get_res(1,3) ","permalink":"https://tzj2006.github.io/leetcode/2025-03-25/","summary":"\u003col start=\"3394\"\u003e\n\u003cli\u003eCheck if Grid can be Cut into Sections\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-03-25"},{"content":"Intuition In this question, we find that if we flip a coin twice, then it is the same as flipping it zero times. Moreover, the only three ways to flip a coin is: Flip this coin Flip the coin before Flip the coin before this coin This means that if we pass this coin, we can no longer flip this coin. Approach In this case, we can iterate from front to end and flip every coin that is 0, and check whether the whole array is 1 at the end. Why is this method correct then? First we know that we cannot flip a coin after we pass this coin.\nThis means that we must flip this coin. If we do not flip this coin, then this coin will remain 0, which does not satisfy the quetion. Therefore, this step is required, missing this step will not give us the array we need.\nWe can also prove that this will lead us to the result for every array that can achieve this step. Because you have to flip this coin no matter what operation you did.\nComplexity Time complexity: $O(N)$, N is the length of the array. Space complexity: $O(1)$. Code class Solution: def minOperations(self, nums: List[int]) -\u0026gt; int: cnt = 0 for i in range(len(nums) - 2): if nums[i] == 0: cnt += 1 nums[i] = 1 nums[i + 1] = 1 - nums[i + 1] nums[i + 2] = 1 - nums[i + 2] if nums[-1] == 1 and nums[-2] == 1: return cnt return -1 ","permalink":"https://tzj2006.github.io/leetcode/2025-03-19/","summary":"\u003col start=\"3191\"\u003e\n\u003cli\u003eMinimum Operations to Make Binary Array Elements Equal to One I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-03-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-punishment-number-of-an-integer/description/\nIntuition Note that all the combinations of a number that is less than $1000^2$ is $2^5 = 32$. Which means that if we use a dfs to decide whether to choose to break from one interval or not cost at most 32 for one number.\nSince we only have $N \\leq 1000$, we can solve it with brute method.\nApproach Use a dfs to check whether it cawn be a punishment number or not.\nComplexity Time complexity: $O(N\\times 2^{log(N)})$, N is the same representation as the description.\nSpace complexity: $O(1)$.\nCode class Solution: def punishmentNumber(self, n: int) -\u0026gt; int: def check(x, now, s, nows, cnt): if now == 0: return (s + nows) == x if s \u0026gt; x: return False flag = check(x, now // 10, s, nows + now % 10 * (10 ** cnt), cnt + 1) if flag: return True flag |= check(x, now // 10, s + nows, now % 10, 1) return flag ans = 0 for i in range(n + 1): if check(i, i*i, 0, 0, 0): ans += i * i return ans class Solution: def punishmentNumber(self, n: int) -\u0026gt; int: punishmentNumbers = [0, 1, 9, 10, 36, 45, 55, 82, 91, 99, 100, 235, 297, 369, 370, 379, 414, 657, 675, 703, 756, 792, 909, 918, 945, 964, 990, 991, 999, 1000] ans = 0 for x in punishmentNumbers: if x \u0026gt; n: return ans ans += x * x return ans Result of normal solution: Result of fastest solution: ","permalink":"https://tzj2006.github.io/leetcode/2025-02-15/","summary":"\u003col start=\"2698\"\u003e\n\u003cli\u003eFind the Punishment Number of an Integer\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-15"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/product-of-the-last-k-numbers/description/\nIntuition Situation 1: if there is no 0: In this case, we can just use a prefix multiplication to do what the question asks. Situation 2: if there is 0: Then it would be 0! Therefore, all we need is to just check whether there is a 0 in the last k numbers of the stream. If yes, them just return 0.\nApproach Use an array to store the multiplication prefix. Check whether there is a zero in the last k streams. Complexity Time complexity: $O(Q)$, Q means the number of operations.\nSpace complexity: $O(Q)$, Q means the number of operations.\nCode class ProductOfNumbers: def __init__(self): self.q = [] self.mul = 1 def add(self, num: int) -\u0026gt; None: self.mul *= num self.q.append(self.mul) if num == 0: self.q = [] self.mul = 1 def getProduct(self, k: int) -\u0026gt; int: if k \u0026gt; len(self.q): return 0 if k == len(self.q): return self.mul return self.mul // self.q[-k - 1] # Your ProductOfNumbers object will be instantiated and called as such: # obj = ProductOfNumbers() # obj.add(num) # param_2 = obj.getProduct(k) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-14/","summary":"\u003col start=\"1352\"\u003e\n\u003cli\u003eProduct of the Last K Numbers\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-14"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-operations-to-exceed-threshold-value-ii/description\nIntuition Do what the question ask using heap.\nApproach Get the minimum two from the list using heap. Put back value $2 * min(x, y) + max(x, y)$ to the heap. If the value you get is all larger or equal to k, then it is all done. Complexity Time complexity: $O(N\\times log(N))$, N is the length of the sequence.\nSpace complexity: $O(1)$, by using heapify, there is no external storage.\nCode class Solution: def minOperations(self, nums: List[int], k: int) -\u0026gt; int: ans = 0 heapify(nums) x = heappop(nums) while(len(nums) \u0026gt; 0 and x \u0026lt; k): y = heappop(nums) nx = min(x, y) * 2 + max(x, y) heappush(nums, nx) x = heappop(nums) ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-13/","summary":"\u003col start=\"3066\"\u003e\n\u003cli\u003eMinimum Operations to Exceed Threshold Value II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-13"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/max-sum-of-a-pair-with-equal-sum-of-digits/description\nIntuition In this question, we can simulate the process described in the question, and then get the answer.\nApproach First write a count function that counts the sum of every integer. Use a dictionary to store the top two values. Compare the current number with two numbers stored in the dictionary. Update ans (initialized by -1), note that we need at least two numbers in the dictionary before we can update the answer. Complexity Time complexity: $O(Nlog(M))$, N is the length of the sequence, M is the maximum number.\nSpace complexity: $O(N)$.\nCode class Solution: def maximumSum(self, nums: List[int]) -\u0026gt; int: def cnt(x): res = 0 while(x \u0026gt; 0): res += x % 10 x //= 10 return res dic = dict() ans = -1 for x in nums: nx = cnt(x) if nx in dic: if x \u0026gt; dic[nx][0]: dic[nx][1] = dic[nx][0] dic[nx][0] = x if dic[nx][1] \u0026gt; 0: ans = max(ans, dic[nx][0] + dic[nx][1]) elif x \u0026gt; dic[nx][1]: dic[nx][1] = x ans = max(ans, dic[nx][0] + dic[nx][1]) else: dic.update({nx: [x, 0]}) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-12/","summary":"\u003col start=\"2342\"\u003e\n\u003cli\u003eMax Sum of a Pair With Equal Sum of Digits\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/remove-all-occurrences-of-a-substring/description/\nIntuition In this case, we need to delete all the occurance of \u0026ldquo;part\u0026rdquo; in \u0026ldquo;s\u0026rdquo;. Therefore, we can check whether \u0026ldquo;s\u0026rdquo; contains \u0026ldquo;part\u0026rdquo;. Then we can delete it from the string.\nApproach Method 1. You can use a stack to do that. When you detect that your stack input a string that is the same to \u0026ldquo;part\u0026rdquo;, then we can delete the string from the stack.\nMethod 2. You can use the python function to find and delete \u0026ldquo;part\u0026rdquo; from the original string S.\nComplexity Time complexity: $O(N\\times M)$, N is the length of s, M is the length of part.\nSpace complexity: $O(1)$\nCode class Solution: def removeOccurrences(self, s: str, part: str) -\u0026gt; str: st = [] N = len(part) for ch in s: st.append(ch) if len(st) \u0026gt;= N: flag = True for i in range(1, N + 1): if st[-i] != part[-i]: print(st[-i], part[-i]) flag = False break if flag: for i in range(N): st.pop() return \u0026#39;\u0026#39;.join(st) Real Python class Solution: def removeOccurrences(self, s: str, part: str) -\u0026gt; str: while part in s: s = s.replace(part,\u0026#34;\u0026#34;,1) return s ","permalink":"https://tzj2006.github.io/leetcode/2025-02-11/","summary":"\u003col start=\"1910\"\u003e\n\u003cli\u003eRemove All Occurrences of a Substring\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-11"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/clear-digits/description/\nIntuition We need to pop the character before the digit in this question.\nApproach Therefore, all we need is just to utilize a stack.\nComplexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, N is the length of the string.\nCode class Solution: def clearDigits(self, s: str) -\u0026gt; str: st = [] for ch in s: if \u0026#39;0\u0026#39; \u0026lt;= ch and ch \u0026lt;= \u0026#39;9\u0026#39;: st.pop() else: st.append(ch) return \u0026#39;\u0026#39;.join(st) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-10/","summary":"\u003col start=\"3174\"\u003e\n\u003cli\u003eClear Digits\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-10"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-number-of-bad-pairs/description/\nIntuition In this question, we find that we need to find the pairs that has the same distance as the difference of nums.\nIn this case, if we distract the distance between these two numbers, then they would be the same.\nThat is: $j - i = nums[j] - nums[i] \\Longrightarrow nums[j] - j = nums[i] = i$.\nThen the question would be easy: we just subtract the index of every number in the list, and then found how many pairs of i,j in the nums array that has the same number.\nWe then subtract these counts from the total counts of answer.\nApproach Count all pairs of i,j; that is, $N \\times (N - 1)$, N is the length of the array. Subtract all the nums[i] by i. Count how many pairs of i,j has the same number. Subtract these i,j pairs from the original answer. Complexity Time complexity: $O(N)$. N is the length of the array.\nSpace complexity: $O(N)$. N is the length of the array.\nCode class Solution: def countBadPairs(self, nums: List[int]) -\u0026gt; int: nums = [nums[i] - i for i in range(len(nums))] cnt = Counter(nums) N = len(nums) ans = N * (N - 1) // 2 for v in cnt.values(): ans -= v * (v - 1) // 2 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-09/","summary":"\u003col start=\"2364\"\u003e\n\u003cli\u003eCount Number of Bad Pairs\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-09"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/design-a-number-container-system/description/\nIntuition In this question, we need to find the smallest index of the number in the list. Note that the index may change when dealing with the list. Because we need the smallest index, so we need a sorted datastructure. Trick: We can get a lazy tag that stores the number of each index, so that we can pop only when we find our current answer does not fullfill the requirement.\nApproach Store a dictionary that stores the sorted sequence of the numbers and the index. Store a dictionary of index and numbers pair. Check whether the answer is valid in the find function. Complexity Time complexity: $O(Q\\ times log(N))$, Q is the time of query, N is the size of the dictionary.\nSpace complexity: $O(Q)$, Q is the time of query.\nCode class NumberContainers: def __init__(self): self.lst = dict() self.idx = dict() def change(self, index: int, number: int) -\u0026gt; None: if number not in self.lst: self.lst.update({number: []}) heappush(self.lst[number], index) self.idx.update({index: number}) def find(self, number: int) -\u0026gt; int: if number not in self.lst: return -1 while self.lst[number]: currIndex = self.lst[number][0] if self.idx[currIndex] != number: heappop(self.lst[number]) else: return currIndex return -1 # Your NumberContainers object will be instantiated and called as such: # obj = NumberContainers() # obj.change(index,number) # param_2 = obj.find(number) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-08/","summary":"\u003col start=\"2349\"\u003e\n\u003cli\u003eDesign a Number Container System\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-number-of-distinct-colors-among-the-balls/description\nIntuition All we need to know is how many colors left in the list. Then we can store how many balls do one color have. If there are 0 balls left, then col_cnt -= 1. If there appears a new color, then col_cnt += 1.\nApproach Store a buket for every ball and every color. Change the color of one ball. If there are 0 balls left, then col_cnt -= 1. If there appears a new color, then col_cnt += 1. Complexity Time complexity: $O(N)$, N is the length of the query.\nSpace complexity: $O(N)$, N is the length of the query.\nPotential follow up question Now I want to change the color of a section? For example, now the imput change into (x,y,z), changing the color of the balls from x to y (inclusive) to z. Then tell me how many balls have distinct colors?\nCode class Solution: def queryResults(self, limit: int, queries: List[List[int]]) -\u0026gt; List[int]: col = 0 ans = [] visCol = dict() balCol = dict() for (x, y) in queries: if x in balCol: visCol[balCol[x]] -= 1 if visCol[balCol[x]] == 0: col -= 1 balCol.update({x: y}) if (y not in visCol) or (visCol[y] == 0): col += 1 visCol.update({y: 1}) else: visCol[y] += 1 ans.append(col) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-07/","summary":"\u003col start=\"3160\"\u003e\n\u003cli\u003eFind the Number of Distinct Colors Among the Balls\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-07"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/tuple-with-same-product/description/\nIntuition In this question, all we need to do is to find the tuple that the numbers in the tuple has the same multiplication value. For example, if there are n pairs of numbers that has the same multiplication value, then the result would be $(^2_n) \\times 4$. This is because we can pick any two pair from the set and form a tuple.\nThe question was, why can you prove that this two pairs are distinct?\nThis is because the original array is distinct. This means that there are no duplicated numbers in the original array. Therefore, if $a \\times b = c \\times d$, we know that $a \\ne c$, then we can now that $b \\ne d$.\nApproach Therefore, all we need to do is to iterate through the whole list and find all tuples that has the same multiplication.\nTrick Among four dictionaries, defaultdict, Counter, dict, and {}, dict has the fastest speed.\nComplexity Time complexity: $O(N ^ 2)$, N is the length of the array.\nSpace complexity: $O(N)$, we need to store the whole array.\nCode class Solution: def tupleSameProduct(self, nums: List[int]) -\u0026gt; int: nums.sort() cnt = dict() N = len(nums) for i in range(N): for j in range(i + 1, N): tmp = nums[i] * nums[j] if tmp not in cnt: cnt[tmp] = 1 else: cnt[tmp] += 1 ans = 0 for v in cnt.values(): ans += 4 * (v) * (v - 1) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-02-06/","summary":"\u003col start=\"1726\"\u003e\n\u003cli\u003eTuple with Same Product\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-06"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-one-string-swap-can-make-strings-equal/description\nIntuition Do what the question ask.\nApproach Do what the question ask.\nCheck that whether there are 0 or exactly 2 position that is different. Check that whether swapping can solve the question. Complexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(1)$.\nCode class Solution: def areAlmostEqual(self, s1: str, s2: str) -\u0026gt; bool: fst = -1 sec = -1 for i in range(len(s1)): if s1[i] != s2[i]: if fst == -1: fst = i elif sec == -1: sec = i else: return False if fst == -1 and sec == -1: return True if fst == -1 or sec == -1: return False if s1[fst] == s2[sec] and s1[sec] == s2[fst]: return True return False ","permalink":"https://tzj2006.github.io/leetcode/2025-02-05/","summary":"\u003col start=\"1790\"\u003e\n\u003cli\u003eCheck if One String Swap Can Make Strings Equal\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-05"},{"content":". \u0026ndash;\u0026gt; find all characters except \u0026lsquo;\\n\u0026rsquo;.\n^ \u0026ndash;\u0026gt; find all the start of the string\n$ \u0026ndash;\u0026gt; find all the end of the string.\n[\u0026hellip;] find a character inside the [].\n[^] find characters not inside the [].\n\\. find the special characters.\n\\d all numbers \u0026lt;=\u0026gt; [0-9]\n\\D all not numbers \u0026lt;=\u0026gt; [^0-9]\n\\s all space characters\n\\S all not space characters\n\\w all letter, num, and . [a-z-A-Z0-9].\n\\W all not letter num and _.\n\\b find side.\nfind find at least once ? find 0 times or once.\n{n} exact n times.\n{n,} at least n times.\n{n, m} at least n times, at most m times.\n| mean or.\n(\u0026hellip;) find all.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-02-04/","summary":"\u003cp\u003e. \u0026ndash;\u0026gt; find all characters except \u0026lsquo;\\n\u0026rsquo;.\u003c/p\u003e\n\u003cp\u003e^ \u0026ndash;\u0026gt; find all the start of the string\u003c/p\u003e\n\u003cp\u003e$ \u0026ndash;\u0026gt; find all the end of the string.\u003c/p\u003e\n\u003cp\u003e[\u0026hellip;] find a character inside the [].\u003c/p\u003e\n\u003cp\u003e[^] find characters not inside the [].\u003c/p\u003e\n\u003cp\u003e\\. find the special characters.\u003c/p\u003e\n\u003cp\u003e\\d all numbers \u0026lt;=\u0026gt; [0-9]\u003c/p\u003e\n\u003cp\u003e\\D all not numbers \u0026lt;=\u0026gt; [^0-9]\u003c/p\u003e\n\u003cp\u003e\\s all space characters\u003c/p\u003e\n\u003cp\u003e\\S all not space characters\u003c/p\u003e\n\u003cp\u003e\\w all letter, num, and \u003cem\u003e. [a-z-A-Z0-9\u003c/em\u003e].\u003c/p\u003e","title":"Bug Journal 2025-02-04"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-ascending-subarray-sum/\nIntuition Same as the problem yesterday, the only difference is changing count to sum.\nApproach Same as the problem yesterday, the only difference is changing count to sum.\nComplexity Time complexity: $O(N)$, N is the length of the array.\nSpace complexity: $O(1)$\nCode class Solution: def maxAscendingSum(self, nums: List[int]) -\u0026gt; int: ans, tmp, pre = nums[0], nums[0], nums[0] for num in nums[1::]: if num \u0026gt; pre: tmp += num else: ans = max(ans, tmp) tmp = num pre = num return max(tmp, ans) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-04/","summary":"\u003col start=\"1800\"\u003e\n\u003cli\u003eMaximum Ascending Subarray Sum\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-04"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/longest-strictly-increasing-or-strictly-decreasing-subarray/\nIntuition Do what the question ask.\nApproach Do what the question ask.\nComplexity Time complexity: $O(N)$, N is the length of the array.\nSpace complexity: $O(1)$.\nCode class Solution: def longestMonotonicSubarray(self, nums: List[int]) -\u0026gt; int: cntI, cntD = 1,1 ans = 1 pre = nums[0] for x in nums[1::]: if x \u0026gt; pre: cntI += 1 ans = max(ans, cntD) cntD = 1 elif x \u0026lt; pre: cntD += 1 ans = max(ans, cntI) cntI = 1 else: ans = max(ans, cntI, cntD) cntI = 1 cntD = 1 pre = x return max(ans, cntI, cntD) ","permalink":"https://tzj2006.github.io/leetcode/2025-02-03/","summary":"\u003col start=\"3105\"\u003e\n\u003cli\u003eLongest Strictly Increasing or Strictly Decreasing Subarray\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-03"},{"content":"Why not use Apple to run LLM: https://github.com/user-attachments/assets/e03bd9e6-0174-44b0-8c99-9a1ab88eeef2\n","permalink":"https://tzj2006.github.io/bugjournal/2025-02-02/","summary":"Speed of M4 Pro","title":"Bug Journal 2025-02-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-array-is-sorted-and-rotated/description/\nIntuition If it is shifted, then it must contain a full original list if we concate two nums together.\nTrick Append the original list to the back of itself it is somehow a ring.\nApproach Concat nums at the end of itself. Then test whether there are at least N non decreasing numbers. Complexity Time complexity: $O(N)$, N is the length of the array\nSpace complexity: $O(1)$\nCode class Solution: def check(self, nums: List[int]) -\u0026gt; bool: n = len(nums) nums += nums cnt = 1 pre = nums[0] for num in nums[1::]: if num \u0026gt;= pre: cnt += 1 else: cnt = 1 pre = num if cnt \u0026gt;= n: return True return False ","permalink":"https://tzj2006.github.io/leetcode/2025-02-02/","summary":"\u003col start=\"1752\"\u003e\n\u003cli\u003eCheck if Array Is Sorted and Rotated\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/divide-nodes-into-the-maximum-number-of-groups/description/\nIntuition The key of this question is $|y - x| = 1$. We know that for a step further, the parity must change. Therefore, when we encounter a case when the parity has a problem (for example, a loop with three nodes and three edges), we would return -1. Otherwise, all we can do is to iterate the starting point of the graph to find which point is the best starting point.\nTherefore, we can have our approach:\nWe can use a dfs to find whether there is a parity issue or not. We can use a bfs to find the starting point. Questions: why using bfs to find the starting point?\nThis is because we need to add a point to the next group if it has an edge connecting the current point and the next point.\nApproach We can use a dfs to find whether there is a parity issue or not. We can use a bfs to find the starting point. Complexity Time complexity: $O(N^2)$\nSpace complexity: $O(N^2)$\nCode class Solution: def magnificentSets(self, n: int, edges: List[List[int]]) -\u0026gt; int: vis = [0] * (n + 1) bvis = [0] * (n + 1) e = [[] for _ in range(n + 1)] for x,y in edges: e[x].append(y) e[y].append(x) ans = 0 clock = 0 def bfs(x): nonlocal clock clock += 1 bvis[x] = clock q = deque([(x,1)]) res = 1 while(len(q) \u0026gt; 0): now, dis = q.popleft() res = max(res, dis) for to in e[now]: if bvis[to] == clock: continue bvis[to] = clock q.append((to, dis + 1)) return res cur = 0 def dfs(x): nonlocal cur cur = max(cur, bfs(x)) # print(cur) tmp = 0 for to in e[x]: if vis[to] == 0: if vis[x] == 0: print(\u0026#34;Warning!\u0026#34;, x) vis[to] = -vis[x] tmp += dfs(to) else: if vis[to] != -vis[x]: return -1 return tmp for i in range(1, n + 1): if vis[i] == 0: cur = 0 vis[i] = 1 if dfs(i) \u0026lt; 0: return -1 # print(\u0026#34;out: \u0026#34;, cur) ans += cur # print(bfs(5)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-30/","summary":"\u003col start=\"2493\"\u003e\n\u003cli\u003eDivide Nodes Into the Maximum Number of Groups\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-30"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/special-array-i/description/\nIntuition Do what the question ask.\nApproach Iterate through the array, then check whether the two number has the are both odd or even or not.\nComplexity Time complexity: $O(N)$\nSpace complexity: $O(1)$\nCode class Solution: def isArraySpecial(self, nums: List[int]) -\u0026gt; bool: for i in range(1, len(nums)): if (nums[i] - nums[i-1]) % 2 == 0: return False return True ","permalink":"https://tzj2006.github.io/leetcode/2025-02-01/","summary":"\u003col start=\"3151\"\u003e\n\u003cli\u003eSpecial Array I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-02-01"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/making-a-large-island/description/\nIntuition In this problem, we need to flip a 0 into a one to calculate the largest connected block. Now, if we can calculate the size of the connected block in the 4 directions of a 0 in the grid, then we just need to add them up and we are all set.\nApproach First we need to calculate the size of each connected block and give a label to each connected block so that we are not adding the same connected block twice.\nThen we iterate all the 0, flip its result is the sum of the unique connected blocks around it in 4 directions.\nComplexity Time complexity: $O(N^2)$\nSpace complexity: $O(N^2)$\nCode class Solution: def largestIsland(self, grid: List[List[int]]) -\u0026gt; int: islandCount = [0,0] dx = [0,0,1,-1] dy = [1,-1,0,0] n = len(grid) m = len(grid[0]) def dfs(x, y, cnt): grid[x][y] = cnt islandCount[cnt] += 1 for i in range(4): nx = x + dx[i] ny = y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or grid[nx][ny] != 1: continue dfs(nx, ny, cnt) cnt = 1 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 1: cnt += 1 islandCount.append(0) dfs(i, j, cnt) ans = max(islandCount) for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 0: tmp = 1 vis = set([]) for k in range(4): nx = i + dx[k] ny = j + dy[k] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or grid[nx][ny] in vis: continue tmp += islandCount[grid[nx][ny]] vis.add(grid[nx][ny]) ans = max(ans, tmp) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-31/","summary":"\u003col start=\"827\"\u003e\n\u003cli\u003eMaking A Large Island\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-31"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/redundant-connection/description/\nIntuition Note that in a ring, every node have more than one index. Therefore, if we delete all the index that has one index, the remaining points will be a ring.\nApproach Note that the label starts from 1 and ends at n, you would like to decrease index by one.\nComplexity Time complexity: $O(N)$\nSpace complexity: $O(N)$\nCode class Solution: def findRedundantConnection(self, edges: List[List[int]]) -\u0026gt; List[int]: N = len(edges) du = [0] * (N) E = [[] for _ in range(N)] for x,y in edges: x -= 1 y -= 1 du[x] += 1 du[y] += 1 E[x].append(y) E[y].append(x) q = deque([]) for i in range(N): if du[i] == 1: q.append(i) # print(du) while(len(q) \u0026gt; 0): x = q.popleft() du[x] = 0 for to in E[x]: if du[to] \u0026gt; 0: du[to] -= 1 if du[to] == 1: q.append(to) # print(du) loop = set([]) for i in range(N): if du[i] \u0026gt; 0: loop.add(i) for i in range(N - 1, -1, -1): x,y = edges[i] if x - 1 in loop and y - 1 in loop: return [x, y] return None ","permalink":"https://tzj2006.github.io/leetcode/2025-01-29/","summary":"\u003col start=\"684\"\u003e\n\u003cli\u003eRedundant Connection\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-29"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-number-of-fish-in-a-grid/description/\nIntuition Find the size of the connected blocks.\nApproach Iterate through the grid to add the size to a connected block, then find the maximum size of the connected block.\nComplexity Time complexity: $O(N\\times M)$, we will visit every point exactly once.\nSpace complexity: $O(1)$, if you do not count the original grid.\nCode class Solution: def findMaxFish(self, grid: List[List[int]]) -\u0026gt; int: ans = 0 dx = [0,0,1,-1] dy = [1,-1,0,0] for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] == 0: continue tmp = grid[i][j] grid[i][j] = 0 q = deque([(i,j)]) while len(q) \u0026gt; 0: x,y = q.popleft() for k in range(4): nx = x + dx[k] ny = y + dy[k] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= len(grid) or ny \u0026gt;= len(grid[0]) or grid[nx][ny] == 0: continue tmp += grid[nx][ny] grid[nx][ny] = 0 # print(i,j,nx, ny, tmp) q.append((nx, ny)) ans = max(ans, tmp) print(grid) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-28/","summary":"\u003col start=\"2658\"\u003e\n\u003cli\u003eMaximum Number of Fish in a Grid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-28"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/course-schedule-iv/description/\nIntuition In this problem, we need to find whether a point is the father of another point or not. In this case, we can simply use one set to store all the fathers of a point and path that set to all its children.\nApproach Use a dfs to iterate all the points. Create a set for every point, then pass it to its children. Complexity Time complexity: $O(N^3 + Q)$. We would at visit each edge at most once. The passing of a set is $O(N)$. So the final time complexity would be $O(N^3 + Q)$.\nSpace complexity: $O(N^2) + Q$. We need to store a set for every point and we also need to store the answer.\nCode class Solution: def checkIfPrerequisite(self, numCourses: int, prerequisites: List[List[int]], queries: List[List[int]]) -\u0026gt; List[bool]: edges = [[] for _ in range(numCourses)] prereq = [set([_]) for _ in range(numCourses)] for x, y in prerequisites: edges[y].append(x) def dfs(x): for to in edges[x]: if len(prereq[to]) == 1: dfs(to) prereq[x] = prereq[x] | prereq[to] for i in range(numCourses): dfs(i) ans = [] for x, y in queries: if x in prereq[y]: ans.append(True) else: ans.append(False) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-27/","summary":"\u003col start=\"1462\"\u003e\n\u003cli\u003eCourse Schedule IV\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-27"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-employees-to-be-invited-to-a-meeting/description/\nIntuition Here we have a directed graph with a ring. To get the ring, we can use topological sort. That is, to find the point which has index 0, and delete the point and its corresponding point.\nBy doing so, the remaining points in the graph are all in a ring.\nIn this problem, there are two ways to put everyone in a seat:\na ring where everyone has its favorite person on his left hand site. when two person are each other\u0026rsquo;s favorite person, they themselves can form a complete ring while people who like them can form a list that points to them. Such structure is special because everyone can find his favorite person without forming a complete ring. Therefore, there could be multiple structures in the room. Approach Use topological sort to find all the rings in the graph. Find all the special case when two people are each others\u0026rsquo; favorite. Return the max size of a ring or return the max size of that multiple structures. Complexity Time complexity: $O(N)$\nSpace complexity: $O(N)$\nCode class Solution: def maximumInvitations(self, favorite: List[int]) -\u0026gt; int: N = len(favorite) du = [0] * N l = [1] * N for x in favorite: du[x] += 1 q = deque([]) for i in range(N): if du[i] == 0: q.append((i, 1)) while(len(q) \u0026gt; 0): x, leng = q.popleft() to = favorite[x] du[to] -= 1 l[to] = max(l[to], leng + 1) if du[to] == 0: q.append((to, leng + 1)) vis = [0] * N def dfs(i): to = favorite[i] vis[i] = 2 if vis[to] == 2: return 1 return dfs(to) + 1 ans = 0 res = 0 for i in range(N): if du[i] != 0 and vis[i] == 0: tmp = dfs(i) # print(i, tmp) if tmp == 2: # print(i, favorite[i], l[i], l[favorite[i]]) res += l[i] + l[favorite[i]] else: ans = max(ans, tmp) return max(ans, res) ","permalink":"https://tzj2006.github.io/leetcode/2025-01-26/","summary":"\u003col start=\"2127\"\u003e\n\u003cli\u003eMaximum Employees to Be Invited to a Meeting\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-26"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/make-lexicographically-smallest-array-by-swapping-elements/description/\nIntuition In this question, we are doing a sorting process that only difference less than limit can swap. Therefore, there forms \u0026ldquo;groups\u0026rdquo;. In a group, two sequential number has a difference less than limit. In this case, if we sort the numbers in a group, then we are all done. Approach We need to get the groups. We need to sort the array first. Then if the difference between two numbers are bigger than limit, then it would belong to two different groups. Then we sort the result for each group. Complexity Time complexity: $O(N\\times log(N))$, N is the length of the array.\nSpace complexity: $O(N)$\nCode class Solution: def lexicographicallySmallestArray(self, nums: List[int], limit: int) -\u0026gt; List[int]: sorted_nums = [] for idx, x in enumerate(nums): sorted_nums.append((x, idx)) sorted_nums.sort() # First we sort the array groups = [] tmp = [] for i in range(len(nums)): if i \u0026gt; 0 and sorted_nums[i][0] - sorted_nums[i-1][0] \u0026gt; limit: tmp.sort() groups.append(tmp) tmp = [] tmp.append(sorted_nums[i][1]) tmp.sort() groups.append(tmp) # Then we form groups idx = 0 pos = 0 ans = [0] * len(nums) for i in range(len(nums)): if pos == len(groups[idx]): pos = 0 idx += 1 ans[groups[idx][pos]] = sorted_nums[i][0] pos += 1 # Then we sort the groups return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-25/","summary":"\u003col start=\"2948\"\u003e\n\u003cli\u003eMake Lexicographically Smallest Array by Swapping Elements\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-25"},{"content":"Some times we want to install a jupyter kernel for our server. Then we would like to use ipykernel\npip install ipykernel ipython kernel install --user --name= To know the PID of a jupyter notebook, you can use:\nimport os os.getpid() ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-24/","summary":"ipykernel, PID","title":"Bug Journal 2025-01-24"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-eventual-safe-states/description/\nNote Outgoing edges means any edge of this point, even if this point connects to itself.\nIntuition The only points that are terminate are the points that have no edges.\nThen we have to find which point connect to these terminate points.\nTherefore, we can use recursive (or any LIFO algorithms) to solve this question.\nApproach Use a DFS. If we find a point that has no outgoing edges, then its a terminate point. If we find a self-loop, all points in the loop are not safety. If a point only connects to safty points, then it is safety. Complexity Time complexity: $O(N)$, all points will be visited only once. Space complexity: $O(N)$. Code class Solution: def eventualSafeNodes(self, graph: List[List[int]]) -\u0026gt; List[int]: n = len(graph) safety = [-1] * n vis = [0] * n ans = [] def dfs(x): if safety[x] != -1: return safety[x] if vis[x] == 1: safety[x] = 0 return 0 vis[x] = 1 if len(graph[x]) == 0: safety[x] = 1 return 1 res = 0 for to in graph[x]: res += dfs(to) if res == len(graph[x]): safety[x] = 1 else: safety[x] = 0 return safety[x] for i in range(n): if(dfs(i) == 1): ans.append(i) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-24/","summary":"\u003col start=\"802\"\u003e\n\u003cli\u003eFind Eventual Safe States\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-24"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-servers-that-communicate/\nIntuition Do what the question ask.\nApproach First count the number of computers in each row and each column. Then count whether a computer has another computer that has the same row or column with it.\nComplexity Time complexity: $O(NM)$, N, M, are the length and the width of the grid.\nSpace complexity: $O(NM)$\nCode class Solution: def countServers(self, grid: List[List[int]]) -\u0026gt; int: cntR = [0] * len(grid) cntC = [0] * len(grid[0]) for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j]: cntR[i] += 1 cntC[j] += 1 ans = 0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] and (cntR[i] \u0026gt; 1 or cntC[j] \u0026gt; 1): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-23/","summary":"\u003col start=\"1267\"\u003e\n\u003cli\u003eCount Servers that Communicate\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-23"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/map-of-highest-peak/description/\nIntuition Because the maximum absolute value of the height difference between two adjacent grid is 1, and the height of water gird is 0. This means that the answer is just the manhattan distance to the nearest water grid.\nApproach Use a BFS to find the nearest manhattan idstance to a water grid.\nComplexity Time complexity: $O(NM)$, N, M are the length and width of the grid.\nSpace complexity: $O(NM)$\nCode class Solution: def highestPeak(self, isWater: List[List[int]]) -\u0026gt; List[List[int]]: q = deque() ans = [[2005 for _ in range(len(isWater[0]))] for _ in range(len(isWater))] for i in range(len(isWater)): for j in range(len(isWater[0])): if isWater[i][j] == 1: q.append((i,j)) ans[i][j] = 0 dx = [0,0,1,-1] dy = [1,-1,0,0] while q: x,y = q.popleft() for i in range(4): nx = x + dx[i] ny = y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= len(isWater) or ny \u0026gt;= len(isWater[0]) or ans[nx][ny] \u0026lt;= ans[x][y] + 1: continue ans[nx][ny] = ans[x][y] + 1 q.append((nx, ny)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-22/","summary":"\u003col start=\"1765\"\u003e\n\u003cli\u003eMap of Highest Peak\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-22"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/grid-game/description/\nIntuition This is a special case where there are only two rows. Moreover, it is also important to note that all number in the grid.\nSince the robot cannot go back whenever we choose to go down, this means that for both robots, there is only one chance to go to the second row.\nIf robot one goes to the second row at index $k$, what will happen?\nNow, in this case, the only numbers not 0 are the numbers that has index less than k in the second row and the numbers that has index more than k in the first row.\nTherefore, to maximize the result for robot 2, it gets to choose to get the numbers in the first row or in the second row because it cannot get back to the first row when it choose to get to the second row.\nApproach Now, all we have to calculate is the sum of all the numbers after index k in the first row, and the sum of all the numbers before index k in the second row.\nTrick Now we have a trick of prefix sum to solve this problem.\nThe sum of all numbers after index in the first row k can be calculated by the sum of all numbers after index k - 1, by subtracting $grid[k][0]$. The sum of all numbers before index in the second row k can be calculated by the sum of all numbers after index k - 1, by adding $grid[k][1]$. The required sum of the first row is always decreasing, while the required sum of the second row is always increasing. Therefore, when $max(sum1, sum2) \u0026gt; presentAns$, we can break the loop as now sum2 \u0026gt; sum1 and will continue increase. Therefore, the answer will not decrease anymore. (Here sum1 means the required sum of row1, sum2 means the required sum of row2, and presentAns means the answer we get at present point when we iterate to index k).\nComplexity Time complexity: $O(N)$, N is the length of the gird.\nSpace complexity: $O(1)$, we only store a few variables.\nCode class Solution: def gridGame(self, grid: List[List[int]]) -\u0026gt; int: x,y = sum(grid[0][1:]), 0 ans = x for i in range(1, len(grid[0])): x -= grid[0][i] y += grid[1][i - 1] if ans \u0026gt;= max(x,y): ans = max(x,y) else: return ans return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-21/","summary":"\u003col start=\"2017\"\u003e\n\u003cli\u003eGrid Game\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-21"},{"content":"Today\u0026rsquo;s Problem https://leetcode.com/problems/first-completely-painted-row-or-column/description\nIntuition Do what the question ask.\nTrick We can store a count array for each row and column, so that we can know how many block painted in any row or column.\nApproach Store the position of each number in the gird. Add one count in each row and column every print. If the print lead to a row or column that is all painted, then output i. Complexity Time complexity: $O(NM)$, N,M are the length and width of the grid.\nSpace complexity: $O(NM)$, we need to store the index of each number.\nCode class Solution: def firstCompleteIndex(self, arr: List[int], mat: List[List[int]]) -\u0026gt; int: col = [0] * (len(arr) + 1) row = [0] * (len(arr) + 1) for i in range(len(mat)): for j in range(len(mat[0])): col[mat[i][j]] = j row[mat[i][j]] = i cntR,cntC = [0] * len(mat), [0] * len(mat[0]) for i, x in enumerate(arr): cntR[row[x]] += 1 cntC[col[x]] += 1 if cntR[row[x]] == len(mat[0]) or cntC[col[x]] == len(mat): return i return -1 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-20/","summary":"\u003col start=\"2661\"\u003e\n\u003cli\u003eFirst Completely Painted Row or Column\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-20"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/trapping-rain-water-ii/description/\nIntuition In yesterday\u0026rsquo;s problem, we talked about the situation when we may consider using graph based methods to solve problems.\nNow we can apply that criteria to this question: in this question, we found that the water can flow from four dirctions, which means that there are aftereffects.\nTherefore, we need to apply graph based methods.\nFor each block, how many water it can store depends on the height difference between it and its lowest neighbor (image a wood bucket with edge heights you may see in psychology classes).\nTo clearify, here, water wall can also be s type of wall that contributes to the height.\nTo know the height of the walls, we first need to genereate a wall. But where is it?\nThe first wall you may consider is the outmost edge of the graph (such as the one shown in case two where stores water using the outmost edge).\nThen we can find the lowest place on the wall to create more walls inside. That is, if its neighbor is higher than the point on the wall, then the point inside will become a new componenet of the wall. Otherwise, that inside point can store enough water to create a water wall as high as the current point.\nBecause we are using the lowest place on the wall, so all other parts of the wall would be higher or equal to the point, which means that the height of the wall is the upper bond of how many water can be stored inside the wall.\nApproach In this case, we can use a priority queue to find the point of the wall efficiently.\nThen follow the algorithm described above:\nCreate the initial wall\nloop:\nfind the lowest point on the wall\ncreate new walls or new water walls\nend loop\nsum up all addition height of water walls\nComplexity Time complexity: $O(NM\\times log(NM))$\nSpace complexity: $O(NM)$\nCode class Solution: def trapRainWater(self, heightMap: List[List[int]]) -\u0026gt; int: if len(heightMap) \u0026lt; 3 or len(heightMap[0]) \u0026lt; 3: return 0 dx = [0,0,1,-1] dy = [1,-1,0,0] vis = [] n, m = len(heightMap), len(heightMap[0]) q = [] for i in range(n): heappush(q, (heightMap[i][0], i, 0)) heappush(q, (heightMap[i][-1], i, m - 1)) vis.append((i, 0)) vis.append((i, m-1)) for i in range(1, m - 1): heappush(q, (heightMap[0][i], 0, i)) heappush(q, (heightMap[-1][i], n - 1, i)) vis.append((0, i)) vis.append((n-1, i)) vis = set(vis) ans = 0 while len(q) \u0026gt; 0: h, x, y = heappop(q) for i in range(4): nx, ny = x + dx[i], y + dy[i] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m or (nx,ny) in vis: continue if heightMap[nx][ny] \u0026lt; h: ans += h - heightMap[nx][ny] heappush(q, (max(h, heightMap[nx][ny]), nx, ny)) vis.add((nx, ny)) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-19/","summary":"\u003col start=\"407\"\u003e\n\u003cli\u003eTrapping Rain Water II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-19"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-cost-to-make-at-least-one-valid-path-in-a-grid/\nIntuition Consider a question:\nHere, there is a graph, each edge has a value 1 or 0, and you should travel from point 0 to point N, what is the shortest path?\nIn this case, you would quickly think of graph algorithms such as Dijkstra, SPFA, or even BFS.\nBut what if I tell you that the question above is the exact same question as what we are solving this quesion? Can you quickly think of the transition between the setting in the question and the setting in this more simple version?\nNow, some of you may think that this problem may be DP question: we have a grid, we may be able to write a DP formular\u0026hellip;\nBut wait, the most important prerequisite of DP is aftereffect. To run a DP, you must make sure that there is not aftereffect. In our situation, because we may need to go from right to left, from down to top, so aftereffect exists. Therefore, we cannot use DP in this question.\nApproach Now, to transfer our question to the question above, we only need to iterate through the graph and create an edge between a point to its neighbor, if this is the neighbor it is pointing at, then the value of the edge will be 0, otherwise it would be 1.\nSome of you may consern the correctness of this solution, as there is also a limitation that \u0026ldquo;You can modify the sign on a cell one time only\u0026rdquo;.\nHowever, the situation is, this graph has not negative edges, which means that your result will always increase if you go through more points. Therefore, you will not even vist the same point more than 1 time, so it is impossible for the solution you get to change the sign of a cell more than 1 time.\nNow, run your Dijkstra (or other shortest path algorithms), and you are all set!\nComplexity Time complexity: $O(N\\times M\\times log(N\\times M))$\nSpace complexity: $O(N\\times M)$\nCode class Solution: def minCost(self, grid: List[List[int]]) -\u0026gt; int: dx = [0,0,1,-1] dy = [1,-1,0,0] n,m = len(grid), len(grid[0]) edges = [[] for i in range(n * m)] def cordinate2d21d(x,y): return x * m + y for i in range(len(grid)): for j in range(len(grid[0])): pos = cordinate2d21d(i, j) for idx in range(4): nx = i + dx[idx] ny = j + dy[idx] if nx \u0026lt; 0 or ny \u0026lt; 0 or nx \u0026gt;= n or ny \u0026gt;= m: continue npos = cordinate2d21d(nx, ny) if idx + 1 == grid[i][j]: edges[pos].append([npos, 0]) else: edges[pos].append([npos, 1]) dis = [inf] * (n * m) dis[0] = 0 q = [(0,0)] while q: d, x = heappop(q) for to, v in edges[x]: if d + v \u0026lt; dis[to]: dis[to] = d + v heappush(q, (dis[to], to)) return dis[cordinate2d21d(n-1, m-1)] ","permalink":"https://tzj2006.github.io/leetcode/2025-01-18/","summary":"\u003col start=\"1368\"\u003e\n\u003cli\u003eMinimum Cost to Make at Least One Valid Path in a Grid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-18"},{"content":"Part Ollama Sometimes someone may already open an ollama server on the HPC. In this case, we need to open a new ollama server, otherwise both of use will run at a slower speed.\nIn this case, what can we do?\nOpen a new personal port!\ne.g:\nenv OLLAMA_MODELS=/orange/qsong1/zt81.duke/Models OLLAMA_HOST=127.0.0.1:11451 ollama serve ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-17/","summary":"Ollama","title":"Bug Journal 2025-01-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/neighboring-bitwise-xor/description/\nIntuition Based on the information of yesterday\u0026rsquo;s problem, we know that $a\\space xor\\space a = 0$. Therefore, $\\large{XOR}_{i=0}^{n} \\small derived[i]$ has to be 0, because all the $original$ offsets. Here, n means the last index of the sequence. Now lets prove that if $\\large{XOR}_{i=0}^{n} \\small derived[i] = 0$ enables us to create the whole $original$ sequence. Let $original[0]=0$, $original[k] = original[k-1]\\space xor\\space derived[k]$. Now all we need to prove is $original[n]\\space xor\\space original[0]=derived[n]$, that is, $original[0] = original[n]\\space xor\\space derived[n]$. According to the formular above, $original[n] = \\large{XOR}_{i=0}^{n-1} \\small derived[i]\\space xor\\space original[0]$ Because $\\large{XOR}_{i=1}^{n} \\small derived[i] = 0$, so $original[n]\\space xor\\space derived[n] = original[0]\\space xor\\space \\large{XOR}_{i=0}^{n} \\small derived[i] = 0 = original[0]$. Therefore, this sequence of $original$ is valid. Trick Now we want to know whether the sequence itself has a xorsum 0 or not. Now, because it is a binary sequence, we can put all 0s together and 1s together, so that now by xor all the 0 and 1s, we find that the result is just the count of 1s. If the count is odd, then the result would be 1, otherwise it would be 0. Therefore, the easiest way to solve this question is to sum up everything in the sequence and check whether it is odd or even. Approach Sum up everything in the sequence and check whether it is odd or even.\nComplexity Time complexity: $O(N)$, N is the length of the sequence.\nSpace complexity: $O(1)$, no other variables stored.\nCode class Solution: def doesValidArrayExist(self, derived: List[int]) -\u0026gt; bool: return (sum(derived) \u0026amp; 1) == 0 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-17/","summary":"\u003col start=\"2683\"\u003e\n\u003cli\u003eNeighboring Bitwise XOR\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-17"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/bitwise-xor-of-all-pairings/description/\nIntuition According to the question, the formular of the output would be: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} \\large{XOR}{\\small j = 1}^{\\small m} nums1[i]\\space xor \\space nums2[j] $$ where $\\large{XOR}$ means the operation that xor from $i$ to $n$. Here, $n$ means the length of $nums1$, and $m$ means the length of $nums2$. According to the properties of xor, xor satisfies the law of commutation and the law of association (more information can be seen here), so we can change the formular to: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} (nums1[i]^m) \\space xor \\space \\large{XOR}{\\small j = 1}^{\\small m} (nums2[j]^n) $$ According to the property that $A\\space xor A = 0$, we now know that the result would be: $$ ans = \\large{XOR}{\\small i = 1}^{\\small n} (nums1[i]^{m % 2}) \\space xor \\space \\large{XOR}{\\small j = 1}^{\\small m} (nums2[j]^{n % 2}) $$\nApproach Iterate two arrays and apply the formular above.\nComplexity Time complexity: $O(N + M)$, N is the length of nums1, M is the length of nums2.\nSpace complexity: $O(1)$, no more space is used.\nCode class Solution: def xorAllNums(self, nums1: List[int], nums2: List[int]) -\u0026gt; int: ans = 0 if len(nums1) % 2 == 1: for x in nums2: ans ^= x if len(nums2) % 2 == 1: for x in nums1: ans ^= x return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-16/","summary":"\u003col start=\"2425\"\u003e\n\u003cli\u003eBitwise XOR of All Pairings\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-16"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimize-xor/description/\nXOR This is the True-False Diagram for $XOR$: In this diagram, we can find that if A and B is the same, then $A\\space XOR\\space B = 0$; else, $A\\space XOR\\space B = 1$.\nIntuition For the first requirement in the question, \u0026ldquo;same number of set bits\u0026rdquo; means \u0026ldquo;same number of bit 1 in the number\u0026rdquo;.\nWhy? This is because the number of 0 in a number can be infinity, while only the number of bit 1 is finite.\nTherefore, we need to count how many 1s are there in number2.\nNow, based on the $XOR$ Diagram we know that to make a number after doing $XOR$, we need to put a one in the same position where number1 has a 1, so that we can decrease it to 0 after doing $XOR$.\nIn this case, we want the \u0026ldquo;decreased\u0026rdquo; 1s from top to down to minimize the result.\nIf there are more 1s in number2 than number1, we would have to add new 1s to the result.\nIn this case, we want to \u0026ldquo;add\u0026rdquo; 1s from down to top to minimize the result.\nApproach Use $bit_count()$ function to count the 1s in num2. Iterate from top to down to decrease 1. Iterate from down to top to add 1. Complexity Time complexity: $O(log_2(N))$, N means the number.\nSpace complexity: $O(1)$, only some variables are stored\nCode class Solution: def minimizeXor(self, num1: int, num2: int) -\u0026gt; int: cnt,ans = num2.bit_count(), 0 for i in range(31, -1, -1): if cnt \u0026gt; 0 and (num1 \u0026amp; (1 \u0026lt;\u0026lt; i)) \u0026gt; 0: ans += (1 \u0026lt;\u0026lt; i) cnt -= 1 for i in range(31): if cnt \u0026gt; 0 and (ans \u0026amp; (1 \u0026lt;\u0026lt; i)) == 0: ans += (1 \u0026lt;\u0026lt; i) cnt -= 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-15/","summary":"\u003col start=\"2429\"\u003e\n\u003cli\u003eMinimize XOR\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-15"},{"content":"Part module load When you want to use something on the server, please first check whether it is on the server or not.\nUse this command:\nmodule avail Then load using this command:\nmodule load [your model name] Important:\nYou can always email rescomputing@duke.edu to get support.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-14/","summary":"\u003ch2 id=\"part-module-load\"\u003ePart module load\u003c/h2\u003e\n\u003cp\u003eWhen you want to use something on the server, please first check whether it is on the server or not.\u003c/p\u003e\n\u003cp\u003eUse this command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emodule avail\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen load using this command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emodule load \u003cspan class=\"o\"\u003e[\u003c/span\u003eyour model name\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eImportant:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou can always email \u003ca href=\"rescomputing@duke.edu\"\u003erescomputing@duke.edu\u003c/a\u003e to get support.\u003c/p\u003e","title":"Bug Journal 2025-01-14"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/find-the-prefix-common-array-of-two-arrays/description/\nIntuition In this question, the only three ways that the answer will increase one is:\n$A[i] == B[i]$. $A[i]$ appears in $B$. $B[i]$ appears in $A$. Approach Iterate through the Array, then check for those three situation. Note that situation 1 conflicts with situation 2 \u0026amp; 3. That is, if cnt is add by 1 through situation 1, then situation 2 \u0026amp; 3 will not increase cnt. But situation 2 \u0026amp; 3 could increase cnt. Complexity Time complexity: $O(N)$, N is the length of the list.\nSpace complexity: $O(N)$, because we need to store a set.\nCode class Solution: def findThePrefixCommonArray(self, A: List[int], B: List[int]) -\u0026gt; List[int]: ans = [] cntA = set([]) cntB = set([]) cnt = 0 for i in range(len(A)): if A[i] == B[i]: cnt += 1 else: if A[i] in cntB: cnt += 1 if B[i] in cntA: cnt += 1 cntA.add(A[i]) cntB.add(B[i]) ans.append(cnt) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-14/","summary":"\u003col start=\"2657\"\u003e\n\u003cli\u003eFind the Prefix Common Array of Two Arrays\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-13"},{"content":"When facing this problem, you can stop using GPU and change to CPU and check the error.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-12/","summary":"\u003cp\u003eWhen facing this problem, you can stop using GPU and change to CPU and check the error.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250112224658415\" loading=\"lazy\" src=\"https://tzj2006.github.io/images/2025-01-12GPUError.jpg\"\u003e\u003c/p\u003e","title":"Bug Journal 2025-01-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-length-of-string-after-operations/\nIntuition In this question, each character is independent. Therefore, we can deal with one character a time. If one character has odd count, than we can delete from left to right and remain 1 character at the end. Otherwise, we will left 2 characters at the end. Approach Therefore, all we need to do is the count each character, and then test whether it has odd count or even count.\nComplexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, I stored a counter.\nCode class Solution: def minimumLength(self, s: str) -\u0026gt; int: cnt = Counter([ch for ch in s]) ans = 0 for x,v in cnt.items(): if v % 2 == 0: ans += 2 else: ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-13/","summary":"\u003col start=\"3223\"\u003e\n\u003cli\u003eMinimum Length of String After Operations\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-13"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/check-if-a-parentheses-string-can-be-valid/\nIntuition To make a parentheses string, we need to find a \u0026lsquo;(\u0026rsquo; for every \u0026lsquo;)\u0026rsquo; on its left side, and a \u0026lsquo;)\u0026rsquo; for every \u0026lsquo;(\u0026rsquo; on its right hand side. We can fulfill the first requirement first as we are iterating from left to right.\nWhen we encounter a \u0026lsquo;)\u0026rsquo;, there are three ways to find him a \u0026lsquo;(\u0026rsquo;, one is a \u0026lsquo;(\u0026rsquo; that is already existing, another is to find him a \u0026lsquo;)\u0026rsquo; that is unlocked, and the last way is to turn him into a \u0026lsquo;(\u0026rsquo; if it is unlocked itself.\nNow, assume that we have find all \u0026lsquo;)\u0026rsquo; a \u0026lsquo;(\u0026rsquo;, then we now need to find all \u0026lsquo;(\u0026rsquo; a \u0026lsquo;)\u0026rsquo;. Now, the only way to find a \u0026lsquo;(\u0026rsquo; on its right side is to find a \u0026lsquo;(\u0026rsquo; that is unlocked, because all \u0026lsquo;)\u0026rsquo; is matched with a \u0026lsquo;(\u0026rsquo;.\nApproach First of all, not that is the length is odd, then it could never be a parentheses string, so please just return False.\nWe can store two arrays, one $anyBracket$ that stores the index all unlocked brackets, and another $openBracket$ stroing all \u0026lsquo;(\u0026rsquo;.\nNow we iterate the whole string from left to right, here are some situations we would meet:\nThis is a unlocked bracket: Then we could put it into our $anyBracket$ stack. This is a \u0026lsquo;(\u0026rsquo;: Then we could put it into our $openBracket$ stack. This is a \u0026lsquo;)\u0026rsquo;: Then we need to find him a \u0026lsquo;(\u0026rsquo;. First we would like to find him a \u0026lsquo;(\u0026rsquo; in our $openBracket$ stack, which will also finish the task that helps a \u0026lsquo;(\u0026rsquo; to find a \u0026lsquo;)\u0026rsquo;. Then if our $openBracket$ stack is empty, then we will find him a \u0026lsquo;(\u0026rsquo; in our $anyBracket$ stack by either a \u0026lsquo;(\u0026rsquo; or a \u0026lsquo;)\u0026rsquo; that is unlocked. If both our $openBracket$ stack and our $anyBracket$ stack are empty, then return False, because we cannot find a \u0026lsquo;(\u0026rsquo; for him. Now we might left some \u0026lsquo;(\u0026rsquo; that is unmatched.\nThen we can iterate every \u0026lsquo;(\u0026rsquo; in our $openBracket$ stack to find whether there is a \u0026lsquo;(\u0026rsquo; or \u0026lsquo;)\u0026rsquo; in our $anyBracket$ stack that has a larger index than our current \u0026lsquo;(\u0026rsquo;.\nIf there is, then we successfully find him a \u0026lsquo;)\u0026rsquo;, congratulations! Otherwise we cannot find him a \u0026lsquo;)\u0026rsquo;, which leads to return False Now if there are even number in our $anyBracket$ stack (which will always be the case because we have already did the singularity test above), please return True, then you are all set!\nImportant Trick Why we need a stack for $openBracket$ and $anyBracket$? Because in this situation, a \u0026lsquo;)\u0026rsquo; will always match to the nearest \u0026lsquo;(\u0026rsquo; on its left hand site, which means we need a FIFO (First in First out) data structure to get \u0026ldquo;the nearest object\u0026rdquo;. Complexity Time complexity: $O(N)$, N is the length of the string.\nSpace complexity: $O(N)$, we stored the indexs of the brackets.\nCode class Solution: def canBeValid(self, s: str, locked: str) -\u0026gt; bool: if len(s) % 2 == 1: return False openBracket = [] anyBracket = [] for i in range(len(s)): if locked[i] == \u0026#39;0\u0026#39;: anyBracket.append(i) else: if s[i] == \u0026#39;(\u0026#39;: openBracket.append(i) else: if len(openBracket) \u0026gt; 0: openBracket.pop() elif len(anyBracket) \u0026gt; 0: anyBracket.pop() else: return False if len(anyBracket) \u0026lt; len(openBracket): return False idx = len(anyBracket) - 1 for i in range(len(openBracket) - 1, -1, -1): if anyBracket[idx] \u0026lt; openBracket[i]: return False idx -= 1 return idx % 2 == 1 ","permalink":"https://tzj2006.github.io/leetcode/2025-01-12/","summary":"\u003col start=\"2116\"\u003e\n\u003cli\u003eCheck if a Parentheses String Can Be Valid\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-12"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/construct-k-palindrome-strings/description/\nIntuition A palindrome can have at most one character with an odd count. Therefore, to create $k$ palindrome strings, there must be at most $k$ characters in string $s$ with an odd count.\nAdditionally, since there are at most 26 different characters, if $26 \\leq k$, the result must be true. However, if the length of $s$ is less than $k$, it would be false.\nTo prove this, let the number of characters with odd counts be $cntO$, and let the count of all remaining characters be $2 \\times cntE$.\nIf all character counts are even, we can always create palindrome strings as long as $k \\leq N$, where $N$ is the total length of $s$. This is because we can place one character on the leftmost side of a palindrome string and its duplicate on the rightmost side, preserving the palindrome structure.\nSince $k \\leq N$, it follows that $k \\leq cntO + 2 \\times cntE$. Thus, $k - cntO \\leq 2 \\times cntE$. This implies that all characters with odd counts can be used to form $cntO$ palindrome strings.\nNow, we have already proved that if all character counts are even, we can always create palindrome strings as long as $k \\leq N$, where $N$ is the total length of $s$. So in this case, if $cntO \\le k$, the result would be ture, otherwise, it would be false.\nFinally, the question reduces to the proposition that $k \\leq N$ when all characters have even counts, which is always true.\nTrick Since there are at most 26 different characters, if $26 \\leq k$, the result must be true.\nApproach Now we only need to calculate the occurence of every character and test whether the odd-count characters are less or equal to $k$ or not.\nComplexity Time complexity: $O(N)$, while N is the length of the string.\nSpace complexity: $O(1)$, while the count of 26 characters are stored.\nCode class Solution: def canConstruct(self, s: str, k: int) -\u0026gt; bool: cnt = [0] * 26 if len(s) \u0026lt; k: return False if k \u0026gt; 25: return True # The code that makes the code run very fast. for ch in s: cnt[ord(ch) - ord(\u0026#39;a\u0026#39;)] += 1 x = 0 for i in range(26): if cnt[i] % 2 == 1: x += 1 return x \u0026lt;= k ","permalink":"https://tzj2006.github.io/leetcode/2025-01-11/","summary":"\u003col start=\"1400\"\u003e\n\u003cli\u003eConstruct K Palindrome Strings\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-11"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/word-subsets/\nIntuition The brute method would take $O(N^2\\times (L+D))$ time, while $N$ is the length of the words, $L$ is the length of each word, and $D$ is the size of the dictionary (that is, 26 characters). However, $N \\le 10^4$, which means that we cannot use brute method. Then we found out that we do not need to compare all $N$ words, instead, we only need to compare the maximum of the occurence of each characters in words2. For example, if \u0026lsquo;a\u0026rsquo; appears 3 times in $words2[1]$, 2 times in $words2[2]$, 4 times in $words2[3]$, then \u0026lsquo;a\u0026rsquo; must appears at least 4 times in a $words1[i]$ to add one to the answer. Therefore, all we need is to count the occurence of each character in word1, and count the maximun occurence of each character in every word2.\nApproach Count the occurence of each character in word1, and count the maximun occurence of each character in every word2.\nComplexity Time complexity: $O(N \\times (L + D))$, while $N$ is the length of the words, $L$ is the length of each word, and $D$ is the size of the dictionary (that is, 26 characters).\nSpace complexity: $O(N \\ times D)$.\nCode class Solution: def wordSubsets(self, words1: List[str], words2: List[str]) -\u0026gt; List[str]: wordcnt1, wordcnt2 = [], [0] * 26 a = ord(\u0026#39;a\u0026#39;) for word in words1: cnt = [0] * 26 for ch in word: x = ord(ch) - a cnt[x] += 1 wordcnt1.append(cnt) for word in words2: cnt = [0] * 26 for ch in word: x = ord(ch) - a cnt[x] += 1 for j in range(26): wordcnt2[j] = max(wordcnt2[j], cnt[j]) ans = [] for i in range(len(words1)): flag = True for k in range(26): if wordcnt1[i][k] \u0026lt; wordcnt2[k]: flag = False break if flag: ans.append(words1[i]) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-10/","summary":"\u003col start=\"916\"\u003e\n\u003cli\u003eWord Subsets\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-10"},{"content":"Part Matplotlib If you want to make the word in matplotlib the a word that could be edited in Adobe illustration, please use the code below at the top of your code:\nmpl.rcParams[\u0026#34;pdf.fonttype\u0026#34;] = 42 mpl.rcParams[\u0026#34;ps.fonttype\u0026#34;] = 42 ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-09/","summary":"\u003ch2 id=\"part-matplotlib\"\u003ePart Matplotlib\u003c/h2\u003e\n\u003cp\u003eIf you want to make the word in matplotlib the a word that could be edited in Adobe illustration, please use the code below at the top of your code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003empl\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ercParams\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;pdf.fonttype\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003empl\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ercParams\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;ps.fonttype\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Bug Journal 2025-01-09"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/counting-words-with-a-given-prefix/description\nIntuition Do what the question ask!\nApproach The question yesterday can give us some insight of how to solve this question with minimal code.\nComplexity Time complexity: $O(N\\times L)$, N is the length of the words, L is the length of a single word.\nSpace complexity: $O(1)$, only some variables are stored.\nCode class Solution: def prefixCount(self, words: List[str], pref: str) -\u0026gt; int: ans = 0 for word in words: if word.startswith(pref): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-09/","summary":"\u003col start=\"2185\"\u003e\n\u003cli\u003eCounting Words With a Given Prefix\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-09"},{"content":"Part SSH To use ssh for UF server, eduVPN app must be downloaded.\nThe tutorial is in this link.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-08/","summary":"\u003ch2 id=\"part-ssh\"\u003ePart SSH\u003c/h2\u003e\n\u003cp\u003eTo use ssh for UF server, eduVPN app must be downloaded.\u003c/p\u003e\n\u003cp\u003eThe tutorial is in \u003ca href=\"https://docs.rc.ufl.edu/access/federated_login/?h=eduvpn#eduvpn-connection\"\u003ethis link\u003c/a\u003e.\u003c/p\u003e","title":"Bug Journal 2025-01-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-prefix-and-suffix-pairs-i/description/\nIntuition Do what the question ask!\nApproach Iterate two times and check the suffix and prefix of the string.\nComplexity Time complexity: $O(N^2\\times L)$, N is the length of the words, and L is the length of the string.\nSpace complexity: $O(1)$\nCode class Solution: def countPrefixSuffixPairs(self, words: List[str]) -\u0026gt; int: def checkpre(str1, str2): if len(str1) \u0026gt; len(str2): return False return str1 == str2[:len(str1)] def checksuf(str1, str2): if len(str1) \u0026gt; len(str2): return False return str1 == str2[len(str2) - len(str1):] ans = 0 for i in range(len(words)): for j in range(i+1, len(words)): if checkpre(words[i],words[j]) and checksuf(words[i], words[j]): ans += 1 return ans class Solution: def countPrefixSuffixPairs(self, words: List[str]) -\u0026gt; int: ans = 0 for i in range(len(words)): for j in range(i+1, len(words)): if words[j].startswith(words[i]) and words[j].endswith(words[i]): ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-08/","summary":"\u003col start=\"3042\"\u003e\n\u003cli\u003eCount Prefix and Suffix Pairs I\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-08"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/string-matching-in-an-array/description/\nIntuition Do what the question ask.\nApproach First sort the words by its length, then iterate all string that has a larger string length. If the current string is a substring of the new string, then put it into the answer list.\nActually you do not need to sort the array, but the sort would accelerate the process.\nComplexity Time complexity: $O(N^2\\times L)$, N is the length of the word list, L is the length of the word.\nSpace complexity: $O(N)$, because we need to store the answer.\nCode class Solution: def stringMatching(self, words: List[str]) -\u0026gt; List[str]: ans = [] words.sort(key = lambda x: len(x)) # This lambda is very import in python for i in range(len(words)): for j in range(i + 1, len(words)): if words[i] in words[j]: ans.append(words[i]) break return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-07/","summary":"\u003col start=\"1408\"\u003e\n\u003cli\u003eString Matching in an Array\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-07"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/minimum-number-of-operations-to-move-all-balls-to-each-box/description\nIntuition For each point, is it easy to find out that the answer of that point is the sum of the distance between i and all the 1s.\nThat is: $ans[i]=\\sum_{j=0}^{n-1} boxes[j] \\times abs(j-i)$.\nNow, imagine that there is a pointer moving left to right from 1 to n, calculating the result.\nWe can find that for $ans[i]$ and $ans[i+1]$, the difference would be the number of 1s from 1 to i minus the number of 1s from i+1 to n.\nThat is: $ans[i+1] - ans[i] = \\sum_{j=0}^{i} boxes[j] - \\sum_{j=i+1}^{n-1} boxes[j]$. Therefore, by calculating the number of 1s on the left hand side of i and the number of all 1s in the sequence, we can calculate all answers by $O(N)$.\nApproach First we need to calculate $ans[0]$ and the number of all 1s in the sequence by using the equation $ans[0]=\\sum_{j=1}^{n} boxes[j] \\times j$. Therefore, we need to iterate through the whole sequence. Next we need to calculate $ans[i+1] - ans[i] = \\sum_{j=0}^{i} boxes[j] - \\sum_{j=i+1}^{n-1} boxes[j]$ for every i from 1 to n-1. Because $ans[i+1] - ans[i] = \\sum_{j=1}^{i} boxes[j] - \\sum_{j=i+1}^{n} boxes[j] = 2 \\times \\sum_{j=0}^{i} boxes[j] - \\sum_{j=0}^{n-1} boxes[j]$. Therefore, all we have to. do is to count the 1s in our iteration to our answer, then apply the fomular above.\nComplexity Time complexity: $O(N)$, N is the size of the boxes.\nSpace complexity: $O(N)$, as we need to store our answer.\nCode class Solution: def minOperations(self, boxes: str) -\u0026gt; List[int]: now = 0 cnt1 = 0 for i in range(len(boxes)): if boxes[i] == \u0026#39;1\u0026#39;: now += i cnt1 += 1 ans = [now] * len(boxes) now_cnt1 = 0 for i in range(1, len(boxes)): if boxes[i-1] == \u0026#39;1\u0026#39;: now_cnt1 += 1 ans[i] = ans[i-1] + 2 * now_cnt1 - cnt1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-06/","summary":"\u003col start=\"1769\"\u003e\n\u003cli\u003eMinimum Number of Operations to Move All Balls to Each Box\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-06"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/shifting-letters-ii/description/\nIntuition We need to write a datastructure to achieve a multi-range addition and a one-time query. Therefore, we can use difference Array and prefix sum.\nApproach In a difference array, we only count the difference of the edges of the change zone. For example, if we add the range $[l,r]$ by $k$, we only pay attention to point l and point r: we add the difference array $diff[l]$ by $k$, and then add the difference array $diff[r + 1]$ by $-k$. Now, if we calculate the prefix sum $s$ in range $[l,r]$, we find that the effect of addition $k$ will be added only in range $[l,r]$ in $s$. Therefore, by using the difference array and the prefix sum, we can deal with the one change in $O(1)$. Now, since we want to change the character, we can first change it into ASCII code, then subtract by the code of \u0026lsquo;a\u0026rsquo;. Then we can use a module of 26 to acheive the effect of \u0026ldquo;character rotation\u0026rdquo;.\nComplexity Time complexity: $O(N + C)$, N is the length of the string, c is the number of changes.\nSpace complexity: $O(N)$, we need to store the difference array.\nCode class Solution: def shiftingLetters(self, s: str, shifts: List[List[int]]) -\u0026gt; str: dif = [0] * (len(s) + 1) for l,r,delta in shifts: if delta == 0: dif[l] -= 1 dif[r + 1] += 1 else: dif[l] += 1 dif[r + 1] -= 1 a = ord(\u0026#39;a\u0026#39;) cnt = 0 ans = \u0026#34;\u0026#34; for i in range(len(s)): cnt += dif[i] ans += chr((ord(s[i]) - a + cnt) % 26 + a) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-05/","summary":"\u003col start=\"2381\"\u003e\n\u003cli\u003eShifting Letters II\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-05"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/unique-length-3-palindromic-subsequences/description/\nIntuition Because the question requires the count of unqiue subsequence. Therefore, the largest count is $26*26 = 676$. Therefore, all we have to do is to count how many unique characters are there between two same characters.\nApproach First we need to calculate the first and last occurance of a character. Then we need to iterate between l and r to count how many unique characters are there between l and r.\nComplexity Time complexity: $O(kN)$, k is the number of unique characters, here it means 26 different character. N is the length of the string.\nSpace complexity: $O(N)$\nCode class Solution: def countPalindromicSubsequence(self, s: str) -\u0026gt; int: st = [inf] * 26 en = [-1] * 26 a = ord(\u0026#39;a\u0026#39;) for i,ch in enumerate(s): nch = ord(ch) - a st[nch] = min(st[nch], i) en[nch] = max(en[nch], i) ans = 0 for x in range(26): if en[x] \u0026lt;= st[x]: continue # print(en[x], st[x], x) vis = set([]) for i in range(st[x] + 1, en[x]): vis.add(s[i]) ans += len(vis) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-04/","summary":"\u003col start=\"1930\"\u003e\n\u003cli\u003eUnique Length-3 Palindromic Subsequences\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-04"},{"content":"Part Ollama To run ollama serve on UF that stores models on large storage, please use:\nenv OLLAMA_MODELS=/orange/qsong1/zt81.duke/Models ollama serve To list all the ollama environment, please use:\nollama list Then check whether all the models pulled are in this list.\nNote that now when dealing with ollama input, multiple role input may cause potential error.\nPart Model Tuning What I did today:\nSet up llama for Sprax task.\nTest1: llama3.2 with reasoning. Result: Nearly everything outputs \u0026ldquo;Sensitive\u0026rdquo; (For both cells that has sensitive and resistent label).\n![image-20250103204922830](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20250103204922830.png)\nGuess: May because there are too many genes that mislead the result of the LLM.\nDecrease the number of genes considered Result: However, the model still outputs \u0026ldquo;Sensitive\u0026rdquo; for nearly every cells.\nTry not using reasoning Result: Very unstable. Sometimes the result is very good, but in most case, it is very bad.\nTry using llama3.3 70B Result: Not much better than guess.\n![image-20250103205231023](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20250103205231023.png)\nFuture work Train MLP Train an MLP using the label, and let LLM to distinguish the embedding.\nIn this case, we can tell whether the LLM is useless or the prompt is useless.\nCustomize tokenizer Change one cell to one token\nCheck Pathway Check whether the LLM is saying nonsense or saying things right.\n","permalink":"https://tzj2006.github.io/bugjournal/2025-01-03/","summary":"run ollama on the server \u0026amp; model tuning","title":"Bug Journal 2025-01-03"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/number-of-ways-to-split-array/description/\nIntuition We need to calculate $sum(a[0] \\space to\\space a[i])$ and $sum(a[i+1]\\space to\\space a[n])$ according to the question. Since $sum(a[0]\\space to\\space a[i + 1]) = sum(a[0]\\space to\\space a[i]) + a[i+1]$ and $sum(a[i+1]\\space to\\space a[n]) = sum(a[0]\\space to\\space a[n]) - sum(a[0]\\space to\\space a[i])$. Therefore, we can new two variables. One $now$ that stores $sum(a[0]\\space to\\space a[i])$ and add $a[i+1]$ to it every iteration, one $summ$ that stores $sum(a[0]\\space to\\space a[n])$.\nApproach In this case, we only need to compare $now$ and $summ - now$. Then count all i that apply.\nComplexity Time complexity: $O(N)$, N is the length of the sequence.\nSpace complexity: $O(1)$, two new varables are stored.\nCode class Solution: def waysToSplitArray(self, nums: List[int]) -\u0026gt; int: now,summ, ans = 0, 0, sum(nums) for num in nums[:-1]: now += num if now \u0026gt;= summ - now: ans += 1 return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-03/","summary":"\u003col start=\"2270\"\u003e\n\u003cli\u003eNumber of Ways to Split Array\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-03"},{"content":"Part Hugo Part Upgrade The best method to upgrade Hugo is to use the exact same method you use when you install Hugo.\nFor example, when you use homebrew to install Hugo, the please use brew to upgrade Hugo.\nThe command of this is:\nbrew upgrade brew upgrade hugo Part config Always the best guides:\nhttps://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\nSometimes the config may have errors.\nFor example, if the baseUrl config does not work, add this to the config file:\nrelativeURLs: false canonifyURLs: true Moreover, we can change what we want to show on the main page, for example, the menu bar:\nmenu: main: - identifier: bugJournal name: bugJournal url: /bugJournal/ weight: 10 - identifier: leetcode name: leetcode url: /leetcode/ weight: 20 - identifier: posts name: posts \u0026amp; notes url: /posts/ weight: 30 To change which part of passage to show on the main page:\nparams: mainSections: - bugJournal - leetcode - posts ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-02/","summary":"update hugo to fix bugs","title":"Bug Journal 2025-01-02"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/count-vowel-strings-in-ranges/description/\nIntuition We need to cacluate the sum of a section, while the sum of the any section remains constant for every query. Therefore we can use prefix sum.\nApproach In one iteration, we can identify whether $words[i]$ is the a vowel string or not. In this case, we can apply a prefix sum to calculate the number of vowel strings between index 1 and index i. Therefore, when we want to calculate the number of vowel strings between index l and index r, we can just use $num(1\\space to\\space r) - num(1\\space to\\space l-1)$ as our result.\nTrick In python, $list[-1]$ means the final index of the list. Therefore, we can add a [0] at the end of our prefix sum list to avoid null index.\nComplexity Time complexity: $O(k\\times N + Q)$, while k is the number of vowels, N is the length of the words, Q is the length of the queries.\nSpace complexity: $O(N)$, because we stored a new list.\nCode class Solution: def vowelStrings(self, words: List[str], queries: List[List[int]]) -\u0026gt; List[int]: sumWords = [0] * (len(words) + 1) vowels = set([\u0026#39;a\u0026#39;,\u0026#39;e\u0026#39;,\u0026#39;i\u0026#39;,\u0026#39;o\u0026#39;,\u0026#39;u\u0026#39;]) for idx, word in enumerate(words): sumWords[idx] = sumWords[idx-1] if word[0] in vowels and word[-1] in vowels: sumWords[idx] += 1 ans = [] for x,y in queries: ans.append(sumWords[y] - sumWords[x-1]) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-02/","summary":"\u003col start=\"2559\"\u003e\n\u003cli\u003eCount Vowel Strings in Ranges\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-02"},{"content":"Part Hugo How to create a Hugo website This is a good tutorial.\nHowever, the themes may contain bugs. (The best way is to download another theme!)\nLanguage Code Error: Some theme use site.LanguageCode and others use site.Lang.LanguageCode Code Highlight Error: Change it to .post-content pre code { word-break: normal !important; white-space: pre !important; } Note 2025.01.03\nAll these error are caused by the version conflict of Hugo, go, and the theme. Update everything to the latest version solves all the problem.\nPart Conda Now I install conda under /orange/qsong1/zt81.duke/miniconda3\nTherefore, to use conda, please use:\ncd /orange/qsong1/zt81.duke/miniconda3 source miniconda3/bin/activate ","permalink":"https://tzj2006.github.io/bugjournal/2025-01-01/","summary":"create hugo website \u0026amp; Install condo","title":"Bug Journal 2025-01-01"},{"content":"Part ChatGPT 4o ChatGPT 4o API demo from openai import OpenAI model_use = \u0026#34;gpt-4o-2024-08-06\u0026#34; client = OpenAI(api_key=\u0026#34;Your-API-key\u0026#34;) completion = client.beta.chat.completions.parse( model=model_use, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Extract the event information.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;}, ], ) event = completion.choices[0].message.parsed Note: I tried to use model \u0026ldquo;gpt-4o\u0026rdquo; but failed.\nHow to create ChatGPT API Key Log in to openai Use the search bar to search \u0026ldquo;API keys\u0026rdquo; Create a new secret key (Shown only once, invisible after closing the tab) Go to billing to add some credit to the account Part UniTox ChatGPT Read from fda.gov Read the label of the drug we are interested in from a .csv file.\nRead the .html file or the .pdf file on the page\nCreate a summary of the .html and .pdf files by ChatGPT\nUse the summary generated by ChatGPT to let ChatGPT decide whether the drug is toxic or not and how toxic the drug is.\nInitial Prompt: Provide a summary of all the parts of the drug label that discuss cardiotoxicity risks and cardiotoxic reactions for this drug. In your summary of each sentence, clearly state whether the drug itself was associated with or caused the cardiotoxicity risk. Output1 Toxidity Score Prompt: Given the above information about a drug, answer \u0026#39;was this drug associated with No Cardiotoxicity, Less Cardiotoxicity, or Most Cardiotoxicity?\u0026#39; Now, answer with just one word: No, Less or Most. Output1 (Summary) OUtput2 Toxidity Test Prompt: Given the above information about a drug, answer \u0026#39;was this drug associated with Cardiotoxicity?\u0026#39; Now, answer with just one word: Yes or No. Output1 Output3 \u0026lt;-\u0026gt; compare GT ![image-20241230175906492](/Users/tongtongtot/Library/Application Support/typora-user-images/image-20241230175906492.png)\nPart Llama Part Ollama First open an ollama server on the server:\nml ollama # activate ollama ollama serve # open ollama server To use ollama in python: (demo)\npip install ollama from ollama import chat, Client, ChatResponse client = Client(host=\u0026#39;http://localhost:11434\u0026#39;) model_use = \u0026#34;llama3.2\u0026#34; completion = client.chat( model=model_use, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Extract the event information.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;}, ], ) completion[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] ","permalink":"https://tzj2006.github.io/posts/llm-study/","summary":"\u003ch2 id=\"part-chatgpt-4o\"\u003ePart ChatGPT 4o\u003c/h2\u003e\n\u003ch3 id=\"chatgpt-4o-api-demo\"\u003eChatGPT 4o API demo\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eopenai\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eOpenAI\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emodel_use\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;gpt-4o-2024-08-06\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eOpenAI\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eapi_key\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Your-API-key\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecompletion\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebeta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecompletions\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparse\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003emodel_use\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Extract the event information.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Alice and Bob are going to a science fair on Friday.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecompletion\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparsed\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: I tried to use model \u0026ldquo;gpt-4o\u0026rdquo; but failed.\u003c/p\u003e\n\u003ch3 id=\"how-to-create-chatgpt-api-key\"\u003eHow to create ChatGPT API Key\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eLog in to \u003ca href=\"https://platform.openai.com/docs/overview\"\u003eopenai\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eUse the search bar to search \u0026ldquo;API keys\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eCreate a new secret key (Shown only once, invisible after closing the tab)\u003c/li\u003e\n\u003cli\u003eGo to billing to add some credit to the account\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"part-unitox-chatgpt\"\u003ePart UniTox ChatGPT\u003c/h2\u003e\n\u003ch3 id=\"read-from-fdagov\"\u003eRead from fda.gov\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eRead the label of the drug we are interested in from a .csv file.\u003c/p\u003e","title":"LLM Study"},{"content":"Today\u0026rsquo;s problem https://leetcode.com/problems/maximum-score-after-splitting-a-string/description\nIntuition Do what the question asks.\nApproach Do what the question asks. Iterate from the back to the front to count how many 1s, iterate from the front to the back to count how many 2s. To speed up the process, we can first count how many 1s are there in the whole sequence, then use another iteration to count the remaining 1s in the sequence by doing a subtraction. Same to the question in Jan.03.2025, the number of 1s in index i + 1 to n = the number of 1s in the sequence - the number of 1s in index 1 to i.\nComplexity Time complexity: $O(N)$, N is the length of s.\nSpace complexity: $O(1)$, only a few new variables are stored.\nCode class Solution: def maxScore(self, s: str) -\u0026gt; int: num1 = 0 for ch in s: if ch == \u0026#39;1\u0026#39;: num1 += 1 now0, now1, ans = 0, 0, 0 for ch in s[:-1]: if ch == \u0026#39;0\u0026#39;: now0 += 1 else: now1 += 1 ans = max(ans, now0 + num1 - now1) return ans ","permalink":"https://tzj2006.github.io/leetcode/2025-01-01/","summary":"\u003col start=\"1422\"\u003e\n\u003cli\u003eMaximum Score After Splitting a String\u003c/li\u003e\n\u003c/ol\u003e\n","title":"LeetCode Daily Question 2025-01-01"},{"content":" 笔记本的 RAM 在关闭屏幕后还耗电吗 markdown 插入图片无法在网站上自动显示 Random 中加一个 checkbox desktop video 英文版 √ 孤波算法是什么 desktop video 多语言切换 √ 是什么成就了一个奢侈品？ Desktop Video 锁屏界面播放 M4 pro V.S. M3 Max ","permalink":"https://tzj2006.github.io/random/","summary":"\u003col\u003e\n\u003cli\u003e笔记本的 RAM 在关闭屏幕后还耗电吗\u003c/li\u003e\n\u003cli\u003emarkdown 插入图片无法在网站上自动显示\u003c/li\u003e\n\u003cli\u003eRandom 中加一个 checkbox\u003c/li\u003e\n\u003cli\u003edesktop video 英文版 √\u003c/li\u003e\n\u003cli\u003e孤波算法是什么\u003c/li\u003e\n\u003cli\u003edesktop video 多语言切换 √\u003c/li\u003e\n\u003cli\u003e是什么成就了一个奢侈品？\u003c/li\u003e\n\u003cli\u003eDesktop Video 锁屏界面播放\u003c/li\u003e\n\u003cli\u003eM4 pro V.S. M3 Max\u003c/li\u003e\n\u003c/ol\u003e","title":"Random Ideas"}]