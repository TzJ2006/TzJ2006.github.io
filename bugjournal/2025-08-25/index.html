<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-08-25 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal">
<meta name="description" content="SOTA VLA">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-08-25/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-08-25/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-08-25/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-08-25">
  <meta property="og:description" content="SOTA VLA">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-08-25T22:37:11-04:00">
    <meta property="article:modified_time" content="2025-08-25T22:37:11-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-08-25">
<meta name="twitter:description" content="SOTA VLA">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-08-25",
      "item": "https://tzj2006.github.io/bugjournal/2025-08-25/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-08-25",
  "name": "Bug Journal 2025-08-25",
  "description": "SOTA VLA",
  "keywords": [
    "Bug Journal"
  ],
  "articleBody": "总结 目前在 Benchmark 1: SQA3D上 GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models 效果最好。达到了 Exact Maching 62.4% 的准确率。 在Benchmark 2: ScanQA上 Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (AAAI 2024) 效果最好。达到了 Exact Maching 31.29% 的准确率。\n动机 想看看VLA / 空间推理VLM 发展得怎么样，SOTA是什么\n调研方式 对于这个Task,我打算从CVPR 2025入手，看看最新的VLA都是如何实现的，又是如何比较的\nPlan 1: 首先，我会先寻找一下CVPR 2025中和VLA有关的任务，并且看看他们的表现 Paper 1: DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering 链接 From 中大 HCP\n结果： 结果一: 这张图用的是 这个benchmark (SQA3D)\n结果二：\n这张图用的是 这个benchmark (ScanQA)\nPaper 2: LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning (CVPR 2024)\n链接\nFrom Fudan, Tencent, and National University of Singapore\n结果 这张图用的是 这个benchmark (ScanQA) 如果横向对比这两个模型，其实有些数据还是这个模型高一些\nPaper 3: Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (AAAI 2024)\n链接\nFrom PKU\n结果 这张图用的是 这个benchmark (ScanQA) 和 这个benchmark (SQA3D)\n如果我们比较这一篇和第一篇CVPR2025 我们会发现，实际上这篇效果更好\nPaper 4: SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding (ECCV 2024)\n链接\nFrom BIGAI, Beijing\n结果 可以看到效果没那么理想\nPaper 5: Scene-LLM: Extending Language Model for 3D Visual Reasoning (WACV 2025) 链接\nFrom Brown University \u0026 Meta\n结果 可以看到这个模型在SQA3D的表现比其他模型更好，达到了54.2%\nPaper 6: Unifying 3D Vision-Language Understanding via Promptable Queries (ECCV 2024)\nFrom BIGAI, Beijing\n链接\n结果 把这个模型放在这里的原因是：这个模型的 MENTOR 和 CIDEr matrix 的表现都比之前的模型好\n注：这个模型和 Paper 4 是同一个组做的\nPaper 7: Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers (NIPS 2024)\nFrom 浙大, 上海AI lab, and 字节\n链接\n结果 这个模型比较了之前的SOTA, 结果SQA3D提高了0.4%的准确率。。。\nPaper 8: GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models (arxiv preprint March 2025)\n链接\nFrom HKU \u0026 上海AI lab\n结果： 这个模型在SQA3D上做到了62.4%的准确率\nPaper 9: Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding (CVPR 2025)\n链接\nFrom The Chinese University of Hong Kong\n结果 这个模型在ScanQA的CIDEr上做到了SOTA\nPaper 10 (To be continued): 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding (arxiv preprint July 2025)\nFrom Shanghai University of Engineering Science \u0026 PKU\n链接\n结果： Report了非常奇怪的结果，需要进一步细看\nBenchmark 1: SQA3D SQA3D\n上图是一个询问的例子\n这个图解释了为什么有些方法会用 “Which” “How” 这些来分类。 总结来说，以这几个词开头的问句较多，且方向不同\n最后统计的是准确率(按照文章中的说法，居然是做一个706维度的分类？) 需要和Ground Truth完全一致\nBenchmark 2: ScanQA ScanQA\n上图是一个询问的例子\n同样是准确率 需要和Ground Truth完全一致\n和上面不同，这里也加入了其他metric去算答案和Ground Truth的相似性\n",
  "wordCount" : "332",
  "inLanguage": "en",
  "datePublished": "2025-08-25T22:37:11-04:00",
  "dateModified": "2025-08-25T22:37:11-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-08-25/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-08-25
    </h1>
    <div class="post-meta"><span title='2025-08-25 22:37:11 -0400 EDT'>August 25, 2025</span>&nbsp;·&nbsp;2 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h3 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h3>
<p>目前在 <a href="#benchmark-1-sqa3d">Benchmark 1: SQA3D</a>上 <a href="#paper-8">GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models</a> 效果最好。达到了 Exact Maching 62.4% 的准确率。
在<a href="#benchmark-2-scanqa">Benchmark 2: ScanQA</a>上 <a href="#paper-3">Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (AAAI 2024)</a> 效果最好。达到了 Exact Maching 31.29% 的准确率。</p>
<h3 id="动机">动机<a hidden class="anchor" aria-hidden="true" href="#动机">#</a></h3>
<p>想看看VLA / 空间推理VLM 发展得怎么样，SOTA是什么</p>
<h3 id="调研方式">调研方式<a hidden class="anchor" aria-hidden="true" href="#调研方式">#</a></h3>
<p>对于这个Task,我打算从CVPR 2025入手，看看最新的VLA都是如何实现的，又是如何比较的</p>
<h3 id="plan-1-首先我会先寻找一下cvpr-2025中和vla有关的任务并且看看他们的表现">Plan 1: 首先，我会先寻找一下CVPR 2025中和VLA有关的任务，并且看看他们的表现<a hidden class="anchor" aria-hidden="true" href="#plan-1-首先我会先寻找一下cvpr-2025中和vla有关的任务并且看看他们的表现">#</a></h3>
<h4 id="paper-1">Paper 1:<a hidden class="anchor" aria-hidden="true" href="#paper-1">#</a></h4>
<p>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering
<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_DSPNet_Dual-vision_Scene_Perception_for_Robust_3D_Question_Answering_CVPR_2025_paper.pdf">链接</a>
From 中大 HCP</p>
<h5 id="结果">结果：<a hidden class="anchor" aria-hidden="true" href="#结果">#</a></h5>
<p><strong>结果一:</strong>
<img alt="1756176421795" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756176421795.png">
这张图用的是 <a href="#benchmark-1-sqa3d">这个benchmark (SQA3D)</a></p>
<p><strong>结果二：</strong></p>
<p><img alt="1756177008320" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756177008320.png">
这张图用的是 <a href="#benchmark-2-scanqa">这个benchmark (ScanQA)</a></p>
<h4 id="paper-2">Paper 2:<a hidden class="anchor" aria-hidden="true" href="#paper-2">#</a></h4>
<p>LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning (CVPR 2024)</p>
<p><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf">链接</a></p>
<p>From Fudan, Tencent, and National University of Singapore</p>
<h5 id="结果-1">结果<a hidden class="anchor" aria-hidden="true" href="#结果-1">#</a></h5>
<p><img alt="1756178028761" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756178028761.png">
这张图用的是 <a href="#benchmark-2-scanqa">这个benchmark (ScanQA)</a>
<img alt="1756177008320" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756177008320.png">
如果横向对比这两个模型，其实有些数据还是这个模型高一些</p>
<h4 id="paper-3">Paper 3:<a hidden class="anchor" aria-hidden="true" href="#paper-3">#</a></h4>
<p>Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (AAAI 2024)</p>
<p><a href="https://www.alphaxiv.org/abs/2402.15933v1">链接</a></p>
<p>From PKU</p>
<h5 id="结果-2">结果<a hidden class="anchor" aria-hidden="true" href="#结果-2">#</a></h5>
<p><img alt="1756178272380" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756178272380.png">
这张图用的是 <a href="#benchmark-2-scanqa">这个benchmark (ScanQA)</a> 和 <a href="#benchmark-1-sqa3d">这个benchmark (SQA3D)</a></p>
<p>如果我们比较这一篇和<a href="#paper-1">第一篇CVPR2025</a>
<img alt="1756176421795" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756176421795.png">
我们会发现，实际上这篇效果更好</p>
<h4 id="paper-4">Paper 4:<a hidden class="anchor" aria-hidden="true" href="#paper-4">#</a></h4>
<p>SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding (ECCV 2024)</p>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-031-72673-6_16">链接</a></p>
<p>From BIGAI, Beijing</p>
<h5 id="结果-3">结果<a hidden class="anchor" aria-hidden="true" href="#结果-3">#</a></h5>
<p><img alt="1756178830668" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756178830668.png">
可以看到效果没那么理想</p>
<h4 id="paper-5">Paper 5:<a hidden class="anchor" aria-hidden="true" href="#paper-5">#</a></h4>
<p>Scene-LLM: Extending Language Model for 3D Visual Reasoning (WACV 2025)
<a href="https://openaccess.thecvf.com/content/WACV2025/papers/Fu_Scene-LLM_Extending_Language_Model_for_3D_Visual_Reasoning_WACV_2025_paper.pdf">链接</a></p>
<p>From Brown University &amp; Meta</p>
<h5 id="结果-4">结果<a hidden class="anchor" aria-hidden="true" href="#结果-4">#</a></h5>
<p><img alt="1756179307643" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756179307643.png">
可以看到这个模型在SQA3D的表现比其他模型更好，达到了54.2%</p>
<h4 id="paper-6">Paper 6:<a hidden class="anchor" aria-hidden="true" href="#paper-6">#</a></h4>
<p>Unifying 3D Vision-Language Understanding via Promptable Queries (ECCV 2024)</p>
<p>From BIGAI, Beijing</p>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-031-72784-9_11">链接</a></p>
<h5 id="结果-5">结果<a hidden class="anchor" aria-hidden="true" href="#结果-5">#</a></h5>
<p><img alt="1756179671294" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756179671294.png">
把这个模型放在这里的原因是：这个模型的 MENTOR 和 CIDEr matrix 的表现都比之前的模型好</p>
<p><em>注：这个模型和 <a href="#paper-4">Paper 4</a> 是同一个组做的</em></p>
<h4 id="paper-7">Paper 7:<a hidden class="anchor" aria-hidden="true" href="#paper-7">#</a></h4>
<p>Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers (NIPS 2024)</p>
<p>From 浙大, 上海AI lab, and 字节</p>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/cebbd24f1e50bcb63d015611fe0fe767-Paper-Conference.pdf">链接</a></p>
<h5 id="结果-6">结果<a hidden class="anchor" aria-hidden="true" href="#结果-6">#</a></h5>
<p><img alt="1756179955898" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756179955898.png">
这个模型比较了之前的SOTA, 结果SQA3D提高了0.4%的准确率。。。</p>
<h4 id="paper-8">Paper 8:<a hidden class="anchor" aria-hidden="true" href="#paper-8">#</a></h4>
<p>GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models (arxiv preprint March 2025)</p>
<p><a href="https://arxiv.org/pdf/2501.01428">链接</a></p>
<p>From HKU &amp; 上海AI lab</p>
<h5 id="结果-7">结果：<a hidden class="anchor" aria-hidden="true" href="#结果-7">#</a></h5>
<p><img alt="1756180371344" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756180371344.png">
这个模型在SQA3D上做到了62.4%的准确率</p>
<h4 id="paper-9">Paper 9:<a hidden class="anchor" aria-hidden="true" href="#paper-9">#</a></h4>
<p>Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene
Understanding (CVPR 2025)</p>
<p><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zheng_Video-3D_LLM_Learning_Position-Aware_Video_Representation_for_3D_Scene_Understanding_CVPR_2025_paper.pdf">链接</a></p>
<p>From The Chinese University of Hong Kong</p>
<h5 id="结果-8">结果<a hidden class="anchor" aria-hidden="true" href="#结果-8">#</a></h5>
<p><img alt="1756180719821" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756180719821.png">
这个模型在ScanQA的CIDEr上做到了SOTA</p>
<h4 id="paper-10-to-be-continued">Paper 10 (To be continued):<a hidden class="anchor" aria-hidden="true" href="#paper-10-to-be-continued">#</a></h4>
<p>3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding (arxiv preprint July 2025)</p>
<p>From Shanghai University of Engineering Science &amp; PKU</p>
<p><a href="https://arxiv.org/pdf/2507.23478">链接</a></p>
<h5 id="结果-9">结果：<a hidden class="anchor" aria-hidden="true" href="#结果-9">#</a></h5>
<p><img alt="1756181023595" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756181023595.png">
Report了非常奇怪的结果，需要进一步细看</p>
<h4 id="benchmark-1-sqa3d">Benchmark 1: SQA3D<a hidden class="anchor" aria-hidden="true" href="#benchmark-1-sqa3d">#</a></h4>
<p><a href="https://axiv.org/pdf/2210.07474">SQA3D</a></p>
<p><img alt="1756176640423" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756176640423.png"></p>
<p>上图是一个询问的例子</p>
<p><img alt="1756176753269" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756176753269.png"></p>
<p>这个图解释了为什么有些方法会用 &ldquo;Which&rdquo; &ldquo;How&rdquo; 这些来分类。
总结来说，以这几个词开头的问句较多，且方向不同</p>
<p><img alt="1756177410073" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756177410073.png">
最后统计的是准确率(按照文章中的说法，居然是做一个706维度的分类？)
需要和Ground Truth完全一致</p>
<h4 id="benchmark-2-scanqa">Benchmark 2: ScanQA<a hidden class="anchor" aria-hidden="true" href="#benchmark-2-scanqa">#</a></h4>
<p><a href="https://arxiv.org/pdf/2112.10482">ScanQA</a></p>
<p><img alt="1756177124363" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756177124363.png"></p>
<p>上图是一个询问的例子</p>
<p><img alt="1756177553329" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756177553329.png">
同样是准确率
需要和Ground Truth完全一致</p>
<p><img alt="1756177596438" loading="lazy" src="https://tzj2006.github.io/images/2025-08-25/1756177596438.png">
和上面不同，这里也加入了其他metric去算答案和Ground Truth的相似性</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
