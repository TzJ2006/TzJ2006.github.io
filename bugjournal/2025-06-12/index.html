<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-06-12 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, Paper Review, CVPR 2025, Robotics">
<meta name="description" content="CVPR 2025 Robotics summary">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-06-12/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-06-12/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-06-12/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-06-12">
  <meta property="og:description" content="CVPR 2025 Robotics summary">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-12T14:49:08+08:00">
    <meta property="article:modified_time" content="2025-06-12T14:49:08+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-06-12">
<meta name="twitter:description" content="CVPR 2025 Robotics summary">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-06-12",
      "item": "https://tzj2006.github.io/bugjournal/2025-06-12/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-06-12",
  "name": "Bug Journal 2025-06-12",
  "description": "CVPR 2025 Robotics summary",
  "keywords": [
    "Bug Journal", "Paper Review", "CVPR 2025", "Robotics"
  ],
  "articleBody": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction CVPR 2025\nFrom 人大 \u0026 上海AI lab\n要达成的事情：\n让机器人能够自我反思到底是哪里做得不好，然后自我调整\n动机：\n人类可以很自然地反思：为什么失败了，为什么机器人不行呢？\n模型实现方式：\n首先，这是一个“语言指导”的RL方法。\n对于这个方法，首先由 LLM 生成一个文字指令：\n比如：现在我要移动一个杯子，我要怎么做\n然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。\n如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。\n现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL\n最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。\n对于修正指令这个步骤，模型一共会输出两条语句，分别是 “语义上的反思” (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)\n最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。\n那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。\n结果：\n更强的学习能力，更强的泛化能力。\n计算要求：\n仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行\n虚拟环境为 RoboMimic 模拟器\n使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs\nRobotic Visual Instruction: A New Paradigm for Human-Robot Communication CVPR 2025\nFrom IC + 上海 AI lab + UCSD …\n目标：\n更好的人机交互\n动机：\n语言有很多冗余信息，那在图片中增加信息不就行了？\n模型实现方式：\n机器人视觉指令 (RoVI) RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：\n箭头：指示运动方向和轨迹 圆圈：标记感兴趣的对象或动作目标 颜色：区分多个指令或动作步骤 数字：指示操作顺序 这种视觉语言具有以下几个优点：\n空间精确性：视觉标记精确地指示3D空间中的位置和路径 时间清晰性：顺序步骤被清晰地划分 直观设计：这些符号易于人类理解和创建 跨文化实用性：视觉指令超越语言障碍 RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。\n靠手动标记了 15K 图片。。。 工作量爆炸💥 而且以后也要人手标。。。\n🤔： 为什么不能自动标，难道作者没有想过这一点吗\n但总之，现在在这个图像的基础上，VLM 会帮忙生成：\n任务的文字描述 可以执行这些任务的代码 代码中包含：\n路径 起点，终点，过程点\n然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取\n算力要求：\nNvidia A40\n现实实验设置：\nUFACTORY X-Arm 6和UR5 两台机械臂 两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。 两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。\n模拟环境设置：\n使用了SAPIEN 作为模拟器。 SIMPLER 作为基础环境。\nMitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation CVPR 2025\nFrom 港科广\n目标：\n缩小人机之间的 Gap\n动机：\n人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap\n问题：\n现在有这些 Gap:\n**视觉环境差异：**人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。\n形态差异： 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。\n尺度和视角： 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。\n解决这一差距的传统方法分为两大类，每类都有显著的局限性：\n预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。\n在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。\n模型实现方式：\nbasically, 就是说希望用一个 Adaptor 来 fill in the gap.\n把从 human demo pretrain embedding 转换成 robot demo embedding.\n对于任务感知也是如此。\n用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。\n结果：\nAlign 一下效果变好了\n算力要求：\n4 * Nvidia A6000\n模拟环境为 RLBench\n真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头\nMoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation CVPR 2025\nFrom 北邮 + 南阳理工 + 清华\n目标：\n导航 + 空间操作\n动机：\n虽然静态的操作已经没问题了，但是若是平台移动就不太好办。\n实现细节：\n这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分\nMoManipVLA使用以下方法实现这些优化问题：\n双退火搜索算法用于基座位置寻找优化 序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案 该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：\nRGB相机进行视觉感知 关节编码器进行本体感知（感知机器人自身位置） (optional) 深度感知以增强障碍物避免\n结果：\n模拟环境：\n模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。\n真实环境（机械臂）：\n在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。\n算力：\n4 * RTX 3090\nROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics CVPR 2025\nFrom OSU and NVIDIA\n目标：\n引导 VLM 2D \u0026 3D 视觉，理解空间结构。\n动机：\nVLM 目前无法理解空间结构。 原因并非 VLM 不行，而是数据不够\"spatial\"\n模型实现细节：\n首先是数据收集：\n输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”\n同时，生成一个俯视图，来看看哪里适合放置一个物体。\n最后在看看这个物体是否适合被放在这个地方。\n对于物体的方位，每一次会从三个角度问问题：\n以机器人为中心的视角 (第一视角) 以物体为中心的视角 (第三视角) 以世界为中心的视角 (fix-cam)\n自我中心：“从您的视角看，书在电脑的左边吗？” 以物体为中心：“从电脑的视角看，书在电脑的左边吗？” 以世界为中心：“从海拔高度看，书在电脑的上方吗？” 这样的好处是可以让 VLM 有更强的空间理解\n结果：\n在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o\n算力要求：\n20-40h * 8 * H100\n模拟环境：\nROBOSPATIAL 数据集，这包括 ScanNet, Matterport3D, 3RScan, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。\n真实环境：\nKinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。 机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。\nThink Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation CVPR 2025\nFrom 上交，复旦，上海 AI lab\n目标：\n如何避免灾难性遗忘\n动机：\n有些动作有相似之处，比如递筷子和递镊子有相似之处 那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？\n模型实现细节：\nPPL 框架的核心组件包括：\n输入编码器： 本体感觉编码器：处理机器人的关节状态和夹爪姿势 视觉编码器：处理场景的 RGB 图像 光流编码器：处理光流信息以捕获运动模式 文本编码器：处理任务的语言指令\n基元提示： 跨任务共享的基本运动模式的学习表示 注入到多头自注意力层的键和值中\n终身提示 (Lifelong Prompts)： 在终身学习期间为新任务学习的特定于任务的提示 与原始提示连接以自定义模型的行为\n运动感知提示查询 (Motion-Aware Prompt Query)： 结合光流和文本指令信息 用于确定不同原始提示的相关性\n扩散Transformer (Diffusion Transformer)： 基于条件输入和提示生成机器人动作\n算力：\n论文中没有直接说明具体的GPU类型和训练时间。\n模拟环境：\n模拟实验是在基于 MimicGen 和 LIBERO 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。\n机械臂：\n真实世界的实验是在 Franka Panda 机械臂上进行的。\n数据集：\n论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。\n获取方式：\n这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。\nGenerating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision CVPR 2025\nFrom Kyoto University\n",
  "wordCount" : "455",
  "inLanguage": "en",
  "datePublished": "2025-06-12T14:49:08+08:00",
  "dateModified": "2025-06-12T14:49:08+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-06-12/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-06-12
    </h1>
    <div class="post-meta"><span title='2025-06-12 14:49:08 +0800 CST'>June 12, 2025</span>&nbsp;·&nbsp;3 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h2 id="phoenix-a-motion-based-self-reflection-framework-for-fine-grained-robotic-action-correction">Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction<a hidden class="anchor" aria-hidden="true" href="#phoenix-a-motion-based-self-reflection-framework-for-fine-grained-robotic-action-correction">#</a></h2>
<p>CVPR 2025</p>
<p>From 人大 &amp; 上海AI lab</p>
<p>要达成的事情：</p>
<p>让机器人能够自我反思到底是哪里做得不好，然后自我调整</p>
<p>动机：</p>
<p>人类可以很自然地反思：为什么失败了，为什么机器人不行呢？</p>
<p>模型实现方式：</p>
<p><img alt="1749711299495" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749711299495.png"></p>
<p>首先，这是一个“语言指导”的RL方法。</p>
<p>对于这个方法，首先由 LLM 生成一个文字指令：</p>
<p>比如：现在我要移动一个杯子，我要怎么做</p>
<p>然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。</p>
<p>如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。</p>
<p>现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL</p>
<p>最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。</p>
<p>对于修正指令这个步骤，模型一共会输出两条语句，分别是 &ldquo;语义上的反思&rdquo; (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)</p>
<p>最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。</p>
<p>那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。</p>
<p>结果：</p>
<p><img alt="1749712321377" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749712321377.png"></p>
<p><img alt="1749712335120" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749712335120.png"></p>
<p>更强的学习能力，更强的泛化能力。</p>
<p>计算要求：</p>
<p>仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行</p>
<p>虚拟环境为 RoboMimic 模拟器</p>
<p>使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs</p>
<hr>
<h2 id="robotic-visual-instruction-a-new-paradigm-for-human-robot-communication">Robotic Visual Instruction: A New Paradigm for Human-Robot Communication<a hidden class="anchor" aria-hidden="true" href="#robotic-visual-instruction-a-new-paradigm-for-human-robot-communication">#</a></h2>
<p>CVPR 2025</p>
<p>From IC + 上海 AI lab + UCSD &hellip;</p>
<p>目标：</p>
<p>更好的人机交互</p>
<p>动机：</p>
<p>语言有很多冗余信息，那在图片中增加信息不就行了？</p>
<p>模型实现方式：</p>
<p>机器人视觉指令 (RoVI)
RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：</p>
<p>箭头：指示运动方向和轨迹
圆圈：标记感兴趣的对象或动作目标
颜色：区分多个指令或动作步骤
数字：指示操作顺序
这种视觉语言具有以下几个优点：</p>
<p>空间精确性：视觉标记精确地指示3D空间中的位置和路径
时间清晰性：顺序步骤被清晰地划分
直观设计：这些符号易于人类理解和创建
跨文化实用性：视觉指令超越语言障碍
RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。</p>
<p><img alt="1749712762668" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749712762668.png"></p>
<p>靠手动标记了 15K 图片。。。
<del>工作量爆炸💥</del>
而且以后也要人手标。。。</p>
<p>🤔： 为什么不能自动标，难道作者没有想过这一点吗</p>
<p>但总之，现在在这个图像的基础上，VLM 会帮忙生成：</p>
<ol>
<li>任务的文字描述</li>
<li>可以执行这些任务的代码</li>
</ol>
<p>代码中包含：</p>
<p>路径 起点，终点，过程点</p>
<p>然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取</p>
<p><img alt="1749713264334" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713264334.png"></p>
<p><img alt="1749713277440" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713277440.png"></p>
<p>算力要求：</p>
<p>Nvidia A40</p>
<p>现实实验设置：</p>
<p>UFACTORY X-Arm 6和UR5 两台机械臂
两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。
两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。</p>
<p>模拟环境设置：</p>
<p>使用了SAPIEN 作为模拟器。
SIMPLER 作为基础环境。</p>
<hr>
<h2 id="mitigating-the-human-robot-domain-discrepancy-in-visual-pre-training-for-robotic-manipulation">Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation<a hidden class="anchor" aria-hidden="true" href="#mitigating-the-human-robot-domain-discrepancy-in-visual-pre-training-for-robotic-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From 港科广</p>
<p>目标：</p>
<p>缩小人机之间的 Gap</p>
<p>动机：</p>
<p>人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap</p>
<p>问题：</p>
<p>现在有这些 Gap:</p>
<p>**视觉环境差异：**人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。</p>
<p><strong>形态差异：</strong> 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。</p>
<p><strong>尺度和视角：</strong> 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。</p>
<p>解决这一差距的传统方法分为两大类，每类都有显著的局限性：</p>
<p>预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。</p>
<p>在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。</p>
<p>模型实现方式：</p>
<p><img alt="1749713649234" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713649234.png"></p>
<p>basically, 就是说希望用一个 Adaptor 来 fill in the gap.</p>
<p>把从 human demo pretrain embedding 转换成 robot demo embedding.</p>
<p><img alt="1749713795748" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713795748.png"></p>
<p>对于任务感知也是如此。</p>
<p>用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。</p>
<p>结果：</p>
<p><img alt="1749713835939" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713835939.png"></p>
<p>Align 一下效果变好了</p>
<p>算力要求：</p>
<p>4 * Nvidia A6000</p>
<p>模拟环境为 RLBench</p>
<p>真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头</p>
<hr>
<h2 id="momanipvla-transferring-vision-language-action-models-for-general-mobile-manipulation">MoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation<a hidden class="anchor" aria-hidden="true" href="#momanipvla-transferring-vision-language-action-models-for-general-mobile-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From 北邮 + 南阳理工 + 清华</p>
<p>目标：</p>
<p>导航 + 空间操作</p>
<p>动机：</p>
<p>虽然静态的操作已经没问题了，但是若是平台移动就不太好办。</p>
<p>实现细节：</p>
<p>这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分</p>
<p>MoManipVLA使用以下方法实现这些优化问题：</p>
<p>双退火搜索算法用于基座位置寻找优化
序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案
该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：</p>
<p>RGB相机进行视觉感知
关节编码器进行本体感知（感知机器人自身位置）
(optional) 深度感知以增强障碍物避免</p>
<p>结果：</p>
<p><img alt="1749714857163" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749714857163.png"></p>
<p>模拟环境：</p>
<p>模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。</p>
<p>真实环境（机械臂）：</p>
<p>在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。</p>
<p>算力：</p>
<p>4 * RTX 3090</p>
<hr>
<h2 id="robospatial-teaching-spatial-understanding-to-2d-and-3d-vision-language-models-for-robotics">ROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics<a hidden class="anchor" aria-hidden="true" href="#robospatial-teaching-spatial-understanding-to-2d-and-3d-vision-language-models-for-robotics">#</a></h2>
<p>CVPR 2025</p>
<p>From OSU and NVIDIA</p>
<p>目标：</p>
<p>引导 VLM 2D &amp; 3D 视觉，理解空间结构。</p>
<p>动机：</p>
<p>VLM 目前无法理解空间结构。
原因并非 VLM 不行，而是数据不够&quot;spatial&quot;</p>
<p>模型实现细节：</p>
<p>首先是数据收集：</p>
<p><img alt="1749717733449" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749717733449.png"></p>
<p>输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”</p>
<p>同时，生成一个俯视图，来看看哪里适合放置一个物体。</p>
<p>最后在看看这个物体是否适合被放在这个地方。</p>
<p>对于物体的方位，每一次会从三个角度问问题：</p>
<p>以机器人为中心的视角 (第一视角)
以物体为中心的视角 (第三视角)
以世界为中心的视角 (fix-cam)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-md" data-lang="md"><span class="line"><span class="cl">自我中心：“从您的视角看，书在电脑的左边吗？”
</span></span><span class="line"><span class="cl">以物体为中心：“从电脑的视角看，书在电脑的左边吗？”  
</span></span><span class="line"><span class="cl">以世界为中心：“从海拔高度看，书在电脑的上方吗？”
</span></span></code></pre></div><p>这样的好处是可以让 VLM 有更强的空间理解</p>
<p>结果：</p>
<p><img alt="1749717985437" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749717985437.png"></p>
<p>在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o</p>
<p>算力要求：</p>
<p>20-40h * 8 * H100</p>
<p>模拟环境：</p>
<p>ROBOSPATIAL 数据集，这包括 <a href="https://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf">ScanNet</a>, <a href="https://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf">Matterport3D</a>, <a href="https://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf">3RScan</a>, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。</p>
<p>真实环境：</p>
<p>Kinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。
机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。</p>
<hr>
<h2 id="think-small-act-big-primitive-prompt-learning-for-lifelong-robot-manipulation">Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation<a hidden class="anchor" aria-hidden="true" href="#think-small-act-big-primitive-prompt-learning-for-lifelong-robot-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From 上交，复旦，上海 AI lab</p>
<p>目标：</p>
<p>如何避免灾难性遗忘</p>
<p>动机：</p>
<p>有些动作有相似之处，比如递筷子和递镊子有相似之处
那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？</p>
<p>模型实现细节：</p>
<p><img alt="1749718882124" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749718882124.png"></p>
<p>PPL 框架的核心组件包括：</p>
<p>输入编码器：
本体感觉编码器：处理机器人的关节状态和夹爪姿势
视觉编码器：处理场景的 RGB 图像
光流编码器：处理光流信息以捕获运动模式
文本编码器：处理任务的语言指令</p>
<p>基元提示：
跨任务共享的基本运动模式的学习表示
注入到多头自注意力层的键和值中</p>
<p>终身提示 (Lifelong Prompts)：
在终身学习期间为新任务学习的特定于任务的提示
与原始提示连接以自定义模型的行为</p>
<p>运动感知提示查询 (Motion-Aware Prompt Query)：
结合光流和文本指令信息
用于确定不同原始提示的相关性</p>
<p>扩散Transformer (Diffusion Transformer)：
基于条件输入和提示生成机器人动作</p>
<p>算力：</p>
<p>论文中没有直接说明具体的GPU类型和训练时间。</p>
<p>模拟环境：</p>
<p>模拟实验是在基于 <a href="https://arxiv.org/pdf/2310.05905">MimicGen</a> 和 <a href="https://arxiv.org/pdf/2310.05905">LIBERO</a> 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。</p>
<p>机械臂：</p>
<p>真实世界的实验是在 Franka Panda 机械臂上进行的。</p>
<p>数据集：</p>
<p>论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。</p>
<p>获取方式：</p>
<p>这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。</p>
<hr>
<h2 id="generating-6dof-object-manipulation-trajectories-from-action-description-in-egocentric-vision">Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision<a hidden class="anchor" aria-hidden="true" href="#generating-6dof-object-manipulation-trajectories-from-action-description-in-egocentric-vision">#</a></h2>
<p>CVPR 2025</p>
<p>From Kyoto University</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
