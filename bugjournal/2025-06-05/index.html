<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-06-05 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, Paper Review">
<meta name="description" content="DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning
发表时间：24 Feb 2025
动机
Robust，还得是Robust。
为什么不加数据(ο´･д･)??
过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。
那怎么办呢(ο´･д･)??
很简单，纯计算模拟不就是了。
主要论点
用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人
这样就不需要实际操作，但是可以直接获得大量数据。
模型流程图

一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.
思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以
机器人一共有两段动作
一段是碰到物体前的动作
一段是碰到物体后的动作
对于第一段，直接用一个变换矩阵变换
对于第二段，直接规划一个新路径 (use RRT-Connect)
现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)
如果可以用的话
然后通过模拟环境生成这个路径的图像
实验设定
虚拟环境：1 条 GroundTruth
真实环境: 1 条真人数据 &#43; 2 次 Replay（机器人自己模拟一遍这个轨迹）
一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)
结果


V.S. RoboGround

  
      
          方面
          DemoGen
          RoboGround
      
  
  
      
          目标问题
          数据高效、空间泛化性差的视觉模仿学习
          多任务泛化能力差、语义-空间信息连接弱
      
      
          核心思想
          从少量人类演示中合成大量视觉演示数据用于模仿学习
          grounding mask（掩码）作为embedding增强泛化
      
      
          数据生成方式
          从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像
          构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 &#43; placement mask
      
      
          人类演示
          一条
          在仿真中自动生成，无需真实 rollouts
      
      
          任务表征形式
          (图像帧, 末端动作)对
          图像 &#43; mask &#43; 指令 &#43; robot state
      
      
          中间表示
          None（直接预测动作）
          掩码（mask）作为空间引导
      
      
          依赖模型
          Immitation Learning
          利用 VLM &#43; Grounded Perceiver 构建 mask-guided policy
      
      
          泛化方式
          利用空间重定向与图像合成覆盖更多初始状态
          通过 grounding masks 和多样 instruction 提升语义-空间泛化
      
  


DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove
">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-06-05/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-06-05/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-06-05/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-06-05">
  <meta property="og:description" content="DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025
动机 Robust，还得是Robust。
为什么不加数据(ο´･д･)??
过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。
那怎么办呢(ο´･д･)??
很简单，纯计算模拟不就是了。
主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人
这样就不需要实际操作，但是可以直接获得大量数据。
模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.
思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以
机器人一共有两段动作
一段是碰到物体前的动作
一段是碰到物体后的动作
对于第一段，直接用一个变换矩阵变换
对于第二段，直接规划一个新路径 (use RRT-Connect)
现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)
如果可以用的话
然后通过模拟环境生成这个路径的图像
实验设定 虚拟环境：1 条 GroundTruth
真实环境: 1 条真人数据 &#43; 2 次 Replay（机器人自己模拟一遍这个轨迹）
一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)
结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 &#43; placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 &#43; mask &#43; 指令 &#43; robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM &#43; Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove ">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-05T10:00:00+08:00">
    <meta property="article:modified_time" content="2025-06-05T10:00:00+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-06-05">
<meta name="twitter:description" content="DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning
发表时间：24 Feb 2025
动机
Robust，还得是Robust。
为什么不加数据(ο´･д･)??
过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。
那怎么办呢(ο´･д･)??
很简单，纯计算模拟不就是了。
主要论点
用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人
这样就不需要实际操作，但是可以直接获得大量数据。
模型流程图

一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.
思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以
机器人一共有两段动作
一段是碰到物体前的动作
一段是碰到物体后的动作
对于第一段，直接用一个变换矩阵变换
对于第二段，直接规划一个新路径 (use RRT-Connect)
现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)
如果可以用的话
然后通过模拟环境生成这个路径的图像
实验设定
虚拟环境：1 条 GroundTruth
真实环境: 1 条真人数据 &#43; 2 次 Replay（机器人自己模拟一遍这个轨迹）
一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)
结果


V.S. RoboGround

  
      
          方面
          DemoGen
          RoboGround
      
  
  
      
          目标问题
          数据高效、空间泛化性差的视觉模仿学习
          多任务泛化能力差、语义-空间信息连接弱
      
      
          核心思想
          从少量人类演示中合成大量视觉演示数据用于模仿学习
          grounding mask（掩码）作为embedding增强泛化
      
      
          数据生成方式
          从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像
          构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 &#43; placement mask
      
      
          人类演示
          一条
          在仿真中自动生成，无需真实 rollouts
      
      
          任务表征形式
          (图像帧, 末端动作)对
          图像 &#43; mask &#43; 指令 &#43; robot state
      
      
          中间表示
          None（直接预测动作）
          掩码（mask）作为空间引导
      
      
          依赖模型
          Immitation Learning
          利用 VLM &#43; Grounded Perceiver 构建 mask-guided policy
      
      
          泛化方式
          利用空间重定向与图像合成覆盖更多初始状态
          通过 grounding masks 和多样 instruction 提升语义-空间泛化
      
  


DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-06-05",
      "item": "https://tzj2006.github.io/bugjournal/2025-06-05/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-06-05",
  "name": "Bug Journal 2025-06-05",
  "description": "DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025\n动机 Robust，还得是Robust。\n为什么不加数据(ο´･д･)??\n过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\n那怎么办呢(ο´･д･)??\n很简单，纯计算模拟不就是了。\n主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\n这样就不需要实际操作，但是可以直接获得大量数据。\n模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\n思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\n机器人一共有两段动作\n一段是碰到物体前的动作\n一段是碰到物体后的动作\n对于第一段，直接用一个变换矩阵变换\n对于第二段，直接规划一个新路径 (use RRT-Connect)\n现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\n如果可以用的话\n然后通过模拟环境生成这个路径的图像\n实验设定 虚拟环境：1 条 GroundTruth\n真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\n一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\n结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 + mask + 指令 + robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM + Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove ",
  "keywords": [
    "Bug Journal", "Paper Review"
  ],
  "articleBody": "DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning 发表时间：24 Feb 2025\n动机 Robust，还得是Robust。\n为什么不加数据(ο´･д･)??\n过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。\n那怎么办呢(ο´･д･)??\n很简单，纯计算模拟不就是了。\n主要论点 用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人\n这样就不需要实际操作，但是可以直接获得大量数据。\n模型流程图 一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.\n思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以\n机器人一共有两段动作\n一段是碰到物体前的动作\n一段是碰到物体后的动作\n对于第一段，直接用一个变换矩阵变换\n对于第二段，直接规划一个新路径 (use RRT-Connect)\n现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)\n如果可以用的话\n然后通过模拟环境生成这个路径的图像\n实验设定 虚拟环境：1 条 GroundTruth\n真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）\n一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)\n结果 V.S. RoboGround 方面 DemoGen RoboGround 目标问题 数据高效、空间泛化性差的视觉模仿学习 多任务泛化能力差、语义-空间信息连接弱 核心思想 从少量人类演示中合成大量视觉演示数据用于模仿学习 grounding mask（掩码）作为embedding增强泛化 数据生成方式 从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像 构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask 人类演示 一条 在仿真中自动生成，无需真实 rollouts 任务表征形式 (图像帧, 末端动作)对 图像 + mask + 指令 + robot state 中间表示 None（直接预测动作） 掩码（mask）作为空间引导 依赖模型 Immitation Learning 利用 VLM + Grounded Perceiver 构建 mask-guided policy 泛化方式 利用空间重定向与图像合成覆盖更多初始状态 通过 grounding masks 和多样 instruction 提升语义-空间泛化 DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove 一个这样的力反馈手套\n可以链接到灵巧手上，然后展示反馈物体的力\n盲抓分辨物体\n盲眼抓杯子\nReactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation 发表时间：23 Apr 2025\n动机 人有触觉，为什么机器人不行？\n所以我们为机器人加上了触觉。\n并且用触觉来优化模型.\n可以是，触觉数据怎么来呢？🤔\n在机器人手上加上触觉组件就好了\n模型流程 方法很简单：\nimage -\u003e 正常的 CV Encoder -\u003e Touch Encoder -\u003e (Action -\u003e Touch Encoder) * N -\u003e image2 …\n实现效果 Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control 发表时间：June 23 2025\n动机 现在机器人的表情不够生动，希望能生动一点\n用一段语音输入进机器人\n为什么选语音？\n因为语音有语气, 语气中可能有细微的表情差别\n“我没事…” “我没事！”和 “我?没事” 是有区别的\n这更适合模型生成表情.\n而现有机器人的表情做得不好，所以这个团队自己做了一个可以做更丰富表情的机器人脸\n实现方式 第一部分，有一个带有丰富语气的语音数据集 + 对应的 3D 人脸建模数据\n第二部分，为了训练模型 -\u003e 电机； 随机生成了 5000 个 3D 建模 让电机去拟合这个表情\n↑以上过程都是在虚拟环境中训练完，然后搬到真实环境做的。\n",
  "wordCount" : "222",
  "inLanguage": "en",
  "datePublished": "2025-06-05T10:00:00+08:00",
  "dateModified": "2025-06-05T10:00:00+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-06-05/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-06-05
    </h1>
    <div class="post-meta"><span title='2025-06-05 10:00:00 +0800 CST'>June 5, 2025</span>&nbsp;·&nbsp;2 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h2 id="demogen-synthetic-demonstration-generation-for-data-efficient-visuomotor-policy-learning">DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning<a hidden class="anchor" aria-hidden="true" href="#demogen-synthetic-demonstration-generation-for-data-efficient-visuomotor-policy-learning">#</a></h2>
<p><a href="https://arxiv.org/abs/2502.16932">发表时间：24 Feb 2025</a></p>
<h4 id="动机">动机<a hidden class="anchor" aria-hidden="true" href="#动机">#</a></h4>
<p>Robust，还得是Robust。</p>
<p>为什么不加数据(ο´･д･)??</p>
<p>过拟合严重，加数据要每个场景都测很多次，哪怕不用人测也要机器人测。</p>
<p>那怎么办呢(ο´･д･)??</p>
<p>很简单，纯计算模拟不就是了。</p>
<h4 id="主要论点">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点">#</a></h4>
<p>用一个演示数据生成很多演示数据，用很多的演示数据 train 机器人</p>
<p>这样就不需要实际操作，但是可以直接获得大量数据。</p>
<h4 id="模型流程图">模型流程图<a hidden class="anchor" aria-hidden="true" href="#模型流程图">#</a></h4>
<p><img alt="1749026226827" loading="lazy" src="https://tzj2006.github.io/images/2025-06-04/1749026226827.png"></p>
<p>一言以蔽之：先算个大概，再看看这个“大概动作”能不能 work.</p>
<p>思路是：一张 PPT 中的图片可以旋转放大缩小，路径也可以</p>
<p>机器人一共有两段动作</p>
<p>一段是碰到物体前的动作</p>
<p>一段是碰到物体后的动作</p>
<p>对于第一段，直接用一个变换矩阵变换</p>
<p>对于第二段，直接规划一个新路径 (use RRT-Connect)</p>
<p>现在我们通过模拟环境验证这个路径行不行 (会不会穿模，会不会碰撞)</p>
<p>如果可以用的话</p>
<p>然后通过模拟环境生成这个路径的图像</p>
<h4 id="实验设定">实验设定<a hidden class="anchor" aria-hidden="true" href="#实验设定">#</a></h4>
<p>虚拟环境：1 条 GroundTruth</p>
<p>真实环境: 1 条真人数据 + 2 次 Replay（机器人自己模拟一遍这个轨迹）</p>
<p>一共模拟 10k 帧左右的数据 (收益递减，10k 属于一个平衡点)</p>
<h4 id="结果">结果<a hidden class="anchor" aria-hidden="true" href="#结果">#</a></h4>
<p><img alt="1749093303146" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749093303146.png"></p>
<p><img alt="1749093327428" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749093327428.png"></p>
<h4 id="vs-roboground">V.S. RoboGround<a hidden class="anchor" aria-hidden="true" href="#vs-roboground">#</a></h4>
<table>
  <thead>
      <tr>
          <th><strong>方面</strong></th>
          <th><strong>DemoGen</strong></th>
          <th><strong>RoboGround</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>目标问题</strong></td>
          <td>数据高效、空间泛化性差的视觉模仿学习</td>
          <td>多任务泛化能力差、语义-空间信息连接弱</td>
      </tr>
      <tr>
          <td><strong>核心思想</strong></td>
          <td>从少量人类演示中<strong>合成大量视觉演示数据</strong>用于模仿学习</td>
          <td>grounding mask（掩码）作为embedding增强泛化</td>
      </tr>
      <tr>
          <td><strong>数据生成方式</strong></td>
          <td>从 BEHAVIOR 数据库中采样演示 → 空间重定向 → 物理优化 → 点云合成图像</td>
          <td>构建大量高复杂度场景 → 使用 LVLM（如 GLaMM）生成目标物体 + placement mask</td>
      </tr>
      <tr>
          <td><strong>人类演示</strong></td>
          <td>一条</td>
          <td>在仿真中自动生成，无需真实 rollouts</td>
      </tr>
      <tr>
          <td><strong>任务表征形式</strong></td>
          <td>(图像帧, 末端动作)<strong>对</strong></td>
          <td>图像 + mask + 指令 + robot state</td>
      </tr>
      <tr>
          <td><strong>中间表示</strong></td>
          <td>None（直接预测动作）</td>
          <td>掩码（mask）作为空间引导</td>
      </tr>
      <tr>
          <td><strong>依赖模型</strong></td>
          <td>Immitation Learning</td>
          <td>利用 VLM + Grounded Perceiver 构建 mask-guided policy</td>
      </tr>
      <tr>
          <td><strong>泛化方式</strong></td>
          <td>利用空间重定向与图像合成覆盖更多初始状态</td>
          <td>通过 grounding masks 和多样 instruction 提升语义-空间泛化</td>
      </tr>
  </tbody>
</table>
<hr>
<h1 id="doglove-dexterous-manipulation-with-a-low-cost-open-source-haptic-force-feedback-glove">DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove<a hidden class="anchor" aria-hidden="true" href="#doglove-dexterous-manipulation-with-a-low-cost-open-source-haptic-force-feedback-glove">#</a></h1>
<p><img alt="1749095773362" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749095773362.png"></p>
<p>一个这样的力反馈手套</p>
<p><img alt="1749095795246" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749095795246.png"></p>
<p>可以链接到灵巧手上，然后展示反馈物体的力</p>
<p><img alt="1749095840431" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749095840431.png"></p>
<p>盲抓分辨物体</p>
<p><img alt="1749096044706" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749096044706.png"></p>
<p>盲眼抓杯子</p>
<p><img alt="1749095932041" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749095932041.png"></p>
<hr>
<h2 id="reactive-diffusion-policy-slow-fast-visual-tactile-policy-learning-for-contact-rich-manipulation">Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation<a hidden class="anchor" aria-hidden="true" href="#reactive-diffusion-policy-slow-fast-visual-tactile-policy-learning-for-contact-rich-manipulation">#</a></h2>
<p><a href="https://arxiv.org/abs/2503.02881">发表时间：23 Apr 2025</a></p>
<h4 id="动机-1">动机<a hidden class="anchor" aria-hidden="true" href="#动机-1">#</a></h4>
<p>人有触觉，为什么机器人不行？</p>
<p>所以我们为机器人加上了触觉。</p>
<p>并且用触觉来优化模型.</p>
<p>可以是，触觉数据怎么来呢？🤔</p>
<p>在机器人手上加上触觉组件就好了</p>
<h4 id="模型流程">模型流程<a hidden class="anchor" aria-hidden="true" href="#模型流程">#</a></h4>
<p><img alt="1749103232421" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749103232421.png"></p>
<p>方法很简单：</p>
<p>image -&gt; 正常的 CV Encoder -&gt; Touch Encoder -&gt; (Action -&gt; Touch Encoder) * N -&gt; image2 &hellip;</p>
<h4 id="实现效果">实现效果<a hidden class="anchor" aria-hidden="true" href="#实现效果">#</a></h4>
<p><img alt="result" loading="lazy" src="http://hxu.rocks/images/pub-pic/rdp.gif"></p>
<hr>
<h2 id="morpheus-a-neural-driven-animatronic-face-with-hybrid-actuation-and-diverse-emotion-control">Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control<a hidden class="anchor" aria-hidden="true" href="#morpheus-a-neural-driven-animatronic-face-with-hybrid-actuation-and-diverse-emotion-control">#</a></h2>
<p><a href="https://roboticsconference.org/program/papers/80/">发表时间：June 23 2025</a></p>
<h4 id="动机-2">动机<a hidden class="anchor" aria-hidden="true" href="#动机-2">#</a></h4>
<p>现在机器人的表情不够生动，希望能生动一点</p>
<p><img alt="1749106962036" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749106962036.png"></p>
<p>用一段语音输入进机器人</p>
<p>为什么选语音？</p>
<p>因为语音有语气, 语气中可能有细微的表情差别</p>
<p><del>“我没事&hellip;” “我没事！”和 “我?没事” 是有区别的</del></p>
<p>这更适合模型生成表情.</p>
<p>而现有机器人的表情做得不好，所以这个团队自己做了一个可以做更丰富表情的机器人脸</p>
<h4 id="实现方式">实现方式<a hidden class="anchor" aria-hidden="true" href="#实现方式">#</a></h4>
<p><img alt="1749107044660" loading="lazy" src="https://tzj2006.github.io/images/2025-06-05/1749107044660.png"></p>
<p>第一部分，有一个带有丰富语气的语音数据集 + 对应的 3D 人脸建模数据</p>
<p>第二部分，为了训练模型 -&gt; 电机； 随机生成了 5000 个 3D 建模 让电机去拟合这个表情</p>
<p>↑以上过程都是在虚拟环境中训练完，然后搬到真实环境做的。</p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
