<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-05-25 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, Robotic, Paper">
<meta name="description" content="Summary of Robotics papers">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-05-14/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-05-14/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-05-14/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-05-25">
  <meta property="og:description" content="Summary of Robotics papers">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-05-15T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-05-15T00:00:00+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-05-25">
<meta name="twitter:description" content="Summary of Robotics papers">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-05-25",
      "item": "https://tzj2006.github.io/bugjournal/2025-05-14/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-05-25",
  "name": "Bug Journal 2025-05-25",
  "description": "Summary of Robotics papers",
  "keywords": [
    "Bug Journal", "Robotic", "Paper"
  ],
  "articleBody": "Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (ACT Algorithm) 主要论点 ALOHA 是一个开放源代码的低成本双臂远程操作硬件系统，整体成本低于 20,000 美元，主要由现成的机器人组件和少量 3D 打印部件组成。该系统支持精细、动态和接触丰富的任务，如穿拉链、乒乓球颠球和链条组装等。\n为了应对模仿学习中政策误差累积的问题，研究人员提出了 ACT 算法。该算法基于 Transformer 架构，采用条件变分自编码器（CVAE）框架，通过预测动作序列（即“动作块”）而非单步动作，减少了有效的预测范围，从而提高了学习效率和稳定性。\n在六个现实世界的精细操作任务中，如打开透明调味杯盖和插入电池等，ALOHA 系统仅通过约 10 分钟的示范数据（约 50 次演示）就实现了 80% 至 90% 的成功率，展示了其在低成本硬件上的高效学习能力。\n创新点 低成本高性能：ALOHA 系统在成本控制的同时，仍能执行复杂的双臂操作任务，降低了高精度机器人研究的门槛。\n动作块预测 ：ACT 算法通过预测动作序列，减少了政策误差的累积，提高了模仿学习的稳定性和效率。\n快速学习能力：系统仅需少量的演示数据即可学习复杂任务，展示了高效的学习能力。\n解决的难点 高精度双臂系统价格昂贵，限制研究和数据获取 。\n模仿学习中长序列预测误差累积问题严重 。\n设计低于 $20,000 的双臂系统 ALOHA + 高效 Transformer 模仿学习方法 ACT。\n通过动作 chunking 预测提升长序列动作稳定性。\n还需要解决的难点 硬件精度限制 ：尽管系统成本低廉，但硬件精度的限制可能影响在更复杂任务中的表现。\n泛化能力：系统在面对未见过的任务或环境时的泛化能力仍需进一步提升。\n实时性能 ：在实际应用中，如何确保系统的实时响应能力和稳定性是一个挑战。\nAutoregressive Action Sequence Learning for Robotic Manipulation 主要论点 论文的核心思想是将机器人动作表示为序列数据，并通过自回归序列建模生成动作序列。为此，作者提出了两项关键技术：\nChunking Causal Transformer (CCT)：该模型扩展了传统因果变换器的单步预测能力，支持在一个步骤中预测多个动作“块”。这种方法提高了对不同控制频率任务的适应性，并通过减少自回归步骤提高了效率 Autoregressive Policy (ARP)：基于CCT，作者设计了ARP架构，用于生成混合动作序列，解决多种机器人操作任务。该架构在Push-T、ALOHA和RLBench等多种机器人操作环境中进行了评估，结果显示ARP作为通用架构，在所有测试基准中匹配或超越了特定环境下的最新技术，同时在计算和参数规模上更为高效 简而言之，CCT 可以预测未来的多个动作 创新点 多动作块预测机制：CCT模型引入了预测多个动作块的能力，使其能够处理不同类型和频率的动作数据，提高了模型的灵活性和效率\n混合动作序列设计：通过将不同类型的动作（如关节位置、2D像素坐标和末端执行器姿态）混合在一个序列中，并为每种动作类型使用不同的块大小，增强了模型对复杂任务的适应能力\n通用策略架构：ARP架构作为一个通用的策略架构，在多个不同的机器人操作环境中表现出色，显示出其广泛的适用性和高效性\n解决的难点 自回归策略效率低: 传统每次只预测一个动作的自回归方法效率低，难以适配高频任务。\n动作混合表示困难：连续值与离散值混合表示在序列学习中不易统一建模。\n还需要解决的难点 动作数据的异质性 ：机器人动作数据通常包括连续值和离散值，如何有效地将这些异质数据表示为序列，并进行建模，是一个挑战。\n高频控制任务的建模 ：现有的自回归架构在处理高频控制任务时存在限制，如何扩展模型以支持高频控制任务，需要进一步研究\n混合动作序列的生成与优化 ：在生成包含多种动作类型的混合序列时，如何确保各动作类型之间的协调性和整体序列的最优性，是一个需要解决的问题\nπ0: A Vision-Language-Action Flow Model for General Robot Control 主要论点 π₀模型的核心是将预训练的视觉-语言模型（VLM）与流匹配架构相结合，形成一个统一的视觉-语言-动作（VLA）模型，用于通用机器人控制。该模型通过在多个灵巧机器人平台（包括单臂、双臂和移动操纵器）的大型多样化数据集上进行训练，学习从视觉和语言输入到动作输出的映射关系。\n模型的训练分为两个阶段：\n预训练阶段 ：在大规模多样化的数据集上进行训练，学习通用的感知和语言理解能力。 微调阶段：在特定任务的高质量数据上进行微调，以提高在特定任务上的性能。 该模型在多个任务上进行了评估，包括折叠衣物、清洁桌子和组装盒子等，展示了其在零样本学习、语言指令遵循和新技能获取方面的能力。\n创新点 流匹配架构 ：引入流匹配技术生成连续的动作分布，适用于高频率和灵巧的任务。\n跨机器人平台训练 ：结合多种机器人类型的数据进行训练，使模型能够适应不同的机器人配置和动作表示。\n高效推理机制 ：模型设计允许高效的推理过程，通过缓存和重用注意力键值对来减少计算量，适应实时控制的需求。\n解决的难点 VLA模型对高频、连续动作建模困难。\n泛化能力差，多机器人平台适应性弱。\n引入流匹配（flow matching）机制，生成连续动作分布，替代离散token生成方式。\n在多机器人、多任务、多平台上训练实现跨平台泛化。\n还需要解决的难点 数据的多样性和质量：虽然模型在多个平台和任务上进行了训练，但如何进一步提高数据的多样性和质量，以增强模型的泛化能力，仍是一个挑战。\n高频动作控制的稳定性 ：在高频率控制任务中，如何确保模型生成的动作序列的稳定性和准确性，需要进一步研究。\n模型的可扩展性和部署 ：如何将该模型部署到实际的机器人系统中，并确保其在不同硬件平台上的性能和效率，是实现其实际应用的关键。\nFAST: Efficient Action Tokenization for Vision-Language-Action Models (Chunking) 主要论点 传统的VLA模型在处理连续的机器人动作信号时，通常采用逐维、逐时间步的简单分箱（binning）策略进行离散化。然而，这种方法在面对高频率、精细操作任务时表现不佳，主要原因在于连续动作之间的强相关性导致模型难以有效学习。\n为解决这一问题，作者提出了FAST（Frequency-space Action Sequence Tokenization）方法，其核心思想包括：\n离散余弦变换（DCT） ：将连续的动作序列转换到频域，捕捉动作信号的主要频率成分，从而减少时间上的冗余信息。（关键帧技术） 量化与字节对编码（BPE） ：对DCT系数进行量化，并采用BPE进行压缩，生成信息密度更高的离散动作标记序列。 此外，作者还推出了 FAST+ ，一个在100万个真实机器人动作轨迹上训练的通用动作标记器，能够适用于多种机器人类型和控制频率的动作序列\n创新点 频域压缩的动作离散化：首次将DCT应用于机器人动作序列的离散化，有效减少了时间上的冗余信息，提高了模型对高频率动作的学习能力。\n通用动作标记器FAST+ ：通过在大规模、多样化的机器人动作数据上训练，FAST+实现了对不同机器人平台和任务的广泛适应性，减少了对特定任务手工设计标记器的需求。\n显著提升训练效率**** ：与传统的扩散模型相比，采用FAST的自回归VLA模型在训练时间上减少了多达5倍，同时在多个任务上达到了相当甚至更优的性能。\n解决的难点 传统动作token推理慢 。\n无法处理高频控制任务和长序列建模 。\n提出 DCT + BPE 的动作频域压缩方法（FAST），大幅压缩动作序列长度，提升效率。\n预训练通用tokenizer（FAST+）跨机器人平台迁移能力强。\n还需要解决的难点 高频动作的精确重建 ：虽然FAST在压缩动作序列方面表现出色，但在某些需要高精度控制的任务中，如何确保压缩后的动作序列能够准确还原原始动作，仍需进一步研究。\n（可不可以设置不同的专家模型，交由模型来判断应该使用原始序列还是压缩后的序列）\n与其他模型架构的兼容性 ：FAST主要与自回归VLA模型结合使用，其在其他类型的模型架构（如非自回归模型）中的表现和适应性尚待探索。\n实时控制的延迟问题 ：在实际机器人控制中，动作的生成和执行需要满足实时性要求，FAST在实际部署中可能面临延迟带来的挑战。\nπ0.5: a Vision-Language-Action Model with Open-World Generalization (More data, multimodel) Basically, 更强的π0。\n更强的 model, 更多的数据\n更强的开放世界泛化能力\nOpenVLA: An Open-Source Vision-Language-Action Model (远程连接 + Chunking + Dino) 主要论点 OpenVLA 是一个拥有 70 亿参数的开源 VLA 模型，基于 Llama 2 语言模型，并结合了 DINOv2 和 SigLIP 的预训练视觉特征。该模型在 Open X-Embodiment 数据集中的 97 万个真实机器人操作轨迹上进行了训练，涵盖了多种机器人形态、任务和场景。OpenVLA 能够直接控制多种机器人，并通过参数高效的微调方法快速适应新的机器人配置\n创新点 开源性与可访问性 ：OpenVLA 是首个完全开源的 VLA 模型，提供了模型检查点、微调笔记本和 PyTorch 训练代码，支持在 Open X-Embodiment 数据集上进行大规模训练。\n融合视觉编码器：模型采用了融合 DINOv2 和 SigLIP 特征的视觉编码器，结合了空间和语义信息，以增强模型对视觉输入的理解能力。\n高效的动作离散化方法：OpenVLA 通过将连续的机器人动作映射到离散的标记上，并使用语言模型的分词器进行处理，提高了模型的训练和推理效率。\n强大的泛化能力 ：在 29 个任务和多种机器人形态上，OpenVLA 的任务成功率比封闭模型 RT-2-X（55B 参数）高出 16.5%，同时参数数量减少了 7 倍。\n解决的难点 缺乏可公开访问、端到端训练的开源VLA模型 。\n跨机器人统一控制策略训练成本高 。\n基于 Llama2 + DINOv2 构建的70亿参数模型开源并附完整工具链。\n使用统一token表示动作（通过BPE），促进模块化、可复用性和可迁移性。\n还需要解决的难点 对未见机器人形态的泛化能力有限 ：OpenVLA 在预训练数据中未包含的机器人形态上，零样本泛化能力有限，需要通过微调适应新的机器人配置。\n动作离散化的精度问题 ：尽管采用了高效的动作离散化方法，但在某些需要高精度控制的任务中，如何确保离散化后的动作能够准确还原原始动作，仍需进一步研究。\n实时控制的计算需求：在实际机器人控制中，动作的生成和执行需要满足实时性要求，OpenVLA 在实际部署中可能面临计算资源和延迟的挑战。\nHi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models (LLM分层) 主要论点 Hi Robot 系统采用了两层策略结构，分别对应心理学中丹尼尔·卡尼曼提出的“系统1”（快速、直觉反应）和“系统2”（慢速、深度推理）模型：\n高层策略（System 2） ：利用预训练的视觉-语言模型（VLM），对复杂的自然语言指令进行解析，结合视觉观察，生成一系列中间步骤的低层语言命令。这一层具备推理能力，能够处理多阶段任务，并根据用户的实时反馈进行调整。 低层策略（System 1） ：基于 π₀ 模型，执行高层策略生成的原子级命令。该层专注于具体动作的执行，如“抓取杯子”，并能根据实时的视觉和状态信息进行快速反应。 该系统在三种不同的机器人平台上进行了测试，包括单臂、双臂和双臂移动机器人，任务涵盖清理杂乱的桌面、制作三明治和杂货购物等，展示了其在处理复杂任务和适应用户反馈方面的能力。\n创新点 分层架构设计：通过将任务解析与动作执行分离，Hi Robot 能够更有效地处理复杂指令和动态反馈，提高了系统的灵活性和适应性。\n“自言自语”机制 ：高层策略在生成低层命令前，会进行内部的语言推理过程，类似于人类在执行复杂任务前的思考过程，增强了系统的推理能力。\n实时反馈整合：系统能够在任务执行过程中接受并理解用户的实时语言反馈，如“那不是垃圾”，并据此调整当前的行为策略。\n广泛的任务适应性 ：通过在多种机器人平台和任务上的测试，验证了该系统在处理多样化任务和环境中的泛化能力。\n解决的难点 单一层次VLA模型无法处理长时序、开放式自然语言指令 。\n缺乏对人类实时反馈的理解和响应能力 。\n引入分层策略（System 1/2）模拟人类推理流程，实现任务分解与动作控制解耦。\n支持语言实时中断和调整（如用户打断、重说等）。\n还需要解决的难点 高层与低层策略的协同：确保高层生成的命令能够被低层准确执行，尤其是在面对未见过的任务或环境时，仍需进一步优化两层之间的接口和协同机制。\n实时性与计算资源的平衡：高层策略的推理过程可能带来计算延迟，如何在保证系统实时响应的同时，维持高层策略的推理深度，是一个需要权衡的问题。\n数据多样性与泛化能力 ：尽管系统在多个任务上表现良好，但在面对极端或未见过的指令和环境时，其泛化能力仍需通过更多样化的数据训练进行提升。\nDiffusion Policy: Visuomotor Policy Learning via Action Diffusion (Diffusion Policy) 主要论点 传统的机器人策略学习方法在处理多模态动作分布和高维动作空间时常面临挑战。为此，作者提出了 Diffusion Policy ，将机器人视觉-动作策略表示为条件去噪扩散过程（Conditional Denoising Diffusion Process）\n该方法的核心思想是：\n训练阶段：在动作空间中添加噪声，并训练模型预测如何从噪声中恢复出原始动作序列。 推理阶段：从随机噪声开始，逐步去噪，生成符合当前视觉观察的动作序列。 通过这种方式，Diffusion Policy 能够有效建模多模态动作分布，适应高维动作空间，并在多个机器人操作任务中表现出色。\n创新点 多模态动作建模能力 ：Diffusion Policy 能够自然地处理多种可能的动作路径，适应复杂任务中的多样性。 高维动作空间适应性：通过在整个动作序列上进行建模，方法在处理高维动作空间时表现稳定，避免了传统方法在高维空间中的不稳定性。 稳定的训练过程：相比于传统的能量模型（Energy-Based Models），Diffusion Policy 避免了对归一化常数的估计，提升了训练的稳定性。 闭环控制机制：结合递归视野控制（Receding Horizon Control），实现了在任务执行过程中的动态重规划，提高了系统的鲁棒性。 解决的难点 能量模型训练不稳定、归一化常数难估计 。\n无法处理多模态动作空间（例如多个解法、连续空间）。\n用扩散模型建模动作序列的分布，天然适配多模态分布。\n在多个真实世界任务中验证其高鲁棒性与学习稳定性。\n还需要解决的难点 推理速度 ：由于扩散过程需要多步去噪，推理速度较慢，可能限制了在实时控制任务中的应用。\n与其他模型的集成 ：如何将 Diffusion Policy 与其他感知或语言模型有效集成，以处理更复杂的任务，仍需进一步研究。\nRT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE 主要论点 RT-1 模型的核心目标是实现一个通用的机器人控制策略，能够处理多种任务、对象和环境。为此，研究团队收集了一个包含 13 台机器人、历时 17 个月、涵盖 700 多个任务、共计 13 万个操作示例的大规模数据集。这些数据包括机器人在实际环境中执行任务的图像序列、自然语言指令和对应的动作序列。\nRT-1 的架构包括以下关键组件：\n图像编码器 ：使用预训练的 EfficientNet-B3 模型处理输入图像，并通过 FiLM 层融合语言指令，提取与任务相关的视觉特征。 TokenLearner 模块 ：对图像特征进行压缩，生成一组紧凑的 token，提高模型的推理效率。 Transformer 模型 ：接收图像和语言的 token 输入，输出离散化的动作 token，控制机器人的执行。 在训练过程中，RT-1 通过模仿学习（Imitation Learning）方法，从收集的示例中学习任务的执行策略。模型能够以每秒 3 次的频率进行闭环控制，直到任务完成或达到预设的时间步数。\n创新点 大规模多任务学习 ：RT-1 在一个包含 13 万个示例、700 多个任务的大规模数据集上进行训练，展示了 Transformer 架构在机器人控制中的强大能力。\n统一的输入输出表示 ：将图像、语言指令和动作统一表示为 token 序列，使得模型能够处理多模态输入，并生成相应的动作输出。\n高效的推理机制：通过 TokenLearner 模块对图像特征进行压缩，显著提高了模型的推理速度，满足实时控制的需求。\n强大的泛化能力 ：RT-1 在未见过的任务、环境和对象上表现出色，展示了其在零样本学习和迁移学习方面的潜力。\n解决的难点 大规模跨任务学习难以整合语言、视觉、动作三模态数据 。\n模型在未见任务和环境上的泛化能力弱 。\n在 130,000+ 真实机器人操作轨迹上训练的 Transformer 模型。\nTokenLearner + FiLM结构实现多模态融合，提升跨任务泛化。\n还需要解决的难点 对新任务的泛化能力有限：尽管 RT-1 在多种任务上表现良好，但在面对完全未见过的任务时，其泛化能力仍有待提升。\n对复杂操作的适应性：当前模型主要针对相对简单的操作任务，对于需要高精度和复杂操作的任务，其性能尚未验证。\n实时性与计算资源的平衡：虽然模型在推理速度上有所优化，但在资源受限的实际部署环境中，如何进一步提高实时性仍是一个挑战。\nOpen-TeleVision: Teleoperation with Immersive Active Visual Feedback 主要论点 Open-TeleVision 系统结合了 VR 设备与机器人控制，允许操作者通过 VR 头显实时感知机器人的立体视觉环境，并将自身的手臂和手部动作映射到机器人上，实现如同“身临其境”的操作体验。\n系统的核心组件包括：\n主动视觉反馈 ：机器人头部配备可动的立体 RGB 摄像头，能够根据操作者的头部运动调整视角，提供实时的第一人称3D 观察。 动作映射机制 ：通过逆运动学（IK）算法和 dex-retargeting 技术，将操作者的手部关键点转换为机器人关节角度，实现精确的动作控制。 远程操作能力 ：系统支持通过互联网进行远程控制，操作者无需与机器人处于同一地点。 在实验中，研究团队使用该系统在两个不同的人形机器人（Unitree H1 和 Fourier GR-1）上完成了包括罐头分类、罐头插入、毛巾折叠和物品卸载等四项长时序、精细操作任务，并成功部署了模仿学习策略。\n创新点 沉浸式第一人称视角：通过主动立体视觉技术，操作者能够以第一人称视角直观感知机器人周围环境，增强了空间感知能力和操作**直觉。\n高精度动作映射：结合逆运动学和 dex-retargeting 技术，实现了操作者动作到机器人动作的高精度映射，支持多指灵巧手的控制。\n远程操作与数据采集 ：系统支持远程操作，操作者可以跨地域控制机器人，并收集高质量的操作数据，促进模仿学习的发展。\n解决的难点 传统远程遥操作缺乏空间沉浸感与操作精度 。\n低质量遥操作数据不利于模仿学习 。\n主动式立体视觉反馈 + VR 映射手部运动实现高保真远程控制。\n为高质量模仿学习数据采集（精细双臂、多指操作）提供有效工具。\n还需要解决的难点 my Questions 如何评估一个机械臂的能力呢？ 高频动作，怎么样算高频呢？\n为什么说机器的精度很重要，到底有多重要呢？\n输出给机器臂的值可以是连续的而不是离散的吗？对应 pi 0\n在不同机器人上训练的难点是？\n为什么需要端到端模型呢？\n机器人上的设备能支持多大的模型运行呢？\n",
  "wordCount" : "504",
  "inLanguage": "en",
  "datePublished": "2025-05-15T00:00:00+08:00",
  "dateModified": "2025-05-15T00:00:00+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-05-14/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-05-25
    </h1>
    <div class="post-meta"><span title='2025-05-15 00:00:00 +0800 CST'>May 15, 2025</span>&nbsp;·&nbsp;3 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h3 id="learning-fine-grained-bimanual-manipulation-with-low-cost-hardware-act-algorithm">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (ACT Algorithm)<a hidden class="anchor" aria-hidden="true" href="#learning-fine-grained-bimanual-manipulation-with-low-cost-hardware-act-algorithm">#</a></h3>
<h4 id="主要论点">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点">#</a></h4>
<p>ALOHA 是一个开放源代码的低成本双臂远程操作硬件系统，整体成本低于 20,000 美元，主要由现成的机器人组件和少量 3D 打印部件组成。该系统支持精细、动态和接触丰富的任务，如穿拉链、乒乓球颠球和链条组装等。</p>
<p>为了应对模仿学习中政策误差累积的问题，研究人员提出了 ACT 算法。该算法基于 Transformer 架构，采用条件变分自编码器（CVAE）框架，通过预测动作序列（即“动作块”）而非单步动作，减少了有效的预测范围，从而提高了学习效率和稳定性。</p>
<p>在六个现实世界的精细操作任务中，如打开透明调味杯盖和插入电池等，ALOHA 系统仅通过约 10 分钟的示范数据（约 50 次演示）就实现了 80% 至 90% 的成功率，展示了其在低成本硬件上的高效学习能力。</p>
<h4 id="创新点">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点">#</a></h4>
<p>低成本高性能：ALOHA 系统在成本控制的同时，仍能执行复杂的双臂操作任务，降低了高精度机器人研究的门槛。</p>
<p>动作块预测 ：ACT 算法通过预测动作序列，减少了政策误差的累积，提高了模仿学习的稳定性和效率。</p>
<p>快速学习能力：系统仅需少量的演示数据即可学习复杂任务，展示了高效的学习能力。</p>
<h4 id="解决的难点">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点">#</a></h4>
<p>高精度双臂系统价格昂贵，限制研究和数据获取 。</p>
<p>模仿学习中长序列预测误差累积问题严重 。</p>
<p>设计低于 $20,000 的双臂系统 ALOHA + 高效 Transformer 模仿学习方法 ACT。</p>
<p>通过动作 chunking 预测提升长序列动作稳定性。</p>
<h4 id="还需要解决的难点">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点">#</a></h4>
<p>硬件精度限制 ：尽管系统成本低廉，但硬件精度的限制可能影响在更复杂任务中的表现。</p>
<p>泛化能力：系统在面对未见过的任务或环境时的泛化能力仍需进一步提升。</p>
<p>实时性能 ：在实际应用中，如何确保系统的实时响应能力和稳定性是一个挑战。</p>
<h3 id="autoregressive-action-sequence-learning-for-robotic-manipulation">Autoregressive Action Sequence Learning for Robotic Manipulation<a hidden class="anchor" aria-hidden="true" href="#autoregressive-action-sequence-learning-for-robotic-manipulation">#</a></h3>
<h4 id="主要论点-1">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-1">#</a></h4>
<p>论文的核心思想是将机器人动作表示为序列数据，并通过自回归序列建模生成动作序列。为此，作者提出了两项关键技术：</p>
<ul>
<li><strong>Chunking Causal Transformer (CCT)</strong>：该模型扩展了传统因果变换器的单步预测能力，支持在一个步骤中预测多个动作“块”。这种方法提高了对不同控制频率任务的适应性，并通过减少自回归步骤提高了效率</li>
<li><strong>Autoregressive Policy (ARP)</strong>：基于CCT，作者设计了ARP架构，用于生成混合动作序列，解决多种机器人操作任务。该架构在Push-T、ALOHA和RLBench等多种机器人操作环境中进行了评估，结果显示ARP作为通用架构，在所有测试基准中匹配或超越了特定环境下的最新技术，同时在计算和参数规模上更为高效</li>
<li>简而言之，CCT 可以预测未来的多个动作</li>
</ul>
<h4 id="创新点-1">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点-1">#</a></h4>
<p><strong>多动作块预测机制</strong>：CCT模型引入了预测多个动作块的能力，使其能够处理不同类型和频率的动作数据，提高了模型的灵活性和效率</p>
<p><strong>混合动作序列设计</strong>：通过将不同类型的动作（如关节位置、2D像素坐标和末端执行器姿态）混合在一个序列中，并为每种动作类型使用不同的块大小，增强了模型对复杂任务的适应能力</p>
<p><strong>通用策略架构</strong>：ARP架构作为一个通用的策略架构，在多个不同的机器人操作环境中表现出色，显示出其广泛的适用性和高效性</p>
<h4 id="解决的难点-1">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点-1">#</a></h4>
<p><strong>自回归策略效率低:</strong> 传统每次只预测一个动作的自回归方法效率低，难以适配高频任务。</p>
<p><strong>动作混合表示困难</strong>：连续值与离散值混合表示在序列学习中不易统一建模。</p>
<h4 id="还需要解决的难点-1">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点-1">#</a></h4>
<p><strong>动作数据的异质性</strong> ：机器人动作数据通常包括连续值和离散值，如何有效地将这些异质数据表示为序列，并进行建模，是一个挑战。</p>
<p><strong>高频控制任务的建模</strong> ：现有的自回归架构在处理高频控制任务时存在限制，如何扩展模型以支持高频控制任务，需要进一步研究</p>
<p><strong>混合动作序列的生成与优化</strong> ：在生成包含多种动作类型的混合序列时，如何确保各动作类型之间的协调性和整体序列的最优性，是一个需要解决的问题</p>
<h3 id="π0-a-vision-language-action-flow-model-for-general-robot-control">π0: A Vision-Language-Action Flow Model for General Robot Control<a hidden class="anchor" aria-hidden="true" href="#π0-a-vision-language-action-flow-model-for-general-robot-control">#</a></h3>
<h4 id="主要论点-2">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-2">#</a></h4>
<p>π₀模型的核心是将预训练的视觉-语言模型（VLM）与流匹配架构相结合，形成一个统一的视觉-语言-动作（VLA）模型，用于通用机器人控制。该模型通过在多个灵巧机器人平台（包括单臂、双臂和移动操纵器）的大型多样化数据集上进行训练，学习从视觉和语言输入到动作输出的映射关系。</p>
<p>模型的训练分为两个阶段：</p>
<ol>
<li>预训练阶段 ：在大规模多样化的数据集上进行训练，学习通用的感知和语言理解能力。</li>
<li>微调阶段：在特定任务的高质量数据上进行微调，以提高在特定任务上的性能。</li>
</ol>
<p>该模型在多个任务上进行了评估，包括折叠衣物、清洁桌子和组装盒子等，展示了其在零样本学习、语言指令遵循和新技能获取方面的能力。</p>
<h4 id="创新点-2">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点-2">#</a></h4>
<p>流匹配架构 ：引入流匹配技术生成连续的动作分布，适用于高频率和灵巧的任务。</p>
<p>跨机器人平台训练 ：结合多种机器人类型的数据进行训练，使模型能够适应不同的机器人配置和动作表示。</p>
<p>高效推理机制 ：模型设计允许高效的推理过程，通过缓存和重用注意力键值对来减少计算量，适应实时控制的需求。</p>
<h4 id="解决的难点-2">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点-2">#</a></h4>
<p>VLA模型对高频、连续动作建模困难。</p>
<p>泛化能力差，多机器人平台适应性弱。</p>
<p>引入流匹配（flow matching）机制，生成<strong>连续<strong><strong>动作</strong></strong>分布</strong>，替代离散token生成方式。</p>
<p>在多机器人、多任务、多平台上训练实现跨平台泛化。</p>
<h4 id="还需要解决的难点-2">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点-2">#</a></h4>
<p>数据的多样性和质量：虽然模型在多个平台和任务上进行了训练，但如何进一步提高数据的多样性和质量，以增强模型的泛化能力，仍是一个挑战。</p>
<p>高频动作控制的稳定性 ：在高频率控制任务中，如何确保模型生成的动作序列的稳定性和准确性，需要进一步研究。</p>
<p>模型的可扩展性和部署 ：如何将该模型部署到实际的机器人系统中，并确保其在不同硬件平台上的性能和效率，是实现其实际应用的关键。</p>
<h3 id="fast-efficient-action-tokenization-for-vision-language-action-models-chunking">FAST: Efficient Action Tokenization for Vision-Language-Action Models (Chunking)<a hidden class="anchor" aria-hidden="true" href="#fast-efficient-action-tokenization-for-vision-language-action-models-chunking">#</a></h3>
<h4 id="主要论点-3">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-3">#</a></h4>
<p>传统的VLA模型在处理连续的机器人动作信号时，通常采用逐维、逐时间步的简单分箱（binning）策略进行离散化。然而，这种方法在面对高频率、精细操作任务时表现不佳，主要原因在于连续动作之间的强相关性导致模型难以有效学习。</p>
<p>为解决这一问题，作者提出了FAST（Frequency-space Action Sequence Tokenization）方法，其核心思想包括：</p>
<ol>
<li>离散余弦变换（DCT） ：将连续的动作序列转换到频域，捕捉动作信号的主要频率成分，从而减少时间上的冗余信息。<strong>（关键帧技术）</strong></li>
<li>量化与字节对编码（BPE） ：对DCT系数进行量化，并采用BPE进行压缩，生成信息密度更高的离散动作标记序列。</li>
</ol>
<p>此外，作者还推出了 FAST+ ，一个在100万个真实机器人动作轨迹上训练的通用动作标记器，能够适用于多种机器人类型和控制频率的动作序列</p>
<h4 id="创新点-3">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点-3">#</a></h4>
<p>频域压缩的动作离散化：首次将DCT应用于机器人动作序列的离散化，有效减少了时间上的冗余信息，提高了模型对高频率动作的学习能力。</p>
<p>通用动作标记器FAST+ ：通过在大规模、多样化的机器人动作数据上训练，FAST+实现了对不同机器人平台和任务的广泛适应性，减少了对特定任务手工设计标记器的需求。</p>
<p>显著提升训练效率**** ：与传统的扩散模型相比，采用FAST的自回归VLA模型在训练时间上减少了多达5倍，同时在多个任务上达到了相当甚至更优的性能。</p>
<h4 id="解决的难点-3">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点-3">#</a></h4>
<p>传统动作token推理慢 。</p>
<p>无法处理高频控制任务和长序列建模 。</p>
<p>提出 DCT + BPE 的动作频域压缩方法（FAST），大幅压缩动作序列长度，提升效率。</p>
<p>预训练通用tokenizer（FAST+）跨机器人平台迁移能力强。</p>
<h4 id="还需要解决的难点-3">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点-3">#</a></h4>
<p>高频动作的精确重建 ：虽然FAST在压缩动作序列方面表现出色，但在某些需要高精度控制的任务中，如何确保压缩后的动作序列能够准确还原原始动作，仍需进一步研究。</p>
<p><strong>（可不可以设置不同的专家模型，交由模型来判断应该使用原始序列还是压缩后的序列）</strong></p>
<p>与其他模型架构的兼容性 ：FAST主要与自回归VLA模型结合使用，其在其他类型的模型架构（如非自回归模型）中的表现和适应性尚待探索。</p>
<p>实时控制的延迟问题 ：在实际机器人控制中，动作的生成和执行需要满足实时性要求，FAST在实际部署中可能面临延迟带来的挑战。</p>
<h3 id="π05-a-vision-language-action-model-with-open-world-generalization-more-data-multimodel">π0.5: a Vision-Language-Action Model with Open-World Generalization (More data, multimodel)<a hidden class="anchor" aria-hidden="true" href="#π05-a-vision-language-action-model-with-open-world-generalization-more-data-multimodel">#</a></h3>
<p>Basically, 更强的π0。</p>
<p>更强的 model, 更多的数据</p>
<p>更强的开放世界泛化能力</p>
<h3 id="openvla-an-open-source-vision-language-action-model-远程连接--chunking--dino">OpenVLA: An Open-Source Vision-Language-Action Model (远程连接 + Chunking + Dino)<a hidden class="anchor" aria-hidden="true" href="#openvla-an-open-source-vision-language-action-model-远程连接--chunking--dino">#</a></h3>
<h4 id="主要论点-4">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-4">#</a></h4>
<p>OpenVLA 是一个拥有 70 亿参数的开源 VLA 模型，基于 Llama 2 语言模型，并结合了 DINOv2 和 SigLIP 的预训练视觉特征。该模型在 Open X-Embodiment 数据集中的 97 万个真实机器人操作轨迹上进行了训练，涵盖了多种机器人形态、任务和场景。OpenVLA 能够直接控制多种机器人，并通过参数高效的微调方法快速适应新的机器人配置</p>
<h4 id="创新点-4">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点-4">#</a></h4>
<p>开源性与可访问性 ：OpenVLA 是首个完全开源的 VLA 模型，提供了模型检查点、微调笔记本和 PyTorch 训练代码，支持在 Open X-Embodiment 数据集上进行大规模训练。</p>
<p>融合视觉编码器：模型采用了融合 DINOv2 和 SigLIP 特征的视觉编码器，结合了空间和语义信息，以增强模型对视觉输入的理解能力。</p>
<p>高效的动作离散化方法：OpenVLA 通过将连续的机器人动作映射到离散的标记上，并使用语言模型的分词器进行处理，提高了模型的训练和推理效率。</p>
<p>强大的泛化能力 ：在 29 个任务和多种机器人形态上，OpenVLA 的任务成功率比封闭模型 RT-2-X（55B 参数）高出 16.5%，同时参数数量减少了 7 倍。</p>
<h4 id="解决的难点-4">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点-4">#</a></h4>
<p>缺乏可公开访问、端到端训练的开源VLA模型 。</p>
<p>跨机器人统一控制策略训练成本高 。</p>
<p>基于 Llama2 + DINOv2 构建的70亿参数模型开源并附完整工具链。</p>
<p>使用统一token表示动作（通过BPE），促进模块化、可复用性和可迁移性。</p>
<h4 id="还需要解决的难点-4">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点-4">#</a></h4>
<p>对未见机器人形态的泛化能力有限 ：OpenVLA 在预训练数据中未包含的机器人形态上，零样本泛化能力有限，需要通过微调适应新的机器人配置。</p>
<p>动作离散化的精度问题 ：尽管采用了高效的动作离散化方法，但在某些需要高精度控制的任务中，如何确保离散化后的动作能够准确还原原始动作，仍需进一步研究。</p>
<p>实时控制的计算需求：在实际机器人控制中，动作的生成和执行需要满足实时性要求，OpenVLA 在实际部署中可能面临计算资源和延迟的挑战。</p>
<h3 id="hi-robot-open-ended-instruction-following-with-hierarchical-vision-language-action-models-llm分层">Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models (LLM分层)<a hidden class="anchor" aria-hidden="true" href="#hi-robot-open-ended-instruction-following-with-hierarchical-vision-language-action-models-llm分层">#</a></h3>
<h4 id="主要论点-5">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-5">#</a></h4>
<p>Hi Robot 系统采用了两层策略结构，分别对应心理学中丹尼尔·卡尼曼提出的“系统1”（快速、直觉反应）和“系统2”（慢速、深度推理）模型：</p>
<ol>
<li>高层策略（System 2） ：利用预训练的视觉-语言模型（VLM），对复杂的自然语言指令进行解析，结合视觉观察，生成一系列中间步骤的低层语言命令。这一层具备推理能力，能够处理多阶段任务，并根据用户的实时反馈进行调整。</li>
<li>低层策略（System 1） ：基于 π₀ 模型，执行高层策略生成的原子级命令。该层专注于具体动作的执行，如“抓取杯子”，并能根据实时的视觉和状态信息进行快速反应。</li>
</ol>
<p>该系统在三种不同的机器人平台上进行了测试，包括单臂、双臂和双臂移动机器人，任务涵盖清理杂乱的桌面、制作三明治和杂货购物等，展示了其在处理复杂任务和适应用户反馈方面的能力。</p>
<h4 id="创新点-5">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点-5">#</a></h4>
<p>分层架构设计：通过将任务解析与动作执行分离，Hi Robot 能够更有效地处理复杂指令和动态反馈，提高了系统的灵活性和适应性。</p>
<p>“自言自语”机制 ：高层策略在生成低层命令前，会进行内部的语言推理过程，类似于人类在执行复杂任务前的思考过程，增强了系统的推理能力。</p>
<p>实时反馈整合：系统能够在任务执行过程中接受并理解用户的实时语言反馈，如“那不是垃圾”，并据此调整当前的行为策略。</p>
<p>广泛的任务适应性 ：通过在多种机器人平台和任务上的测试，验证了该系统在处理多样化任务和环境中的泛化能力。</p>
<h4 id="解决的难点-5">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点-5">#</a></h4>
<p>单一层次VLA模型无法处理长时序、开放式自然语言指令 。</p>
<p>缺乏对人类实时反馈的理解和响应能力 。</p>
<p>引入分层策略（System 1/2）模拟人类推理流程，实现<strong>任务分解</strong>与动作控制解耦。</p>
<p>支持语言实时中断和调整（如用户打断、重说等）。</p>
<h4 id="还需要解决的难点-5">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点-5">#</a></h4>
<p>高层与低层策略的协同：确保高层生成的命令能够被低层准确执行，尤其是在面对未见过的任务或环境时，仍需进一步优化两层之间的接口和协同机制。</p>
<p>实时性与计算资源的平衡：高层策略的推理过程可能带来计算延迟，如何在保证系统实时响应的同时，维持高层策略的推理深度，是一个需要权衡的问题。</p>
<p>数据多样性与泛化能力 ：尽管系统在多个任务上表现良好，但在面对极端或未见过的指令和环境时，其泛化能力仍需通过更多样化的数据训练进行提升。</p>
<h3 id="diffusion-policy-visuomotor-policy-learning-via-action-diffusion-diffusion-policy">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion (Diffusion Policy)<a hidden class="anchor" aria-hidden="true" href="#diffusion-policy-visuomotor-policy-learning-via-action-diffusion-diffusion-policy">#</a></h3>
<h4 id="主要论点-6">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-6">#</a></h4>
<p>传统的机器人策略学习方法在处理多模态动作分布和高维动作空间时常面临挑战。为此，作者提出了 Diffusion Policy ，将机器人视觉-动作策略表示为条件去噪扩散过程（Conditional Denoising Diffusion Process）</p>
<p>该方法的核心思想是：</p>
<ul>
<li>训练阶段：在动作空间中添加噪声，并训练模型预测如何从噪声中恢复出原始动作序列。</li>
<li>推理阶段：从随机噪声开始，逐步去噪，生成符合当前视觉观察的动作序列。</li>
</ul>
<p>通过这种方式，Diffusion Policy 能够有效建模多模态动作分布，适应高维动作空间，并在多个机器人操作任务中表现出色。</p>
<h4 id="创新点-6">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点-6">#</a></h4>
<ul>
<li>多模态动作建模能力 ：Diffusion Policy 能够自然地处理多种可能的动作路径，适应复杂任务中的多样性。</li>
<li>高维动作空间适应性：通过在整个动作序列上进行建模，方法在处理高维动作空间时表现稳定，避免了传统方法在高维空间中的不稳定性。</li>
<li>稳定的训练过程：相比于传统的能量模型（Energy-Based Models），Diffusion Policy 避免了对归一化常数的估计，提升了训练的稳定性。</li>
<li>闭环控制机制：结合递归视野控制（Receding Horizon Control），实现了在任务执行过程中的动态重规划，提高了系统的鲁棒性。</li>
</ul>
<h4 id="解决的难点-6">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点-6">#</a></h4>
<p>能量模型训练不稳定、归一化常数难估计 。</p>
<p>无法处理多模态动作空间（例如多个解法、连续空间）。</p>
<p>用扩散模型建模动作序列的分布，天然适配多模态分布。</p>
<p>在多个真实世界任务中验证其高鲁棒性与学习稳定性。</p>
<h4 id="还需要解决的难点-6">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点-6">#</a></h4>
<p>推理速度 ：由于扩散过程需要多步去噪，推理速度较慢，可能限制了在实时控制任务中的应用。</p>
<p>与其他模型的集成 ：如何将 Diffusion Policy 与其他感知或语言模型有效集成，以处理更复杂的任务，仍需进一步研究。</p>
<h3 id="rt-1-robotics-transformer-for-real-world-control-at-scale">RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE<a hidden class="anchor" aria-hidden="true" href="#rt-1-robotics-transformer-for-real-world-control-at-scale">#</a></h3>
<h4 id="主要论点-7">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-7">#</a></h4>
<p>RT-1 模型的核心目标是实现一个通用的机器人控制策略，能够处理多种任务、对象和环境。为此，研究团队收集了一个包含 13 台机器人、历时 17 个月、涵盖 700 多个任务、共计 13 万个操作示例的大规模数据集。这些数据包括机器人在实际环境中执行任务的图像序列、自然语言指令和对应的动作序列。</p>
<p>RT-1 的架构包括以下关键组件：</p>
<ul>
<li>图像编码器 ：使用预训练的 EfficientNet-B3 模型处理输入图像，并通过 FiLM 层融合语言指令，提取与任务相关的视觉特征。</li>
<li>TokenLearner 模块 ：对图像特征进行压缩，生成一组紧凑的 token，提高模型的推理效率。</li>
<li>Transformer 模型 ：接收图像和语言的 token 输入，输出离散化的动作 token，控制机器人的执行。</li>
</ul>
<p>在训练过程中，RT-1 通过模仿学习（Imitation Learning）方法，从收集的示例中学习任务的执行策略。模型能够以每秒 3 次的频率进行闭环控制，直到任务完成或达到预设的时间步数。</p>
<h4 id="创新点-7">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点-7">#</a></h4>
<p>大规模多任务学习 ：RT-1 在一个包含 13 万个示例、700 多个任务的大规模数据集上进行训练，展示了 Transformer 架构在机器人控制中的强大能力。</p>
<p>统一的输入输出表示 ：将图像、语言指令和动作统一表示为 token 序列，使得模型能够处理多模态输入，并生成相应的动作输出。</p>
<p>高效的推理机制：通过 TokenLearner 模块对图像特征进行压缩，显著提高了模型的推理速度，满足实时控制的需求。</p>
<p>强大的泛化能力 ：RT-1 在未见过的任务、环境和对象上表现出色，展示了其在零样本学习和迁移学习方面的潜力。</p>
<h4 id="解决的难点-7">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点-7">#</a></h4>
<p>大规模跨任务学习难以整合语言、视觉、动作三模态数据 。</p>
<p>模型在未见任务和环境上的泛化能力弱 。</p>
<p>在 130,000+ 真实机器人操作轨迹上训练的 Transformer 模型。</p>
<p>TokenLearner + FiLM结构实现多模态融合，提升跨任务泛化。</p>
<h4 id="还需要解决的难点-7">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点-7">#</a></h4>
<p>对新任务的泛化能力有限：尽管 RT-1 在多种任务上表现良好，但在面对完全未见过的任务时，其泛化能力仍有待提升。</p>
<p>对复杂操作的适应性：当前模型主要针对相对简单的操作任务，对于需要高精度和复杂操作的任务，其性能尚未验证。</p>
<p>实时性与计算资源的平衡：虽然模型在推理速度上有所优化，但在资源受限的实际部署环境中，如何进一步提高实时性仍是一个挑战。</p>
<h3 id="open-television-teleoperation-with-immersive-active-visual-feedback">Open-TeleVision: Teleoperation with Immersive Active Visual Feedback<a hidden class="anchor" aria-hidden="true" href="#open-television-teleoperation-with-immersive-active-visual-feedback">#</a></h3>
<h4 id="主要论点-8">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-8">#</a></h4>
<p>Open-TeleVision 系统结合了 VR 设备与机器人控制，允许操作者通过 VR 头显实时感知机器人的立体视觉环境，并将自身的手臂和手部动作映射到机器人上，实现如同“身临其境”的操作体验。</p>
<p>系统的核心组件包括：</p>
<ul>
<li>主动视觉反馈 ：机器人头部配备可动的立体 RGB 摄像头，能够根据操作者的头部运动调整视角，提供实时的第一人称3D 观察。</li>
<li>动作映射机制 ：通过逆运动学（IK）算法和 dex-retargeting 技术，将操作者的手部关键点转换为机器人关节角度，实现精确的动作控制。</li>
<li>远程操作能力 ：系统支持通过互联网进行远程控制，操作者无需与机器人处于同一地点。</li>
</ul>
<p>在实验中，研究团队使用该系统在两个不同的人形机器人（Unitree H1 和 Fourier GR-1）上完成了包括罐头分类、罐头插入、毛巾折叠和物品卸载等四项长时序、精细操作任务，并成功部署了模仿学习策略。</p>
<h4 id="创新点-8">创新点<a hidden class="anchor" aria-hidden="true" href="#创新点-8">#</a></h4>
<p>沉浸式第一人称视角：通过主动立体视觉技术，操作者能够以第一人称视角直观感知机器人周围环境，增强了空间感知能力和操作**直觉。</p>
<p>高精度动作映射：结合逆运动学和 dex-retargeting 技术，实现了操作者动作到机器人动作的高精度映射，支持多指灵巧手的控制。</p>
<p>远程操作与数据采集 ：系统支持远程操作，操作者可以跨地域控制机器人，并收集高质量的操作数据，促进模仿学习的发展。</p>
<h4 id="解决的难点-8">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点-8">#</a></h4>
<p>传统远程遥操作缺乏空间沉浸感与操作精度 。</p>
<p>低质量遥操作数据不利于模仿学习 。</p>
<p>主动式立体视觉反馈 + VR 映射手部运动实现高保真远程控制。</p>
<p>为高质量模仿学习数据采集（精细双臂、多指操作）提供有效工具。</p>
<h4 id="还需要解决的难点-8">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点-8">#</a></h4>
<h2 id="my-questions">my Questions<a hidden class="anchor" aria-hidden="true" href="#my-questions">#</a></h2>
<p>如何评估一个机械臂的能力呢？
<img alt="1746872165191" loading="lazy" src="https://tzj2006.github.io/images/2025-05-10/1746872165191.png"></p>
<p>高频动作，怎么样算高频呢？</p>
<p>为什么说机器的精度很重要，到底有多重要呢？</p>
<p>输出给机器臂的值可以是连续的而不是离散的吗？对应 pi 0</p>
<p>在不同机器人上训练的难点是？</p>
<p>为什么需要端到端模型呢？</p>
<p>机器人上的设备能支持多大的模型运行呢？</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
