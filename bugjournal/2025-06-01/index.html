<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-06-01 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, Computer Vision, LLM">
<meta name="description" content="主流视觉-文本多模态模型技术分析
近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。
CLIP (OpenAI, 2021)

整体架构设计： CLIP 采用 双编码器架构 ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到相同维度的向量空间lightly.ai。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型lightly.ai。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到共享的多模态嵌入空间lightly.ai。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。
模态对齐方式： CLIP通过对比学习实现视觉-语言对齐。训练时，模型给定一批图文对，学习预测哪张图像与哪段文本匹配lightly.ai。具体而言，CLIP使用 对称的跨模态对比损失 （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度lightly.ai。这种训练使图像和文本编码器产生的特征在共享空间中成对靠近lightly.ai。
输入 token 表达统一： CLIP并未显式统一图像和文本的输入表示格式。 图像和文本各有独立的token化和编码流程 ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征lightly.ai。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过独立编码&#43;对齐空间的方式，规避了直接将图像作为序列token处理的不统一问题。
损失函数与训练策略： 损失采用 对比学习的InfoNCE损失 （实现为带温度系数的归一化softmax交叉熵）research.google。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了大批量训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化lightly.ai。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应lightly.ai。CLIP从随机初始化开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型lightly.ai。
数据集及伪标签： CLIP在一个超大规模的图文配对数据集上预训练，包含约4亿对图像-文本arxiv.org（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖自然语言的弱监督arxiv.org。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。
六大难点应对：

模态对齐困难：  CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远lightly.ai。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。
token格式不统一：  采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对lightly.ai。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。
语义粒度不匹配： CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如Fine-Grained CLIP等正是受限于CLIP在局部语义对齐上的不足lightly.ailightly.ai。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。
多模态上下文保持： CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 多模态上下文 （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。
训练数据稀缺： CLIP通过大规模弱标注数据从根本上缓解了数据稀缺的问题arxiv.org。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征research.google。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。
计算开销高： 训练CLIP确实需要巨大的算力和显存，但相对来说，其双塔架构使训练可并行展开，推理时也可分别预编码图文后做相似度计算，具有一定的效率优势lightly.ai。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化lightly.ai。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现lightly.ai。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。



参考： CLIP 的论文arxiv.org详细描述了其对比预训练方法，OpenAI 的博客也提供了概述lightly.ai。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库huggingface.co。
ALIGN (Google, 2021)

整体架构设计： ALIGN（ A Large-scale ImaGe and Noisy-Text embedding ）延续了与CLIP相同的双编码器对比学习架构research.google。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：EfficientNet-L2卷积网络作为图像编码器，BERT-Large作为文本编码器，并均从随机初始化开始训练research.google。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。
模态对齐方式： ALIGN采用对比损失（normalized softmax）来训练，使匹配的图文对嵌入向量接近，不匹配的远离research.google。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以批为单位的跨模态对比训练，使模型学到强大的图文对齐表示。
输入 token 表达统一： ALIGN同样没有将图像直接离散为token序列，而是通过双通道处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出embedding空间实现统一表示research.google。
损失函数与训练策略： 使用 对比学习损失 （InfoNCE变体），在大批量上训练模型research.google。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 最小程度的过滤 ，通过数据规模来弥补噪声research.google。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。
数据集及伪标签： ALIGN的亮点在于使用了超过10亿对图像-Alt文本的超大规模数据集research.google。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 无需人工标注 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN 放宽过滤标准 ，只做了基于频率的简单过滤，最终得到约18亿对图文数据research.google。这些文本描述可能包含噪声甚至与图像无关，但研究表明 规模弥补质量 ：如此海量的数据使模型学到泛化的视觉语言表示research.google。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题research.google。
六大难点应对：

模态对齐困难： ALIGN证明了数据规模在对齐中的重要作用。通过十倍于CLIP的数据规模和强大的对比损失，模型学到了稳健的跨模态对齐能力research.googleresearch.google。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型research.google。
token格式不统一： 与CLIP类似，ALIGN通过双编码器架构回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可research.google。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。
语义粒度不匹配： ALIGN的训练目标依旧作用在 全局图像-句子层面 ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对细粒度语义的不匹配没有特殊解决方案。
多模态上下文保持： ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力research.google，未涉及多模态对话等情境。因此，ALIGN在多轮交互或长上下文问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。
训练数据稀缺： ALIGN的策略是极端扩增数据规模以消除数据稀缺瓶颈research.google。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型research.google。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 弱标注的大数据 。
计算开销高： 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展research.google。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源research.google。尽管计算开销高昂，ALIGN通过 冻结架构复杂性 （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，优先扩大数据规模比增加模型复杂度更有效research.googleresearch.google。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。



参考： ALIGN 的研究细节发表于 ICML 2021research.google。Google Research 官方博客提供了对ALIGN的通俗描述research.googleresearch.google。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-06-01/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-06-01/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-06-01/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-06-01">
  <meta property="og:description" content="主流视觉-文本多模态模型技术分析 近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。
CLIP (OpenAI, 2021) 整体架构设计： CLIP 采用 双编码器架构 ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到相同维度的向量空间lightly.ai。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型lightly.ai。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到共享的多模态嵌入空间lightly.ai。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。 模态对齐方式： CLIP通过对比学习实现视觉-语言对齐。训练时，模型给定一批图文对，学习预测哪张图像与哪段文本匹配lightly.ai。具体而言，CLIP使用 对称的跨模态对比损失 （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度lightly.ai。这种训练使图像和文本编码器产生的特征在共享空间中成对靠近lightly.ai。 输入 token 表达统一： CLIP并未显式统一图像和文本的输入表示格式。 图像和文本各有独立的token化和编码流程 ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征lightly.ai。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过独立编码&#43;对齐空间的方式，规避了直接将图像作为序列token处理的不统一问题。 损失函数与训练策略： 损失采用 对比学习的InfoNCE损失 （实现为带温度系数的归一化softmax交叉熵）research.google。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了大批量训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化lightly.ai。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应lightly.ai。CLIP从随机初始化开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型lightly.ai。 数据集及伪标签： CLIP在一个超大规模的图文配对数据集上预训练，包含约4亿对图像-文本arxiv.org（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖自然语言的弱监督arxiv.org。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。 六大难点应对： 模态对齐困难： CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远lightly.ai。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。 token格式不统一： 采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对lightly.ai。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。 语义粒度不匹配： CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如Fine-Grained CLIP等正是受限于CLIP在局部语义对齐上的不足lightly.ailightly.ai。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。 多模态上下文保持： CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 多模态上下文 （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。 训练数据稀缺： CLIP通过大规模弱标注数据从根本上缓解了数据稀缺的问题arxiv.org。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征research.google。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。 计算开销高： 训练CLIP确实需要巨大的算力和显存，但相对来说，其双塔架构使训练可并行展开，推理时也可分别预编码图文后做相似度计算，具有一定的效率优势lightly.ai。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化lightly.ai。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现lightly.ai。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。 参考： CLIP 的论文arxiv.org详细描述了其对比预训练方法，OpenAI 的博客也提供了概述lightly.ai。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库huggingface.co。
ALIGN (Google, 2021) 整体架构设计： ALIGN（ A Large-scale ImaGe and Noisy-Text embedding ）延续了与CLIP相同的双编码器对比学习架构research.google。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：EfficientNet-L2卷积网络作为图像编码器，BERT-Large作为文本编码器，并均从随机初始化开始训练research.google。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。 模态对齐方式： ALIGN采用对比损失（normalized softmax）来训练，使匹配的图文对嵌入向量接近，不匹配的远离research.google。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以批为单位的跨模态对比训练，使模型学到强大的图文对齐表示。 输入 token 表达统一： ALIGN同样没有将图像直接离散为token序列，而是通过双通道处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出embedding空间实现统一表示research.google。 损失函数与训练策略： 使用 对比学习损失 （InfoNCE变体），在大批量上训练模型research.google。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 最小程度的过滤 ，通过数据规模来弥补噪声research.google。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。 数据集及伪标签： ALIGN的亮点在于使用了超过10亿对图像-Alt文本的超大规模数据集research.google。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 无需人工标注 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN 放宽过滤标准 ，只做了基于频率的简单过滤，最终得到约18亿对图文数据research.google。这些文本描述可能包含噪声甚至与图像无关，但研究表明 规模弥补质量 ：如此海量的数据使模型学到泛化的视觉语言表示research.google。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题research.google。 六大难点应对： 模态对齐困难： ALIGN证明了数据规模在对齐中的重要作用。通过十倍于CLIP的数据规模和强大的对比损失，模型学到了稳健的跨模态对齐能力research.googleresearch.google。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型research.google。 token格式不统一： 与CLIP类似，ALIGN通过双编码器架构回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可research.google。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。 语义粒度不匹配： ALIGN的训练目标依旧作用在 全局图像-句子层面 ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对细粒度语义的不匹配没有特殊解决方案。 多模态上下文保持： ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力research.google，未涉及多模态对话等情境。因此，ALIGN在多轮交互或长上下文问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。 训练数据稀缺： ALIGN的策略是极端扩增数据规模以消除数据稀缺瓶颈research.google。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型research.google。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 弱标注的大数据 。 计算开销高： 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展research.google。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源research.google。尽管计算开销高昂，ALIGN通过 冻结架构复杂性 （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，优先扩大数据规模比增加模型复杂度更有效research.googleresearch.google。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。 参考： ALIGN 的研究细节发表于 ICML 2021research.google。Google Research 官方博客提供了对ALIGN的通俗描述research.googleresearch.google。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-01T12:00:14+08:00">
    <meta property="article:modified_time" content="2025-06-01T12:00:14+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-06-01">
<meta name="twitter:description" content="主流视觉-文本多模态模型技术分析
近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。
CLIP (OpenAI, 2021)

整体架构设计： CLIP 采用 双编码器架构 ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到相同维度的向量空间lightly.ai。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型lightly.ai。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到共享的多模态嵌入空间lightly.ai。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。
模态对齐方式： CLIP通过对比学习实现视觉-语言对齐。训练时，模型给定一批图文对，学习预测哪张图像与哪段文本匹配lightly.ai。具体而言，CLIP使用 对称的跨模态对比损失 （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度lightly.ai。这种训练使图像和文本编码器产生的特征在共享空间中成对靠近lightly.ai。
输入 token 表达统一： CLIP并未显式统一图像和文本的输入表示格式。 图像和文本各有独立的token化和编码流程 ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征lightly.ai。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过独立编码&#43;对齐空间的方式，规避了直接将图像作为序列token处理的不统一问题。
损失函数与训练策略： 损失采用 对比学习的InfoNCE损失 （实现为带温度系数的归一化softmax交叉熵）research.google。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了大批量训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化lightly.ai。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应lightly.ai。CLIP从随机初始化开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型lightly.ai。
数据集及伪标签： CLIP在一个超大规模的图文配对数据集上预训练，包含约4亿对图像-文本arxiv.org（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖自然语言的弱监督arxiv.org。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。
六大难点应对：

模态对齐困难：  CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远lightly.ai。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。
token格式不统一：  采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对lightly.ai。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。
语义粒度不匹配： CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如Fine-Grained CLIP等正是受限于CLIP在局部语义对齐上的不足lightly.ailightly.ai。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。
多模态上下文保持： CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 多模态上下文 （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。
训练数据稀缺： CLIP通过大规模弱标注数据从根本上缓解了数据稀缺的问题arxiv.org。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征research.google。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。
计算开销高： 训练CLIP确实需要巨大的算力和显存，但相对来说，其双塔架构使训练可并行展开，推理时也可分别预编码图文后做相似度计算，具有一定的效率优势lightly.ai。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化lightly.ai。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现lightly.ai。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。



参考： CLIP 的论文arxiv.org详细描述了其对比预训练方法，OpenAI 的博客也提供了概述lightly.ai。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库huggingface.co。
ALIGN (Google, 2021)

整体架构设计： ALIGN（ A Large-scale ImaGe and Noisy-Text embedding ）延续了与CLIP相同的双编码器对比学习架构research.google。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：EfficientNet-L2卷积网络作为图像编码器，BERT-Large作为文本编码器，并均从随机初始化开始训练research.google。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。
模态对齐方式： ALIGN采用对比损失（normalized softmax）来训练，使匹配的图文对嵌入向量接近，不匹配的远离research.google。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以批为单位的跨模态对比训练，使模型学到强大的图文对齐表示。
输入 token 表达统一： ALIGN同样没有将图像直接离散为token序列，而是通过双通道处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出embedding空间实现统一表示research.google。
损失函数与训练策略： 使用 对比学习损失 （InfoNCE变体），在大批量上训练模型research.google。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 最小程度的过滤 ，通过数据规模来弥补噪声research.google。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。
数据集及伪标签： ALIGN的亮点在于使用了超过10亿对图像-Alt文本的超大规模数据集research.google。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 无需人工标注 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN 放宽过滤标准 ，只做了基于频率的简单过滤，最终得到约18亿对图文数据research.google。这些文本描述可能包含噪声甚至与图像无关，但研究表明 规模弥补质量 ：如此海量的数据使模型学到泛化的视觉语言表示research.google。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题research.google。
六大难点应对：

模态对齐困难： ALIGN证明了数据规模在对齐中的重要作用。通过十倍于CLIP的数据规模和强大的对比损失，模型学到了稳健的跨模态对齐能力research.googleresearch.google。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型research.google。
token格式不统一： 与CLIP类似，ALIGN通过双编码器架构回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可research.google。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。
语义粒度不匹配： ALIGN的训练目标依旧作用在 全局图像-句子层面 ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对细粒度语义的不匹配没有特殊解决方案。
多模态上下文保持： ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力research.google，未涉及多模态对话等情境。因此，ALIGN在多轮交互或长上下文问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。
训练数据稀缺： ALIGN的策略是极端扩增数据规模以消除数据稀缺瓶颈research.google。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型research.google。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 弱标注的大数据 。
计算开销高： 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展research.google。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源research.google。尽管计算开销高昂，ALIGN通过 冻结架构复杂性 （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，优先扩大数据规模比增加模型复杂度更有效research.googleresearch.google。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。



参考： ALIGN 的研究细节发表于 ICML 2021research.google。Google Research 官方博客提供了对ALIGN的通俗描述research.googleresearch.google。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-06-01",
      "item": "https://tzj2006.github.io/bugjournal/2025-06-01/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-06-01",
  "name": "Bug Journal 2025-06-01",
  "description": "主流视觉-文本多模态模型技术分析 近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。\nCLIP (OpenAI, 2021) 整体架构设计： CLIP 采用 双编码器架构 ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到相同维度的向量空间lightly.ai。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型lightly.ai。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到共享的多模态嵌入空间lightly.ai。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。 模态对齐方式： CLIP通过对比学习实现视觉-语言对齐。训练时，模型给定一批图文对，学习预测哪张图像与哪段文本匹配lightly.ai。具体而言，CLIP使用 对称的跨模态对比损失 （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度lightly.ai。这种训练使图像和文本编码器产生的特征在共享空间中成对靠近lightly.ai。 输入 token 表达统一： CLIP并未显式统一图像和文本的输入表示格式。 图像和文本各有独立的token化和编码流程 ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征lightly.ai。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过独立编码+对齐空间的方式，规避了直接将图像作为序列token处理的不统一问题。 损失函数与训练策略： 损失采用 对比学习的InfoNCE损失 （实现为带温度系数的归一化softmax交叉熵）research.google。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了大批量训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化lightly.ai。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应lightly.ai。CLIP从随机初始化开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型lightly.ai。 数据集及伪标签： CLIP在一个超大规模的图文配对数据集上预训练，包含约4亿对图像-文本arxiv.org（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖自然语言的弱监督arxiv.org。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。 六大难点应对： 模态对齐困难： CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远lightly.ai。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。 token格式不统一： 采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对lightly.ai。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。 语义粒度不匹配： CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如Fine-Grained CLIP等正是受限于CLIP在局部语义对齐上的不足lightly.ailightly.ai。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。 多模态上下文保持： CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 多模态上下文 （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。 训练数据稀缺： CLIP通过大规模弱标注数据从根本上缓解了数据稀缺的问题arxiv.org。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征research.google。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。 计算开销高： 训练CLIP确实需要巨大的算力和显存，但相对来说，其双塔架构使训练可并行展开，推理时也可分别预编码图文后做相似度计算，具有一定的效率优势lightly.ai。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化lightly.ai。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现lightly.ai。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。 参考： CLIP 的论文arxiv.org详细描述了其对比预训练方法，OpenAI 的博客也提供了概述lightly.ai。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库huggingface.co。\nALIGN (Google, 2021) 整体架构设计： ALIGN（ A Large-scale ImaGe and Noisy-Text embedding ）延续了与CLIP相同的双编码器对比学习架构research.google。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：EfficientNet-L2卷积网络作为图像编码器，BERT-Large作为文本编码器，并均从随机初始化开始训练research.google。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。 模态对齐方式： ALIGN采用对比损失（normalized softmax）来训练，使匹配的图文对嵌入向量接近，不匹配的远离research.google。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以批为单位的跨模态对比训练，使模型学到强大的图文对齐表示。 输入 token 表达统一： ALIGN同样没有将图像直接离散为token序列，而是通过双通道处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出embedding空间实现统一表示research.google。 损失函数与训练策略： 使用 对比学习损失 （InfoNCE变体），在大批量上训练模型research.google。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 最小程度的过滤 ，通过数据规模来弥补噪声research.google。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。 数据集及伪标签： ALIGN的亮点在于使用了超过10亿对图像-Alt文本的超大规模数据集research.google。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 无需人工标注 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN 放宽过滤标准 ，只做了基于频率的简单过滤，最终得到约18亿对图文数据research.google。这些文本描述可能包含噪声甚至与图像无关，但研究表明 规模弥补质量 ：如此海量的数据使模型学到泛化的视觉语言表示research.google。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题research.google。 六大难点应对： 模态对齐困难： ALIGN证明了数据规模在对齐中的重要作用。通过十倍于CLIP的数据规模和强大的对比损失，模型学到了稳健的跨模态对齐能力research.googleresearch.google。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型research.google。 token格式不统一： 与CLIP类似，ALIGN通过双编码器架构回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可research.google。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。 语义粒度不匹配： ALIGN的训练目标依旧作用在 全局图像-句子层面 ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对细粒度语义的不匹配没有特殊解决方案。 多模态上下文保持： ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力research.google，未涉及多模态对话等情境。因此，ALIGN在多轮交互或长上下文问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。 训练数据稀缺： ALIGN的策略是极端扩增数据规模以消除数据稀缺瓶颈research.google。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型research.google。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 弱标注的大数据 。 计算开销高： 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展research.google。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源research.google。尽管计算开销高昂，ALIGN通过 冻结架构复杂性 （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，优先扩大数据规模比增加模型复杂度更有效research.googleresearch.google。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。 参考： ALIGN 的研究细节发表于 ICML 2021research.google。Google Research 官方博客提供了对ALIGN的通俗描述research.googleresearch.google。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。\n",
  "keywords": [
    "Bug Journal", "Computer Vision", "LLM"
  ],
  "articleBody": "主流视觉-文本多模态模型技术分析 近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。\nCLIP (OpenAI, 2021) 整体架构设计： CLIP 采用 双编码器架构 ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到相同维度的向量空间lightly.ai。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型lightly.ai。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到共享的多模态嵌入空间lightly.ai。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。 模态对齐方式： CLIP通过对比学习实现视觉-语言对齐。训练时，模型给定一批图文对，学习预测哪张图像与哪段文本匹配lightly.ai。具体而言，CLIP使用 对称的跨模态对比损失 （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度lightly.ai。这种训练使图像和文本编码器产生的特征在共享空间中成对靠近lightly.ai。 输入 token 表达统一： CLIP并未显式统一图像和文本的输入表示格式。 图像和文本各有独立的token化和编码流程 ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征lightly.ai。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过独立编码+对齐空间的方式，规避了直接将图像作为序列token处理的不统一问题。 损失函数与训练策略： 损失采用 对比学习的InfoNCE损失 （实现为带温度系数的归一化softmax交叉熵）research.google。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了大批量训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化lightly.ai。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应lightly.ai。CLIP从随机初始化开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型lightly.ai。 数据集及伪标签： CLIP在一个超大规模的图文配对数据集上预训练，包含约4亿对图像-文本arxiv.org（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖自然语言的弱监督arxiv.org。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。 六大难点应对： 模态对齐困难： CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远lightly.ai。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。 token格式不统一： 采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对lightly.ai。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。 语义粒度不匹配： CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如Fine-Grained CLIP等正是受限于CLIP在局部语义对齐上的不足lightly.ailightly.ai。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。 多模态上下文保持： CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 多模态上下文 （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。 训练数据稀缺： CLIP通过大规模弱标注数据从根本上缓解了数据稀缺的问题arxiv.org。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征research.google。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。 计算开销高： 训练CLIP确实需要巨大的算力和显存，但相对来说，其双塔架构使训练可并行展开，推理时也可分别预编码图文后做相似度计算，具有一定的效率优势lightly.ai。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化lightly.ai。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现lightly.ai。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。 参考： CLIP 的论文arxiv.org详细描述了其对比预训练方法，OpenAI 的博客也提供了概述lightly.ai。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库huggingface.co。\nALIGN (Google, 2021) 整体架构设计： ALIGN（ A Large-scale ImaGe and Noisy-Text embedding ）延续了与CLIP相同的双编码器对比学习架构research.google。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：EfficientNet-L2卷积网络作为图像编码器，BERT-Large作为文本编码器，并均从随机初始化开始训练research.google。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。 模态对齐方式： ALIGN采用对比损失（normalized softmax）来训练，使匹配的图文对嵌入向量接近，不匹配的远离research.google。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以批为单位的跨模态对比训练，使模型学到强大的图文对齐表示。 输入 token 表达统一： ALIGN同样没有将图像直接离散为token序列，而是通过双通道处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出embedding空间实现统一表示research.google。 损失函数与训练策略： 使用 对比学习损失 （InfoNCE变体），在大批量上训练模型research.google。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 最小程度的过滤 ，通过数据规模来弥补噪声research.google。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。 数据集及伪标签： ALIGN的亮点在于使用了超过10亿对图像-Alt文本的超大规模数据集research.google。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 无需人工标注 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN 放宽过滤标准 ，只做了基于频率的简单过滤，最终得到约18亿对图文数据research.google。这些文本描述可能包含噪声甚至与图像无关，但研究表明 规模弥补质量 ：如此海量的数据使模型学到泛化的视觉语言表示research.google。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题research.google。 六大难点应对： 模态对齐困难： ALIGN证明了数据规模在对齐中的重要作用。通过十倍于CLIP的数据规模和强大的对比损失，模型学到了稳健的跨模态对齐能力research.googleresearch.google。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型research.google。 token格式不统一： 与CLIP类似，ALIGN通过双编码器架构回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可research.google。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。 语义粒度不匹配： ALIGN的训练目标依旧作用在 全局图像-句子层面 ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对细粒度语义的不匹配没有特殊解决方案。 多模态上下文保持： ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力research.google，未涉及多模态对话等情境。因此，ALIGN在多轮交互或长上下文问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。 训练数据稀缺： ALIGN的策略是极端扩增数据规模以消除数据稀缺瓶颈research.google。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型research.google。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 弱标注的大数据 。 计算开销高： 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展research.google。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源research.google。尽管计算开销高昂，ALIGN通过 冻结架构复杂性 （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，优先扩大数据规模比增加模型复杂度更有效research.googleresearch.google。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。 参考： ALIGN 的研究细节发表于 ICML 2021research.google。Google Research 官方博客提供了对ALIGN的通俗描述research.googleresearch.google。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。\nBLIP (Salesforce, 2022) 整体架构设计： BLIP（ Bootstrapping Language-Image Pre-training ）提出了一种 多模态混合编码-解码架构（MED） ，旨在同时支持视觉-语言的理解和生成任务lightly.ai。具体而言，BLIP的模型包含三个不同模式的子模型：① 单模态编码器 ：对图像或文本单独编码，用于提取各自模态的表示，并采用图文对比损失（ITC）对齐两种模态的全局特征lightly.ai；② 图像引导的文本编码器 ：在文本Transformer中引入跨模态注意力，让文本编码能够利用图像特征，训练时使用 图文匹配（ITM）损失来判断给定图文是否匹配lightly.ai；③ 图像引导的文本解码器 ：使用因果自注意（单向）以实现文本生成，能够在给定图像的条件下生成描述或回答，训练时使用语言模型（LM）损失lightly.ai。这三部分共享同一个视觉编码器（ViT）和大部分文本编码-解码参数，仅在自注意力层是否双向/单向上有所区别lightly.ai。这种设计使一个模型即可兼顾判别任务 （如检索、VQA判断）和 生成任务 （如图像描述）。 模态对齐方式： BLIP结合多种目标实现模态对齐和融合：(1) 图文对比学习（ITC）让图像和文本的全局表示对齐，获得CLIP类似的跨模态嵌入空间lightly.ai；(2) 图文匹配（ITM）通过引入交叉注意力的文本编码器，对图文对的匹配与否进行二分类训练，从而细粒度地对齐图像内容和文本语义lightly.ai；(3) 图像条件文本生成（LM损失）使模型学会在视觉条件下生成合适的语言输出。这三种损失共同训练，迫使模型不同层面地对齐：既有 全局对齐 （ITC保证embedding空间一致），又有 局部对齐 （ITM通过注意力机制关注图像局部来判断匹配），还有跨模态 生成对齐 （LM确保图像信息融入生成过程）。尤其ITM子任务，需要模型理解图像细节与文本句子细微差异，提高了模态对齐的精细程度。 输入 token 表达统一： BLIP并未将图像直接当作序列token交给文本Transformer，而是采用部分共享参数的编码器-解码器架构来统一多模态信息。图像首先由视觉Transformer编码为一组图像embedding序列，文本则通过词嵌入得到文本token表示。在图文交互阶段，图像embedding通过跨模态注意力机制供文本编码器/解码器读取，从而在Transformer中融合lightly.ai。由于文本端Transformer参数在编码和解码模式下共享（仅注意力方向不同），图像信息可以以一致的方式融入文本token处理流程lightly.ai。简言之，BLIP通过在Transformer中引入图像作为钥匙/值的跨注意力，将图像内容注入文本token序列的处理，使两种模态的信息在Transformer中统一表达和交互。 损失函数与训练策略： BLIP在预训练阶段联合优化三种损失lightly.ai：图文对比损失、图文匹配损失、语言建模损失。一次训练迭代中，通过共享的图像编码器提取视觉特征后，分别送入上述三种模式的网络计算损失lightly.ai。为提高效率，BLIP 让文本编码器和解码器共享参数 （仅自注意力模块不同），这样图像特征可一次读取，多头任务不会成倍增加参数lightly.ai。训练策略上，BLIP先在大规模图文数据上这样多任务预训练，然后可以微调到下游具体任务。由于同时优化多目标，需平衡各损失的权重，论文中选择了适当的超参使模型在理解和生成性能上均有提升。此外，为了利用噪声较大的网络数据，BLIP设计了 两阶段Bootstrapping策略 ：首先用预训练好的模型作为图像描述生成器（Captioner）为图像生成候选描述，然后用一个筛选器（Filter）剔除不匹配的图文对，再将清洗/补充后的数据用于进一步训练lightly.ai。这种“Captioner+Filter”流程有效降低了训练数据中的噪声，并引入了合成的伪标签描述，提高了数据利用率arxiv.org。 使用的数据集及伪标签： BLIP的预训练使用了数百万规模的公众图文对数据（如COCO Caption、Visual Genome等常用数据集的组合，以及从网上爬取的图文对）arxiv.org。相对于CLIP/ALIGN那样十亿级的弱标注数据，BLIP使用的数据规模较小但质量更高（经过一定清洗）。为了进一步扩充数据，BLIP引入 伪标签机制 ：利用自己模型生成图像描述（Captioner生成synthetic captions），再通过训练好的匹配模型（Filter）过滤噪声lightly.ai。结果是，原本嘈杂的网络图文数据被“自举”出较为可靠的新图文对，从而缓解了高质量标注数据不足的问题arxiv.org。这一过程中生成的图像描述相当于 合成标签 ，极大丰富了训练语料。在预训练后，BLIP在下游如Flickr30K检索、COCO描述、VQA等数据集上进行微调或直接评估，均取得领先性能arxiv.org。值得一提的是，BLIP的整个预训练不依赖外部标注工具（如不使用额外OCR或检测模型），完全通过多任务训练和自举数据来提高性能。 六大难点应对： 模态对齐困难： BLIP通过多重训练目标从不同层次对齐图像和文本。ITC损失提供全局嵌入对齐，ITM损失迫使模型关注细粒度关联来判断真伪配对，LM损失则确保图像信息能融入自然语言生成lightly.ailightly.ai。此外，BLIP在训练中让图像参与文本Transformer的注意力计算，直接在模型内部融合模态，这比单纯对比学习的对齐更深入。综合来看，BLIP有效缓解了模态对齐难的问题，使模型不仅对整体匹配敏感，也能对局部语义对齐做出正确判断。 token格式不统一： BLIP没有一刀切地将图像转为离散token，而是采用跨注意力融合策略保持各模态表征方式的优势。图像以连续向量形式存在，通过跨模态注意力供文本Transformer使用，实现类似“在Transformer中把图像当成一串记忆token”的效果。这种方法避免了人为定义图像token格式的问题，由模型自学怎样将图像特征融入文本语境。因此，BLIP在不显式统一输入格式的情况下，通过模型结构实现了功能上的token统一处理。 语义粒度不匹配： BLIP在架构上引入了细粒度语义对齐机制。ITM子任务要求模型判别图文是否匹配，这通常取决于对图像细节和文本词语是否对应的判断（例如一句描述中某个细节是否在图中存在）。模型通过跨注意力可以聚焦图像的局部区域来对应文本片段，从而解决了图像区域与文本词汇粒度不对齐的问题。在生成阶段，图像引导的解码器也能描述具体对象或属性，实现细粒度描述lightly.ai。因此BLIP比CLIP这类全局对齐模型更好地兼顾了细粒度的语义对齐。 多模态上下文保持： 虽然BLIP主要针对单幅图像与单段文本的配对任务，但其架构天然适用于 多轮交互的扩展 。因为BLIP的文本Transformer本质是一个语言模型，经过适当调整可以接受前文对话作为文本输入，然后结合图像生成回答。事实上，BLIP的设计理念已被后续多模态对话模型（如 InstructBLIP 等）继承，用于处理多轮对话。不过BLIP原始模型并未显示多轮对话训练。在单轮情景下，BLIP利用Transformer的长序列能力，一定程度上可以处理 更长的文本上下文 （如图像说明+问题一起作为前缀，然后生成回答）。因此在上下文保持上，BLIP通过Transformer结构具备了潜在优势，但需通过特定训练来充分发挥。 训练数据稀缺： BLIP以 自举（Bootstrapping）的方式缓解数据不足。面对高质量标注数据有限的问题，BLIP使用初步模型为大量未标注图像生成描述（相当于自动标注），再过滤噪声后加入训练arxiv.org。这种方式有效放大了训练集规模且成本低，因为生成伪标签比人工标注快得多。结果，BLIP无需像CLIP那样依赖上亿数据，就能取得优异表现arxiv.org。此外，多任务联合训练也提高了数据利用效率：同一数据同时为对比、匹配、生成三种任务服务，信息提取更加充分。总之，BLIP通过模型自生成数据+多任务学习 ，成功在有限数据下逼近甚至超越了依赖海量数据的方法。 计算开销高： BLIP的模型大小适中（基于ViT-B/16等视觉主干，文本部分与BERT-base级别相当），但同时优化三种目标确实增加了训练复杂度。不过，通过 参数共享 （文本编码器和解码器共享大部分参数）lightly.ai和 模块复用 （同一个视觉编码器和Transformer用于多任务），BLIP将训练开销控制在可接受范围。相较于为理解和生成训练两个模型，BLIP训练单个模型完成两类任务，实际上 节省了总体算力 。当然，多任务训练需要更长时间收敛，但Salesforce的实验表明收益是值得的arxiv.org。在推理阶段，BLIP可以根据任务切换模式，例如执行检索时只用编码器部分，做描述时用编码-解码器，全模型参数无需全部参与，从而 推理开销也相对可控 。综上，BLIP通过架构设计在性能和计算成本之间取得了平衡，使得大型多模态模型的训练变得更加高效。 参考： BLIP的论文发表在 ICML 2022arxiv.orgarxiv.org。官方代码已开源在 GitHubarxiv.org（salesforce/BLIP仓库），提供了预训练模型和下游任务的fine-tune实现，方便复现论文结果。\nBLIP-2 (Salesforce, 2023) 整体架构设计： BLIP-2的核心思想是利用现有的预训练模型来高效构建多模态模型arxiv.org。它冻结了图像编码器（如ViT系列）和大型语言模型（LLM，如OPT、Flan-T5等），通过引入一个轻量级的Query Transformer（Q-Former）将二者连接起来arxiv.org。架构上包括：冻结的视觉编码器-\u003e Q-Former -\u003e 冻结的文本生成模型。Q-Former本质是一个Transformer模块，接受视觉特征作为输入，输出一组固定数量的查询向量lightly.ai。这组向量经过投影后，作为虚拟的“视觉token”，嵌入到LLM的输入序列中，从而让LLM能够接收图像信息lightly.ai。由于LLM参数冻结，BLIP-2主要训练Q-Former和少量连接层。 模态对齐方式： BLIP-2将跨模态对齐的主要难点转移到Q-Former上。它采用两阶段训练lightly.ai：第一阶段，让Q-Former结合冻结视觉编码器进行 图文表示学习 （类似BLIP-1的方法，使用图文对比或匹配损失），使Q-Former学会提取与文本语义相关的视觉概念lightly.ai。第二阶段，将训练好的Q-Former输出连接到冻结的LLM输入embedding，利用图文对话/描述数据训练生成任务，使整个系统能够端到端地产生对应输入图像的文本lightly.ai。在对齐过程中，Q-Former充当中介：它一头通过跨注意力读取视觉特征，另一头输出的查询向量要能和LLM的语义空间对接。因此，通过专门设计的损失（如阶段一的对比/ITC+ITM，阶段二的语言建模），BLIP-2成功将视觉空间对齐到语言空间。直观来说， Q-Former学会生成“描述图像的语言向量” ，这些向量插入LLM提示中后，LLM即可理解并基于图像内容作出回答。 输入 token 表达统一： 在BLIP-2中，输入给LLM的是标准的文本token序列，但其中混入了由图像生成的 特殊嵌入向量 。具体实现是：LLM的词表中引入若干个保留位置，用来放置Q-Former生成的视觉查询向量（不一定真的映射为离散token，而是直接作为embedding）lightly.ailightly.ai。因此，从LLM角度看，它接收到了一串长度为N（固定）的“视觉token”嵌入，后面可能跟随文本token，例如问题或提示语。通过这种方式，图像信息被格式统一地并入LLM的输入序列，就好像视觉也被表示成了一组特殊的单词embedding。值得注意的是，这里的视觉token并非通过人工词典获得，而是Q-Former自由学习产生的向量。不过，对于LLM来说，无论是真实文字embedding还是视觉embedding，它都一视同仁地通过自注意力机制处理。这实现了在架构上的 输入统一 ：图像被转换成等价于文本embedding的形式，与文本共同作用于下游生成。 损失函数与训练策略： BLIP-2采用 分阶段训练策略 。阶段一使用与BLIP类似的目标（ITC对比学习、ITM匹配等）训练Q-Former，使其能够对齐视觉和文本表示lightly.ai。阶段二则固定视觉编码器和Q-Former不变（或仅微调Q-Former），仅训练将Q-Former输出喂入LLM后的生成能力lightly.ai。阶段二通常采用 语言模型损失 ：给定图像和（可选的）文本提示，让LLM输出描述或答案，与GT文本计算交叉熵损失，从而调整Q-Former和连接层使LLM的输出正确。在这一阶段，LLM本身参数冻结，所以训练信号主要作用于Q-Former，使其输出的视觉查询能被LLM高效利用。此外，BLIP-2可能使用了混合数据训练策略：既包含纯图文对话数据，也包含传统图文描述数据，以增强模型泛化能力。总结来说，第一阶段注重表示对齐，第二阶段注重生成对接arxiv.org。两个阶段结合，使模型以较低的训练成本达到对大语言模型“喂图”的效果。 使用的数据集及伪标签： BLIP-2所使用的数据包括现成的大规模图文对以及 对话式多模态数据 。阶段一使用的数据类似BLIP-1，例如COCO Caption、Visual Genome Caption以及LAION-400M等开放图文集，用于学习跨模态表示。阶段二则需要图像输入/文本输出的监督数据，如VQA问答、图像描述，以及自制的指令数据集等。由于BLIP-2本身是在2023年提出，可能利用了当时兴起的多模态指令数据（例如由GPT生成的对话）来增强模型的对话能力。关于伪标签，BLIP-2相比BLIP-1更少需要合成描述，原因是它直接利用预训练的LLM已经具备生成流畅文本的能力。相反，BLIP-2更关注 如何高效利用预训练资源 。它没有从零开始生成伪标签，而是通过降低训练需求（冻结大模型）来避免需要海量新标注数据arxiv.org。因此，除非为了特定任务，BLIP-2通常不依赖额外的伪标注数据。不过，在一些研究和开源实现中，会将BLIP-2作为基础，再用GPT-4生成的指令数据进行微调（如InstructBLIP），那属于后续fine-tuning阶段。总的来说，BLIP-2本身强调 利用已有数据与模型 ，而非采集新数据，这是一种不同于以往“大规模爬取”的范式。 六大难点应对： 模态对齐困难： BLIP-2的巧妙之处在于借助预训练模型降低对齐门槛：视觉编码器（如CLIP的ViT）本身已具有与文本对齐的表示能力，LLM则有强大的语言理解和生成能力。Q-Former经过专门训练，学习如何从图像提取出能解释文本的关键视觉概念lightly.ai。它将视觉信号压缩成几十个查询向量，使之恰好能被LLM理解。通过两阶段训练，BLIP-2成功将视觉信息嵌入LLM的上下文中，实现 模态隐式对齐 。尤其第二阶段训练，让模型生成正确描述，确保了视觉表示和语言表示在语义空间上对齐，以至于LLM可以将来自图像的embedding视作自身词汇的一部分。这解决了LLM未看过图像的难题，将跨模态对齐转换为一个中等规模Transformer训练就完成了arxiv.org。因此，BLIP-2在对齐上绕过了直接训练巨型多模态模型的难关，以更低成本达到对齐效果。 token格式不统一： BLIP-2通过 Query Transformer输出固定长度视觉token向量 ，使得图像信息以接近文本token的形式输入LLMlightly.ai。这些视觉token不是离散符号，但在Transformer中发挥的作用与普通词嵌入相同。LLM在位置嵌入上也不区分它们，这样视觉和文本序列实际上融合为一个统一的序列处理lightly.ai。因此，虽然没有显式定义图像的词汇表，BLIP-2达成了功能上的token格式统一：模型把连续视觉特征转换为离散的若干embedding插入序列。LLM可以像处理句子一样处理“图像句子”。这一设计继承了Flamingo等模型的思路，但更轻量（因为只有Q-Former承担额外计算）。 语义粒度不匹配： BLIP-2输出的视觉token向量本质上可以被视为图像的 语义摘要 。Q-Former通过训练，会针对文本任务提取图像中与语义相关的细粒度信息。例如，若任务是描述图像，Q-Former会聚焦显著对象和属性；若任务是回答问题，Q-Former会提取与问题相关的视觉线索。这种机制使图像的大量低层次像素信息被压缩，仅保留语义层面的关键内容，从而匹配LLM处理的语言粒度（概念级别）lightly.ai。因此，语义粒度的鸿沟通过Q-Former的提炼得到弥合——图像的细节被提升到语义概念后才提供给LLM。实践证明，BLIP-2能够让LLM正确识别图中具体对象并生成相应描述，说明语义层次基本匹配了语言空间arxiv.org。当然，如果图像中有非常细微的局部信息，固定数量的查询可能略有不足，但总体上BLIP-2在保持主要语义同时过滤冗余细节方面是成功的。 多模态上下文保持： BLIP-2本身不直接处理多轮对话，但由于它的输出接口是对接LLM，而LLM天然支持长上下文对话，因此BLIP-2具备扩展为多模态对话的潜力。事实上，将BLIP-2生成的视觉token视为对话的一部分，就可以实现 ChatGPT+图像 的效果。BLIP-2的论文主要评估的是单轮任务（如VQA回答、图像描述），但把它用于对话时，可以每次在提示中加入视觉token并配合已有的聊天上下文，LLM即可持续参考视觉信息进行对话。这意味着BLIP-2间接实现了 视觉上下文在多轮对话中的保持 ：视觉token可以在对话prompt中重复出现或被引用，使LLM记住之前提到的图像要点。不过，在一个会话过程中，BLIP-2通常针对每张新图像各自运行一次，不会像Flamingo那样显式处理多张图共同存在的情况。因此严格来说，BLIP-2原生支持 单图上下文保持 ，多图或连续对话需借助LLM的记忆机制来维系。 训练数据稀缺： BLIP-2的策略是 以预训练模型替代海量数据 。因为直接训练一个看图的LLM需要海量图文数据，但BLIP-2通过使用预先训练好的ViT和LLM，将主要学习任务转为训练Q-Former。Q-Former的参数规模（约1.9亿）远小于LLM，所需训练数据也相对少lightly.ai。实验表明，在已有的大模型基础上，只需在相对有限的图文数据上微调，就能达到甚至超过训练80亿参数模型（如Flamingo）的效果arxiv.org。这等于用模型知识弥补了数据量不足。此外，BLIP-2本身利用了BLIP-1时期的图文数据清洗经验，挑选高质量数据进行两阶段训练，以较小的数据量取得高性能arxiv.org。因此，对研究者而言，BLIP-2降低了训练多模态模型的数据门槛——无需爬取上亿样本，有几百万高质量样本配合预训练模型就够用。 计算开销高： 相比从头训练一个多模态Transformer（参数往往数十亿），BLIP-2的训练开销显著降低。冻结LLM和视觉编码器意味着大部分参数不需要反向传播更新，只训练Q-Former等少部分参数，使内存和算力需求下降。作者报告，BLIP-2仅有少量可训练参数却超越了一些体量大几十倍的模型arxiv.org。同时，由于分阶段训练，第一阶段可在相对小模型上完成，第二阶段虽然用LLM但只进行embedding层和Q-Former的调优，计算效率高。综合来看，BLIP-2通过迁移学习和 参数高效微调 ，极大缓和了算力需求。这也体现在推理阶段：因为LLM冻结且对话时只需将视觉token拼接输入，不增加额外推理步骤，实时性有保障。当然，BLIP-2依赖的LLM本身推理开销不低（如果LLM很大），但相较于训练一个同等大小的多模态模型，BLIP-2的总计算代价小得多。因此，在算力有限的环境下，BLIP-2提供了一种实用可行的多模态方案。 参考： BLIP-2的论文在 arXiv 发布arxiv.orgarxiv.org（ICLR 2023），详细介绍了其两阶段训练方法和在零样本VQA等任务上的性能。代码已开源在 GitHub（salesforce/LAVIS库中提供了BLIP-2实现）。BLIP-2的效果也推动了许多衍生工作（如开放对话系统 MiniGPT-4 等），这些都建立在BLIP-2提供的视觉-语言接口之上。\nGIT (Microsoft, 2022) 整体架构设计： GIT（ Generative Image-to-text Transformer ）尝试将视觉-语言任务完全统一到一个生成式Transformer框架下ar5iv.labs.arxiv.org。其架构极为简洁： 一个图像编码器 + 一个文本解码器 ，二者共同组成一个端到端的序列到序列模型ar5iv.labs.arxiv.org。图像编码器提取图像特征（采用预训练的CLIP视觉Transformer或自训练的ViT等，输出二维特征序列ar5iv.labs.arxiv.org），然后通过线性层投影并加上位置嵌入，作为文本解码器的跨注意力键值输入ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。文本解码器是标准的Transformer解码架构（多层自注意力+交叉注意力），以语言模型方式生成文本ar5iv.labs.arxiv.org。不同于许多早期方法，GIT不使用任何物体检测器或OCR模型来预处理图像，也不引入额外的多模态编码器，一切融合在Transformer解码器中完成ar5iv.labs.arxiv.org。这种纯粹的“图像到文本”架构使模型在预训练和微调阶段的结构完全一致，能够方便地泛化到各种以文本为输出的视觉任务。 模态对齐方式： GIT没有采用显式的对比对齐或ITM损失，而是通过单一的语言建模任务隐式地实现模态对齐ar5iv.labs.arxiv.org。在预训练时，模型接收图像并 直接生成整段描述文本 （或回答），训练目标是最小化生成文本与真实文本之间的交叉熵损失ar5iv.labs.arxiv.org。这种方式迫使图像编码器提取的特征必须包含生成正确文本所需的所有信息，同时解码器的交叉注意力会学习将文本词汇与相应的图像区域关联，以便正确生成。这意味着图像和文本的对齐并不是通过拉近embedding距离实现的，而是在Transformer解码过程中，通过注意力权重对齐：模型只有在正确对齐图像内容与生成词语时才能取得低损失。例如，当解码器生成单词“狗”时，跨模态注意力会自然地关注图像中狗所在的特征区域，从而将视觉语义与该单词绑定。经过大规模训练后，这种注意力驱动的软对齐形成模型内隐的模态对齐机制。值得一提的是，作者在预训练时 扩充了任务种类 ，不仅包括图像描述，还有图像问答等，这些任务都要求正确关联图像和文本才能解答，从而进一步强化了模态对齐ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。 输入 token 表达统一： GIT通过 将图像特征序列拼接进Transformer解码器的输入 ，实现了一种隐式的token统一表示ar5iv.labs.arxiv.org。具体而言，图像编码器输出经过投影变换后，作为一组“图像token”（连续向量）排列在Transformer解码器的输入序列最前ar5iv.labs.arxiv.org。紧随其后的是文本的标记和需要生成的文本token（初始化为待预测状态）。在Transformer内部，采用一个特别的序列到序列注意力掩码ar5iv.labs.arxiv.org：文本token可以看见所有图像token和之前的文本token，而图像token之间也可以相互看到（便于图像特征全局建模）ar5iv.labs.arxiv.org。这样，Transformer解码器实际上同时处理了图像token和文本token的序列。对模型而言，图像token与普通文本embedding在同一计算图中，只是通过mask控制了注意力方向。通过这种机制，GIT无需修改Transformer结构，就实现了 图像+文本统一序列建模 ：图像被视作解码开始时的一段前缀序列。这保证了图像信息能够像前文一样参与生成过程，从而让图像上下文与文本自然融合。另外，这种方法也不需要离散化图像，只要提供足够的图像token分辨率，模型就能以连续表示处理视觉信息。 损失函数与训练策略： GIT采用 纯粹的自回归语言模型损失 。给定图像（以及可选的提示文本），让模型生成目标文本序列，计算标准的交叉熵损失来训练ar5iv.labs.arxiv.org。在预训练期间，为了让模型适应多样任务，训练数据中包含了各种形式：图像标题生成、图像问答（在这种情况下，会在图像token后加入问题文本作为前缀，然后生成答案）等ar5iv.labs.arxiv.orgar5iv.labs.arxiv.org。例如，对于VQA，输入序列是「… 问题：… 答案：」，模型学习在看到“问题”后生成正确“答案”ar5iv.labs.arxiv.org。这种统一的语言模型策略使预训练和下游任务能够共享同一套参数和目标，不需要为不同任务切换架构或损失函数。此外，作者强调扩大预训练数据和模型规模对性能至关重要ar5iv.labs.arxiv.org。他们使用了比以往更大规模的图文数据，以及训练了不同尺寸的模型（从Base到巨型）进行对比，在多个任务上取得新的SOTAar5iv.labs.arxiv.org。训练策略上没有使用教师模型或多阶段训练，而是一阶段大一统模型学尽可能多的任务。这种“无技巧（no bells and whistles）”的方法充分依赖海量数据和模型容量来获得性能ar5iv.labs.arxiv.org。 使用的数据集及合成数据： GIT的预训练数据非常广泛和庞大。微软在论文中没有公布确切的数据量，但提到**“扩大了预训练数据规模”ar5iv.labs.arxiv.org。推测他们使用了公共的大型图文数据集合集（如COYO、LAION等），以及内部收集的数据，包括图片描述和问答标注。此外，他们还将视频字幕数据扩充到模型中，使模型能处理视频（选帧作为序列的一部分）ar5iv.labs.arxiv.org。在下游微调时，GIT在12个具有挑战性的基准上测试，包括COCO、nocaps（开放词汇描述）、VizWiz（盲人拍照求助）、TextCaps（需要OCR的图片描述）、多种VQA和视频caption等arxiv.org。令人瞩目的是，GIT在TextCaps数据集上首次超越了人类表现arxiv.org，说明模型学会了相当程度的场景文本识别和理解——这归功于预训练涵盖了带文字的图像以及无需OCR模块的端到端学习。GIT并未借助合成的伪标签数据；相反，它直接在真实任务数据上大规模训练**。例如，为了让模型具备OCR能力，他们可能在预训练中加入了带文字的图像及其文字描述（如OCR-VQA等），让模型自己去学习文字区域的特征提取ar5iv.labs.arxiv.org。因此，GIT更多是通过多任务训练覆盖各种模态难点，而不是通过额外生成数据来弥补。当然，训练这样一个模型本身需要巨量的数据，但微软具备这样的资源优势。 六大难点应对： 模态对齐困难： GIT选择了端到端生成作为对齐手段。由于模型只能通过正确生成文本来降低损失，它被迫在内部对齐图像与文本。例如，Transformer解码器的交叉注意力会在训练中自动调整，使得每个生成的词与相应的图像内容关联。这种隐式对齐不需要额外的对比损失，却在模型Attention权重中形成了图像区域-文本词汇的映射关系。再加上GIT预训练涵盖问答等任务，模型学会在回答问题时关注相关图像部分，在描述时依照图像内容组织语言——这些都属于模态对齐的体现。可见，尽管没有显式对齐Loss，GIT通过任务驱动对齐实现了高质量的模态对齐ar5iv.labs.arxiv.org。模型的成功表明，只要任务设计合理，生成式训练本身就能让模型学会跨模态对齐。 token格式不统一： GIT通过序列到序列Transformer架构，巧妙地让图像和文本“同列于一个序列”。图像编码器输出一系列向量，这些向量在解码器里被视作一段上下文序列ar5iv.labs.arxiv.org。这样，虽然图像不是离散单词，但在Transformer看来，它们只是前若干个特殊的输入embedding。后续文本token可以自然地参考这些图像embedding，就如同参考句首提供的提示一样。这个设计避免了需要定义图像词典或修改模型输入结构，使 格式统一的问题迎刃而解 。换言之，Transformer模型对图像和文本一视同仁，只是通过mask控制依赖关系ar5iv.labs.arxiv.org。因此，GIT内部已经实现了对不同模态信息的格式融合，不存在单独处理再对齐的问题。 语义粒度不匹配： GIT直接使用CNN/ViT提取图像特征，并通过Transformer将其转换为语言。没有显式区域级别的对齐机制，但Transformer的交叉注意力可以细粒度地处理图像patch与词的关系。例如，模型在生成某个名词时，会极大地注意对应物体的那些视觉token，实现类似局部对齐的效果。这相当于让细粒度对齐在注意力机制中自发完成。此外，作者使用了一个trick：他们用对比学习预训练好的图像编码器ar5iv.labs.arxiv.org，保证图像特征本身具有较高级的语义表示能力（对比预训练会让相同类别/语义的图像特征聚类）。这意味着图像特征一开始就带有一定的语义概括性，减少了视觉低层细节与语言高级概念的不匹配ar5iv.labs.arxiv.org。因此，在语义粒度上，GIT通过预训练的视觉语义特征+解码器注意力两方面，较好地解决了粒度差异问题。模型的OCR能力说明它可以从小区域拼写出单词，说明精细粒度也能捕获；而在描述整图时又能抓大放小，生成整体语义，这体现了粒度上的灵活性。 多模态上下文保持： GIT的设计初衷不在对话，而在统一各种 静态视觉任务 。因此原版GIT不具备多轮对话记忆。然而，它提供了 统一的生成框架 ，理论上可以扩展对话：只需在输入序列中加入之前对话的文本，即可将历史作为上下文。而图像如果需要在对话中反复参考，可以在每轮答复时都把同样的图像token放入输入。但这会受到模型最大序列长度限制。微软没有在论文中报道对话实验，但在VQA任务里，GIT通过将“问题”作为前缀文本与图像共同输入ar5iv.labs.arxiv.org来回答，已经体现了处理图文混合上下文的能力。对于多张图像，GIT可以一次编码多张图的特征串联作为更长的图像token序列，只是论文未深入探索。这种架构天然支持多模态上下文的扩展，但需要注意计算成本会随序列长度增长。在视频场景中，作者已经验证了能处理多帧（通过给每帧加上时间嵌入再串联）ar5iv.labs.arxiv.org。所以GIT显示出一定的上下文扩展性，但要真正保持多轮对话语境，可能还需在生成策略上做些改动（如引入特殊标记区分说话人等）。总的说来，GIT为多模态上下文提供了一个统一容器，但对话管理不在其预训练范围内，需要额外设计。 训练数据稀缺： GIT依赖大规模多样化数据取得成功。它的理念是与其设计复杂模型，不如用简单模型配合巨量数据ar5iv.labs.arxiv.org。虽然作者未公开数据细节，但可以推测其使用近十亿级别的图文对进行训练（极可能包括微软内部的ALIGN-类数据或JFT系列）。通过大量数据，GIT在各任务上都达到新的高度arxiv.org。对于普通研究者而言，如此数据难以获得。但GIT证明，大模型+大数据可以在无需额外标注和复杂技巧的情况下解决很多问题。因此，GIT没有使用伪标签，它体现的是另一种思路： 以规模取胜 。这在一定程度上回避了数据稀缺，因为一旦数据够多，很多小数据集的问题都变得可以零样本解决ar5iv.labs.arxiv.org。此外，统一模型能跨任务共享知识：例如在描述任务学到的知识对VQA有帮助，这其实提高了每条数据的利用率。这种多任务迁移也缓和了单任务数据不足的情况。因此，虽然GIT本身消耗了巨大数据，但相对于分别训练多个任务专用模型，其综合效率反而更高。 计算开销高： 训练一个像GIT这样的模型（尤其是大尺寸版本）需要相当高的计算投入。微软通过大规模并行和分布式训练完成了这一过程。幸运的是，GIT架构简单统一，没有多分支，这使并行效率较高。模型参数虽多，但Transformer易于在GPU/TPU上加速。而且作者在论文中提供了不同模型规模的对比如Base、Large、Huge等ar5iv.labs.arxiv.org。在实际应用中，可根据算力选择较小的模型进行fine-tune。推理方面，由于没有双塔或额外模块，GIT生成一次回复需要完整地跑Transformer，对于长序列仍较耗时。但没有交叉模块切换开销。值得注意的是，GIT证明了 统一模型减少了重复计算 ：比起每个任务训练不同模型，一个预训练模型fine-tune各任务总计算量更小ar5iv.labs.arxiv.org。同时，它也展示了Transformer在CV任务中的威力，使GPU上的Transformer算力得以充分利用，不像以前CNN+RNN需要异构处理。所以总体看，GIT的 训练成本虽然高，但回报是一个通用模型 。随着算力的提升，这种“大一统预训练”将变得越来越现实。 参考： GIT论文发表于 2023 年CVPRarxiv.org（OpenReview提供了审稿意见）。论文附带的代码已在GitHub开源arxiv.org（microsoft/GenerativeImage2Text），方便社区使用。有关GIT的更深入讲解，可参考微软研究博客和OpenAI笔记等资源对比GIT与同类模型的设计理念。\nFlamingo (DeepMind, 2022) 整体架构设计： Flamingo是DeepMind提出的一种 少样本视觉语言模型 ，它将预训练的视觉编码器和预训练的大型语言模型结合，通过插入跨模态注意力层实现图文融合lilianweng.github.iolilianweng.github.io。具体来说，Flamingo采用了CLIP的ViT作为图像编码器（提取每张图像的一组视觉特征），采用类似GPT-3风格的大型Transformer作为文本生成模型（如Chinchilla 70B）lilianweng.github.io。在两者之间，Flamingo引入一个 Perceiver Resampler模块 ，将任意长度的视觉特征压缩成固定数量的 视觉tokens （如每张图像压缩成N≈64个token）lilianweng.github.io。然后，在语言模型的每层若干位置，插入“门控跨注意力层”，让文本流在生成过程中可以多次访问这些视觉tokenslilianweng.github.io。这些跨注意力层在语言模型层之间交织，使模型在生成每个词时，都能参考图像信息。值得强调的是，Flamingo在训练时 冻结了原有的语言模型和视觉编码器权重 ，只训练中间的新组分（包括Perceiver和跨模态层）lilianweng.github.io。这种设计确保了预训练模型的语言和视觉知识被最大程度保留，同时通过新组件实现模态融合。 模态对齐方式： Flamingo的模态对齐依赖于 预训练模型的知识+少量新的连接参数 。图像编码器CLIP本身已提供高质量的视觉表示，语言模型也有丰富的语言常识。Flamingo只训练连接部分，通过自回归语言模型目标来让视觉信息对接语言输出lilianweng.github.io。训练过程中，模型读取一串交织的图像和文本（例如一个网页内容，其中有文字和插入的图片），试图按照出现顺序预测下一个文本tokenlilianweng.github.io。这隐含地要求模型学会对齐：当遇到需要描述图像的地方，就必须利用视觉tokens提供的信息来正确地产生文字。因此，Flamingo没有明确的对比或匹配损失，而是在 序列建模过程中完成对齐 。尤其得益于CLIP提供的视觉特征空间和语言模型的语义空间都非常成熟，跨注意力层只需学会将二者关联即可。例如，Flamingo使用一个门控机制控制每个跨注意力头对视觉的依赖程度，这保证了模型不会过度依赖或忽略视觉信息，而是渐进式地融合lilianweng.github.io。经过训练，Flamingo实现了图文对齐，以至于在推理时，可以在看到图像后正确地继续对话生成相关文本。这种对齐能力在它的few-shot学习中表现突出：只需给出少量图文示例，模型就能对新图像输出合理描述或回答，表明模态对齐已经内化在模型中了lilianweng.github.io。 输入 token 表达统一： Flamingo通过 对文本序列进行特殊标记和掩码 ，实现了对图像和文本交替输入的统一处理lilianweng.github.io。他们在训练语料的文本中插入特殊标记 代表图像占位符，当遇到该标记时，模型会取下一张图像的视觉tokens作为输入lilianweng.github.io。在Transformer内部，通过设计注意力mask，使得文本token只能看见最近一次出现的图像tokens以前的文本，而不能看见更早图像，以此处理多图场景lilianweng.github.io。同时，由于视觉tokens长度固定，每当有图像时，就把那N个视觉tokens嵌入序列。这样，整个输入序列可能形如：“文本段1 文本段2 文本段3…”。对于Transformer来说，标记只是一种指示，它实际会被替换为N个视觉embedding。最终，模型看到的是一个混合序列，其中既有文本token embedding也有视觉token embedding。Flamingo的跨注意力层保证文本可以从视觉embedding汲取信息lilianweng.github.io。总之，Flamingo实现了 在同一序列中交织图像和文本 ：在位置编码上，文本和图像embedding各据其位，模型通过mask确保因果关系正确lilianweng.github.io。这种方式处理输入使得模型能够自然地接受任意交替的多模态输入，而不需要显式地将图像转成离散标签或one-hot表示。 损失函数与训练策略： Flamingo以自回归下一个词预测作为唯一的训练目标lilianweng.github.io。训练数据是精心构造的 多模态序列 ：DeepMind构建了一个名为“M3W”（MassiveWeb）的大型数据集，从网络抓取包含图像和文字的网页片段共4300万条lilianweng.github.io。这些数据被处理成长度为256的token序列（其中可能包含最多5张图像）lilianweng.github.io。此外，Flamingo还混合了传统的 图文对数据 （如ALIGN的1.8亿图文对）和 视频-文本数据 （如从视频中抽帧及对应描述）进行训练lilianweng.github.iolilianweng.github.io。整个训练在不同数据源上采用 分布式多任务训练 ：每个batch随机抽取来自网页、多图文对、视频的样本分别计算NLL损失，再按设定权重求和优化lilianweng.github.io。这样的策略使模型同时适应多种输入形式。训练中需要注意各数据集的权重分配，作者采用均衡采样避免小数据集被忽略，同时也调整过不同任务损失的比重lilianweng.github.io。最后，通过大量算力（语言模型80B参数，加上新插入层）训练，Flamingo可以在不微调的情况下实现few-shot学习，即给定少数示例即可在16个下游任务中取得接近或超过有监督SOTA的成绩lilianweng.github.io。模型也支持进一步微调，但由于参数量巨大且新的门控层较敏感，微调需要小心调参。不过，一旦训练完成，Flamingo在多模态few-shot方面展示了卓越的能力。 使用的数据集及伪标签： 如上所述，Flamingo的主要预训练数据包括三个部分：网页多模态语料M3Wlilianweng.github.io、 图片-文本对数据 （如ALIGN 1.8B对、LAION等）lilianweng.github.io、 视频-文本数据 （如Instagram/Twitter短视频说明等，文中代号LTIP和VTP）lilianweng.github.io。M3W的构建无需人工标注，纯粹爬取网页，这可以视为引入了 大量弱监督数据 。那些网页上的文本并非专门描述图像，但模型会通过上下文学习其中关联。这有点类似伪标签，因为并非每句话都准确描述对应图像，但模型会自己找关系。另外，Flamingo并未使用生成模型来自行生产描述，它依赖真实世界的数据多样性。值得注意的是，Flamingo训练所需的监督非常少，几乎全是弱标注或无标注数据。few-shot能力使它在下游不需要大规模微调数据。因此，Flamingo充分体现了用海量弱标注数据替代高质量标注的理念。没有迹象表明Flamingo使用了由其他模型生成的伪标签数据；它更像是把互联网当作最大的标注来源，在文本和图像并存的自然场景中学习。 六大难点应对： 模态对齐困难： Flamingo借助冻结的CLIP提供良好的图文先验表示，并通过少量参数训练将其输出嵌入语言模型上下文。这意味着视觉和语言模态的大体对齐已经由CLIP和预训练LM保证，Flamingo只需学习 在具体上下文中关联 。通过跨注意力层，Flamingo学会在需要时提取视觉token信息用于生成下一个词，从而实现对齐。其few-shot性能表明，训练后模型能够快速对齐新任务的图文语义，这得益于大量多样化训练让对齐泛化良好lilianweng.github.io。换言之，Flamingo用数据多样性+强大基础模型平稳地度过了模态对齐难关。 token格式不统一： Flamingo直接在Transformer中处理交替的多模态序列，将图像表示为固定长度token插入序列，这相当于统一了输入格式lilianweng.github.io。虽然图像token不是离散符号，但它们像文本token一样有自己的位置，与前后文本共同组成序列输入Transformer。同时，引入 标记作为占位符，使文本流认识到何处有图像lilianweng.github.io。这种方案无需对图像进行离散化编码，而是用连续向量表示并通过mask和标记融入序列，实现了格式统一。实验证明，这样模型可以灵活处理任意交替顺序的图文输入，这正是统一输入格式带来的好处。 语义粒度不匹配： Flamingo利用CLIP的高层视觉特征（ViT-L/14等）作为输入，这些特征本身具有较丰富的语义信息（CLIP已对齐过标签文本）。再通过Perceiver压缩，Flamingo获得一组紧凑的视觉tokens，每个可能聚合了图像若干部分信息lilianweng.github.io。这会损失一些低层细节，但保留主要语义，匹配语言模型处理的概念粒度。对于非常细的细节，如图像中的文字或小目标，Flamingo如果训练数据涵盖这类任务也能捕捉（但Flamingo主要没专门练OCR类任务，表现可能一般）。总体而言，Flamingo的设计旨在 抓主要语义 ：用几百个视觉token代表整张图lilianweng.github.io。语言模型生成注重全局语义和上下文，微观细节在few-shot场景下可能需要提示引导才能关注。不过，通过web数据训练，Flamingo也学习了不少细节（如定位照片里的物体等）。因此，它在语义粒度上采取以语义为主，细节为辅的策略，符合few-shot应用的需求。 多模态上下文保持： 这是Flamingo最大的强项之一。模型专门设计来处理 任意长度的交互式多模态上下文 。通过mask策略，Flamingo可以应对多张图和多段文本交替：保证每段文本只能看最近的图像，从而按顺序关联图像和文字lilianweng.github.io。这使模型在一个序列中可以包含多轮图文对话——实际上Flamingo天生就是支持图文混合对话的。训练中它看过网页内容的多次图文交替，因此对多模态上下文延续性有经验lilianweng.github.io。few-shot推理时，可以先给几个示例（图+问+答），模型就能在持续的多模态对话下发挥作用lilianweng.github.io。这种能力是一般模型不具备的。因此Flamingo很适合多轮对话、讲故事等需要保持上下文的场景。需要注意长序列涉及的内存和计算成本，但Flamingo通过稀疏注意力等优化应对。总的来说，Flamingo在多模态上下文保持方面达到了当时的新高度，真正实现了在Transformer中融合长上下文的多模态信息。 训练数据稀缺： Flamingo通过大规模弱标注数据和 多数据源混合 ，在没有显式人工标注的情况下取得了卓越性能lilianweng.github.io。它所需的只是网络上已有的大量图文并茂内容，而不需要额外的人工作答或描述数据（除了验证集）。这证明了利用海量的非结构化数据也能训练出强大的多模态模型。few-shot学习的优势在于，模型可以适应新任务而不需要对每个任务都有成千上万标注数据。Flamingo在16个任务上的结果显示，即使这些任务的数据对模型来说是新的，它依然靠few-shot提示达到不错效果lilianweng.github.io。这极大缓解了对监督数据的需求。因此，Flamingo的方案是 用预训练+提示学习替代下游数据 。当然，预训练本身用了43M网页和十亿级对，耗资巨大，但都是低成本获取的数据。可以说，它把收集标注的钱换成了算力钱。一旦模型训练完毕，同样权重可以few-shot解决多个任务，再也不需要逐个任务大量标注了。 计算开销高： Flamingo包含一个80B规模的语言模型（如Chinchilla 70B）和一系列新插入的层，总参数量非常高，训练消耗巨大的TPU/GPU资源。这显然是非常高的计算开销。然而，Flamingo通过 冻结大模型 ，大幅减少了需要更新的参数量lilianweng.github.io。仅训练新加的几千万参数，使得训练收敛更快、更稳定，同时避免灾难性遗忘。此外，相比从零训练80B多模态模型，这种“夹心”微调的成本要低得多。推理阶段，Flamingo的计算与一个同等大小的LM相当，外加一些跨注意力计算，可以在多卡并行生成。在few-shot时，不需要反复fine-tune，从而节省了针对每个任务微调的算力。因此，对于拥有训练超大模型能力的团队来说，Flamingo的 性价比反而不错 ：用额外\u003c1B参数的代价，把一个纯语言模型变成了多模态模型。总之，Flamingo依然属于算力投入极高的模型，但在架构上做了取舍，通过参数冻结和高并行设计，把这笔开销控制在可能范围，并用其泛化能力回收了在多个任务上的成本。 参考： Flamingo的论文（Alayrac et al. 2022）可在arXiv获取lilianweng.github.iolilianweng.github.io。其中详述了模型架构和训练数据构成。DeepMind未公开Flamingo的代码，但有社区复现项目（如lucidrains的PyTorch实现）。Lilian Weng的博客对Flamingo进行了通俗讲解lilianweng.github.io。Flamingo在Few-shot VQA等任务上的表现促使后续多模态聊天模型（如OpenAI的GPT-4V）采用类似思想。\nGPT-4V (OpenAI, 2023) 整体架构设计： GPT-4V是GPT-4模型的视觉增强版本，能够接受图像和文本输入，输出文本en.wikipedia.org。虽然OpenAI并未公开GPT-4V的具体架构和参数en.wikipedia.org，“V”版的实现大致可推测为在GPT-4大型Transformer架构中融合了视觉处理模块。很可能GPT-4V采用了单一Transformer模型来同时处理图像和文本：图像通过一个卷积或ViT编码器提取特征，然后以某种形式馈入Transformer。例如，有推测称GPT-4V使用类似Flamingo的方法——一个 预训练的ViT作为图像编码器 ，将其输出作为额外的输入embedding，通过新添的跨注意力机制注入到原GPT-4的Transformer中lilianweng.github.io。也有可能GPT-4V将图像编码为若干“视觉token”直接拼接到文本token序列中处理（类似BLIP-2/GIT那样）。不管实现细节如何，GPT-4V的架构原则应是 在不大幅改变GPT-4语言能力的前提下，赋予其视觉输入通路 。因此，它很可能保留了GPT-4的大部分层和参数，仅在输入嵌入层或中间插入层增加视觉接口，使模型能够在Self-Attention中同时考虑图像和文本信息。作为一个多模态LLM，GPT-4V仍以Transformer为核心en.wikipedia.org。 模态对齐方式： GPT-4V在开发过程中应该经历了 大量多模态预训练和对齐调优 。预训练阶段，模型接受图文混合数据，学习以生成下一个token为目标（无论下一个是文字还是需要根据图像产生的文字）。这种训练会驱动模型自动建立图像与文本语义的映射关系。由于GPT-4本身非常强大，GPT-4V可能仅需较少的额外数据就能学会模态对齐。然而OpenAI可能使用了多种辅助手段：包括 对比损失 （确保图像相关的文本embedding靠近）或者多模态一致性约束等，但具体未知。可以肯定的是，GPT-4V经过了 强化学习人类反馈（RLHF）的对齐环节en.wikipedia.org：人工反馈不仅针对文本回答质量，也包括对视觉理解正确性的评价。这种人工调教确保模型在视觉问答中对齐人类期望。例如，人类监督会奖赏模型正确描述图像、严惩胡编乱造，从而促使模型更好地学习视觉-语言对齐关系。总的说来，GPT-4V的模态对齐来自两部分 ：一是模型大规模多模态训练的自我监督对齐（让模型预测正确的多模态输出而被迫对齐），二是 对抗性和人类反馈微调 （纠正不准确的对齐，如图像内容误解）以达成人类满意的对齐度。最终结果是GPT-4V在各种视觉描述、问答任务上表现出强大的理解力和对齐度，甚至可以准确解释复杂图片、阅读图中文字并将之融入答案——这说明其视觉语义已与语言很好地结合。 输入 token 表达统一： 从用户接口看，GPT-4V接受的输入是图像（像素形式）和文本，自然语言以token形式进入，图像则以文件上传形式进入API。但在模型内部，必须将图像转化为与文本token可交互的表示。根据业界经验，GPT-4V可能采用两种方式之一：其一， 离线视觉编码+前缀嵌入 。即通过一个CNN/ViT将图像转成一串embedding，然后在Transformer输入端用特殊标记占位，将这些embedding作为“视觉前缀”插入。这类似BLIP-2和GIT的策略，让视觉embedding在Transformer序列中，与后续文本共同处理。其二， 中途插入跨模态层 ，即模型运行过程中，当需要处理图像时，调用一个微型视觉Transformer将图像转成键值供专门的跨注意力层使用（类似Flamingo做法）。无论哪种，最终效果是 模型看到了一系列向量表示，部分来自图像，部分来自文本 ，并通过统一的注意力机制处理它们。因此，GPT-4V实现了 视觉信息向等价文本向量的转换 ：这些向量可能没有离散token对应，但Transformer无差别对待它们，把它们当作上下文的一部分。OpenAI也定义了GPT-4V的token计费方式：图像按一定像素大小折算成若干token成本platform.openai.com, 这暗示他们内部将图像信息映射为了固定数量的embedding，相当于一些token。这与输入统一表示的思路一致。此外，GPT-4V支持在对话中多次输入图像，模型通过聊天记忆可以连续参照多幅图像。这种灵活性也表明输入的图像已经嵌入Transformer上下文，模型可以在内部“记住”它，就像记住前文一样。因此可以说，GPT-4V在实现上做到了图像和文本输入的格式统一，至少从Transformer的视角来看是一致的序列信息流。 损失函数与训练策略： GPT-4V的训练包括两个阶段： 预训练（Self-Supervised）和对齐微调（Supervised + RLHF） en.wikipedia.org。预训练损失是标准的 因果语言建模损失 ，扩展到多模态场景，即给定之前的文本token和图像embedding，预测下一个文本tokenen.wikipedia.org。这一步可能使用了大量图文对数据和合成任务数据，让模型具备基础视觉理解和描述能力。接下来，OpenAI对GPT-4V进行了 监督微调 ，包括让模型跟随指令、可靠回答问题、避免不当输出等。这一步使用有人类标注答案的图像问答数据和对话数据，损失为交叉熵对标参考答案。最后还有 RLHF阶段 ，通过人类反馈训练一个奖励模型，对模型回答质量评分，再用策略梯度或近端策略优化调整模型参数，使之产生更符合人类期望的回答en.wikipedia.org。在RLHF中，人类会比较两版对同一图像问题的回答优劣，以训练奖励模型。这确保GPT-4V不仅正确，还要解释清楚、详尽并遵守安全守则。训练策略方面，GPT-4V很可能采用了 混合训练 ：例如让模型在大约80%时间学习纯文本任务（以不损害其语言能力），20%时间学习带图像的任务，以逐渐融合视觉能力而不遗忘语言能力。这符合OpenAI对GPT-4统一多模态模型的描述，称其在巨量算力下进行预测性能的平稳扩展arxiv.org。因此，GPT-4V训练过程相当复杂，但核心损失仍是让模型预测正确的输出序列（文本），只是过程中施加了各种人类知识和偏好约束。 使用的数据集及伪标签： OpenAI未公开GPT-4V使用的数据细节en.wikipedia.org。推测其预训练数据包含 互联网爬取的大规模图文对 （如可能使用LAION、ALIGN数据，或者自建的10亿级别数据集），涵盖多样领域。还可能有 OCR场景数据 （扫描文档及文本）、 图表数据 、网页截图和说明等，因为GPT-4V表现出识别文档、读表格、看图编程等广泛能力。监督微调阶段，他们可能编纂了一个多模态指令数据集，类似InstructGPT，但带图像：比如让标注员提供图像并提问，写出高质量参考答案。这部分数据可能较小（数万到数十万对），但涵盖不同任务（描述、定位、分类、推理等）。此外，社区猜测OpenAI可能利用GPT-4自身生成了一部分训练数据（即“判师”策略），但官方未证实。相较于开源做法（如LLaVA用GPT-4生成对COCO的问答作为训练集），OpenAI有资源直接人工标注，所以GPT-4V的关键数据更可能是人类精标而非伪标签。唯一确定的是，GPT-4V 融合了多源数据 ：文本数据（与GPT-4共享）、图像+文本数据，以及人类反馈数据en.wikipedia.org。这种多阶段、多样本训练使模型具有极其广泛的视觉语言知识。基于效果推断，GPT-4V肯定见过各种真实世界图像场景，包括照片、插画、截图、漫画等，也了解了不少常见视觉任务的问答格式。这正是其在未知图片上一样游刃有余的原因。 六大难点应对： 模态对齐困难： GPT-4V可被视为目前模态对齐最成功的例子之一。OpenAI通过 统一模型训练+精细对齐调优 ，使得GPT-4V在视觉和语言之间建立了深度联系。模型能将图像中的元素转换成文字描述或用于推理，说明跨模态概念高度统一。例如，它可以看图进行幽默理解、数学分析，这意味着不仅低层语义对齐，高层推理也对齐了。相比CLIP等需下游配对的模型，GPT-4V内部产生了 端到端的对齐 ：一幅图像输入，其内部生成的表征能直接触发与之对应的知识和词汇。RLHF过程中，人类引导模型关注正确区域、忠实描述，进一步强化了 精准对齐 （比如不编造不存在的物体）。因此，对以前悬而未决的模态对齐难题，GPT-4V以超级规模训练+人类校正的方法给出了答案：几乎可以对任意复杂图文实现正确对齐。 token格式不统一： GPT-4V在接口上依然区分图像上传和文本输入，但在模型内部已经实现了格式统一。如上推测，图像被编码成embedding插入Transformer，相当于模型看到的是统一的向量序列，其中没有本质区别区分来源。OpenAI甚至提供了一个token折算方法来计价图像，这暗示他们定义了一个统一本质的token空间包含图像platform.openai.com。GPT-4V也许没有明确的视觉词表，但通过扩展embedding层，模型接受了一批额外的向量（视觉patch的embedding或者Resampler输出）作为“视觉token”。Transformer处理自注意力时，对这些向量和普通文字embedding执行相同的矩阵计算。因此可以说，GPT-4V在实现上 消除了模态输入格式差异 ，达到了真正的多模态Transformer形态。这也是为什么用户能对它自由提问“图中有什么字”或“这个人是谁”，模型像读文字一样“读”图。这种统一在OpenAI的技术报告中虽未明说，但从其行为特征和架构趋势能推断出来。 语义粒度不匹配： GPT-4V展现出处理各种粒度语义的能力，从辨认具体细节（如图中小字、微小物品）到理解抽象场景（如人物关系、场景氛围）。这表明模型采用了高分辨率的视觉表征和 强大的分层理解 。一种可能方式是多级特征：基础ViT提供细粒度patch特征，然后Transformer多层逐步汇总，像人类视觉系统一样先看细节再理解整体。此外，OpenAI可能特意在训练集中加入了一些需要细粒度识别的任务（OCR、细分类），迫使模型关注局部细节。同时，大语言模型部分拥有强大的上下文推理能力，能从细节推导整体意义。这两方面结合，使GPT-4V能较好地弥合视觉像素级信息与语言概念级信息之间的鸿沟。例如，对一张复杂的漫画，模型既能识别面部表情这样的细节，又能归纳出搞笑之处这样的高层语义。可以认为，GPT-4V通过多尺度注意力解决了语义粒度不匹配：低层注意力抓取细节，高层Self-Attention整合语义，并在输出时选择恰当的语言粒度表述。 多模态上下文保持： GPT-4V本质是ChatGPT的扩展版，因而天然具备对话上下文记忆能力。用户可以在一次对话中连续上传多张相关图像并配以提问，模型能够参考对话历史和所有已提供的图像信息来回答。比如，用户先上传一家谱照片问“这是谁？”，再上传另一张照片问“他和前面那人是什么关系？”，GPT-4V可以基于前文记忆，将两图人物联系起来回答。这说明模型内部对多轮图像和文本都建立了表示，并通过对话状态维持了跨轮次的多模态上下文。OpenAI很可能在微调阶段加入了这类多轮、多图对话的数据，使模型学会使用 引用之前的图像。在推理实现上，ChatGPT系统会给每张图一个编号，将其embedding保存在对话状态，后续提问如果引用，模型就会重新利用。这种机制虽未明示，但从体验上看GPT-4V确实支持相当长的多模态对话。因此，它在多模态上下文保持上达到了目前最强水平：既能处理长文本对话，又能记忆多张图像的内容并综合推理。这一能力是之前模型（如Flamingo）few-shot模拟的更高级形式，因为GPT-4V经过明确的对话格式训练和强化，对话管理更加可靠。 训练数据稀缺： 对于普通研究者来说，高质量大规模多模态数据稀缺是难点，但OpenAI通过自身积累和合作，可能获取了十分丰富的数据。GPT-4V可以被视为以数据和算力硬碰硬解决问题的典型。它用规模（模型、数据）换性能，不太依赖小技巧。值得注意的是，虽然OpenAI未公布数据，但推测很大一部分来自现有开放数据（LAION、COCO、Visual Genome等）以及定制采集的数据（比如购买版权图片、内部生成的数据等）。此外，人类标注在对齐阶段起了决定性作用，这是另一种形式的数据： 专家知识数据 。OpenAI投入了大量人力去微调模型的行为，使得最终模型的能力远超仅靠原始数据训练的版本。这相当于通过人类反馈来弥补数据集不足之处——对于一些模型自己难以领会的任务，人类示范和偏好指导提供了额外信息。这种做法开创了用少量高质量人工数据引导海量机器学习的范式。简而言之，GPT-4V应对数据稀缺的方案在于： 一手抓“大”（扩展预训练数据广度），一手抓“精”（收集人类高质量指令/反馈数据） 。两者结合，使模型既见多识广，又合乎人意。 计算开销高： GPT-4V毫无疑问是在极其庞大的算力支持下训练的。传闻GPT-4基础模型参数在数千亿以上，训练消耗数千万美元级别GPU成本。加入视觉模态后，训练复杂度进一步提高。不过OpenAI通过一些工程手段控制了成本：据报道，他们使用了训练性能预测方法，在较小模型上估计大模型表现，从而少走弯路arxiv.org。另外采用混合精度、模型并行、流水线并行等技术提高效率。模型结构上，使用统一Transformer而非多分支，可以充分利用成熟的Transformer优化器和加速器。这些都帮助缓解了计算压力。在推理阶段，GPT-4V同样需要强大算力支持，但OpenAI通过托管API方式，用优化过的推理服务器提供服务，单次调用成本对于终端用户来说隐藏在付费中。可以说，GPT-4V目前的计算开销不是一般机构能承担的，但它也展示了高投入带来高性能的路线。随着硬件进步和可能的压缩蒸馏技术，未来GPT-4V的成本有望下降。就当前而言，OpenAI通过自身资源攻克了这一难题，对外提供一个无需本地计算就能调用的强大多模态模型，这在客观上绕过了许多用户对算力的需求。 参考： GPT-4 的技术报告en.wikipedia.orgen.wikipedia.org提到其多模态能力和训练方法，但未披露细节。维基百科也指出OpenAI未公布GPT-4的架构和数据en.wikipedia.org。尽管如此，我们可以参考类似的研究（如Google PaLM-E、DeepMind Flamingolilianweng.github.io）来推测GPT-4V的设计思路。OpenAI的GPT-4发布博客openai.com和官方FAQ也提供了一些线索（如图像计费折算）。目前没有公开的GPT-4V代码或模型，但已有一些开源项目（如MiniGPT-4、LLaVA等）尝试复现其部分功能，可供了解实现原理。总的来说，GPT-4V代表了当前多模态模型技术的前沿，将视觉和语言能力融合达到了前所未有的高度。\n",
  "wordCount" : "537",
  "inLanguage": "en",
  "datePublished": "2025-06-01T12:00:14+08:00",
  "dateModified": "2025-06-01T12:00:14+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-06-01/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-06-01
    </h1>
    <div class="post-meta"><span title='2025-06-01 12:00:14 +0800 +0800'>June 1, 2025</span>&nbsp;·&nbsp;3 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h1 id="主流视觉-文本多模态模型技术分析">主流视觉-文本多模态模型技术分析<a hidden class="anchor" aria-hidden="true" href="#主流视觉-文本多模态模型技术分析">#</a></h1>
<p>近年来，视觉和文本结合的多模态模型取得了显著进展。在此，我们选取当前主流的几种模型（包括但不限于 CLIP、ALIGN、BLIP、BLIP-2、GIT、Flamingo、GPT-4V），逐一分析它们的架构设计、模态对齐方式、输入 token 统一表示方法、损失函数与训练策略、数据集与伪标签使用情况，并讨论它们如何在架构或训练上应对多模态学习的六大难点。</p>
<h2 id="clip-openai-2021">CLIP (OpenAI, 2021)<a hidden class="anchor" aria-hidden="true" href="#clip-openai-2021">#</a></h2>
<ul>
<li><strong>整体架构设计：</strong> CLIP 采用 <strong>双编码器架构</strong> ：包括一个图像编码器和一个文本编码器，两者分别将图像和文本映射到<strong>相同维度</strong>的向量空间<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder">lightly.ai</a>。图像编码器可以使用 ResNet 或 Vision Transformer 等架构，文本编码器则是基于 Transformer 的语言模型<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder">lightly.ai</a>。在输出端，两个编码器各自接一个线性投影，将图像和文本特征投影到<strong>共享的多模态嵌入空间</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder">lightly.ai</a>。这种架构并不在中途融合图像和文本特征，而是各自编码后在嵌入空间对齐。</li>
<li><strong>模态对齐方式：</strong> CLIP通过<strong>对比学习</strong>实现视觉-语言对齐。训练时，模型给定一批图文对，学习<strong>预测哪张图像与哪段文本匹配</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations">lightly.ai</a>。具体而言，CLIP使用 <strong>对称的跨模态对比损失</strong> （即分别以图像检索文本和文本检索图像计算两个方向的softmax交叉熵损失），最大化真实匹配的图文嵌入相似度，最小化非匹配对的相似度<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations">lightly.ai</a>。这种训练使图像和文本编码器产生的特征在共享空间中<strong>成对靠近</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations">lightly.ai</a>。</li>
<li><strong>输入 token 表达统一：</strong> CLIP并未显式统一图像和文本的输入表示格式。 <strong>图像和文本各有独立的token化和编码流程</strong> ：文本用BPE分词作为离散token输入Transformer，图像则以像素或patch为输入到CNN/ViT得到连续特征<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder">lightly.ai</a>。两种模态的数据直到嵌入空间才进行统一对齐。因此CLIP通过<strong>独立编码+对齐空间</strong>的方式，规避了直接将图像作为序列token处理的不统一问题。</li>
<li><strong>损失函数与训练策略：</strong> 损失采用 <strong>对比学习的InfoNCE损失</strong> （实现为带温度系数的归一化softmax交叉熵）<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and">research.google</a>。在一个batch中，真实的图文对作为正样本，不匹配的组合作为负样本，通过softmax拉开正负样本的评分差距。为提高训练效率，CLIP使用了<strong>大批量</strong>训练（成千上万的对/批）以提供足够的负样本，同时在实现上对大batch的softmax作了数值稳定和分布式计算的优化<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,2023%2C%20in%20their">lightly.ai</a>。图像预处理上，只用了基本的数据增广（如随机裁剪）以保持图片内容与文本描述对应<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=insensitive%20to%20the%20capacity%20of,the%20text%20encoder">lightly.ai</a>。CLIP从<strong>随机初始化</strong>开始端到端训练图像和文本编码器，没有使用预训练的视觉或语言模型<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder">lightly.ai</a>。</li>
<li><strong>数据集及伪标签：</strong> CLIP在一个超大规模的图文配对数据集上预训练，包含约<strong>4亿对图像-文本</strong><a href="https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model">arxiv.org</a>（主要来自互联网的图片及其旁白文本）。这些文本多为图像的标题或描述，具有较大噪声，但规模极其庞大。CLIP没有使用合成的文本标签或额外的人工标注数据，完全依赖<strong>自然语言的弱监督</strong><a href="https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model">arxiv.org</a>。如此大量的图文对无需人工清洗，使模型学习到广泛的视觉概念。训练中未使用伪标签技术，也未针对数据稀缺问题额外生成合成数据，因为其数据规模本身就非常巨大。</li>
<li><strong>六大难点应对：</strong>
<ol>
<li><em>模态对齐困难：</em>  CLIP通过对比损失直接学习图像和文本的全局语义对齐，将匹配的图文特征拉近，不匹配的推远<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations">lightly.ai</a>。超大规模多样化数据和对比学习目标极大缓解了模态对齐难题，使模型学习到稳健的跨模态表示空间。</li>
<li><em>token格式不统一：</em>  采用双编码器架构有效避免了将图像离散化为“词”的问题。图像由专门的视觉编码器处理，文本由文本编码器处理，二者输出相同格式的向量后再比对<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder">lightly.ai</a>。这种后期对齐的方法在不统一输入token格式的前提下，实现了多模态表示的统一。</li>
<li><em>语义粒度不匹配：</em> CLIP对图像和文本都是提取整体级别的表示（整幅图像对应整段文本）。它没有显式对齐图像局部区域与文本片段，因此可能无法细粒度对齐具体对象。但庞大的训练语料涵盖各种粒度描述，在全局对比目标下模型隐式学到了一定程度的细粒度关联。后续研究如<strong>Fine-Grained CLIP</strong>等正是受限于CLIP在局部语义对齐上的不足<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20architecture%20consists%20of%20separate,capacity%20of%20the%20text%20encoder">lightly.ai</a><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=SigLIP%3A%20Optimising%20the%20loss%20function,for%20better%20scaling">lightly.ai</a>。总体而言，CLIP主要对齐全局语义，对于细粒度语义不匹配问题未作专门架构设计。</li>
<li><em>多模态上下文保持：</em> CLIP一次只处理单一的图-文对，不涉及序列化的多轮交互或多张图像输入。因此 <strong>多模态上下文</strong> （如对话历史或多图情景）并未在架构中体现。CLIP更侧重于独立图文对的匹配，对跨时序或多轮情境无法建模。这一限制在CLIP应用于对话或多图任务时需要借助外部机制解决。</li>
<li><em>训练数据稀缺：</em> CLIP通过<strong>大规模弱标注数据</strong>从根本上缓解了数据稀缺的问题<a href="https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model">arxiv.org</a>。无需逐张图人工标注类别，而是利用网络中丰富的图像文本对，实现“以量补质”。即使这些网络文本存在噪声，作者证明只要规模足够大，模型仍能学到有效特征<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>。因此CLIP没有使用数据增强或伪标注技术，而是倾向于直接收集海量数据。</li>
<li><em>计算开销高：</em> 训练CLIP确实需要巨大的算力和显存，但相对来说，其<strong>双塔架构</strong>使训练可并行展开，推理时也可分别预编码图文后做相似度计算，<strong>具有一定的效率优势</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=between%20positive%20image,between%20visual%20and%20textual%20representations">lightly.ai</a>。与需要跨模态交叉注意力的单体模型相比，CLIP的对比学习目标在实现分布式并行时稍有挑战，需要对大batch软max做特殊优化<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20loss%20function%20employed%20during,Training%E2%80%9D%2C%20propose%20to">lightly.ai</a>。后续研究提出用sigmoid替代softmax（如 SigLIP）来简化分布式实现<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=implementation%20is%20numerically%20unstable%2C%20and,additional%20bias%20terms%2C%20and%20calculations">lightly.ai</a>。总的来说，OpenAI通过合理的工程和算法优化，使CLIP在可能的范围内降低了计算开销，但其预训练仍是大规模的（数百万 GPU 时）。值得一提的是，CLIP模型体积适中（例如ViT-B/32版约有数亿参数），推理可在单GPU上高效完成，实现了训练成本和推理效率的折中。</li>
</ol>
</li>
</ul>
<p><strong>参考：</strong> CLIP 的论文<a href="https://arxiv.org/abs/2103.00020#:~:text=supervision,grained%20object%20classification.%20The%20model">arxiv.org</a>详细描述了其对比预训练方法，OpenAI 的博客也提供了概述<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=trained%20to%20identify%20the%20correct,between%20visual%20and%20textual%20representations">lightly.ai</a>。代码实现可参考 OpenAI 提供的开源版本或 HuggingFace 的CLIP模型库<a href="https://huggingface.co/docs/transformers/en/model_doc/clip#:~:text=CLIP%20uses%20an%20image%20encoder,the%20same%20number%20of">huggingface.co</a>。</p>
<h2 id="align-google-2021">ALIGN (Google, 2021)<a hidden class="anchor" aria-hidden="true" href="#align-google-2021">#</a></h2>
<ul>
<li><strong>整体架构设计：</strong> ALIGN（ <strong>A Large-scale ImaGe and Noisy-Text embedding</strong> ）延续了与CLIP相同的<strong>双编码器对比学习架构</strong><a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained">research.google</a>。它包含独立的图像编码器和文本编码器，两者输出到同一向量空间。与CLIP不同的是，Google使用了更大规模的主干：<strong>EfficientNet-L2卷积网络</strong>作为图像编码器，<strong>BERT-Large</strong>作为文本编码器，并均从随机初始化开始训练<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and">research.google</a>。模型最终投影到一个共享嵌入空间，用于图文检索和匹配任务。</li>
<li><strong>模态对齐方式：</strong> ALIGN采用<strong>对比损失（normalized softmax）<strong>来训练，使匹配的图文对嵌入向量接近，不匹配的远离<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and">research.google</a>。具体而言，对每个图像-文本对，计算嵌入的余弦相似度，并通过softmax使得正确匹配对在一批样本中得到最高的似然。损失在图到文和文到图两个方向对称计算，如同CLIP的做法。这种以</strong>批为单位的跨模态对比</strong>训练，使模型学到强大的图文对齐表示。</li>
<li><strong>输入 token 表达统一：</strong> ALIGN同样没有将图像直接离散为token序列，而是通过<strong>双通道</strong>处理：图像经过CNN提取特征向量，文本经过BERT编码为文本向量。两模态输出向量通过各自的投影层映射到同维空间。由于采用独立编码器，ALIGN不要求图像和文本的输入格式统一，而是在输出<strong>embedding空间</strong>实现统一表示<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and">research.google</a>。</li>
<li><strong>损失函数与训练策略：</strong> 使用 <strong>对比学习损失</strong> （InfoNCE变体），在<strong>大批量</strong>上训练模型<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained">research.google</a>。Google收集了极其庞大的图文数据，使每个训练step都含有大量负例，从而有效训练对比目标。与CLIP类似，ALIGN需要跨GPU同步计算softmax分母，这在工程上有所挑战。值得注意的是，作者强调尽管数据非常嘈杂，他们仅做 <strong>最小程度的过滤</strong> ，通过<strong>数据规模</strong>来弥补噪声<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>。训练策略上，ALIGN从头训练EfficientNet-L2和BERT-Large，这意味着需要相当长的训练时间，但好处是模型能够充分适配新的数据分布。</li>
<li><strong>数据集及伪标签：</strong> ALIGN的亮点在于使用了<strong>超过10亿对图像-Alt文本</strong>的超大规模数据集<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>。这些图像来自网络，文本是网页提供的替代文本（alt-text），数据 <strong>无需人工标注</strong> 。与早期精心清洗的小型数据集（如Conceptual Captions 300万对）不同，ALIGN <strong>放宽过滤标准</strong> ，只做了基于频率的简单过滤，最终得到约<strong>18亿对</strong>图文数据<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20this%20work%2C%20we%20follow,text%20pairs">research.google</a>。这些文本描述可能包含噪声甚至与图像无关，但研究表明 <strong>规模弥补质量</strong> ：如此海量的数据使模型学到泛化的视觉语言表示<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>。ALIGN未使用任何合成标签或伪标注技术——完全依赖真实的网络数据，其策略是用极大规模的弱标注数据来缓解数据稀缺和噪声问题<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>。</li>
<li><strong>六大难点应对：</strong>
<ol>
<li><em>模态对齐困难：</em> ALIGN证明了<strong>数据规模</strong>在对齐中的重要作用。通过<strong>十倍于CLIP的数据规模</strong>和强大的对比损失，模型学到了稳健的跨模态对齐能力<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a><a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,L2">research.google</a>。即使数据噪声较高，足够的样本多样性也促使模型捕获图像和文本的正确对应关系，在Flickr30K、COCO等检索任务上超越了之前更复杂的有交叉注意力的模型<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>。</li>
<li><em>token格式不统一：</em> 与CLIP类似，ALIGN通过<strong>双编码器架构</strong>回避了统一输入格式的问题。图像和文本分别编码，各自发挥最适合的网络结构（CNN对像素、Transformer对文本），最终只需统一embedding空间即可<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained">research.google</a>。因此，不需要对图像进行文本化表示，也不需要修改文本token空间来容纳图像信息。</li>
<li><em>语义粒度不匹配：</em> ALIGN的训练目标依旧作用在 <strong>全局图像-句子层面</strong> ，没有显式对齐局部区域与词语。它依靠CNN提取图像总体特征，并用整句文本描述来监督。对于图像细节（如小物体或局部属性）与文本词汇的对应，没有专门机制来处理。这方面的不足在ALIGN中仍然存在，不过由于EfficientNet-L2具有强大的表征能力，加上海量数据涵盖各种描述粒度，一定程度上模型可以借助上下文学到细粒度信息。但总体来说，ALIGN主要解决宏观对齐，对<strong>细粒度语义</strong>的不匹配没有特殊解决方案。</li>
<li><em>多模态上下文保持：</em> ALIGN同样一次仅处理单一图文对，没有上下文记忆能力。它不支持在模型内部串联多轮对话或多张图片。Google在ALIGN中更关注开放域的检索和零样本分类能力<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>，未涉及多模态对话等情境。因此，ALIGN在<strong>多轮交互</strong>或<strong>长上下文</strong>问题上没有新的设计。后续若需要保持多模态上下文，需要在ALIGN提取的embedding之外搭建额外机制（例如将ALIGN作为编码器，配合语言模型处理对话历史）。</li>
<li><em>训练数据稀缺：</em> ALIGN的策略是<strong>极端扩增数据规模</strong>以消除数据稀缺瓶颈<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>。无需昂贵的人力标注，直接爬取网络 alt-text 即可得到十亿级别的配对。作者证明即使数据噪声大，但数量上去了，同样可以训练出SOTA模型<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a>。因此，在数据方面，ALIGN并没有使用数据增强或伪标签，而是依赖互联网海量数据源。这一思路对于普通研究者来说难以复现，但从方法上证明了解决数据匮乏的一条可行路径：即利用 <strong>弱标注的大数据</strong> 。</li>
<li><em>计算开销高：</em> 为训练ALIGN，Google使用了更大模型和更多数据，训练开销比CLIP更为惊人。然而ALIGN架构简单（无交叉注意力交互），这使得训练可以高效并行扩展<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=representations%20of%20the%20image%20and,used%20for%20downstream%20visual%20and">research.google</a>。他们成功训练了一个EfficientNet-L2（参数接近10亿级）和BERT-Large的组合，可见投入了巨大的计算资源<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=softmax%29%20towardsdatascience,used%20for%20downstream%20visual%20and">research.google</a>。尽管计算开销高昂，ALIGN通过 <strong>冻结架构复杂性</strong> （只用双塔，不引入额外模块）来保障多机并行效率，也说明在同等资源下，<strong>优先扩大数据规模</strong>比增加模型复杂度更有效<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a><a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained">research.google</a>。因此，ALIGN在可用算力内追求最大的数据量和模型规模，以取得最佳性能。</li>
</ol>
</li>
</ul>
<p><strong>参考：</strong> ALIGN 的研究细节发表于 ICML 2021<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=In%20,We">research.google</a>。Google Research 官方博客提供了对ALIGN的通俗描述<a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=language%20models,attention%20models%2C%20and%20enable">research.google</a><a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/#:~:text=For%20the%20purpose%20of%20building,large%20%28text%20encoder%29%20trained">research.google</a>。由于该模型未开源，实现细节可参考开源的对比学习框架或OpenCLIP等类似项目。</p>
<h2 id="blip-salesforce-2022">BLIP (Salesforce, 2022)<a hidden class="anchor" aria-hidden="true" href="#blip-salesforce-2022">#</a></h2>
<ul>
<li><strong>整体架构设计：</strong> BLIP（ <strong>Bootstrapping Language-Image Pre-training</strong> ）提出了一种 <strong>多模态混合编码-解码架构（MED）</strong> ，旨在同时支持视觉-语言的理解和生成任务<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=,g">lightly.ai</a>。具体而言，BLIP的模型包含三个不同模式的子模型：①  <strong>单模态编码器</strong> ：对图像或文本单独编码，用于提取各自模态的表示，并采用图文对比损失（ITC）对齐两种模态的全局特征<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=,g">lightly.ai</a>；②  <strong>图像引导的文本编码器</strong> ：在文本Transformer中引入跨模态注意力，让文本编码能够利用图像特征，训练时使用 <strong>图文匹配（ITM）<strong>损失来判断给定图文是否匹配<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=,LM%29%20loss">lightly.ai</a>；③  <strong>图像引导的文本解码器</strong> ：使用因果自注意（单向）以实现文本生成，能够在给定图像的条件下生成描述或回答，训练时使用语言模型（LM）损失<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=cross,LM%29%20loss">lightly.ai</a>。这三部分共享同一个视觉编码器（ViT）和大部分文本编码-解码参数，仅在自注意力层是否双向/单向上有所区别<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=While%20pre,attention%20layers">lightly.ai</a>。这种设计使一个模型即可兼顾</strong>判别任务</strong> （如检索、VQA判断）和 <strong>生成任务</strong> （如图像描述）。</li>
<li><strong>模态对齐方式：</strong> BLIP结合多种目标实现模态对齐和融合：(1) 图文对比学习（ITC）让图像和文本的全局表示对齐，获得CLIP类似的跨模态嵌入空间<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=,g">lightly.ai</a>；(2) 图文匹配（ITM）通过引入<strong>交叉注意力</strong>的文本编码器，对图文对的匹配与否进行二分类训练，从而细粒度地对齐图像内容和文本语义<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=,LM%29%20loss">lightly.ai</a>；(3) 图像条件文本生成（LM损失）使模型学会在视觉条件下生成合适的语言输出。这三种损失共同训练，迫使模型不同层面地对齐：既有 <strong>全局对齐</strong> （ITC保证embedding空间一致），又有 <strong>局部对齐</strong> （ITM通过注意力机制关注图像局部来判断匹配），还有跨模态 <strong>生成对齐</strong> （LM确保图像信息融入生成过程）。尤其ITM子任务，需要模型理解图像细节与文本句子细微差异，提高了模态对齐的精细程度。</li>
<li><strong>输入 token 表达统一：</strong> BLIP并未将图像直接当作序列token交给文本Transformer，而是采用<strong>部分共享参数的编码器-解码器架构</strong>来统一多模态信息。图像首先由视觉Transformer编码为一组图像embedding序列，文本则通过词嵌入得到文本token表示。在图文交互阶段，<strong>图像embedding通过跨模态注意力机制</strong>供文本编码器/解码器读取，从而在Transformer中融合<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=,LM%29%20loss">lightly.ai</a>。由于文本端Transformer参数在编码和解码模式下共享（仅注意力方向不同），图像信息可以以一致的方式融入文本token处理流程<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=While%20pre,attention%20layers">lightly.ai</a>。简言之，BLIP通过在Transformer中引入<strong>图像作为钥匙/值</strong>的跨注意力，将图像内容注入文本token序列的处理，使两种模态的信息在Transformer中统一表达和交互。</li>
<li><strong>损失函数与训练策略：</strong> BLIP在预训练阶段<strong>联合优化三种损失</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=While%20pre,attention%20layers">lightly.ai</a>：图文对比损失、图文匹配损失、语言建模损失。一次训练迭代中，通过共享的图像编码器提取视觉特征后，分别送入上述三种模式的网络计算损失<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=While%20pre,attention%20layers">lightly.ai</a>。为提高效率，BLIP <strong>让文本编码器和解码器共享参数</strong> （仅自注意力模块不同），这样图像特征可一次读取，多头任务不会成倍增加参数<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=While%20pre,attention%20layers">lightly.ai</a>。训练策略上，BLIP先在大规模图文数据上这样多任务预训练，然后可以微调到下游具体任务。由于同时优化多目标，需平衡各损失的权重，论文中选择了适当的超参使模型在理解和生成性能上均有提升。此外，为了利用噪声较大的网络数据，BLIP设计了 <strong>两阶段Bootstrapping策略</strong> ：首先用预训练好的模型作为<strong>图像描述生成器（Captioner）<strong>为图像生成候选描述，然后用一个</strong>筛选器（Filter）<strong>剔除不匹配的图文对，再将清洗/补充后的数据用于进一步训练<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=To%20reduce%20the%20number%20of,encoder%2C%20respectively">lightly.ai</a>。这种“Captioner+Filter”流程有效降低了训练数据中的噪声，并</strong>引入了合成的伪标签</strong>描述，提高了数据利用率<a href="https://arxiv.org/abs/2201.12086#:~:text=supervision,at%20%2016%20this%20https">arxiv.org</a>。</li>
<li><strong>使用的数据集及伪标签：</strong> BLIP的预训练使用了数百万规模的公众图文对数据（如COCO Caption、Visual Genome等常用数据集的组合，以及从网上爬取的图文对）<a href="https://arxiv.org/abs/2201.12086#:~:text=supervision,at%20%2016%20this%20https">arxiv.org</a>。相对于CLIP/ALIGN那样十亿级的弱标注数据，BLIP使用的数据规模较小但质量更高（经过一定清洗）。为了进一步扩充数据，BLIP引入 <strong>伪标签机制</strong> ：利用自己模型生成图像描述（Captioner生成synthetic captions），再通过训练好的匹配模型（Filter）过滤噪声<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=To%20reduce%20the%20number%20of,encoder%2C%20respectively">lightly.ai</a>。结果是，原本嘈杂的网络图文数据被“自举”出较为可靠的新图文对，从而缓解了高质量标注数据不足的问题<a href="https://arxiv.org/abs/2201.12086#:~:text=supervision,at%20%2016%20this%20https">arxiv.org</a>。这一过程中生成的图像描述相当于 <strong>合成标签</strong> ，极大丰富了训练语料。在预训练后，BLIP在下游如Flickr30K检索、COCO描述、VQA等数据集上进行微调或直接评估，均取得领先性能<a href="https://arxiv.org/abs/2201.12086#:~:text=effectively%20utilizes%20the%20noisy%20web,released%20at%20this%20https%20URL">arxiv.org</a>。值得一提的是，BLIP的整个预训练不依赖外部标注工具（如不使用额外OCR或检测模型），完全通过多任务训练和自举数据来提高性能。</li>
<li><strong>六大难点应对：</strong>
<ol>
<li><em>模态对齐困难：</em> BLIP通过<strong>多重训练目标</strong>从不同层次对齐图像和文本。ITC损失提供全局嵌入对齐，ITM损失迫使模型关注<strong>细粒度关联</strong>来判断真伪配对，LM损失则确保图像信息能<strong>融入自然语言生成</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=,g">lightly.ai</a><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=,LM%29%20loss">lightly.ai</a>。此外，BLIP在训练中让图像参与文本Transformer的注意力计算，直接在模型内部融合模态，这比单纯对比学习的对齐更深入。综合来看，BLIP有效缓解了模态对齐难的问题，使模型不仅对整体匹配敏感，也能对局部语义对齐做出正确判断。</li>
<li><em>token格式不统一：</em> BLIP没有一刀切地将图像转为离散token，而是采用<strong>跨注意力融合</strong>策略保持各模态表征方式的优势。图像以连续向量形式存在，通过跨模态注意力供文本Transformer使用，实现类似“在Transformer中把图像当成一串记忆token”的效果。这种方法避免了人为定义图像token格式的问题，由模型自学怎样将图像特征融入文本语境。因此，BLIP在不显式统一输入格式的情况下，通过模型结构实现了功能上的token统一处理。</li>
<li><em>语义粒度不匹配：</em> BLIP在架构上引入了<strong>细粒度语义对齐</strong>机制。ITM子任务要求模型判别图文是否匹配，这通常取决于对图像细节和文本词语是否对应的判断（例如一句描述中某个细节是否在图中存在）。模型通过跨注意力可以聚焦图像的局部区域来对应文本片段，从而<strong>解决了图像区域与文本词汇粒度不对齐</strong>的问题。在生成阶段，图像引导的解码器也能描述具体对象或属性，实现细粒度描述<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=cross,g">lightly.ai</a>。因此BLIP比CLIP这类全局对齐模型更好地兼顾了细粒度的语义对齐。</li>
<li><em>多模态上下文保持：</em> 虽然BLIP主要针对单幅图像与单段文本的配对任务，但其架构天然适用于 <strong>多轮交互的扩展</strong> 。因为BLIP的文本Transformer本质是一个语言模型，经过适当调整可以接受前文对话作为文本输入，然后结合图像生成回答。事实上，BLIP的设计理念已被后续多模态对话模型（如 InstructBLIP 等）继承，用于处理多轮对话。不过BLIP原始模型并未显示多轮对话训练。在单轮情景下，BLIP利用Transformer的长序列能力，一定程度上可以处理 <strong>更长的文本上下文</strong> （如图像说明+问题一起作为前缀，然后生成回答）。因此在上下文保持上，BLIP通过Transformer结构具备了潜在优势，但需通过特定训练来充分发挥。</li>
<li><em>训练数据稀缺：</em> BLIP以 <strong>自举（Bootstrapping）<strong>的方式缓解数据不足。面对高质量标注数据有限的问题，BLIP使用初步模型为大量未标注图像生成描述（相当于自动标注），再过滤噪声后加入训练<a href="https://arxiv.org/abs/2201.12086#:~:text=supervision,at%20%2016%20this%20https">arxiv.org</a>。这种方式有效放大了训练集规模且成本低，因为生成伪标签比人工标注快得多。结果，BLIP无需像CLIP那样依赖上亿数据，就能取得优异表现<a href="https://arxiv.org/abs/2201.12086#:~:text=effectively%20utilizes%20the%20noisy%20web,released%20at%20this%20https%20URL">arxiv.org</a>。此外，多任务联合训练也提高了数据利用效率：同一数据同时为对比、匹配、生成三种任务服务，信息提取更加充分。总之，BLIP通过</strong>模型自生成数据+多任务学习</strong> ，成功在有限数据下逼近甚至超越了依赖海量数据的方法。</li>
<li><em>计算开销高：</em> BLIP的模型大小适中（基于ViT-B/16等视觉主干，文本部分与BERT-base级别相当），但同时优化三种目标确实增加了训练复杂度。不过，通过 <strong>参数共享</strong> （文本编码器和解码器共享大部分参数）<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=While%20pre,attention%20layers">lightly.ai</a>和 <strong>模块复用</strong> （同一个视觉编码器和Transformer用于多任务），BLIP将训练开销控制在可接受范围。相较于为理解和生成训练两个模型，BLIP训练单个模型完成两类任务，实际上 <strong>节省了总体算力</strong> 。当然，多任务训练需要更长时间收敛，但Salesforce的实验表明收益是值得的<a href="https://arxiv.org/abs/2201.12086#:~:text=effectively%20utilizes%20the%20noisy%20web,released%20at%20this%20https%20URL">arxiv.org</a>。在推理阶段，BLIP可以根据任务切换模式，例如执行检索时只用编码器部分，做描述时用编码-解码器，全模型参数无需全部参与，从而 <strong>推理开销也相对可控</strong> 。综上，BLIP通过架构设计在性能和计算成本之间取得了平衡，使得大型多模态模型的训练变得更加高效。</li>
</ol>
</li>
</ul>
<p><strong>参考：</strong> BLIP的论文发表在 ICML 2022<a href="https://arxiv.org/abs/2201.12086#:~:text=Title%3ABLIP%3A%20Bootstrapping%20Language,Language%20Understanding%20and%20Generation">arxiv.org</a><a href="https://arxiv.org/abs/2201.12086#:~:text=supervision,at%20%2016%20this%20https">arxiv.org</a>。官方代码已开源在 GitHub<a href="https://arxiv.org/abs/2201.12086#:~:text=CIDEr%29%2C%20and%20VQA%20%28%2B1.6,released%20at%20this%20https%20URL">arxiv.org</a>（<code>salesforce/BLIP</code>仓库），提供了预训练模型和下游任务的fine-tune实现，方便复现论文结果。</p>
<h2 id="blip-2-salesforce-2023">BLIP-2 (Salesforce, 2023)<a hidden class="anchor" aria-hidden="true" href="#blip-2-salesforce-2023">#</a></h2>
<ul>
<li><strong>整体架构设计：</strong> BLIP-2的核心思想是<strong>利用现有的预训练模型</strong>来高效构建多模态模型<a href="https://arxiv.org/abs/2301.12597#:~:text=%3E%20Abstract%3AThe%20cost%20of%20vision,art">arxiv.org</a>。它冻结了图像编码器（如ViT系列）和大型语言模型（LLM，如OPT、Flan-T5等），通过引入一个<strong>轻量级的Query Transformer（Q-Former）<strong>将二者连接起来<a href="https://arxiv.org/abs/2301.12597#:~:text=prohibitive%20due%20to%20end,tasks%2C%20despite%20having%20significantly%20fewer">arxiv.org</a>。架构上包括：冻结的视觉编码器-&gt; Q-Former -&gt; 冻结的文本生成模型。Q-Former本质是一个Transformer模块，接受视觉特征作为输入，输出一组</strong>固定数量的查询向量</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20Q,attention%20layers">lightly.ai</a>。这组向量经过投影后，作为虚拟的“视觉token”，嵌入到LLM的输入序列中，从而让LLM能够接收图像信息<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=To%20alleviate%20this%20issue%2C%20they,Former%20to%20the%20frozen%20LLM">lightly.ai</a>。由于LLM参数冻结，BLIP-2主要训练Q-Former和少量连接层。</li>
<li><strong>模态对齐方式：</strong> BLIP-2将跨模态对齐的主要难点转移到Q-Former上。它采用<strong>两阶段训练</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=To%20alleviate%20this%20issue%2C%20they,Former%20to%20the%20frozen%20LLM">lightly.ai</a>：第一阶段，让Q-Former结合冻结视觉编码器进行 <strong>图文表示学习</strong> （类似BLIP-1的方法，使用图文对比或匹配损失），使Q-Former学会提取与文本语义相关的视觉概念<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=stage%20strategy%20and%20acts%20as,Former%20to%20the%20frozen%20LLM">lightly.ai</a>。第二阶段，将训练好的Q-Former输出连接到冻结的LLM输入embedding，利用图文对话/描述数据训练生成任务，使整个系统能够<strong>端到端地产生</strong>对应输入图像的文本<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=stage%20strategy%20and%20acts%20as,Former%20to%20the%20frozen%20LLM">lightly.ai</a>。在对齐过程中，Q-Former充当中介：它一头通过跨注意力读取视觉特征，另一头输出的查询向量要能和LLM的语义空间对接。因此，通过专门设计的损失（如阶段一的对比/ITC+ITM，阶段二的语言建模），BLIP-2成功将视觉空间对齐到语言空间。直观来说， <strong>Q-Former学会生成“描述图像的语言向量”</strong> ，这些向量插入LLM提示中后，LLM即可理解并基于图像内容作出回答。</li>
<li><strong>输入 token 表达统一：</strong> 在BLIP-2中，输入给LLM的是标准的文本token序列，但其中混入了由图像生成的 <strong>特殊嵌入向量</strong> 。具体实现是：LLM的词表中引入若干个保留位置，用来放置Q-Former生成的视觉查询向量（不一定真的映射为离散token，而是直接作为embedding）<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=To%20alleviate%20this%20issue%2C%20they,Former%20to%20the%20frozen%20LLM">lightly.ai</a><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20Q,attention%20layers">lightly.ai</a>。因此，从LLM角度看，它接收到了一串长度为N（固定）的“视觉token”嵌入，后面可能跟随文本token，例如问题或提示语。通过这种方式，图像信息被<strong>格式统一</strong>地并入LLM的输入序列，就好像视觉也被表示成了一组特殊的单词embedding。值得注意的是，这里的视觉token并非通过人工词典获得，而是Q-Former自由学习产生的向量。不过，对于LLM来说，无论是真实文字embedding还是视觉embedding，它都一视同仁地通过自注意力机制处理。这实现了在架构上的 <strong>输入统一</strong> ：图像被转换成等价于文本embedding的形式，与文本共同作用于下游生成。</li>
<li><strong>损失函数与训练策略：</strong> BLIP-2采用 <strong>分阶段训练策略</strong> 。阶段一使用与BLIP类似的目标（ITC对比学习、ITM匹配等）训练Q-Former，使其能够对齐视觉和文本表示<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=stage%20strategy%20and%20acts%20as,Former%20to%20the%20frozen%20LLM">lightly.ai</a>。阶段二则固定视觉编码器和Q-Former不变（或仅微调Q-Former），仅训练将Q-Former输出喂入LLM后的生成能力<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=stage%20strategy%20and%20acts%20as,Former%20to%20the%20frozen%20LLM">lightly.ai</a>。阶段二通常采用 <strong>语言模型损失</strong> ：给定图像和（可选的）文本提示，让LLM输出描述或答案，与GT文本计算交叉熵损失，从而调整Q-Former和连接层使LLM的输出正确。在这一阶段，LLM本身参数冻结，所以训练信号主要作用于Q-Former，使其输出的视觉查询能被LLM高效利用。此外，BLIP-2可能使用了混合数据训练策略：既包含纯图文对话数据，也包含传统图文描述数据，以增强模型泛化能力。总结来说，<strong>第一阶段注重表示对齐，第二阶段注重生成对接</strong><a href="https://arxiv.org/abs/2301.12597#:~:text=vision,We">arxiv.org</a>。两个阶段结合，使模型以较低的训练成本达到对大语言模型“喂图”的效果。</li>
<li><strong>使用的数据集及伪标签：</strong> BLIP-2所使用的数据包括<strong>现成的大规模图文对</strong>以及 <strong>对话式多模态数据</strong> 。阶段一使用的数据类似BLIP-1，例如COCO Caption、Visual Genome Caption以及LAION-400M等开放图文集，用于学习跨模态表示。阶段二则需要图像输入/文本输出的监督数据，如VQA问答、图像描述，以及自制的指令数据集等。由于BLIP-2本身是在2023年提出，可能利用了当时兴起的多模态指令数据（例如由GPT生成的对话）来增强模型的对话能力。关于伪标签，BLIP-2相比BLIP-1更少需要合成描述，原因是它<strong>直接利用预训练的LLM</strong>已经具备生成流畅文本的能力。相反，BLIP-2更关注 <strong>如何高效利用预训练资源</strong> 。它没有从零开始生成伪标签，而是通过降低训练需求（冻结大模型）来避免需要海量新标注数据<a href="https://arxiv.org/abs/2301.12597#:~:text=%3E%20Abstract%3AThe%20cost%20of%20vision,art">arxiv.org</a>。因此，除非为了特定任务，BLIP-2通常不依赖额外的伪标注数据。不过，在一些研究和开源实现中，会将BLIP-2作为基础，再用GPT-4生成的指令数据进行微调（如InstructBLIP），那属于后续fine-tuning阶段。总的来说，BLIP-2本身强调 <strong>利用已有数据与模型</strong> ，而非采集新数据，这是一种不同于以往“大规模爬取”的范式。</li>
<li><strong>六大难点应对：</strong>
<ol>
<li><em>模态对齐困难：</em> BLIP-2的巧妙之处在于借助<strong>预训练模型</strong>降低对齐门槛：视觉编码器（如CLIP的ViT）本身已具有与文本对齐的表示能力，LLM则有强大的语言理解和生成能力。Q-Former经过专门训练，学习如何从图像提取出能解释文本的<strong>关键视觉概念</strong><a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=stage%20strategy%20and%20acts%20as,Former%20to%20the%20frozen%20LLM">lightly.ai</a>。它将视觉信号压缩成几十个查询向量，使之恰好能被LLM理解。通过两阶段训练，BLIP-2成功将视觉信息嵌入LLM的上下文中，实现 <strong>模态隐式对齐</strong> 。尤其第二阶段训练，让模型生成正确描述，确保了视觉表示和语言表示在语义空间上对齐，以至于LLM可以将来自图像的embedding视作自身词汇的一部分。这解决了LLM未看过图像的难题，将跨模态对齐转换为一个中等规模Transformer训练就完成了<a href="https://arxiv.org/abs/2301.12597#:~:text=prohibitive%20due%20to%20end,tasks%2C%20despite%20having%20significantly%20fewer">arxiv.org</a>。因此，BLIP-2在对齐上绕过了直接训练巨型多模态模型的难关，以更低成本达到对齐效果。</li>
<li><em>token格式不统一：</em> BLIP-2通过 <strong>Query Transformer输出固定长度视觉token向量</strong> ，使得图像信息以接近文本token的形式输入LLM<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20Q,attention%20layers">lightly.ai</a>。这些视觉token不是离散符号，但在Transformer中发挥的作用与普通词嵌入相同。LLM在位置嵌入上也不区分它们，这样视觉和文本序列实际上融合为一个统一的序列处理<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=To%20alleviate%20this%20issue%2C%20they,Former%20to%20the%20frozen%20LLM">lightly.ai</a>。因此，虽然没有显式定义图像的词汇表，BLIP-2达成了功能上的token格式统一：模型把连续视觉特征转换为离散的若干embedding插入序列。LLM可以像处理句子一样处理“图像句子”。这一设计继承了<strong>Flamingo</strong>等模型的思路，但更轻量（因为只有Q-Former承担额外计算）。</li>
<li><em>语义粒度不匹配：</em> BLIP-2输出的视觉token向量本质上可以被视为图像的 <strong>语义摘要</strong> 。Q-Former通过训练，会针对文本任务提取图像中与语义相关的细粒度信息。例如，若任务是描述图像，Q-Former会聚焦显著对象和属性；若任务是回答问题，Q-Former会提取与问题相关的视觉线索。这种机制使图像的大量低层次像素信息被压缩，仅保留语义层面的关键内容，从而匹配LLM处理的语言粒度（概念级别）<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=stage%20strategy%20and%20acts%20as,Former%20to%20the%20frozen%20LLM">lightly.ai</a>。因此，语义粒度的鸿沟通过Q-Former的提炼得到弥合——图像的细节被提升到语义概念后才提供给LLM。实践证明，BLIP-2能够让LLM正确识别图中具体对象并生成相应描述，说明<strong>语义层次基本匹配</strong>了语言空间<a href="https://arxiv.org/abs/2301.12597#:~:text=image%20encoder,can%20follow%20natural%20language%20instructions">arxiv.org</a>。当然，如果图像中有非常细微的局部信息，固定数量的查询可能略有不足，但总体上BLIP-2在保持主要语义同时过滤冗余细节方面是成功的。</li>
<li><em>多模态上下文保持：</em> BLIP-2本身不直接处理多轮对话，但由于它的输出接口是对接LLM，而LLM天然支持长上下文对话，因此BLIP-2具备扩展为多模态对话的潜力。事实上，将BLIP-2生成的视觉token视为对话的一部分，就可以实现 ChatGPT+图像 的效果。BLIP-2的论文主要评估的是单轮任务（如VQA回答、图像描述），但把它用于对话时，可以每次在提示中加入视觉token并配合已有的聊天上下文，LLM即可持续参考视觉信息进行对话。这意味着BLIP-2间接实现了 <strong>视觉上下文在多轮对话中的保持</strong> ：视觉token可以在对话prompt中重复出现或被引用，使LLM记住之前提到的图像要点。不过，在一个会话过程中，BLIP-2通常针对每张新图像各自运行一次，不会像Flamingo那样显式处理多张图共同存在的情况。因此严格来说，BLIP-2原生支持 <strong>单图上下文保持</strong> ，多图或连续对话需借助LLM的记忆机制来维系。</li>
<li><em>训练数据稀缺：</em> BLIP-2的策略是 <strong>以预训练模型替代海量数据</strong> 。因为直接训练一个看图的LLM需要海量图文数据，但BLIP-2通过使用预先训练好的ViT和LLM，将主要学习任务转为训练Q-Former。Q-Former的参数规模（约1.9亿）远小于LLM，所需训练数据也相对少<a href="https://www.lightly.ai/blog/clip-and-friends#:~:text=The%20Q,attention%20layers">lightly.ai</a>。实验表明，在已有的大模型基础上，只需在相对有限的图文数据上微调，就能达到甚至超过训练80亿参数模型（如Flamingo）的效果<a href="https://arxiv.org/abs/2301.12597#:~:text=image%20encoder,can%20follow%20natural%20language%20instructions">arxiv.org</a>。这等于用模型知识弥补了数据量不足。此外，BLIP-2本身利用了BLIP-1时期的图文数据清洗经验，挑选高质量数据进行两阶段训练，以较小的数据量取得高性能<a href="https://arxiv.org/abs/2301.12597#:~:text=image%20encoder,can%20follow%20natural%20language%20instructions">arxiv.org</a>。因此，对研究者而言，BLIP-2降低了训练多模态模型的数据门槛——无需爬取上亿样本，有几百万高质量样本配合预训练模型就够用。</li>
<li><em>计算开销高：</em> 相比从头训练一个多模态Transformer（参数往往数十亿），BLIP-2的训练开销显著降低。冻结LLM和视觉编码器意味着大部分参数不需要反向传播更新，只训练Q-Former等少部分参数，使内存和算力需求下降。作者报告，BLIP-2仅有<strong>少量可训练参数</strong>却超越了一些体量大几十倍的模型<a href="https://arxiv.org/abs/2301.12597#:~:text=image%20encoder,can%20follow%20natural%20language%20instructions">arxiv.org</a>。同时，由于分阶段训练，第一阶段可在相对小模型上完成，第二阶段虽然用LLM但只进行embedding层和Q-Former的调优，计算效率高。综合来看，BLIP-2通过<strong>迁移学习</strong>和 <strong>参数高效微调</strong> ，极大缓和了算力需求。这也体现在推理阶段：因为LLM冻结且对话时只需将视觉token拼接输入，不增加额外推理步骤，实时性有保障。当然，BLIP-2依赖的LLM本身推理开销不低（如果LLM很大），但相较于训练一个同等大小的多模态模型，BLIP-2的总计算代价小得多。因此，在算力有限的环境下，BLIP-2提供了一种实用可行的多模态方案。</li>
</ol>
</li>
</ul>
<p><strong>参考：</strong> BLIP-2的论文在 arXiv 发布<a href="https://arxiv.org/abs/2301.12597#:~:text=%3E%20Abstract%3AThe%20cost%20of%20vision,art">arxiv.org</a><a href="https://arxiv.org/abs/2301.12597#:~:text=prohibitive%20due%20to%20end,tasks%2C%20despite%20having%20significantly%20fewer">arxiv.org</a>（ICLR 2023），详细介绍了其两阶段训练方法和在零样本VQA等任务上的性能。代码已开源在 GitHub（<code>salesforce/LAVIS</code>库中提供了BLIP-2实现）。BLIP-2的效果也推动了许多衍生工作（如开放对话系统 MiniGPT-4 等），这些都建立在BLIP-2提供的视觉-语言接口之上。</p>
<h2 id="git-microsoft-2022">GIT (Microsoft, 2022)<a hidden class="anchor" aria-hidden="true" href="#git-microsoft-2022">#</a></h2>
<ul>
<li><strong>整体架构设计：</strong> GIT（ <strong>Generative Image-to-text Transformer</strong> ）尝试将视觉-语言任务完全统一到一个<strong>生成式Transformer</strong>框架下<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=structures%20%28uni%2Fmulti,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。其架构极为简洁： <strong>一个图像编码器 + 一个文本解码器</strong> ，二者共同组成一个端到端的序列到序列模型<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=structures%20%28uni%2Fmulti,Without%20bells%20and%20whistles">ar5iv.labs.arxiv.org</a>。图像编码器提取图像特征（采用预训练的CLIP视觉Transformer或自训练的ViT等，输出二维特征序列<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=3">ar5iv.labs.arxiv.org</a>），然后通过线性层投影并加上位置嵌入，作为文本解码器的跨注意力键值输入<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20image%20encoder%20is%20based,based%20approaches%2C%20e.g">ar5iv.labs.arxiv.org</a><a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20text%20decoder%20is%20a,3%20is">ar5iv.labs.arxiv.org</a>。文本解码器是标准的Transformer解码架构（多层自注意力+交叉注意力），以语言模型方式生成文本<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=structures%20%28uni%2Fmulti,arts%20on%20numerous%20challenging%20benchmarks">ar5iv.labs.arxiv.org</a>。不同于许多早期方法，GIT<strong>不使用任何物体检测器或OCR模型</strong>来预处理图像，也不引入额外的多模态编码器，一切融合在Transformer解码器中完成<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=structures%20%28uni%2Fmulti,arts%20on%20numerous%20challenging%20benchmarks">ar5iv.labs.arxiv.org</a>。这种纯粹的“图像到文本”架构使模型在预训练和微调阶段的结构完全一致，能够方便地泛化到各种以文本为输出的视觉任务。</li>
<li><strong>模态对齐方式：</strong> GIT没有采用显式的对比对齐或ITM损失，而是通过<strong>单一的语言建模任务</strong>隐式地实现模态对齐<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=match%20at%20L109%20decoder,margin%2C%20as%20summarized%20in%20Table%C2%A01">ar5iv.labs.arxiv.org</a>。在预训练时，模型接收图像并 <strong>直接生成整段描述文本</strong> （或回答），训练目标是最小化生成文本与真实文本之间的交叉熵损失<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=match%20at%20L109%20decoder,margin%2C%20as%20summarized%20in%20Table%C2%A01">ar5iv.labs.arxiv.org</a>。这种方式迫使图像编码器提取的特征必须包含生成正确文本所需的所有信息，同时解码器的交叉注意力会学习将文本词汇与相应的图像区域关联，以便正确生成。这意味着图像和文本的对齐并不是通过拉近embedding距离实现的，而是在<strong>Transformer解码过程</strong>中，通过注意力权重对齐：模型只有在正确对齐图像内容与生成词语时才能取得低损失。例如，当解码器生成单词“狗”时，跨模态注意力会自然地关注图像中狗所在的特征区域，从而将视觉语义与该单词绑定。经过大规模训练后，这种<strong>注意力驱动的软对齐</strong>形成模型内隐的模态对齐机制。值得一提的是，作者在预训练时 <strong>扩充了任务种类</strong> ，不仅包括图像描述，还有图像问答等，这些任务都要求正确关联图像和文本才能解答，从而进一步强化了模态对齐<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=In%20this%20paper%2C%20we%20design,Without%20bells%20and%20whistles">ar5iv.labs.arxiv.org</a><a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=both%20pre,initialized%20as%200">ar5iv.labs.arxiv.org</a>。</li>
<li><strong>输入 token 表达统一：</strong> GIT通过 <strong>将图像特征序列拼接进Transformer解码器的输入</strong> ，实现了一种隐式的token统一表示<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20text%20decoder%20is%20a,3%20is">ar5iv.labs.arxiv.org</a>。具体而言，图像编码器输出经过投影变换后，作为一组“图像token”（连续向量）排列在Transformer解码器的输入序列最前<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20text%20decoder%20is%20a,3%20is">ar5iv.labs.arxiv.org</a>。紧随其后的是文本的<code>&lt;BOS&gt;</code>标记和需要生成的文本token（初始化为待预测状态）。在Transformer内部，采用一个特别的<strong>序列到序列注意力掩码</strong><a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20text%20decoder%20is%20a,the%20preceding%20tokens%20and%20all">ar5iv.labs.arxiv.org</a>：文本token可以看见所有图像token和之前的文本token，而图像token之间也可以相互看到（便于图像特征全局建模）<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20text%20decoder%20is%20a,the%20preceding%20tokens%20and%20all">ar5iv.labs.arxiv.org</a>。这样，Transformer解码器实际上同时处理了图像token和文本token的序列。对模型而言，图像token与普通文本embedding在同一计算图中，只是通过mask控制了注意力方向。通过这种机制，GIT无需修改Transformer结构，就实现了 <strong>图像+文本统一序列建模</strong> ：图像被视作解码开始时的一段前缀序列。这保证了图像信息能够像前文一样参与生成过程，从而让图像上下文与文本自然融合。另外，这种方法也不需要离散化图像，只要提供足够的图像token分辨率，模型就能以连续表示处理视觉信息。</li>
<li><strong>损失函数与训练策略：</strong> GIT采用 <strong>纯粹的自回归语言模型损失</strong> 。给定图像（以及可选的提示文本），让模型生成目标文本序列，计算标准的交叉熵损失来训练<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=match%20at%20L109%20decoder,margin%2C%20as%20summarized%20in%20Table%C2%A01">ar5iv.labs.arxiv.org</a>。在预训练期间，为了让模型适应多样任务，训练数据中包含了各种形式：图像标题生成、图像问答（在这种情况下，会在图像token后加入问题文本作为前缀，然后生成答案）等<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=Image%3A%20Refer%20to%20caption%20Figure,before%20concatenation">ar5iv.labs.arxiv.org</a><a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=both%20pre,initialized%20as%200">ar5iv.labs.arxiv.org</a>。例如，对于VQA，输入序列是「<code>&lt;img&gt;</code>&hellip;<code>&lt;img&gt;</code> 问题：&hellip; 答案：」，模型学习在看到“问题”后生成正确“答案”<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=both%20pre,initialized%20as%200">ar5iv.labs.arxiv.org</a>。这种统一的语言模型策略使预训练和下游任务能够共享同一套参数和目标，不需要为不同任务切换架构或损失函数。此外，作者强调<strong>扩大预训练数据和模型规模</strong>对性能至关重要<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=GIT%2C%20we%20simplify%20the%20architecture,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。他们使用了比以往更大规模的图文数据，以及训练了不同尺寸的模型（从Base到巨型）进行对比，在多个任务上取得新的SOTA<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=under%20a%20single%20language%20modeling,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。训练策略上没有使用教师模型或多阶段训练，而是一阶段大一统模型学尽可能多的任务。这种“无技巧（no bells and whistles）”的方法充分依赖海量数据和模型容量来获得性能<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=GIT%2C%20we%20simplify%20the%20architecture,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。</li>
<li><strong>使用的数据集及合成数据：</strong> GIT的预训练数据非常广泛和庞大。微软在论文中没有公布确切的数据量，但提到**“扩大了预训练数据规模”<strong><a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=GIT%2C%20we%20simplify%20the%20architecture,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。推测他们使用了公共的大型图文数据集合集（如COYO、LAION等），以及内部收集的数据，包括图片描述和问答标注。此外，他们还将</strong>视频字幕数据<strong>扩充到模型中，使模型能处理视频（选帧作为序列的一部分）<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=In%20this%20paper%2C%20we%20design,Without%20bells%20and%20whistles">ar5iv.labs.arxiv.org</a>。在下游微调时，GIT在12个具有挑战性的基准上测试，包括COCO、nocaps（开放词汇描述）、VizWiz（盲人拍照求助）、TextCaps（需要OCR的图片描述）、多种VQA和视频caption等<a href="https://arxiv.org/abs/2205.14100#:~:text=one%20text%20decoder%20under%20a,this%20https%20URL">arxiv.org</a>。令人瞩目的是，GIT在TextCaps数据集上首次超越了人类表现<a href="https://arxiv.org/abs/2205.14100#:~:text=one%20text%20decoder%20under%20a,this%20https%20URL">arxiv.org</a>，说明模型学会了相当程度的</strong>场景文本识别<strong>和理解——这归功于预训练涵盖了带文字的图像以及无需OCR模块的端到端学习。GIT并未借助合成的伪标签数据；相反，它</strong>直接在真实任务数据上大规模训练**。例如，为了让模型具备OCR能力，他们可能在预训练中加入了带文字的图像及其文字描述（如OCR-VQA等），让模型自己去学习文字区域的特征提取<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=under%20a%20single%20language%20modeling,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。因此，GIT更多是通过<strong>多任务训练</strong>覆盖各种模态难点，而不是通过额外生成数据来弥补。当然，训练这样一个模型本身需要巨量的数据，但微软具备这样的资源优势。</li>
<li><strong>六大难点应对：</strong>
<ol>
<li><em>模态对齐困难：</em> GIT选择了<strong>端到端生成</strong>作为对齐手段。由于模型只能通过正确生成文本来降低损失，它被迫在内部对齐图像与文本。例如，Transformer解码器的交叉注意力会在训练中自动调整，使得每个生成的词与相应的图像内容关联。这种隐式对齐不需要额外的对比损失，却在模型Attention权重中形成了<strong>图像区域-文本词汇</strong>的映射关系。再加上GIT预训练涵盖问答等任务，模型学会在回答问题时关注相关图像部分，在描述时依照图像内容组织语言——这些都属于模态对齐的体现。可见，尽管没有显式对齐Loss，GIT通过<strong>任务驱动对齐</strong>实现了高质量的模态对齐<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=match%20at%20L109%20decoder,margin%2C%20as%20summarized%20in%20Table%C2%A01">ar5iv.labs.arxiv.org</a>。模型的成功表明，只要任务设计合理，生成式训练本身就能让模型学会跨模态对齐。</li>
<li><em>token格式不统一：</em> GIT通过<strong>序列到序列Transformer</strong>架构，巧妙地让图像和文本“同列于一个序列”。图像编码器输出一系列向量，这些向量在解码器里被视作一段上下文序列<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20text%20decoder%20is%20a,3%20is">ar5iv.labs.arxiv.org</a>。这样，虽然图像不是离散单词，但在Transformer看来，它们只是前若干个特殊的输入embedding。后续文本token可以自然地参考这些图像embedding，就如同参考句首提供的提示一样。这个设计避免了需要定义图像词典或修改模型输入结构，使 <strong>格式统一的问题迎刃而解</strong> 。换言之，Transformer模型对图像和文本一视同仁，只是通过mask控制依赖关系<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20text%20decoder%20is%20a,the%20preceding%20tokens%20and%20all">ar5iv.labs.arxiv.org</a>。因此，GIT内部已经实现了对不同模态信息的格式融合，不存在单独处理再对齐的问题。</li>
<li><em>语义粒度不匹配：</em> GIT直接使用CNN/ViT提取图像特征，并通过Transformer将其转换为语言。没有显式区域级别的对齐机制，但Transformer的交叉注意力可以细粒度地处理图像patch与词的关系。例如，模型在生成某个名词时，会极大地注意对应物体的那些视觉token，实现类似局部对齐的效果。这相当于让细粒度对齐在<strong>注意力机制</strong>中自发完成。此外，作者使用了一个trick：他们用对比学习预训练好的图像编码器<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20image%20encoder%20is%20based,This%20is">ar5iv.labs.arxiv.org</a>，保证图像特征本身具有较高级的语义表示能力（对比预训练会让相同类别/语义的图像特征聚类）。这意味着图像特征一开始就带有一定的语义概括性，减少了视觉低层细节与语言高级概念的不匹配<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=The%20image%20encoder%20is%20based,based%20approaches%2C%20e.g">ar5iv.labs.arxiv.org</a>。因此，在语义粒度上，GIT通过<strong>预训练的视觉语义特征+解码器注意力</strong>两方面，较好地解决了粒度差异问题。模型的OCR能力说明它可以从小区域拼写出单词，说明精细粒度也能捕获；而在描述整图时又能抓大放小，生成整体语义，这体现了粒度上的灵活性。</li>
<li><em>多模态上下文保持：</em> GIT的设计初衷不在对话，而在统一各种 <strong>静态视觉任务</strong> 。因此原版GIT不具备多轮对话记忆。然而，它提供了 <strong>统一的生成框架</strong> ，理论上可以扩展对话：只需在输入序列中加入之前对话的文本，即可将历史作为上下文。而图像如果需要在对话中反复参考，可以在每轮答复时都把同样的图像token放入输入。但这会受到模型最大序列长度限制。微软没有在论文中报道对话实验，但在VQA任务里，GIT通过将“问题”作为前缀文本与图像共同输入<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=both%20pre,initialized%20as%200">ar5iv.labs.arxiv.org</a>来回答，已经体现了处理<strong>图文混合上下文</strong>的能力。对于多张图像，GIT可以一次编码多张图的特征串联作为更长的图像token序列，只是论文未深入探索。这种架构天然支持多模态上下文的扩展，但需要注意计算成本会随序列长度增长。在视频场景中，作者已经验证了能处理多帧（通过给每帧加上时间嵌入再串联）<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=both%20pre,before%20concatenation">ar5iv.labs.arxiv.org</a>。所以GIT显示出一定的上下文扩展性，但要真正保持多轮对话语境，可能还需在生成策略上做些改动（如引入特殊标记区分说话人等）。总的说来，GIT为多模态上下文提供了一个统一容器，但<strong>对话管理</strong>不在其预训练范围内，需要额外设计。</li>
<li><em>训练数据稀缺：</em> GIT依赖<strong>大规模多样化数据</strong>取得成功。它的理念是与其设计复杂模型，不如用简单模型配合巨量数据<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=GIT%2C%20we%20simplify%20the%20architecture,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。虽然作者未公开数据细节，但可以推测其使用近十亿级别的图文对进行训练（极可能包括微软内部的<strong>ALIGN-类数据</strong>或JFT系列）。通过大量数据，GIT在各任务上都达到新的高度<a href="https://arxiv.org/abs/2205.14100#:~:text=one%20text%20decoder%20under%20a,this%20https%20URL">arxiv.org</a>。对于普通研究者而言，如此数据难以获得。但GIT证明，大模型+大数据可以在无需额外标注和复杂技巧的情况下解决很多问题。因此，GIT没有使用伪标签，它体现的是另一种思路： <strong>以规模取胜</strong> 。这在一定程度上回避了数据稀缺，因为一旦数据够多，很多小数据集的问题都变得可以零样本解决<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=under%20a%20single%20language%20modeling,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。此外，统一模型能跨任务共享知识：例如在描述任务学到的知识对VQA有帮助，这其实提高了每条数据的利用率。这种多任务迁移也缓和了单任务数据不足的情况。因此，虽然GIT本身消耗了巨大数据，但相对于分别训练多个任务专用模型，其综合效率反而更高。</li>
<li><em>计算开销高：</em> 训练一个像GIT这样的模型（尤其是大尺寸版本）需要相当高的计算投入。微软通过大规模并行和分布式训练完成了这一过程。幸运的是，GIT架构简单统一，没有多分支，这使并行效率较高。模型参数虽多，但Transformer易于在GPU/TPU上加速。而且作者在论文中提供了不同模型规模的对比如Base、Large、Huge等<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=match%20at%20L209%20et%C2%A0al,training%20data%20scales%20%289">ar5iv.labs.arxiv.org</a>。在实际应用中，可根据算力选择较小的模型进行fine-tune。推理方面，由于没有双塔或额外模块，GIT生成一次回复需要完整地跑Transformer，对于长序列仍较耗时。但没有交叉模块切换开销。值得注意的是，GIT证明了 <strong>统一模型减少了重复计算</strong> ：比起每个任务训练不同模型，一个预训练模型fine-tune各任务总计算量更小<a href="https://ar5iv.labs.arxiv.org/html/2205.14100#:~:text=GIT%2C%20we%20simplify%20the%20architecture,image%20classification%20and%20scene%20text">ar5iv.labs.arxiv.org</a>。同时，它也展示了Transformer在CV任务中的威力，使GPU上的Transformer算力得以充分利用，不像以前CNN+RNN需要异构处理。所以总体看，GIT的 <strong>训练成本虽然高，但回报是一个通用模型</strong> 。随着算力的提升，这种“大一统预训练”将变得越来越现实。</li>
</ol>
</li>
</ul>
<p><strong>参考：</strong> GIT论文发表于 2023 年CVPR<a href="https://arxiv.org/abs/2205.14100#:~:text=,the%20arts%20on%2012%20challenging">arxiv.org</a>（OpenReview提供了审稿意见）。论文附带的代码已在GitHub开源<a href="https://arxiv.org/abs/2205.14100#:~:text=Furthermore%2C%20we%20present%20a%20new,this%20https%20URL">arxiv.org</a>（microsoft/GenerativeImage2Text），方便社区使用。有关GIT的更深入讲解，可参考微软研究博客和OpenAI笔记等资源对比GIT与同类模型的设计理念。</p>
<h2 id="flamingo-deepmind-2022">Flamingo (DeepMind, 2022)<a hidden class="anchor" aria-hidden="true" href="#flamingo-deepmind-2022">#</a></h2>
<ul>
<li><strong>整体架构设计：</strong> Flamingo是DeepMind提出的一种 <strong>少样本视觉语言模型</strong> ，它将预训练的视觉编码器和预训练的大型语言模型结合，通过插入<strong>跨模态注意力层</strong>实现图文融合<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Flamingo%20%28Alayrac%20et%20al,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=,on%20the%20above%20visual%20tokens">lilianweng.github.io</a>。具体来说，Flamingo采用了CLIP的ViT作为图像编码器（提取每张图像的一组视觉特征），采用类似GPT-3风格的大型Transformer作为文本生成模型（如Chinchilla 70B）<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Flamingo%20%28Alayrac%20et%20al,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。在两者之间，Flamingo引入一个 <strong>Perceiver Resampler模块</strong> ，将任意长度的视觉特征压缩成固定数量的 <strong>视觉tokens</strong> （如每张图像压缩成N≈64个token）<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=signals%2C%20Flamingo%20adopts%20a%20Perceiver,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。然后，在语言模型的每层若干位置，插入“门控跨注意力层”，让文本流在生成过程中可以多次访问这些视觉tokens<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=connects%20a%20pretrained%20LM%20and,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。这些跨注意力层在语言模型层之间交织，使模型在生成每个词时，都能参考图像信息。值得强调的是，Flamingo在训练时 <strong>冻结了原有的语言模型和视觉编码器权重</strong> ，只训练中间的新组分（包括Perceiver和跨模态层）<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Similar%20to%20ClipCap%2C%20both%20pretrained,more%20training%20data%20than%20ClipCap">lilianweng.github.io</a>。这种设计确保了预训练模型的语言和视觉知识被最大程度保留，同时通过新组件实现模态融合。</li>
<li><strong>模态对齐方式：</strong> Flamingo的模态对齐依赖于 <strong>预训练模型的知识+少量新的连接参数</strong> 。图像编码器CLIP本身已提供高质量的视觉表示，语言模型也有丰富的语言常识。Flamingo只训练连接部分，通过<strong>自回归语言模型目标</strong>来让视觉信息对接语言输出<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=and%20then%20use%20cross,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。训练过程中，模型读取一串交织的图像和文本（例如一个网页内容，其中有文字和插入的图片），试图按照出现顺序预测下一个文本token<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=and%20then%20use%20cross,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。这隐含地要求模型学会对齐：当遇到需要描述图像的地方，就必须利用视觉tokens提供的信息来正确地产生文字。因此，Flamingo没有明确的对比或匹配损失，而是在 <strong>序列建模过程中完成对齐</strong> 。尤其得益于CLIP提供的视觉特征空间和语言模型的语义空间都非常成熟，跨注意力层只需学会将二者关联即可。例如，Flamingo使用一个<strong>门控机制</strong>控制每个跨注意力头对视觉的依赖程度，这保证了模型不会过度依赖或忽略视觉信息，而是<strong>渐进式地融合</strong><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Flamingo%20is%20only%20trained%20to,In">lilianweng.github.io</a>。经过训练，Flamingo实现了图文对齐，以至于在推理时，可以在看到图像后正确地继续对话生成相关文本。这种对齐能力在它的few-shot学习中表现突出：只需给出少量图文示例，模型就能对新图像输出合理描述或回答，表明模态对齐已经内化在模型中了<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=At%20test%20time%2C%20Flamingo%20naturally,context%20contribute%20to%20better%20performance">lilianweng.github.io</a>。</li>
<li><strong>输入 token 表达统一：</strong> Flamingo通过 <strong>对文本序列进行特殊标记和掩码</strong> ，实现了对图像和文本交替输入的统一处理<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=To%20easily%20handle%20text%20with,of%20images%20in%20the%20context">lilianweng.github.io</a>。他们在训练语料的文本中插入特殊标记 <code>&lt;image&gt;</code>代表图像占位符，当遇到该标记时，模型会取下一张图像的视觉tokens作为输入<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=They%20scraped%2043%20million%20webpages,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>。在Transformer内部，通过设计注意力mask，使得文本token只能看见<strong>最近一次出现的图像tokens</strong>以前的文本，而不能看见更早图像，以此处理多图场景<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=To%20easily%20handle%20text%20with,of%20images%20in%20the%20context">lilianweng.github.io</a>。同时，由于视觉tokens长度固定，每当有图像时，就把那N个视觉tokens嵌入序列。这样，整个输入序列可能形如：“文本段1 <code>&lt;image&gt;</code> 文本段2 <code>&lt;image&gt;</code> 文本段3&hellip;”。对于Transformer来说，<code>&lt;image&gt;</code>标记只是一种指示，它实际会被替换为N个视觉embedding。最终，模型看到的是一个混合序列，其中既有文本token embedding也有视觉token embedding。Flamingo的跨注意力层保证文本可以从视觉embedding汲取信息<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=connects%20a%20pretrained%20LM%20and,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。总之，Flamingo实现了 <strong>在同一序列中交织图像和文本</strong> ：在位置编码上，文本和图像embedding各据其位，模型通过mask确保因果关系正确<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=To%20easily%20handle%20text%20with,of%20images%20in%20the%20context">lilianweng.github.io</a>。这种方式处理输入使得模型能够自然地接受任意交替的多模态输入，而不需要显式地将图像转成离散标签或one-hot表示。</li>
<li><strong>损失函数与训练策略：</strong> Flamingo以<strong>自回归下一个词预测</strong>作为唯一的训练目标<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=and%20then%20use%20cross,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。训练数据是精心构造的 <strong>多模态序列</strong> ：DeepMind构建了一个名为“M3W”（MassiveWeb）的大型数据集，从网络抓取包含图像和文字的网页片段共4300万条<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=They%20scraped%2043%20million%20webpages,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>。这些数据被处理成长度为256的token序列（其中可能包含最多5张图像）<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=,padding%20to%20%24N%24%20if%20fewer">lilianweng.github.io</a>。此外，Flamingo还混合了传统的 <strong>图文对数据</strong> （如ALIGN的1.8亿图文对）和 <strong>视频-文本数据</strong> （如从视频中抽帧及对应描述）进行训练<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=match%20at%20L390%20,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=They%20scraped%2043%20million%20webpages,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>。整个训练在不同数据源上采用 <strong>分布式多任务训练</strong> ：每个batch随机抽取来自网页、多图文对、视频的样本分别计算NLL损失，再按设定权重求和优化<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Since%20Flamingo%20is%20trained%20on,each%20dataset%20and%20apply%20a">lilianweng.github.io</a>。这样的策略使模型同时适应多种输入形式。训练中需要注意各数据集的权重分配，作者采用均衡采样避免小数据集被忽略，同时也调整过不同任务损失的比重<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=is%20very%20important%20for%20the,each%20dataset%20and%20apply%20a">lilianweng.github.io</a>。最后，通过大量算力（语言模型80B参数，加上新插入层）训练，Flamingo可以在不微调的情况下实现few-shot学习，即给定少数示例即可在16个下游任务中取得接近或超过有监督SOTA的成绩<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=match%20at%20L423%20Flamingo%20outperforms,does%20lead%20to%20better%20results">lilianweng.github.io</a>。模型也支持进一步微调，但由于参数量巨大且新的门控层较敏感，微调需要小心调参。不过，一旦训练完成，Flamingo在多模态few-shot方面展示了卓越的能力。</li>
<li><strong>使用的数据集及伪标签：</strong> 如上所述，Flamingo的主要预训练数据包括三个部分：<strong>网页多模态语料M3W</strong><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=They%20scraped%2043%20million%20webpages,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>、 <strong>图片-文本对数据</strong> （如ALIGN 1.8B对、LAION等）<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=They%20scraped%2043%20million%20webpages,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>、 <strong>视频-文本数据</strong> （如Instagram/Twitter短视频说明等，文中代号LTIP和VTP）<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>。M3W的构建无需人工标注，纯粹爬取网页，这可以视为引入了 <strong>大量弱监督数据</strong> 。那些网页上的文本并非专门描述图像，但模型会通过上下文学习其中关联。这有点类似伪标签，因为并非每句话都准确描述对应图像，但模型会自己找关系。另外，Flamingo并未使用生成模型来自行生产描述，它依赖真实世界的数据多样性。值得注意的是，Flamingo训练所需的监督非常少，几乎全是弱标注或无标注数据。few-shot能力使它在下游不需要大规模微调数据。因此，Flamingo充分体现了<strong>用海量弱标注数据替代高质量标注</strong>的理念。没有迹象表明Flamingo使用了由其他模型生成的伪标签数据；它更像是把互联网当作最大的标注来源，在文本和图像并存的自然场景中学习。</li>
<li><strong>六大难点应对：</strong>
<ol>
<li><em>模态对齐困难：</em> Flamingo借助<strong>冻结的CLIP</strong>提供良好的图文先验表示，并通过少量参数训练将其输出嵌入语言模型上下文。这意味着视觉和语言模态的大体对齐已经由CLIP和预训练LM保证，Flamingo只需学习 <strong>在具体上下文中关联</strong> 。通过跨注意力层，Flamingo学会在需要时提取视觉token信息用于生成下一个词，从而实现对齐。其few-shot性能表明，训练后模型能够快速对齐新任务的图文语义，这得益于大量多样化训练让对齐泛化良好<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=match%20at%20L423%20Flamingo%20outperforms,does%20lead%20to%20better%20results">lilianweng.github.io</a>。换言之，Flamingo用<strong>数据多样性+强大基础模型</strong>平稳地度过了模态对齐难关。</li>
<li><em>token格式不统一：</em> Flamingo直接在Transformer中处理交替的多模态序列，将图像表示为固定长度token插入序列，这相当于<strong>统一了输入格式</strong><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=signals%2C%20Flamingo%20adopts%20a%20Perceiver,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。虽然图像token不是离散符号，但它们像文本token一样有自己的位置，与前后文本共同组成序列输入Transformer。同时，引入 <code>&lt;image&gt;</code>标记作为占位符，使文本流认识到何处有图像<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=,padding%20to%20%24N%24%20if%20fewer">lilianweng.github.io</a>。这种方案无需对图像进行离散化编码，而是用连续向量表示并通过mask和标记融入序列，实现了格式统一。实验证明，这样模型可以灵活处理任意交替顺序的图文输入，这正是统一输入格式带来的好处。</li>
<li><em>语义粒度不匹配：</em> Flamingo利用CLIP的高层视觉特征（ViT-L/14等）作为输入，这些特征本身具有较丰富的语义信息（CLIP已对齐过标签文本）。再通过Perceiver压缩，Flamingo获得一组紧凑的视觉tokens，每个可能聚合了图像若干部分信息<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=signals%2C%20Flamingo%20adopts%20a%20Perceiver,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>。这会损失一些低层细节，但保留主要语义，匹配语言模型处理的概念粒度。对于非常细的细节，如图像中的文字或小目标，Flamingo如果训练数据涵盖这类任务也能捕捉（但Flamingo主要没专门练OCR类任务，表现可能一般）。总体而言，Flamingo的设计旨在 <strong>抓主要语义</strong> ：用几百个视觉token代表整张图<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=signals%2C%20Flamingo%20adopts%20a%20Perceiver,The%20training%20objective%20is">lilianweng.github.io</a>。语言模型生成注重全局语义和上下文，微观细节在few-shot场景下可能需要提示引导才能关注。不过，通过web数据训练，Flamingo也学习了不少细节（如定位照片里的物体等）。因此，它在语义粒度上采取<strong>以语义为主，细节为辅</strong>的策略，符合few-shot应用的需求。</li>
<li><em>多模态上下文保持：</em> 这是Flamingo最大的强项之一。模型专门设计来处理 <strong>任意长度的交互式多模态上下文</strong> 。通过mask策略，Flamingo可以应对多张图和多段文本交替：保证每段文本只能看最近的图像，从而<strong>按顺序关联图像和文字</strong><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=To%20easily%20handle%20text%20with,of%20images%20in%20the%20context">lilianweng.github.io</a>。这使模型在一个序列中可以包含多轮图文对话——实际上Flamingo天生就是支持图文混合对话的。训练中它看过网页内容的多次图文交替，因此对多模态上下文延续性有经验<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=They%20scraped%2043%20million%20webpages,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>。few-shot推理时，可以先给几个示例（图+问+答），模型就能在<strong>持续的多模态对话</strong>下发挥作用<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=At%20test%20time%2C%20Flamingo%20naturally,context%20contribute%20to%20better%20performance">lilianweng.github.io</a>。这种能力是一般模型不具备的。因此Flamingo很适合多轮对话、讲故事等需要保持上下文的场景。需要注意长序列涉及的内存和计算成本，但Flamingo通过稀疏注意力等优化应对。总的来说，Flamingo在多模态上下文保持方面达到了当时的新高度，真正实现了在Transformer中融合长上下文的多模态信息。</li>
<li><em>训练数据稀缺：</em> Flamingo通过<strong>大规模弱标注数据</strong>和 <strong>多数据源混合</strong> ，在没有显式人工标注的情况下取得了卓越性能<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=They%20scraped%2043%20million%20webpages,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>。它所需的只是网络上已有的大量图文并茂内容，而不需要额外的人工作答或描述数据（除了验证集）。这证明了利用海量的非结构化数据也能训练出强大的多模态模型。few-shot学习的优势在于，模型可以适应新任务而不需要对每个任务都有成千上万标注数据。Flamingo在16个任务上的结果显示，即使这些任务的数据对模型来说是新的，它依然靠few-shot提示达到不错效果<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=match%20at%20L423%20Flamingo%20outperforms,does%20lead%20to%20better%20results">lilianweng.github.io</a>。这极大缓解了对监督数据的需求。因此，Flamingo的方案是 <strong>用预训练+提示学习替代下游数据</strong> 。当然，预训练本身用了43M网页和十亿级对，耗资巨大，但都是低成本获取的数据。可以说，它把收集标注的钱换成了算力钱。一旦模型训练完毕，同样权重可以few-shot解决多个任务，再也不需要逐个任务大量标注了。</li>
<li><em>计算开销高：</em> Flamingo包含一个80B规模的语言模型（如Chinchilla 70B）和一系列新插入的层，总参数量非常高，训练消耗巨大的TPU/GPU资源。这显然是非常高的计算开销。然而，Flamingo通过 <strong>冻结大模型</strong> ，大幅减少了需要更新的参数量<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Similar%20to%20ClipCap%2C%20both%20pretrained,more%20training%20data%20than%20ClipCap">lilianweng.github.io</a>。仅训练新加的几千万参数，使得训练收敛更快、更稳定，同时避免灾难性遗忘。此外，相比从零训练80B多模态模型，这种“夹心”微调的成本要低得多。推理阶段，Flamingo的计算与一个同等大小的LM相当，外加一些跨注意力计算，可以在多卡并行生成。在few-shot时，不需要反复fine-tune，从而节省了针对每个任务微调的算力。因此，对于拥有训练超大模型能力的团队来说，Flamingo的 <strong>性价比反而不错</strong> ：用额外&lt;1B参数的代价，把一个纯语言模型变成了多模态模型。总之，Flamingo依然属于算力投入极高的模型，但在架构上做了取舍，通过参数冻结和高并行设计，把这笔开销控制在可能范围，并用其泛化能力回收了在多个任务上的成本。</li>
</ol>
</li>
</ul>
<p><strong>参考：</strong> Flamingo的论文（Alayrac et al. 2022）可在arXiv获取<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Flamingo%20%28Alayrac%20et%20al,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=They%20scraped%2043%20million%20webpages,including%20ALIGN%2C%20LTIP%20and%20VTP">lilianweng.github.io</a>。其中详述了模型架构和训练数据构成。DeepMind未公开Flamingo的代码，但有社区复现项目（如lucidrains的PyTorch实现）。Lilian Weng的博客对Flamingo进行了通俗讲解<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Flamingo%20%28Alayrac%20et%20al,The%20training%20objective%20is">lilianweng.github.io</a>。Flamingo在Few-shot VQA等任务上的表现促使后续多模态聊天模型（如OpenAI的GPT-4V）采用类似思想。</p>
<h2 id="gpt-4v-openai-2023">GPT-4V (OpenAI, 2023)<a hidden class="anchor" aria-hidden="true" href="#gpt-4v-openai-2023">#</a></h2>
<ul>
<li><strong>整体架构设计：</strong> GPT-4V是GPT-4模型的视觉增强版本，能够接受图像和文本输入，输出文本<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=some%20of%20the%20problems%20with,7">en.wikipedia.org</a>。虽然OpenAI并未公开GPT-4V的具体架构和参数<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=some%20of%20the%20problems%20with,7">en.wikipedia.org</a>，“V”版的实现大致可推测为在GPT-4大型Transformer架构中融合了视觉处理模块。很可能GPT-4V采用了<strong>单一Transformer模型</strong>来同时处理图像和文本：图像通过一个卷积或ViT编码器提取特征，然后以某种形式馈入Transformer。例如，有推测称GPT-4V使用类似Flamingo的方法——一个 <strong>预训练的ViT作为图像编码器</strong> ，将其输出作为额外的输入embedding，通过新添的跨注意力机制注入到原GPT-4的Transformer中<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=,on%20the%20above%20visual%20tokens">lilianweng.github.io</a>。也有可能GPT-4V将图像编码为若干“视觉token”直接拼接到文本token序列中处理（类似BLIP-2/GIT那样）。不管实现细节如何，GPT-4V的架构原则应是 <strong>在不大幅改变GPT-4语言能力的前提下，赋予其视觉输入通路</strong> 。因此，它很可能保留了GPT-4的大部分层和参数，仅在输入嵌入层或中间插入层增加视觉接口，使模型能够在Self-Attention中同时考虑图像和文本信息。作为一个多模态LLM，GPT-4V仍以<strong>Transformer为核心</strong><a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=Microsoft%20Copilot%20.,3%20%5D%3A%202">en.wikipedia.org</a>。</li>
<li><strong>模态对齐方式：</strong> GPT-4V在开发过程中应该经历了 <strong>大量多模态预训练和对齐调优</strong> 。预训练阶段，模型接受图文混合数据，学习以生成下一个token为目标（无论下一个是文字还是需要根据图像产生的文字）。这种训练会驱动模型自动建立图像与文本语义的映射关系。由于GPT-4本身非常强大，GPT-4V可能仅需较少的额外数据就能学会模态对齐。然而OpenAI可能使用了多种辅助手段：包括 <strong>对比损失</strong> （确保图像相关的文本embedding靠近）或者<strong>多模态一致性约束</strong>等，但具体未知。可以肯定的是，GPT-4V经过了 <strong>强化学习人类反馈（RLHF）<strong>的对齐环节<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=Microsoft%20Copilot%20.,3%20%5D%3A%202">en.wikipedia.org</a>：人工反馈不仅针对文本回答质量，也包括对视觉理解正确性的评价。这种人工调教确保模型在视觉问答中对齐人类期望。例如，人类监督会奖赏模型正确描述图像、严惩胡编乱造，从而促使模型更好地学习视觉-语言对齐关系。总的说来，GPT-4V的模态对齐来自</strong>两部分</strong> ：一是<strong>模型大规模多模态训练</strong>的自我监督对齐（让模型预测正确的多模态输出而被迫对齐），二是 <strong>对抗性和人类反馈微调</strong> （纠正不准确的对齐，如图像内容误解）以达成人类满意的对齐度。最终结果是GPT-4V在各种视觉描述、问答任务上表现出强大的理解力和对齐度，甚至可以准确解释复杂图片、阅读图中文字并将之融入答案——这说明其视觉语义已与语言很好地结合。</li>
<li><strong>输入 token 表达统一：</strong> 从用户接口看，GPT-4V接受的输入是图像（像素形式）和文本，自然语言以token形式进入，图像则以文件上传形式进入API。但在模型内部，必须将图像转化为与文本token可交互的表示。根据业界经验，GPT-4V可能采用两种方式之一：其一， <strong>离线视觉编码+前缀嵌入</strong> 。即通过一个CNN/ViT将图像转成一串embedding，然后在Transformer输入端用特殊标记占位，将这些embedding作为“视觉前缀”插入。这类似BLIP-2和GIT的策略，让视觉embedding在Transformer序列中，与后续文本共同处理。其二， <strong>中途插入跨模态层</strong> ，即模型运行过程中，当需要处理图像时，调用一个微型视觉Transformer将图像转成键值供专门的跨注意力层使用（类似Flamingo做法）。无论哪种，最终效果是 <strong>模型看到了一系列向量表示，部分来自图像，部分来自文本</strong> ，并通过统一的注意力机制处理它们。因此，GPT-4V实现了 <strong>视觉信息向等价文本向量的转换</strong> ：这些向量可能没有离散token对应，但Transformer无差别对待它们，把它们当作上下文的一部分。OpenAI也定义了GPT-4V的token计费方式：图像按一定像素大小折算成若干token成本<a href="https://platform.openai.com/docs/guides/images-vision#:~:text=GPT,image%20is%20determined%20as">platform.openai.com</a>, 这暗示他们内部将图像信息映射为了固定数量的embedding，相当于一些token。这与输入统一表示的思路一致。此外，GPT-4V支持在对话中多次输入图像，模型通过聊天记忆可以连续参照多幅图像。这种灵活性也表明输入的图像已经嵌入Transformer上下文，模型可以在内部“记住”它，就像记住前文一样。因此可以说，GPT-4V在实现上做到了图像和文本输入的格式统一，至少从Transformer的视角来看是一致的序列信息流。</li>
<li><strong>损失函数与训练策略：</strong> GPT-4V的训练包括两个阶段： <strong>预训练（Self-Supervised）<strong>和</strong>对齐微调（Supervised + RLHF）</strong> <a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=Microsoft%20Copilot%20.,3%20%5D%3A%202">en.wikipedia.org</a>。预训练损失是标准的 <strong>因果语言建模损失</strong> ，扩展到多模态场景，即给定之前的文本token和图像embedding，预测下一个文本token<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=Microsoft%20Copilot%20.,3%20%5D%3A%202">en.wikipedia.org</a>。这一步可能使用了大量图文对数据和合成任务数据，让模型具备基础视觉理解和描述能力。接下来，OpenAI对GPT-4V进行了 <strong>监督微调</strong> ，包括让模型跟随指令、可靠回答问题、避免不当输出等。这一步使用有人类标注答案的图像问答数据和对话数据，损失为交叉熵对标参考答案。最后还有 <strong>RLHF阶段</strong> ，通过人类反馈训练一个奖励模型，对模型回答质量评分，再用策略梯度或近端策略优化调整模型参数，使之产生更符合人类期望的回答<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=Microsoft%20Copilot%20.,3%20%5D%3A%202">en.wikipedia.org</a>。在RLHF中，人类会比较两版对同一图像问题的回答优劣，以训练奖励模型。这确保GPT-4V不仅正确，还要解释清楚、详尽并遵守安全守则。训练策略方面，GPT-4V很可能采用了 <strong>混合训练</strong> ：例如让模型在大约80%时间学习纯文本任务（以不损害其语言能力），20%时间学习带图像的任务，以逐渐融合视觉能力而不遗忘语言能力。这符合OpenAI对GPT-4统一多模态模型的描述，称其在巨量算力下进行预测性能的平稳扩展<a href="https://arxiv.org/abs/2303.08774#:~:text=simulated%20bar%20exam%20with%20a,4">arxiv.org</a>。因此，GPT-4V训练过程相当复杂，但核心损失仍是让模型预测正确的输出序列（文本），只是过程中施加了各种人类知识和偏好约束。</li>
<li><strong>使用的数据集及伪标签：</strong> OpenAI未公开GPT-4V使用的数据细节<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=some%20of%20the%20problems%20with,7">en.wikipedia.org</a>。推测其预训练数据包含 <strong>互联网爬取的大规模图文对</strong> （如可能使用LAION、ALIGN数据，或者自建的10亿级别数据集），涵盖多样领域。还可能有 <strong>OCR场景数据</strong> （扫描文档及文本）、 <strong>图表数据</strong> 、<strong>网页截图和说明</strong>等，因为GPT-4V表现出识别文档、读表格、看图编程等广泛能力。监督微调阶段，他们可能编纂了一个多模态指令数据集，类似InstructGPT，但带图像：比如让标注员提供图像并提问，写出高质量参考答案。这部分数据可能较小（数万到数十万对），但涵盖不同任务（描述、定位、分类、推理等）。此外，社区猜测OpenAI可能利用GPT-4自身生成了一部分训练数据（即“判师”策略），但官方未证实。相较于开源做法（如LLaVA用GPT-4生成对COCO的问答作为训练集），OpenAI有资源直接人工标注，所以GPT-4V的关键数据更可能是<strong>人类精标</strong>而非伪标签。唯一确定的是，GPT-4V <strong>融合了多源数据</strong> ：文本数据（与GPT-4共享）、图像+文本数据，以及人类反馈数据<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=Microsoft%20Copilot%20.,3%20%5D%3A%202">en.wikipedia.org</a>。这种多阶段、多样本训练使模型具有极其广泛的视觉语言知识。基于效果推断，GPT-4V肯定见过各种真实世界图像场景，包括照片、插画、截图、漫画等，也了解了不少常见视觉任务的问答格式。这正是其在未知图片上一样游刃有余的原因。</li>
<li><strong>六大难点应对：</strong>
<ol>
<li><em>模态对齐困难：</em> GPT-4V可被视为目前模态对齐最成功的例子之一。OpenAI通过 <strong>统一模型训练+精细对齐调优</strong> ，使得GPT-4V在视觉和语言之间建立了深度联系。模型能将图像中的元素转换成文字描述或用于推理，说明跨模态概念高度统一。例如，它可以看图进行幽默理解、数学分析，这意味着不仅低层语义对齐，高层推理也对齐了。相比CLIP等需下游配对的模型，GPT-4V内部产生了 <strong>端到端的对齐</strong> ：一幅图像输入，其内部生成的表征能直接触发与之对应的知识和词汇。RLHF过程中，人类引导模型关注正确区域、忠实描述，进一步强化了 <strong>精准对齐</strong> （比如不编造不存在的物体）。因此，对以前悬而未决的模态对齐难题，GPT-4V以<strong>超级规模训练+人类校正</strong>的方法给出了答案：几乎可以对任意复杂图文实现正确对齐。</li>
<li><em>token格式不统一：</em> GPT-4V在接口上依然区分图像上传和文本输入，但在模型内部已经实现了格式统一。如上推测，图像被编码成embedding插入Transformer，相当于模型看到的是统一的向量序列，其中没有本质区别区分来源。OpenAI甚至提供了一个token折算方法来计价图像，这暗示他们定义了一个统一本质的token空间包含图像<a href="https://platform.openai.com/docs/guides/images-vision#:~:text=GPT,image%20is%20determined%20as">platform.openai.com</a>。GPT-4V也许没有明确的视觉词表，但通过扩展embedding层，模型接受了一批额外的向量（视觉patch的embedding或者Resampler输出）作为“视觉token”。Transformer处理自注意力时，对这些向量和普通文字embedding执行相同的矩阵计算。因此可以说，GPT-4V在实现上 <strong>消除了模态输入格式差异</strong> ，达到了真正的多模态Transformer形态。这也是为什么用户能对它自由提问“图中有什么字”或“这个人是谁”，模型像读文字一样“读”图。这种统一在OpenAI的技术报告中虽未明说，但从其行为特征和架构趋势能推断出来。</li>
<li><em>语义粒度不匹配：</em> GPT-4V展现出处理各种粒度语义的能力，从辨认具体细节（如图中小字、微小物品）到理解抽象场景（如人物关系、场景氛围）。这表明模型采用了<strong>高分辨率的视觉表征</strong>和 <strong>强大的分层理解</strong> 。一种可能方式是多级特征：基础ViT提供细粒度patch特征，然后Transformer多层逐步汇总，像人类视觉系统一样先看细节再理解整体。此外，OpenAI可能特意在训练集中加入了一些需要细粒度识别的任务（OCR、细分类），迫使模型关注局部细节。同时，大语言模型部分拥有强大的上下文推理能力，能从细节推导整体意义。这两方面结合，使GPT-4V能较好地弥合视觉像素级信息与语言概念级信息之间的鸿沟。例如，对一张复杂的漫画，模型既能识别面部表情这样的细节，又能归纳出搞笑之处这样的高层语义。可以认为，GPT-4V通过<strong>多尺度注意力</strong>解决了语义粒度不匹配：低层注意力抓取细节，高层Self-Attention整合语义，并在输出时选择恰当的语言粒度表述。</li>
<li><em>多模态上下文保持：</em> GPT-4V本质是ChatGPT的扩展版，因而天然具备对话上下文记忆能力。用户可以在一次对话中连续上传多张相关图像并配以提问，模型能够参考<strong>对话历史</strong>和<strong>所有已提供的图像信息</strong>来回答。比如，用户先上传一家谱照片问“这是谁？”，再上传另一张照片问“他和前面那人是什么关系？”，GPT-4V可以基于前文记忆，将两图人物联系起来回答。这说明模型内部对多轮图像和文本都建立了表示，并通过对话状态维持了跨轮次的多模态上下文。OpenAI很可能在微调阶段加入了这类多轮、多图对话的数据，使模型学会使用 <code>&lt;image_n&gt;</code>引用之前的图像。在推理实现上，ChatGPT系统会给每张图一个编号，将其embedding保存在对话状态，后续提问如果引用，模型就会重新利用。这种机制虽未明示，但从体验上看GPT-4V确实支持相当长的多模态对话。因此，它在<strong>多模态上下文保持</strong>上达到了目前最强水平：既能处理长文本对话，又能记忆多张图像的内容并综合推理。这一能力是之前模型（如Flamingo）few-shot模拟的更高级形式，因为GPT-4V经过明确的对话格式训练和强化，对话管理更加可靠。</li>
<li><em>训练数据稀缺：</em> 对于普通研究者来说，高质量大规模多模态数据稀缺是难点，但OpenAI通过自身积累和合作，可能获取了十分丰富的数据。GPT-4V可以被视为<strong>以数据和算力硬碰硬</strong>解决问题的典型。它用规模（模型、数据）换性能，不太依赖小技巧。值得注意的是，虽然OpenAI未公布数据，但推测很大一部分来自现有开放数据（LAION、COCO、Visual Genome等）以及定制采集的数据（比如购买版权图片、内部生成的数据等）。此外，人类标注在对齐阶段起了决定性作用，这是另一种形式的数据： <strong>专家知识数据</strong> 。OpenAI投入了大量人力去微调模型的行为，使得最终模型的能力远超仅靠原始数据训练的版本。这相当于通过<strong>人类反馈</strong>来弥补数据集不足之处——对于一些模型自己难以领会的任务，人类示范和偏好指导提供了额外信息。这种做法开创了用少量高质量人工数据引导海量机器学习的范式。简而言之，GPT-4V应对数据稀缺的方案在于： <strong>一手抓“大”（扩展预训练数据广度），一手抓“精”（收集人类高质量指令/反馈数据）</strong> 。两者结合，使模型既见多识广，又合乎人意。</li>
<li><em>计算开销高：</em> GPT-4V毫无疑问是在极其庞大的算力支持下训练的。传闻GPT-4基础模型参数在数千亿以上，训练消耗数千万美元级别GPU成本。加入视觉模态后，训练复杂度进一步提高。不过OpenAI通过一些工程手段控制了成本：据报道，他们使用了<strong>训练性能预测</strong>方法，在较小模型上估计大模型表现，从而少走弯路<a href="https://arxiv.org/abs/2303.08774#:~:text=post,4">arxiv.org</a>。另外采用混合精度、模型并行、流水线并行等技术提高效率。模型结构上，使用统一Transformer而非多分支，可以充分利用成熟的Transformer优化器和加速器。这些都帮助缓解了计算压力。在推理阶段，GPT-4V同样需要强大算力支持，但OpenAI通过托管API方式，用优化过的推理服务器提供服务，单次调用成本对于终端用户来说隐藏在付费中。可以说，GPT-4V目前的计算开销不是一般机构能承担的，但它也展示了<strong>高投入带来高性能</strong>的路线。随着硬件进步和可能的压缩蒸馏技术，未来GPT-4V的成本有望下降。就当前而言，OpenAI通过自身资源攻克了这一难题，对外提供一个无需本地计算就能调用的强大多模态模型，这在客观上绕过了许多用户对算力的需求。</li>
</ol>
</li>
</ul>
<p><strong>参考：</strong> GPT-4 的技术报告<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=Microsoft%20Copilot%20.,3%20%5D%3A%202">en.wikipedia.org</a><a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=some%20of%20the%20problems%20with,7">en.wikipedia.org</a>提到其多模态能力和训练方法，但未披露细节。维基百科也指出OpenAI未公布GPT-4的架构和数据<a href="https://en.wikipedia.org/wiki/GPT-4#:~:text=some%20of%20the%20problems%20with,7">en.wikipedia.org</a>。尽管如此，我们可以参考类似的研究（如Google PaLM-E、DeepMind Flamingo<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#:~:text=Flamingo%20%28Alayrac%20et%20al,is%20an%20autoregressive%2C%20NLL%20loss">lilianweng.github.io</a>）来推测GPT-4V的设计思路。OpenAI的GPT-4发布博客<a href="https://openai.com/index/gpt-4-research/#:~:text=GPT,">openai.com</a>和官方FAQ也提供了一些线索（如图像计费折算）。目前没有公开的GPT-4V代码或模型，但已有一些开源项目（如MiniGPT-4、LLaVA等）尝试复现其部分功能，可供了解实现原理。总的来说，GPT-4V代表了当前多模态模型技术的前沿，将视觉和语言能力融合达到了前所未有的高度。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
