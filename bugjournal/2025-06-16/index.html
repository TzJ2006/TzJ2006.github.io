<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-06-16 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, Paper Summary">
<meta name="description" content="Avoid Catastrophy forget">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-06-16/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-06-16/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-06-16/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-06-16">
  <meta property="og:description" content="Avoid Catastrophy forget">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-16T14:14:45+08:00">
    <meta property="article:modified_time" content="2025-06-16T14:14:45+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-06-16">
<meta name="twitter:description" content="Avoid Catastrophy forget">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-06-16",
      "item": "https://tzj2006.github.io/bugjournal/2025-06-16/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-06-16",
  "name": "Bug Journal 2025-06-16",
  "description": "Avoid Catastrophy forget",
  "keywords": [
    "Bug Journal", "Paper Summary"
  ],
  "articleBody": "持续学习中避免灾难性遗忘：具身智能领域的研究进展综述 引言与问题背景 持续学习（Continual Learning，也称终身学习）指模型在数据分布和学习目标不断变化的情境下，能够连续学习新任务且不忘记已有知识的能力arxiv.orgar5iv.org。传统深度学习假设训练数据 i.i.d.且一次性可用，这在现实具身智能（如机器人、自主系统）中往往不成立ar5iv.orgar5iv.org。其中最大挑战之一是灾难性遗忘（catastrophic forgetting） ，即模型在顺序学习多个任务时，新知识的获取会导致旧知识被快速、大幅遗忘en.wikipedia.org。这一现象最早由 McCloskey 和 Cohen 于1989年在联结主义神经网络中发现arxiv.org，体现了所谓 稳定-可塑性权衡 ：模型既要对新信息足够可塑（plasticity），又要对既有知识保持稳定（stability）en.wikipedia.org。与生物神经系统相比，人工神经网络在顺序学习时更容易“灾难性”遗忘过去经验，而人类和动物通常表现出渐进、有选择的遗忘cell.comcell.com。如何在保持模型泛化能力的同时避免旧知识被破坏，成为持续学习研究的核心问题cell.com。为此，大量研究围绕不同技术路线展开，包括利用附加数据回放、正则化约束、动态模块化架构和外部记忆等方法来缓解遗忘arxiv.org。本文将按时间脉络梳理持续学习避免遗忘的关键进展，重点聚焦具身智能场景的应用，并比较不同方法在这些场景中的优劣差异。\n早期探索：灾难性遗忘的提出与初步对策 灾难性遗忘现象的提出（1980s–1990s）: 神经网络中的灾难性遗忘问题由 McCloskey 和 Cohen (1989)en.wikipedia.org以及 Ratcliff (1990) 首次严谨描述en.wikipedia.org。他们发现序列训练两个任务时，后学任务会显著干扰先前任务的记忆en.wikipedia.orgen.wikipedia.org。这一问题可视为Grossberg提出的稳定-可塑性两难的极端表现en.wikipedia.org。此后，研究者逐步意识到，若要让人工智能具备人类般持续学习的能力，必须解决神经网络的遗忘灾难。1990年代中期，一些学者开始探索初步对策。例如，Robins (1995) 提出了重演/伪重演 (rehearsal/pseudorehearsal) 方法，即在学习新任务时将旧任务样本或由模型生成的“伪样本”一并训练，以巩固旧知识ar5iv.org。这种方法模拟了生物大脑在睡眠中重放记忆的过程，缓解了遗忘问题，被视为后续生成式回放方法的雏形ar5iv.orgar5iv.org。与此同时，Grossberg 等人在稳定-塑性理论指导下发展**自适应共振理论 (ART)**网络，通过网络结构与记忆单元的设计减少旧记忆被覆盖的风险。这一时期的工作揭示了灾难性遗忘的严重性，并奠定了若干基本思想：通过保留过去经验（真实或模拟）或限制参数剧烈更新来保护已有知识cell.com。然而，由于当时神经网络规模和应用场景有限，这些早期方法未形成统一框架，但为后续深度学习时代的持续学习研究提供了宝贵思路。\n深度学习时代兴起前的持续学习理念: 在2000年前后，机器学习领域也出现了一些“终身学习”思想，例如 Thrun 和 Mitchell 等人在机器人领域讨论让机器人不断积累知识、自主适应新环境的算法。但受限于模型能力，这些工作多偏向理论构想或特定场景下的增量学习算法。值得一提的是，Silver et al. (2013) 等人在认知科学领域提出**“永不停歇学习”(Never-Ending Learning)的愿景，旨在构建能无限获取新知识的AI系统。这些理念上的探索进一步强调了持续学习的重要性，但真正有效的算法突破还要等待深度学习的成熟。进入2010年代，深度神经网络在图像、语音等任务上取得突破，但其遗忘现象依然明显**。Goodfellow et al. (2014) 的实证研究表明，即使现代深度网络在顺序学习多任务（如不同MNIST变换）时，仍然发生严重的性能遗忘，他们尝试用Dropout等正则策略略微缓解遗忘cs.uic.edu。这一阶段的研究重新量化了深度模型遗忘的程度，引发了学界对持续学习的关注，也为随后的关键方法发明做好了铺垫。\n深度学习时代的方法演进（2016–2020） 进入深度学习时代后，大量持续学习算法被提出。总体而言，这些方法可分为以下几类： 参数正则化 、 经验回放 （或生成回放）、 参数隔离/模块化 、以及外部记忆等arxiv.org。各类方法都有经典代表工作，我们按时间演进介绍主要方法及其贡献。\n参数正则化方法 Learning without Forgetting (LwF, 2016): Li 和 Hoiem 提出“学习不遗忘”算法arxiv.org。他们假设只能获取新任务数据，无法重温旧任务数据的现实情况，通过让模型在学习新任务时蒸馏(distillation)旧任务模型的输出分布来正则化模型参数变化arxiv.org。具体而言，在训练新任务时对旧任务模型的预测进行保持，使新模型尽可能产生与旧模型相似的输出，从而保护原有能力。LwF是知识蒸馏用于持续学习的开创性工作，实现了只用新任务数据也能较好保留旧任务性能arxiv.org。该方法在2016年ECCV发表，此后蒸馏正则化成为持续学习的重要手段之一。\nElastic Weight Consolidation (EWC, 2017): Kirkpatrick 等人（DeepMind）在PNAS 2017发表了著名的EWC算法arxiv.org。EWC通过近似计算每个参数对旧任务的重要程度（利用费舍尔信息矩阵对参数敏感度进行估计），在学习新任务的损失中添加项，惩罚对重要参数的大幅更新arxiv.org。直观来说，模型会“放慢”那些对旧任务重要参数的学习速率，以免遗忘旧知识arxiv.org。EWC在经典分类任务（如顺序MNIST）和强化学习任务（顺序Atari游戏）上验证了有效性arxiv.org。作为持续学习领域里程碑，EWC证明通过软约束参数更新，可以在一定程度上同时保持先前任务性能和新任务学习能力arxiv.org。其思路后来衍生出许多变体，例如利用更精细近似二阶信息的方法等ar5iv.org。\nSynaptic Intelligence (SI, 2017): Zenke 等人在ICML 2017提出了另一本质类似EWC的正则化方法SIarxiv.org。不同于EWC预先计算参数重要度，SI在训练过程中实时累积每个参数对损失的贡献度，结束当前任务时将其视为该参数的重要性arxiv.org。这种“智能突触”机制借鉴了生物突触强度调节的复杂性，每个突触（参数）在多个任务中累积“任务相关信息”，新任务来时利用这些信息调整学习率arxiv.org。SI在多个连续分类任务上显著降低了遗忘，同时保持了计算高效arxiv.org。与EWC相比，SI无需存储旧任务样本，同样不增加模型容量，对于资源受限的设备具有吸引力arxiv.org。但正如多数正则化方法的局限，当任务数量增多或差异较大时，单纯靠增加惩罚会导致模型学习新任务受限，需要在稳定与塑性间权衡ar5iv.orgar5iv.org。\n其他正则化进展: 除上述，2017年前后还出现了多种参数正则化方案。例如 Li et al. (2017) 的增量时刻匹配（IMM）方法通过匹配参数分布的方式融合旧新任务模型；Aljundi et al. (2018) 提出的MAS（Memory Aware Synapses）利用输出敏感度评估参数重要性。这些方法本质均为在损失函数中添加某种形式的正则项，使模型避免过度调整关键参数以保留旧知识ar5iv.orgar5iv.org。正则化方法的优点是 无需存储旧样本，内存占用低 ，易于在嵌入式/机器人等设备上实现ar5iv.org。它们的不足在于当任务间差异巨大、参数冲突严重时效果受限，而且累积过多任务后模型可能进入“过度稳定”状态难以学习新任务ar5iv.org。在具身智能场景中，由于设备算力和存储有限，正则化方法仍是常用选择之一。例如 EWC 被用于机器人连续控制任务以保护低层政策参数不被遗忘arxiv.org。但如果机器人遇到全新领域任务，正则化可能限制其适应新技能的能力，这是后续方法力图解决的问题。\n经验重放与生成式回放方法 显式经验重放 (Experience Replay): 针对持续学习，最直接的思路是 在学习新任务时重温部分旧任务的数据 ，仿佛让模型“复习”以前的知识ar5iv.org。Rebuffi 等人在CVPR 2017提出的 iCaRL 方法将这一思想与深度学习结合arxiv.org。iCaRL在每学新类别时， 保存每个旧类别少量代表样本（记忆库） ，训练时将这些旧样本与新数据一起用于更新模型，并对模型输出进行知识蒸馏以防决策边界偏移arxiv.org。通过同时学习分类新类别和回顾旧类别，iCaRL实现了深度网络在长时间增量学习许多类时，比仅新数据训练的策略遗忘显著减少arxiv.org。iCaRL开创了样本记忆回放+蒸馏结合的范式。之后许多增量学习方法沿用了“小样本记忆”思路，如 Hou et al. (2019) 的UCIR、Wu et al. (2019) 的BiC等，都在如何精选和高效利用少量旧样本上下功夫。经验重放策略也直接应用于强化学习领域——在非平稳环境中，智能体可将过去经历的轨迹保存一部分，在策略更新时混入重放，以避免策略完全偏离先前成功经验。这与深度Q网络（DQN）的经验回放缓冲理念一脉相承，只是这里目的是防遗忘而不仅是破除相关性。在机器人学习中，经验重放意味着机器人在学习新技能时定期练习已掌握的技能（通过模拟旧技能的传感器输入等），这在实践中提高了多技能机器人系统的鲁棒性。\nGradient Episodic Memory (GEM, 2017): Lopez-Paz \u0026 Ranzato 在 NeurIPS 2017 提出的 GEM 则进一步创新了重放的使用方式arxiv.org。与直接将旧样本混入训练不同，GEM把少量旧任务样本存入 episodic memory ，在每次参数更新时，通过约束新梯度与旧样本梯度的内积为非负，确保新任务训练不会增加旧任务损失arxiv.org。这种基于优化约束的方法保证了模型对记忆样本性能不下降，实现了一定的“向后迁移”能力：在学习新任务的同时还有可能改进旧任务表现arxiv.org。GEM开创了利用回放样本的梯度信息指导优化的思路，其后续简化版A-GEM (Chaudhry et al. 2018)降低了计算成本。对于具身智能而言，GEM这类方法的优势在于即使少量记忆也能通过优化约束起效，而且不需要明确任务边界（可以对任意过去经验施加约束）。不过其劣势是需要实时计算并存储梯度，复杂度较高，且仍需维护一个小型记忆库。\n生成式回放 (Generative Replay): 当直接存储原始旧样本受限时，另一策略是训练生成模型来产生日前学过的数据，从而实现回放。Shin et al. 在 NeurIPS 2017 提出的 Deep Generative Replay (DGR) 是该思路的里程碑arxiv.org。DGR构建了一个生成模型（如GAN或VAE）作为“记忆仿真器”，在学习新任务时利用生成模型产生日前各任务的合成样本，并与新数据混合训练Solver模型arxiv.org。在每完成一个任务后，Solver的参数固定，然后训练生成模型去拟合更新后的Solver分布，以便下次产生更新的数据分布ar5iv.org。这种双模型协同框架受到大脑“海马-新皮层”互作机制的启发，即利用快速变化的“海马体”生成回忆来训练慢更新的“皮层”网络arxiv.org。DGR实验证明，即使不保存任何真实旧样本，模型仍能通过生成模拟数据达到与有存储时相近的效果ar5iv.orgarxiv.org。生成回放的优势是 不直接占用存储真实数据 ，在隐私敏感或内存极小的设备上尤为有用arxiv.org。其缺点在于生成模型本身也面临持续学习问题（如何不忘记早期的数据分布），且训练开销较大ar5iv.org。后续不少工作改进了生成回放，如 Wu et al. (2018) 将GAN与变分特征结合（MeRGAN），Rios et al. (2018) 用生成对抗网络生成特征而非像素，提高效率等ar5iv.orgar5iv.org。生成回放还被应用到强化学习的状态生成中，例如 Caselles-Dupré et al. (2019) 提出的自触发生成回放（S-TRIGGER）用于连续学习环境状态表示ar5iv.org。总的来看，回放类方法（包括经验重放和生成回放）在各种基准上往往表现突出，被认为是目前抗遗忘最有效的范式之一。然而它们对存储或生成能力有要求，在具身智能中需权衡内存/算力和性能：对于机器人等设备，存储少量关键经验（如图像片段、关键帧）进行回放在实践中较常用，而实时训练复杂生成模型则相对少见。\n模块化架构与参数隔离方法 Progressive Neural Networks (PNN, 2016): Rusu 等人提出的渐进神经网络是持续学习的架构派代表arxiv.org。PNN在每遇到新任务时 冻结已有网络 ，并侧旁新增一组“列”网络用于学习新任务arxiv.org。同时，通过旁路连接让新任务列能够利用之前各列学到的特征（实现知识迁移）arxiv.org。这种架构确保旧任务的参数永不修改，从而彻底避免遗忘arxiv.org；而新增模块可以专门学习新任务，有充足的模型容量。PNN在Atari游戏和3D迷宫导航等一系列强化学习任务上取得优于微调的成绩，并显示出显著的前向迁移能力arxiv.org。其缺点也很明显：每增加一个任务网络规模就线性增长，在任务数很多时不切实际arxiv.org。尽管如此，PNN证明了模块隔离在避免遗忘上的有效性，许多后续方法受此启发引入可扩展或可选择激活的架构。\nPathNet (2017) 与 PackNet (2018): 为了缓解PNN网络爆炸的问题，Fernando 等人 (2017) 提出的 PathNet 利用进化算法在固定网络中为每个任务选择一条互不干扰的子网络路径，相当于在共享参数的前提下实现参数隔离。Mallya 和 Lazebnik (2018) 则提出 PackNet ，通过反复剪枝和重训练来为新任务腾出参数空间arxiv.org。具体来说，PackNet先训练初始任务模型，然后剪除一定比例不重要的参数（权重置零但保留位置），学习第二个任务时仅利用空闲参数；如此迭代，将多个任务“打包”进单个网络中arxiv.org。实验表明，在ImageNet等大型数据上，PackNet可在一个VGG模型中连续容纳多个细粒度分类任务，性能接近于单独训练arxiv.org。PackNet无需存储旧数据，也不引入新参数，因此相比PNN更高效arxiv.org。但PackNet需要预先设定剪枝比例，且剪枝过多可能损害旧任务性能，过少则限制新任务空间。后来一些变体如 “Piggyback” (Mallya, 2018) 则改为学习任务特定的掩码，更灵活地实现参数复用。总体而言，参数隔离类方法（含动态扩张和网络剪枝）通过结构上的硬约束避免了遗忘，其优势是旧知识完全保留、无干扰arxiv.org。在机器人等具身智能中，如果任务集是离散且有限的，这类方法可考虑使用。例如在多任务机器人控制中，可为每个任务分配专属网络模块或参数子集，新任务加入时扩展网络并冻结旧模块，从而保持以往技能arxiv.org。然而，在开放环境下任务可能连续涌现且无法预知数量，单纯无限扩展网络不切实际。因此近期一些工作尝试结合元学习或 条件网络 ，自动决定何时复用旧参数、何时增加新参数，以兼顾模型规模和遗忘防护。比如 Serra et al. (2018) 提出的 HAT 方法对每层参数学习可训练门控，通过门控向量的稀疏化实现在相同网络中隔离不同任务的激活区域，从而在不显著增加参数的情况下减少干扰。\n脑启发的双记忆体系: 值得注意的是，一些方法从神经科学的双重内存理论汲取灵感，将快速学习模块和稳定长时模块结合起来应对遗忘。例如 Kemker 和 Kanan (2018) 提出的 FearNet 模型采用“大脑 海马-新皮层 ”的架构arxiv.org：用一个类似海马体的小网络专门快速学习当前任务，并在适当时机（模拟睡眠）将新知识整合（consolidate）到另一个类似皮层的大网络中做长期存储arxiv.org。同时还有一个类似杏仁核的模块，根据输入判断应该用哪套记忆系统回答arxiv.org。FearNet不需存储旧样本，依赖生成式机制回忆旧类数据，达到与iCaRL相当的性能arxiv.org。这类方法实质上属于架构+回放的混合策略（因为短期网本身可看作一种内生记忆生成器）。双记忆策略对具身智能有自然的意义：机器人或代理可以配置一个“小而快”的在线学习器来及时适应新变化，同时定期将知识固化到“大而稳”的长期模型中，从而两全其美。不过如何确定巩固频率以及双网络的容量匹配仍在探索中。\n方法对比与小结 不同持续学习方法各有优劣，在具身智能场景下需要平衡选择ar5iv.org。正则化方法不需保存样本、开销低，适合嵌入式设备在线更新，但在任务变化剧烈时可能束缚新知识获取ar5iv.org。回放方法往往效果最佳，即便少量样本重放也能显著降低遗忘arxiv.org；对于机器人这种可反复与环境交互的场景，还可通过自主采样过去环境状态进行重演。然而存储真实数据可能受限于隐私或容量，而训练生成模型又对计算资源有较高要求arxiv.org。模块化/参数隔离方法彻底杜绝了遗忘，在多任务机器人系统（任务有限且可拆分）中很有价值，但在开放任务中扩展性受限arxiv.org。外部记忆和双重内存策略提供了一种折中：通过引入专门的记忆模块，模型可以在不反复调整主要网络权重的情况下查询和更新知识。例如在多人对话交互机器人中，引入一个可读写的记忆单元存储历史对话要点，有助于长期一致的对话理解。但引入记忆也增大了系统复杂度，需要设计高效的检索和写入机制。\n此外，许多先进方法不再局限于单一策略，而是混合多种机制以取长补短ar5iv.org。例如 Schwarz 等人 (2018) 提出的 Progress \u0026 Compress 框架将动态架构与蒸馏结合：使用Progressive Network扩展新任务列，然后通过蒸馏将新列知识压缩回主干网络，从而既避免遗忘又控制模型规模arxiv.org。再如 von Oswald et al. (2019) 的 MER 方法将元学习思想融入记忆重放，通过元训练提高模型表示对新旧任务的解耦，从而辅助减少干扰。这些综合方法在近年不断涌现，说明持续学习领域正朝着多策略融合与自动适应方向发展。\n具身智能场景中的持续学习应用 具身智能领域（如机器人、自主车辆、智能代理）为持续学习提供了最实际也最具挑战的用武之地arxiv.org。与静态数据集不同，具身智能体在物理世界中连续感知和行动，环境非平稳且任务边界往往不明确ar5iv.orgar5iv.org。以下我们重点考察持续学习方法在几个具身场景的应用进展：\n机器人视觉与物体识别： 服务机器人需要在不断变化的环境中识别新对象、适应新场景，这正是开放域的持续学习问题。为评测算法，2020年提出了OpenLORIS-Object机器人视觉数据集，包含随时间推移环境光照、视角、物距等变化的数据流sciencedirect.com。在该数据集上，Lomonaco 等人组织了持续学习挑战，促进了算法在真实机器人感知条件下的比较。一系列方法被测试：如 iCaRL 的小样本存储结合知识蒸馏策略在这种增量物体识别中取得稳健表现；又如 IROS 2020 的 Latent Replay 方法，Pellegrini et al. 提出只在特征空间保存和重放旧数据link.springer.com。具体而言，机器人摄像头图像经卷积网络得到中间表示，将这些低维激活缓存代替原始高维图像，可大幅减少存储并实现实时回放ieeexplore.ieee.org。实验表明，在OpenLORIS这种持续视觉任务中，Latent Replay比直接存图像几乎不降性能，却更高效满足机器人实时性需求ieeexplore.ieee.org。另一最新进展是 Hajizada et al. (2024) 提出的 Continually Learning Prototypes (CLP) 算法arxiv.orgarxiv.org。CLP针对机器人少样本在线学习和开放世界场景设计：它采用原型向量表征每类知识，并通过元可塑性机制动态调整每个原型的学习速率来平衡新旧知识稳定性arxiv.orgarxiv.org。同时CLP具备新类别自我检测与无监督学习能力（即机器人遇到未知物体时可判断新类别并自主创建原型学习）arxiv.orgarxiv.org。重要的是，CLP不使用任何显式回放数据且兼容神经形态芯片，实现了超低能耗下的持续学习arxiv.orgarxiv.org。这对于内存和电池有限的移动机器人具有现实意义。总的来看，在机器人视觉领域，混合使用 小样本记忆 、 特征回放 、适应性学习率等技术已取得显著效果，使机器人能逐步扩展认知能力且遗忘受控。\n人机交互与多模态学习： 具身智能体常涉及多模态感知（视觉、听觉、语言）和人机交互，这带来了持续学习的新课题。例如社交机器人需要持续学习新的对话内容、新的手势动作等。NLP领域已有针对增量学习的综述（如 Biesialska et al. , 2020link.springer.com），其中提到自然语言处理任务在持续学习中面临词汇和语义随时间演变的问题。一些方法通过动态扩充词典或嵌入空间缓解了“遗忘”早期语义的现象。对于多模态交互，Kulkarni et al. (2019) 提出在对话系统中使用弹性权重约束来保留模型早期对话技能，同时新增新领域对话意图。交互学习中一个重要方面是 用户在环（human-in-the-loop） ：机器人可通过用户反馈实时修正知识。近期有工作探索 交互式持续学习 ，如 Hazifa et al. (2022) 结合神经形态计算，利用片上在线学习快速吸收用户教授的新知识，同时通过正则保护已有知识dl.acm.org。虽然具体算法仍在早期，但这些尝试指出了方向——未来的具身智能体应能通过持续人机交互 自我进化 ，并且做到“学而不忘”。\n连续控制与强化学习： 在自主驾驶、机器人控制等连续决策场景，持续学习同样关键。例如自动驾驶车辆遇到新道路场景，需要学习新策略而不忘记基本驾驶技能。Shaheen et al. (2022) 的综述link.springer.comlink.springer.com总结了三类自主系统（无人车、无人机和移动机器人）中的持续学习挑战：模型需在在线方式从大量顺序数据中学习，且资源受限、须保障安全稳定link.springer.comlink.springer.com。一些研究采用策略蒸馏或迁移学习避免遗忘旧任务策略。如 Rusu et al. 在DeepMind的机器人实验中，用渐进网络将仿真训练的技能迁移到现实机器人上，同时保持仿真技能不丢失arxiv.org。又如 Traoré et al. (2019) 提出的 DiscoRL 框架，将旧策略压缩为策略库，再用Policy Distillation（策略蒸馏）技术在新环境中融合旧策略以加速学习，同时旧策略作为教师防止遗忘ar5iv.org。在连续控制中，策略往往以神经网络表示，类似分类任务的遗忘也会发生：新环境下调整策略网络，会导致旧环境下性能下降。为此 Rolnick et al. (2019) 提出的 CLEAR 方法，将off-policy经验重放引入强化学习的策略梯度训练，既提高新任务样本效率又维持旧任务价值函数不变。该方法在Atari游戏顺序学习中取得好结果，被视为强化学习领域对抗遗忘的有效方案之一。需要强调的是，强化学习场景中任务界限往往模糊，甚至代理可能在一个不断演变的环境中持续学习（如运营多年的家庭服务机器人，会不断遇到新任务）。这接近无任务标签 (task-agnostic)的持续学习。Aljundi et al. (2019) 针对此提出了在线持续学习方案：通过检测网络对新数据的干扰程度动态触发记忆重放（MIR）link.springer.com，以及使用梯度稀疏化挑选对旧任务干扰最大的记忆样本来更新，从而在无明确任务边界下也能抑制遗忘。此类方法在机器人持续感知与导航中具有潜力，因为现实中机器人很难知道自己何时“切换了任务”，只能根据环境变化连续调整。\n开放世界和自主适应: 具身智能体经常处于开放世界，可能遇到训练时未见过的全新情况。持续学习的终极目标是在这种开放环境中实现持续适应而不崩溃。Open-world持续学习需要综合上述技术，还涉及新知识的自主发现和 主动学习 。比如前述CLP方法引入了新类检测机制，让机器人在开放世界下识别何时需要学习新对象arxiv.orgarxiv.org。又如 Mundt et al. (2020) 探讨了结合异常检测和持续学习，使模型在检测到输入分布偏移时能触发新任务学习流程。对自主车而言，面对从未见过的道路情况（极端天气、新施工区域），如果能自动检测出“新情境”并调用持续学习模块更新模型，将大幅提高安全性。当然，这也带来安全约束下的学习稳定性问题，需要确保新学习不会在尚未充分验证时投入决策。近期一些研究主张引入 不确定性估计 （如Bayesian NN）判断模型何时需要学习新任务，以及学习后的性能变化link.springer.com。这些探索尚属前沿，但对于真正长期自主运作的智能体至关重要。\n近年新进展（2020–2025）与展望 过去五年中，持续学习领域涌现了一系列新趋势和方法，进一步提高了模型在复杂环境中的持续适应能力：\n任务无关持续学习: 越来越多工作关注在无明确任务边界、数据连续流动场景下的学习link.springer.comlink.springer.com。这更贴近现实中的机器人/代理感知流。为此，方法上强调在线更新、有限内存和即时评估。例如 2020 年的 GDumb 方法提出一种极端简单但强大的baseline：始终只训练当前模型在收到的全部数据上（存储一定量最近数据），每次新数据到来直接从头训练。这种方法虽谈不上高明，却在一些线上学习赛道表现接近更复杂的方法，提示我们需要重新审视评价指标。在可预见的将来， 线上持续学习 （Single-Pass Continual Learning）将成为研究热点，它要求算法一次遍历数据且不泄露未来信息，在边学习边推理的同时抗遗忘arxiv.orgarxiv.org。具身智能如实时视频流分析、持续语音识别都属于这种场景。 持续学习评测基准丰富化: 近年构建了许多新数据集和基准来评测持续学习算法在更复杂任务上的性能。如 CORe50、OpenLORIS 等视觉序列数据集用于评测在线物体识别link.springer.com；ACL 2021 Lifelong NLP挑战提供持续自然语言理解任务；还有不同领域的持续强化学习基准、连续无人驾驶仿真环境等。这些基准推动算法从依赖任务ID的小规模实验，走向更贴近真实的情境。评测指标也越发丰富，除了遗忘率、累计精度外，开始考虑模型的计算效率、内存开销以及在长期学习中的稳定性link.springer.comlink.springer.com。这些综合指标对于具身智能系统尤为重要，因为实际应用中资源受限且需要持续运行。 联邦持续学习与分布式学习: 在物联网和边缘计算兴起的背景下，联邦持续学习成为新方向arxiv.org。即多个分散设备（如一群机器人或智能传感器）在各自持续学习的同时，定期交流模型更新，从而在保证隐私下实现知识共享和共同进化。诸如 FedWeIT、FCL 等算法探索了如何在联邦场景下减少遗忘并高效通信arxiv.org。对具身智能来说，这意味着例如一队协作机器人的经验可以融合，使整体学习速度加快且每个体遗忘降低。该领域仍在起步，面临异步学习、设备差异等挑战，但前景值得期待。 理论分析与可解释性: 持续学习理论方面，近年有人尝试从信息论和最优化角度给出遗忘的分析框架，如用Fisher信息界定参数迁移平衡ar5iv.org。另外，对持续学习过程中的可解释性要求也在提高——在机器人应用中，理解模型为何遗忘某能力、何时需要触发新学习，对于建立用户信任很重要。一些研究利用可视化技术观察随着任务增添，网络内部表示如何演化，以寻找缓解遗忘的线索。还有工作将神经符号方法引入持续学习，以借助符号逻辑的约束保持旧知识。这些方向虽属于前沿探索，但表明社区已不仅满足于经验提升性能，也在寻求持续学习更深层的理论和可解释支撑。 综上，持续学习作为迈向真正智能系统的关键一步，近年来在算法和应用上都取得了显著进展。从最初发现问题、提出启发式对策，到如今各种融合策略在复杂环境中落地，我们离“像人一样终身学习”的AI越来越近。在具身智能领域，实现持续学习将赋予机器人和自主代理长久的自主适应能力，使其能够随着环境和任务变化不断成长，而无需频繁人工干预。展望未来，持续学习研究需要进一步结合元学习、强化学习、因果推断等范式，研发更加通用高效的算法。同时，在真实世界大规模部署持续学习系统时，还需重视 安全机制 （防止在学习过程中性能突然退化）、 伦理与隐私 （学习过程中对用户数据的处理）等问题。可以预见，随着研究的深化，持续学习将在机器人自主导航、智能助理、自动驾驶乃至通用人工智能等领域扮演日益重要的角色，推动人工智能从“静态聪明”走向“动态成长”。\n参考文献：\nMcCloskey, M. \u0026 Cohen, N. J. (1989). Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem . In Psychology of Learning and Motivation , vol. 24, pp. 109–165en.wikipedia.org. ( 首次揭示神经网络顺序学习遗忘问题 ) French, R. M. (1999). Catastrophic forgetting in connectionist networks . Trends in Cognitive Sciences, 3 (4):128–135cell.com. ( 灾难性遗忘综述，分析原因并讨论可能解决方案 ) Robins, A. (1995). Catastrophic forgetting, rehearsal and pseudorehearsal . Connection Science, 7 (2):123–146ar5iv.org. ( 提出伪重演方法，用随机伪样本重放旧知识 ) Goodfellow, I. et al. (2014). An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks . In ICLR 2014 . ( 实证分析深度网络遗忘现象，评估基本缓解策略 ) Li, Z. \u0026 Hoiem, D. (2016). Learning without Forgetting . In ECCV 2016arxiv.org. ( 知识蒸馏用于持续学习，只用新任务数据保持旧任务性能 ) Kirkpatrick, J. et al. (2017). Overcoming catastrophic forgetting in neural networks . PNAS, 114 (13):3521–3526arxiv.org. ( 提出EWC，通过弹性权重凝固保护重要参数arxiv.org ) Zenke, F. et al. (2017). Continual Learning Through Synaptic Intelligence . In ICML 2017arxiv.org. ( 提出SI算法，智能累积参数重要性减少遗忘arxiv.org ) Rebuffi, S.-A. et al. (2017). iCaRL: Incremental Classifier and Representation Learning . In CVPR 2017arxiv.org. ( 提出增量分类策略，结合样本保存和蒸馏避免遗忘arxiv.org ) Lopez-Paz, D. \u0026 Ranzato, M. (2017). Gradient Episodic Memory for Continual Learning . In NeurIPS 2017arxiv.org. ( 提出GEM算法，用梯度约束保证新任务不增大旧任务损失arxiv.org ) Shin, H. et al. (2017). Continual Learning with Deep Generative Replay . In NeurIPS 2017arxiv.org. ( 提出深度生成回放DGR，通过生成模型重现旧样本融合训练arxiv.org ) Rusu, A. A. et al. (2016). Progressive Neural Networks . arXiv:1606.04671arxiv.org. ( 提出渐进网络架构，扩展新列避免遗忘并实现知识迁移arxiv.org ) Mallya, A. \u0026 Lazebnik, S. (2018). PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning . In CVPR 2018arxiv.org. ( 通过迭代剪枝为新任务腾出容量，实现单网络多任务无遗忘arxiv.org ) Schwarz, J. et al. (2018). Progress \u0026 Compress: A scalable framework for continual learning . In ICML 2018arxiv.org. ( 提出进展-压缩框架，结合渐进扩展和蒸馏压缩，实现无增长持续学习arxiv.org ) Aljundi, R. et al. (2019). Online Continual Learning with Maximal Interfered Retrieval . In NeurIPS 2019 . ( 提出在线持续学习算法MIR，选择干扰最大的记忆样本回放，任务无关场景有效 ) Pellegrini, L. et al. (2020). Latent Replay for Real-Time Continual Learning . In IROS 2020link.springer.com. ( 提出在特征空间进行重放，支持机器人实时持续学习，降低存储与计算需求 ) Kemker, R. \u0026 Kanan, C. (2018). FearNet: Brain-Inspired Model for Incremental Learning . In ICLR 2018arxiv.org. ( 提出双内存脑启发模型，不存原始数据通过生物式记忆系统整合知识arxiv.org ) Hajizada, E. et al. (2024). Continually Learning Prototypes . arXiv:2404.00418arxiv.orgarxiv.org. ( 提出原型持续学习方法，少样本在线学习并支持开放世界新类发现，无需存储回放 ) Shaheen, K. et al. (2022). Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks . Journal of Intelligent \u0026 Robotic Systems, 105 (9)link.springer.comlink.springer.com. ( 面向自主系统的持续学习综述，分析算法在无人车、无人机等中的性能和挑战 ) Lesort, T. et al. (2020). Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges . Information Fusion, 58 :52–68arxiv.orgar5iv.org. ( 持续学习在机器人领域的综述，提出评测框架和跨领域方法借鉴思路 ) 引用\n[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[2312.10549] Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomyhttps://arxiv.org/abs/2312.10549Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[PDF] Continual Learning and Catastrophic Forgettinghttps://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1706.08840] Gradient Episodic Memory for Continual Learninghttps://arxiv.org/abs/1706.08840[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1805.06370] Progress \u0026 Compress: A scalable framework for continual learninghttps://arxiv.org/abs/1805.06370[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Towards lifelong object recognition: A dataset and benchmarkhttps://www.sciencedirect.com/science/article/abs/pii/S0031320322003004Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccLatent Replay for Real-Time Continual Learning - IEEE Xplorehttps://ieeexplore.ieee.org/document/9341460/Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccInteractive continual learning for robots: a neuromorphic approachhttps://dl.acm.org/doi/10.1145/3546790.3546791Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfcc[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfcchttps://arxiv.org/pdf/2302.00487https://arxiv.org/pdf/2302.00487Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccFederated Continual Learning for Edge-AI: A Comprehensive Surveyhttps://arxiv.org/html/2411.13740v1\n",
  "wordCount" : "1426",
  "inLanguage": "en",
  "datePublished": "2025-06-16T14:14:45+08:00",
  "dateModified": "2025-06-16T14:14:45+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-06-16/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-06-16
    </h1>
    <div class="post-meta"><span title='2025-06-16 14:14:45 +0800 CST'>June 16, 2025</span>&nbsp;·&nbsp;7 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h1 id="持续学习中避免灾难性遗忘具身智能领域的研究进展综述">持续学习中避免灾难性遗忘：具身智能领域的研究进展综述<a hidden class="anchor" aria-hidden="true" href="#持续学习中避免灾难性遗忘具身智能领域的研究进展综述">#</a></h1>
<h2 id="引言与问题背景">引言与问题背景<a hidden class="anchor" aria-hidden="true" href="#引言与问题背景">#</a></h2>
<p>持续学习（Continual Learning，也称终身学习）指模型在数据分布和学习目标不断变化的情境下，能够连续学习新任务且不忘记已有知识的能力<a href="https://arxiv.org/abs/1907.00182#:~:text=,learning%20is%20not%20necessarily%20finding">arxiv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=An%20important%20challenge%20for%20machine,of%20complex%20skills%20and%20knowledge">ar5iv.org</a>。传统深度学习假设训练数据 <em>i.i.d.<em>且一次性可用，这在现实具身智能（如机器人、自主系统）中往往不成立<a href="https://ar5iv.org/pdf/1907.00182#:~:text=1%20Introduction">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=You%20want%20to%20learn%20from,not%20know%20how%20and%20when">ar5iv.org</a>。其中最大挑战之一是</em>灾难性遗忘（catastrophic forgetting）</em> ，即模型在顺序学习多个任务时，新知识的获取会导致旧知识被<strong>快速、大幅</strong>遗忘<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=cognitive%20science,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>。这一现象最早由 McCloskey 和 Cohen 于1989年在联结主义神经网络中发现<a href="https://arxiv.org/abs/2312.10549#:~:text=Catastrophic%20Forgetting%20%28CF%29,solutions%2C%20proposes%20a%20taxonomy%20to">arxiv.org</a>，体现了所谓 <em>稳定-可塑性权衡</em> ：模型既要对新信息足够可塑（plasticity），又要对既有知识保持稳定（stability）<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>。与生物神经系统相比，人工神经网络在顺序学习时更容易“灾难性”遗忘过去经验，而人类和动物通常表现出渐进、有选择的遗忘<a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=All%20natural%20cognitive%20systems%2C%20and%2C,generalize%2C%20to%20function%20in%20the">cell.com</a><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=Unfortunately%2C%20though%2C%20catastrophic%20forgetting%20does,brain%20might%20have%20overcome%20this">cell.com</a>。如何在保持模型泛化能力的同时避免旧知识被破坏，成为持续学习研究的核心问题<a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=catastrophic%20forgetting,solution%20for%20distributed%20connectionist%20networks">cell.com</a>。为此，大量研究围绕不同技术路线展开，包括利用附加数据回放、正则化约束、动态模块化架构和外部记忆等方法来缓解遗忘<a href="https://arxiv.org/html/2404.00418v1#:~:text=phenomenon%20that%20reflects%20the%20trade,18%2C%203">arxiv.org</a>。本文将按时间脉络梳理持续学习避免遗忘的关键进展，重点聚焦具身智能场景的应用，并比较不同方法在这些场景中的优劣差异。</p>
<h2 id="早期探索灾难性遗忘的提出与初步对策">早期探索：灾难性遗忘的提出与初步对策<a hidden class="anchor" aria-hidden="true" href="#早期探索灾难性遗忘的提出与初步对策">#</a></h2>
<p><strong>灾难性遗忘现象的提出（1980s–1990s）:</strong> 神经网络中的灾难性遗忘问题由 McCloskey 和 Cohen (1989)<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=cognitive%20science,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>以及 Ratcliff (1990) 首次严谨描述<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>。他们发现序列训练两个任务时，后学任务会显著干扰先前任务的记忆<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=McCloskey%20and%20Cohen%20,with%20backpropagation%20neural%20network%20modelling">en.wikipedia.org</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=each%20learning%20trial%20on%20the,Furthermore%2C%20the%20problems%202%2B1">en.wikipedia.org</a>。这一问题可视为Grossberg提出的稳定-可塑性两难的极端表现<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>。此后，研究者逐步意识到，若要让人工智能具备人类般持续学习的能力，必须解决神经网络的遗忘灾难。1990年代中期，一些学者开始探索初步对策。例如，Robins (1995) 提出了<strong>重演/伪重演 (rehearsal/pseudorehearsal)</strong> 方法，即在学习新任务时将旧任务样本或由模型生成的“伪样本”一并训练，以巩固旧知识<a href="https://ar5iv.org/pdf/1907.00182#:~:text=,In%202016%20IEEE%2055th%20Conference">ar5iv.org</a>。这种方法模拟了生物大脑在睡眠中<strong>重放记忆</strong>的过程，缓解了遗忘问题，被视为后续生成式回放方法的雏形<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Instead%20of%20modeling%20the%20past,based%20learning%2C%20where%20the%20model">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=the%20past%20is%20not%20forgotten,91%2C%2020%20%2C%20%20110">ar5iv.org</a>。与此同时，Grossberg 等人在稳定-塑性理论指导下发展**自适应共振理论 (ART)**网络，通过网络结构与记忆单元的设计减少旧记忆被覆盖的风险。这一时期的工作揭示了灾难性遗忘的严重性，并奠定了若干基本思想：通过保留过去经验（真实或模拟）或限制参数剧烈更新来保护已有知识<a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=Unfortunately%2C%20though%2C%20catastrophic%20forgetting%20does,brain%20might%20have%20overcome%20this">cell.com</a>。然而，由于当时神经网络规模和应用场景有限，这些早期方法未形成统一框架，但为后续深度学习时代的持续学习研究提供了宝贵思路。</p>
<p><strong>深度学习时代兴起前的持续学习理念:</strong> 在2000年前后，机器学习领域也出现了一些“终身学习”思想，例如 Thrun 和 Mitchell 等人在机器人领域讨论让机器人不断积累知识、自主适应新环境的算法。但受限于模型能力，这些工作多偏向理论构想或特定场景下的增量学习算法。值得一提的是，Silver <em>et al.</em> (2013) 等人在认知科学领域提出**“永不停歇学习”<strong>(Never-Ending Learning)的愿景，旨在构建能无限获取新知识的AI系统。这些理念上的探索进一步强调了持续学习的重要性，但真正有效的算法突破还要等待深度学习的成熟。进入2010年代，深度神经网络在图像、语音等任务上取得突破，但其</strong>遗忘现象依然明显**。Goodfellow <em>et al.</em> (2014) 的实证研究表明，即使现代深度网络在顺序学习多任务（如不同MNIST变换）时，仍然发生严重的性能遗忘，他们尝试用<strong>Dropout</strong>等正则策略略微缓解遗忘<a href="https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf#:~:text=,Hinton%20et%20al.%2C">cs.uic.edu</a>。这一阶段的研究重新量化了深度模型遗忘的程度，引发了学界对持续学习的关注，也为随后的关键方法发明做好了铺垫。</p>
<h2 id="深度学习时代的方法演进20162020">深度学习时代的方法演进（2016–2020）<a hidden class="anchor" aria-hidden="true" href="#深度学习时代的方法演进20162020">#</a></h2>
<p>进入深度学习时代后，大量持续学习算法被提出。总体而言，这些方法可分为以下几类： <strong>参数正则化</strong> 、 <strong>经验回放</strong> （或生成回放）、 <strong>参数隔离/模块化</strong> 、以及<strong>外部记忆</strong>等<a href="https://arxiv.org/html/2404.00418v1#:~:text=phenomenon%20that%20reflects%20the%20trade,18%2C%203">arxiv.org</a>。各类方法都有经典代表工作，我们按时间演进介绍主要方法及其贡献。</p>
<h3 id="参数正则化方法">参数正则化方法<a hidden class="anchor" aria-hidden="true" href="#参数正则化方法">#</a></h3>
<p><strong>Learning without Forgetting (LwF, 2016):</strong> Li 和 Hoiem 提出“学习不遗忘”算法<a href="https://arxiv.org/abs/1606.09282#:~:text=retraining%20on%20such%20data%20becomes,and%20new%20task%20datasets%20for">arxiv.org</a>。他们假设只能获取新任务数据，无法重温旧任务数据的现实情况，通过让模型在学习新任务时<strong>蒸馏(distillation)<strong>旧任务模型的输出分布来</strong>正则化</strong>模型参数变化<a href="https://arxiv.org/abs/1606.09282#:~:text=retraining%20on%20such%20data%20becomes,and%20new%20task%20datasets%20for">arxiv.org</a>。具体而言，在训练新任务时对旧任务模型的预测进行保持，使新模型尽可能产生与旧模型相似的输出，从而保护原有能力。LwF是<strong>知识蒸馏用于持续学习</strong>的开创性工作，实现了只用新任务数据也能较好保留旧任务性能<a href="https://arxiv.org/abs/1606.09282#:~:text=for%20its%20existing%20capabilities%20are,for%20improved%20new%20task%20performance">arxiv.org</a>。该方法在2016年ECCV发表，此后蒸馏正则化成为持续学习的重要手段之一。</p>
<p><strong>Elastic Weight Consolidation (EWC, 2017):</strong> Kirkpatrick 等人（DeepMind）在PNAS 2017发表了著名的EWC算法<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。EWC通过近似计算每个参数对旧任务的重要程度（利用费舍尔信息矩阵对参数敏感度进行估计），在学习新任务的损失中添加项，<strong>惩罚对重要参数的大幅更新</strong><a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。直观来说，模型会“放慢”那些对旧任务重要参数的学习速率，以免遗忘旧知识<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。EWC在经典分类任务（如顺序MNIST）和强化学习任务（顺序Atari游戏）上验证了有效性<a href="https://arxiv.org/abs/1612.00796#:~:text=tasks%20by%20selectively%20slowing%20down,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。作为持续学习领域里程碑，EWC证明通过软约束参数更新，可以在一定程度上同时保持先前任务性能和新任务学习能力<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。其思路后来衍生出许多变体，例如利用更精细近似二阶信息的方法等<a href="https://ar5iv.org/pdf/1907.00182#:~:text=weights%20and%20produce%20an%20adapted,Fisher%20matrix%20using%20the%20Kronecker">ar5iv.org</a>。</p>
<p><strong>Synaptic Intelligence (SI, 2017):</strong> Zenke 等人在ICML 2017提出了另一本质类似EWC的正则化方法SI<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。不同于EWC预先计算参数重要度，SI在训练过程中<strong>实时累积</strong>每个参数对损失的贡献度，结束当前任务时将其视为该参数的重要性<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。这种“智能突触”机制借鉴了生物突触强度调节的复杂性，每个突触（参数）在多个任务中累积“任务相关信息”，新任务来时利用这些信息调整学习率<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。SI在多个连续分类任务上显著降低了遗忘，同时保持了计算高效<a href="https://arxiv.org/abs/1703.04200#:~:text=intelligent%20synapses%20that%20bring%20some,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。与EWC相比，SI无需存储旧任务样本，同样不增加模型容量，对于资源受限的设备具有吸引力<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。但正如多数正则化方法的局限，当任务数量增多或差异较大时，单纯靠增加惩罚会导致模型学习新任务受限，需要在稳定与塑性间权衡<a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=also%20generative%20models%20,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a>。</p>
<p><strong>其他正则化进展:</strong> 除上述，2017年前后还出现了多种参数正则化方案。例如 Li <em>et al.</em> (2017) 的增量时刻匹配（IMM）方法通过匹配参数分布的方式融合旧新任务模型；Aljundi <em>et al.</em> (2018) 提出的MAS（Memory Aware Synapses）利用输出敏感度评估参数重要性。这些方法本质均为在损失函数中添加某种形式的正则项，使模型<strong>避免过度调整关键参数</strong>以保留旧知识<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Basic%20regularization%20techniques%20that%20could,More%20complex%20methods%20consist%20in">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a>。正则化方法的优点是 <strong>无需存储旧样本，内存占用低</strong> ，易于在嵌入式/机器人等设备上实现<a href="https://ar5iv.org/pdf/1907.00182#:~:text=streams%20of%20data%20presented%20sequentially,speed%20during%20the%20learning%20process">ar5iv.org</a>。它们的不足在于当任务间差异巨大、参数冲突严重时效果受限，而且累积过多任务后模型可能进入“过度稳定”状态难以学习新任务<a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a>。在具身智能场景中，由于设备算力和存储有限，正则化方法仍是常用选择之一。例如 EWC 被用于机器人连续控制任务以保护低层政策参数不被遗忘<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。但如果机器人遇到全新领域任务，正则化可能限制其适应新技能的能力，这是后续方法力图解决的问题。</p>
<h3 id="经验重放与生成式回放方法">经验重放与生成式回放方法<a hidden class="anchor" aria-hidden="true" href="#经验重放与生成式回放方法">#</a></h3>
<p><strong>显式经验重放 (Experience Replay):</strong> 针对持续学习，最直接的思路是 <strong>在学习新任务时重温部分旧任务的数据</strong> ，仿佛让模型“复习”以前的知识<a href="https://ar5iv.org/pdf/1907.00182#:~:text=One%20solution%20to%20Continual%20Learning,where%20continual%20learning%20is%20necessary">ar5iv.org</a>。Rebuffi 等人在CVPR 2017提出的 <strong>iCaRL</strong> 方法将这一思想与深度学习结合<a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail">arxiv.org</a>。iCaRL在每学新类别时， <strong>保存每个旧类别少量代表样本（记忆库）</strong> ，训练时将这些旧样本与新数据一起用于更新模型，并对模型输出进行知识蒸馏以防决策边界偏移<a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail">arxiv.org</a>。通过同时学习分类新类别和回顾旧类别，iCaRL实现了深度网络在长时间增量学习许多类时，比仅新数据训练的策略遗忘显著减少<a href="https://arxiv.org/abs/1611.07725#:~:text=only%20the%20training%20data%20for,where%20other%20strategies%20quickly%20fail">arxiv.org</a>。iCaRL开创了<strong>样本记忆回放+蒸馏</strong>结合的范式。之后许多增量学习方法沿用了“小样本记忆”思路，如 Hou <em>et al.</em> (2019) 的UCIR、Wu <em>et al.</em> (2019) 的BiC等，都在如何精选和高效利用少量旧样本上下功夫。经验重放策略也直接应用于强化学习领域——在非平稳环境中，智能体可将过去经历的<strong>轨迹</strong>保存一部分，在策略更新时混入重放，以避免策略完全偏离先前成功经验。这与深度Q网络（DQN）的经验回放缓冲理念一脉相承，只是这里目的是防遗忘而不仅是破除相关性。在机器人学习中，经验重放意味着机器人在学习新技能时定期练习已掌握的技能（通过模拟旧技能的传感器输入等），这在实践中提高了多技能机器人系统的鲁棒性。</p>
<p><strong>Gradient Episodic Memory (GEM, 2017):</strong> Lopez-Paz &amp; Ranzato 在 NeurIPS 2017 提出的 GEM 则进一步创新了重放的使用方式<a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>。与直接将旧样本混入训练不同，GEM把少量旧任务样本存入 <em>episodic memory</em> ，在每次参数更新时，通过约束新梯度与旧样本梯度的内积为非负，<strong>确保新任务训练不会增加旧任务损失</strong><a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>。这种基于<em>优化约束</em>的方法保证了模型对记忆样本性能不下降，实现了一定的“向后迁移”能力：在学习新任务的同时还有可能改进旧任务表现<a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>。GEM开创了利用<strong>回放样本的梯度信息</strong>指导优化的思路，其后续简化版A-GEM (Chaudhry et al. 2018)降低了计算成本。对于具身智能而言，GEM这类方法的优势在于即使少量记忆也能通过优化约束起效，而且不需要明确任务边界（可以对任意过去经验施加约束）。不过其劣势是需要实时计算并存储梯度，复杂度较高，且仍需维护一个小型记忆库。</p>
<p><strong>生成式回放 (Generative Replay):</strong> 当直接存储原始旧样本受限时，另一策略是训练<strong>生成模型</strong>来产生日前学过的数据，从而实现回放。Shin <em>et al.</em> 在 NeurIPS 2017 提出的 <strong>Deep Generative Replay (DGR)</strong> 是该思路的里程碑<a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks">arxiv.org</a>。DGR构建了一个生成模型（如GAN或VAE）作为“记忆仿真器”，在学习新任务时利用生成模型产生日前各任务的合成样本，并与新数据混合训练<strong>Solver</strong>模型<a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks">arxiv.org</a>。在每完成一个任务后，Solver的参数固定，然后训练生成模型去拟合更新后的Solver分布，以便下次产生更新的数据分布<a href="https://ar5iv.org/pdf/1907.00182#:~:text=A%20classical%20method%20implementing%20a,model%20to%20learn%20next%20task">ar5iv.org</a>。这种<strong>双模型协同</strong>框架受到大脑“海马-新皮层”互作机制的启发，即利用快速变化的“海马体”生成回忆来训练慢更新的“皮层”网络<a href="https://arxiv.org/abs/1705.08690#:~:text=the%20problem%2C%20it%20requires%20large,sequential%20learning%20settings%20involving%20image">arxiv.org</a>。DGR实验证明，即使不保存任何真实旧样本，模型仍能通过生成模拟数据达到与有存储时相近的效果<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Instead%20of%20modeling%20the%20past,based%20learning%2C%20where%20the%20model">ar5iv.org</a><a href="https://arxiv.org/abs/1705.08690#:~:text=dual%20model%20architecture%20consisting%20of,settings%20involving%20image%20classification%20tasks">arxiv.org</a>。生成回放的优势是 <strong>不直接占用存储真实数据</strong> ，在隐私敏感或内存极小的设备上尤为有用<a href="https://arxiv.org/abs/1705.08690#:~:text=solving%20multiple%20tasks%20have%20been,those%20for%20a%20new%20task">arxiv.org</a>。其缺点在于生成模型本身也面临持续学习问题（如何不忘记早期的数据分布），且训练开销较大<a href="https://ar5iv.org/pdf/1907.00182#:~:text=While%20most%20of%20the%20Generative,62">ar5iv.org</a>。后续不少工作改进了生成回放，如 Wu <em>et al.</em> (2018) 将GAN与变分特征结合（MeRGAN），Rios <em>et al.</em> (2018) 用生成对抗网络生成特征而非像素，提高效率等<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Lee%20et%20al.%20,%E2%9C%93%20%20%20%E2%9C%93">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Achille%20et%20al.%20,%E2%9C%93%20%20%20%E2%9C%93">ar5iv.org</a>。生成回放还被应用到强化学习的状态生成中，例如 Caselles-Dupré <em>et al.</em> (2019) 提出的自触发生成回放（S-TRIGGER）用于连续学习环境状态表示<a href="https://ar5iv.org/pdf/1907.00182#:~:text=%2A%20%20%5B20%5D%20H.%C2%A0Caselles,Di%C2%A0Stefano%2C%20and">ar5iv.org</a>。总的来看，回放类方法（包括经验重放和生成回放）在各种基准上往往表现突出，被认为是目前<strong>抗遗忘最有效</strong>的范式之一。然而它们对存储或生成能力有要求，在具身智能中需权衡内存/算力和性能：对于机器人等设备，存储少量关键经验（如图像片段、关键帧）进行回放在实践中较常用，而实时训练复杂生成模型则相对少见。</p>
<h3 id="模块化架构与参数隔离方法">模块化架构与参数隔离方法<a hidden class="anchor" aria-hidden="true" href="#模块化架构与参数隔离方法">#</a></h3>
<p><strong>Progressive Neural Networks (PNN, 2016):</strong> Rusu 等人提出的渐进神经网络是持续学习的架构派代表<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。PNN在每遇到新任务时 <strong>冻结已有网络</strong> ，并侧旁新增一组“列”网络用于学习新任务<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。同时，通过旁路连接让新任务列能够利用之前各列学到的特征（实现知识迁移）<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a>。这种架构确保旧任务的参数永不修改，从而<strong>彻底避免遗忘</strong><a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a>；而新增模块可以专门学习新任务，有充足的模型容量。PNN在Atari游戏和3D迷宫导航等一系列强化学习任务上取得优于微调的成绩，并显示出显著的<strong>前向迁移</strong>能力<a href="https://arxiv.org/abs/1606.04671#:~:text=a%20step%20forward%20in%20this,layers%20of%20the%20learned%20policy">arxiv.org</a>。其缺点也很明显：每增加一个任务网络规模就线性增长，在任务数很多时不切实际<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。尽管如此，PNN证明了模块隔离在避免遗忘上的有效性，许多后续方法受此启发引入<em>可扩展</em>或<em>可选择激活</em>的架构。</p>
<p><strong>PathNet (2017) 与 PackNet (2018):</strong> 为了缓解PNN网络爆炸的问题，Fernando 等人 (2017) 提出的 PathNet 利用进化算法在固定网络中为每个任务选择一条互不干扰的子网络路径，相当于在共享参数的前提下实现参数隔离。Mallya 和 Lazebnik (2018) 则提出  <strong>PackNet</strong> ，通过反复<strong>剪枝</strong>和<strong>重训练</strong>来为新任务腾出参数空间<a href="https://arxiv.org/abs/1711.05769#:~:text=,scale">arxiv.org</a>。具体来说，PackNet先训练初始任务模型，然后剪除一定比例不重要的参数（权重置零但保留位置），学习第二个任务时仅利用空闲参数；如此迭代，将多个任务“打包”进单个网络中<a href="https://arxiv.org/abs/1711.05769#:~:text=pruning%20techniques%2C%20we%20exploit%20redundancies,grained">arxiv.org</a>。实验表明，在ImageNet等大型数据上，PackNet可在一个VGG模型中连续容纳多个细粒度分类任务，性能接近于单独训练<a href="https://arxiv.org/abs/1711.05769#:~:text=minimal%20storage%20overhead,available%20at%20this%20https%20URL">arxiv.org</a>。PackNet无需存储旧数据，也不引入新参数，因此相比PNN更高效<a href="https://arxiv.org/abs/1711.05769#:~:text=,scale">arxiv.org</a>。但PackNet需要预先设定剪枝比例，且剪枝过多可能损害旧任务性能，过少则限制新任务空间。后来一些变体如 “Piggyback” (Mallya, 2018) 则改为学习任务特定的掩码，更灵活地实现参数复用。总体而言，<em>参数隔离</em>类方法（含动态扩张和网络剪枝）通过<strong>结构上的硬约束</strong>避免了遗忘，其优势是旧知识完全保留、无干扰<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a>。在机器人等具身智能中，如果任务集是离散且有限的，这类方法可考虑使用。例如在多任务机器人控制中，可为每个任务分配专属网络模块或参数子集，新任务加入时扩展网络并冻结旧模块，从而保持以往技能<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a>。然而，在开放环境下任务可能连续涌现且无法预知数量，单纯无限扩展网络不切实际。因此近期一些工作尝试结合元学习或 <strong>条件网络</strong> ，自动决定何时复用旧参数、何时增加新参数，以兼顾模型规模和遗忘防护。比如 Serra <em>et al.</em> (2018) 提出的 HAT 方法对每层参数学习可训练门控，通过门控向量的稀疏化实现在相同网络中隔离不同任务的激活区域，从而在不显著增加参数的情况下减少干扰。</p>
<p><strong>脑启发的双记忆体系:</strong> 值得注意的是，一些方法从神经科学的<strong>双重内存</strong>理论汲取灵感，将快速学习模块和稳定长时模块结合起来应对遗忘。例如 Kemker 和 Kanan (2018) 提出的 <strong>FearNet</strong> 模型采用“大脑 <strong>海马-新皮层</strong> ”的架构<a href="https://arxiv.org/abs/1711.10563#:~:text=for%20each%20class%2C%20making%20it,art%20performance%20at%20incremental%20class">arxiv.org</a>：用一个类似海马体的小网络专门快速学习当前任务，并在适当时机（模拟睡眠）将新知识整合（consolidate）到另一个类似皮层的大网络中做长期存储<a href="https://arxiv.org/abs/1711.10563#:~:text=previous%20examples%2C%20making%20it%20memory,AudioSet%29%20benchmarks">arxiv.org</a>。同时还有一个类似杏仁核的模块，根据输入判断应该用哪套记忆系统回答<a href="https://arxiv.org/abs/1711.10563#:~:text=recent%20memories%20inspired%20by%20the,art%20performance%20at%20incremental%20class">arxiv.org</a>。FearNet不需存储旧样本，依赖<strong>生成式</strong>机制回忆旧类数据，达到与iCaRL相当的性能<a href="https://arxiv.org/abs/1711.10563#:~:text=to%20suffer%20from%20catastrophic%20forgetting,FearNet%20also%20uses%20a%20module">arxiv.org</a>。这类方法实质上属于架构+回放的混合策略（因为短期网本身可看作一种内生记忆生成器）。双记忆策略对具身智能有自然的意义：机器人或代理可以配置一个“小而快”的在线学习器来及时适应新变化，同时定期将知识固化到“大而稳”的长期模型中，从而两全其美。不过如何确定巩固频率以及双网络的容量匹配仍在探索中。</p>
<h3 id="方法对比与小结">方法对比与小结<a hidden class="anchor" aria-hidden="true" href="#方法对比与小结">#</a></h3>
<p>不同持续学习方法各有优劣，在具身智能场景下需要平衡选择<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Moreover%2C%20most%20of%20continual%20learning,and%20the%20strategies%20they%20propose">ar5iv.org</a>。<strong>正则化方法</strong>不需保存样本、开销低，适合嵌入式设备在线更新，但在任务变化剧烈时可能束缚新知识获取<a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a>。<strong>回放方法</strong>往往效果最佳，即便少量样本重放也能显著降低遗忘<a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>；对于机器人这种可<strong>反复与环境交互</strong>的场景，还可通过自主采样过去环境状态进行重演。然而存储真实数据可能受限于隐私或容量，而训练生成模型又对计算资源有较高要求<a href="https://arxiv.org/abs/1705.08690#:~:text=solving%20multiple%20tasks%20have%20been,those%20for%20a%20new%20task">arxiv.org</a>。<strong>模块化/参数隔离方法</strong>彻底杜绝了遗忘，在多任务机器人系统（任务有限且可拆分）中很有价值，但在开放任务中扩展性受限<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。<strong>外部记忆和双重内存</strong>策略提供了一种折中：通过引入专门的记忆模块，模型可以在不反复调整主要网络权重的情况下查询和更新知识。例如在多人对话交互机器人中，引入一个可读写的记忆单元存储历史对话要点，有助于长期一致的对话理解。但引入记忆也增大了系统复杂度，需要设计高效的检索和写入机制。</p>
<p>此外，许多先进方法不再局限于单一策略，而是<strong>混合多种机制</strong>以取长补短<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Moreover%2C%20most%20of%20continual%20learning,and%20the%20strategies%20they%20propose">ar5iv.org</a>。例如 Schwarz 等人 (2018) 提出的 <strong>Progress &amp; Compress</strong> 框架将<strong>动态架构</strong>与<strong>蒸馏</strong>结合：使用Progressive Network扩展新任务列，然后通过蒸馏将新列知识压缩回主干网络，从而既避免遗忘又控制模型规模<a href="https://arxiv.org/abs/1805.06370#:~:text=problems,alphabets%20as%20well%20as%20two">arxiv.org</a>。再如 von Oswald <em>et al.</em> (2019) 的 <strong>MER</strong> 方法将元学习思想融入记忆重放，通过元训练提高模型表示对新旧任务的解耦，从而辅助减少干扰。这些综合方法在近年不断涌现，说明持续学习领域正朝着<strong>多策略融合</strong>与<strong>自动适应</strong>方向发展。</p>
<h2 id="具身智能场景中的持续学习应用">具身智能场景中的持续学习应用<a hidden class="anchor" aria-hidden="true" href="#具身智能场景中的持续学习应用">#</a></h2>
<p>具身智能领域（如机器人、自主车辆、智能代理）为持续学习提供了最实际也最具挑战的用武之地<a href="https://arxiv.org/abs/1907.00182#:~:text=,most%20recent%20papers%20on%20continual">arxiv.org</a>。与静态数据集不同，具身智能体在物理世界中连续感知和行动，环境非平稳且任务边界往往不明确<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Robotic%20agents%20have%20to%20learn,robotics%20approaches%20in%20a%20way">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=You%20want%20to%20learn%20from,not%20know%20how%20and%20when">ar5iv.org</a>。以下我们重点考察持续学习方法在几个具身场景的应用进展：</p>
<p><strong>机器人视觉与物体识别：</strong> 服务机器人需要在不断变化的环境中识别新对象、适应新场景，这正是开放域的持续学习问题。为评测算法，2020年提出了<strong>OpenLORIS-Object</strong>机器人视觉数据集，包含随时间推移环境光照、视角、物距等变化的数据流<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322003004#:~:text=Towards%20lifelong%20object%20recognition%3A%20A,vision%20with%20quantifiable%20environmental%20factors">sciencedirect.com</a>。在该数据集上，Lomonaco 等人组织了持续学习挑战，促进了算法在真实机器人感知条件下的比较。一系列方法被测试：如 iCaRL 的小样本存储结合知识蒸馏策略在这种增量物体识别中取得稳健表现；又如 IROS 2020 的 <strong>Latent Replay</strong> 方法，Pellegrini <em>et al.</em> 提出<strong>只在特征空间保存和重放</strong>旧数据<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=16,IEEE">link.springer.com</a>。具体而言，机器人摄像头图像经卷积网络得到中间表示，将这些<strong>低维激活</strong>缓存代替原始高维图像，可大幅减少存储并实现实时回放<a href="https://ieeexplore.ieee.org/document/9341460/#:~:text=Xplore%20ieeexplore,space%2C%20we%20store%20activations">ieeexplore.ieee.org</a>。实验表明，在OpenLORIS这种持续视觉任务中，Latent Replay比直接存图像几乎不降性能，却更高效满足机器人实时性需求<a href="https://ieeexplore.ieee.org/document/9341460/#:~:text=Xplore%20ieeexplore,space%2C%20we%20store%20activations">ieeexplore.ieee.org</a>。另一最新进展是 Hajizada <em>et al.</em> (2024) 提出的 <strong>Continually Learning Prototypes (CLP)</strong> 算法<a href="https://arxiv.org/html/2404.00418v1#:~:text=require%20buffering%20and%20a%20balanced,free%2C%20hence%20does%20not">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=novelties%20and%20learning%20new%20items,simple%20version%20of%20CLP%20in">arxiv.org</a>。CLP针对机器人<strong>少样本在线学习</strong>和<strong>开放世界</strong>场景设计：它采用<em>原型</em>向量表征每类知识，并通过<strong>元可塑性</strong>机制动态调整每个原型的学习速率来平衡新旧知识稳定性<a href="https://arxiv.org/html/2404.00418v1#:~:text=scenarios%20where%20robots%20must%20learn,free%2C%20hence%20does%20not">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=Learning%20Prototypes%20%28CLP%29,In%20a%20low">arxiv.org</a>。同时CLP具备新类别<strong>自我检测与无监督学习</strong>能力（即机器人遇到未知物体时可判断新类别并自主创建原型学习）<a href="https://arxiv.org/html/2404.00418v1#:~:text=networks%20and%20the%20more%20natural,towards%20realistic%20learning%20for%20robots">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=supervision%C2%A0%5B8%5D.%20Therefore%2C%20we%20extend%20FS,OWCL">arxiv.org</a>。重要的是，CLP<strong>不使用任何显式回放数据</strong>且兼容神经形态芯片，实现了超低能耗下的持续学习<a href="https://arxiv.org/html/2404.00418v1#:~:text=forgetting%2C%20CLP%20utilizes%20a%20novel,In%20the%20open%20world">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=dynamic%20adaptation%20mechanism%20for%20the,part%20of%20the%20Lava%20software">arxiv.org</a>。这对于内存和电池有限的移动机器人具有现实意义。总的来看，在机器人视觉领域，混合使用 <strong>小样本记忆</strong> 、 <strong>特征回放</strong> 、<strong>适应性学习率</strong>等技术已取得显著效果，使机器人能逐步扩展认知能力且遗忘受控。</p>
<p><strong>人机交互与多模态学习：</strong> 具身智能体常涉及多模态感知（视觉、听觉、语言）和人机交互，这带来了持续学习的新课题。例如社交机器人需要持续学习新的对话内容、新的手势动作等。NLP领域已有针对增量学习的综述（如 Biesialska  <em>et al.</em> , 2020<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=7,6523%E2%80%936541%20%282020">link.springer.com</a>），其中提到自然语言处理任务在持续学习中面临词汇和语义随时间演变的问题。一些方法通过动态扩充词典或嵌入空间缓解了“遗忘”早期语义的现象。对于多模态交互，Kulkarni <em>et al.</em> (2019) 提出在对话系统中使用<strong>弹性权重约束</strong>来保留模型早期对话技能，同时新增新领域对话意图。交互学习中一个重要方面是 <strong>用户在环（human-in-the-loop）</strong> ：机器人可通过用户反馈实时修正知识。近期有工作探索 <strong>交互式持续学习</strong> ，如 Hazifa <em>et al.</em> (2022) 结合神经形态计算，利用片上在线学习快速吸收用户教授的新知识，同时通过正则保护已有知识<a href="https://dl.acm.org/doi/10.1145/3546790.3546791#:~:text=Interactive%20continual%20learning%20for%20robots%3A,footprint%20and%20interactive%20learning%20capability">dl.acm.org</a>。虽然具体算法仍在早期，但这些尝试指出了方向——未来的具身智能体应能通过持续人机交互 <strong>自我进化</strong> ，并且做到“学而不忘”。</p>
<p><strong>连续控制与强化学习：</strong> 在自主驾驶、机器人控制等连续决策场景，持续学习同样关键。例如自动驾驶车辆遇到新道路场景，需要学习新策略而不忘记基本驾驶技能。Shaheen <em>et al.</em> (2022) 的综述<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Continual%20learning%20is%20essential%20for,implementations%20of%20continuous%20learning%20algorithms">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=under%20three%20main%20autonomous%20systems%2C,extensively%20explored%20in%20this%20article">link.springer.com</a>总结了三类自主系统（无人车、无人机和移动机器人）中的持续学习挑战：模型需在<strong>在线方式</strong>从大量顺序数据中学习，且资源受限、须保障安全稳定<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Continual%20learning%20is%20essential%20for,implementations%20of%20continuous%20learning%20algorithms">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=learning%20algorithms%20that%20perform%20continuous,are%20extensively%20explored%20in%20this">link.springer.com</a>。一些研究采用<strong>策略蒸馏</strong>或<strong>迁移学习</strong>避免遗忘旧任务策略。如 Rusu <em>et al.</em> 在DeepMind的机器人实验中，用<strong>渐进网络</strong>将仿真训练的技能迁移到现实机器人上，同时保持仿真技能不丢失<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。又如 Traoré <em>et al.</em> (2019) 提出的 <strong>DiscoRL</strong> 框架，将旧策略压缩为策略库，再用<strong>Policy Distillation（策略蒸馏）<strong>技术在新环境中融合旧策略以加速学习，同时旧策略作为教师防止遗忘<a href="https://ar5iv.org/pdf/1907.00182#:~:text=,Task%20and%20Lifelong%20Learning%2C%202019">ar5iv.org</a>。在连续控制中，策略往往以神经网络表示，类似分类任务的遗忘也会发生：新环境下调整策略网络，会导致旧环境下性能下降。为此 Rolnick <em>et al.</em> (2019) 提出的 <strong>CLEAR</strong> 方法，将</strong>off-policy经验重放</strong>引入强化学习的策略梯度训练，既提高新任务样本效率又维持旧任务价值函数不变。该方法在Atari游戏顺序学习中取得好结果，被视为强化学习领域对抗遗忘的有效方案之一。需要强调的是，强化学习场景中任务界限往往模糊，甚至代理可能在<em>一个</em>不断演变的环境中持续学习（如运营多年的家庭服务机器人，会不断遇到新任务）。这接近<strong>无任务标签 (task-agnostic)<strong>的持续学习。Aljundi <em>et al.</em> (2019) 针对此提出了</strong>在线持续学习</strong>方案：通过检测网络对新数据的<strong>干扰程度</strong>动态触发记忆重放（MIR）<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=18,2019">link.springer.com</a>，以及使用<strong>梯度稀疏化</strong>挑选对旧任务干扰最大的记忆样本来更新，从而在无明确任务边界下也能抑制遗忘。此类方法在机器人持续感知与导航中具有潜力，因为现实中机器人很难知道自己何时“切换了任务”，只能根据环境变化连续调整。</p>
<p><strong>开放世界和自主适应:</strong> 具身智能体经常处于开放世界，可能遇到训练时未见过的全新情况。持续学习的终极目标是在这种开放环境中实现持续适应而不崩溃。Open-world持续学习需要综合上述技术，还涉及<strong>新知识的自主发现</strong>和 <strong>主动学习</strong> 。比如前述CLP方法引入了<strong>新类检测</strong>机制，让机器人在开放世界下识别何时需要学习新对象<a href="https://arxiv.org/html/2404.00418v1#:~:text=Yet%2C%20FS,as%20such%20a%20system%20should">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=supervision%C2%A0%5B8%5D.%20Therefore%2C%20we%20extend%20FS,OWCL">arxiv.org</a>。又如 Mundt <em>et al.</em> (2020) 探讨了结合<strong>异常检测</strong>和持续学习，使模型在检测到输入分布偏移时能触发新任务学习流程。对自主车而言，面对从未见过的道路情况（极端天气、新施工区域），如果能自动检测出“新情境”并调用持续学习模块更新模型，将大幅提高安全性。当然，这也带来安全约束下的学习稳定性问题，需要确保新学习不会在尚未充分验证时投入决策。近期一些研究主张引入 <strong>不确定性估计</strong> （如Bayesian NN）判断模型何时需要学习新任务，以及学习后的性能变化<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Google%20Scholar">link.springer.com</a>。这些探索尚属前沿，但对于真正长期自主运作的智能体至关重要。</p>
<h2 id="近年新进展20202025与展望">近年新进展（2020–2025）与展望<a hidden class="anchor" aria-hidden="true" href="#近年新进展20202025与展望">#</a></h2>
<p>过去五年中，持续学习领域涌现了一系列新趋势和方法，进一步提高了模型在复杂环境中的持续适应能力：</p>
<ul>
<li><strong>任务无关持续学习:</strong> 越来越多工作关注在无明确任务边界、数据连续流动场景下的学习<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=15,11254%E2%80%9311263%20%282019">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=18,2019">link.springer.com</a>。这更贴近现实中的机器人/代理感知流。为此，方法上强调在线更新、有限内存和即时评估。例如 2020 年的 GDumb 方法提出一种极端简单但强大的baseline：始终只训练当前模型在收到的全部数据上（存储一定量最近数据），每次新数据到来直接从头训练。这种方法虽谈不上高明，却在一些线上学习赛道表现接近更复杂的方法，提示我们需要重新审视评价指标。在可预见的将来， <strong>线上持续学习</strong> （Single-Pass Continual Learning）将成为研究热点，它要求算法一次遍历数据且不泄露未来信息，在边学习边推理的同时抗遗忘<a href="https://arxiv.org/pdf/2302.00487#:~:text=Realistic%20applications%20present%20particular%20challenges,other%20vision%20domains%20such%20as">arxiv.org</a><a href="https://arxiv.org/pdf/2302.00487#:~:text=latter%2C%20although%20current%20advances%20mainly,methods%20are%20adapted%20to%20them">arxiv.org</a>。具身智能如实时视频流分析、持续语音识别都属于这种场景。</li>
<li><strong>持续学习评测基准丰富化:</strong> 近年构建了许多新数据集和基准来评测持续学习算法在更复杂任务上的性能。如 CORe50、OpenLORIS 等视觉序列数据集用于评测<strong>在线物体识别</strong><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=16,IEEE">link.springer.com</a>；ACL 2021 Lifelong NLP挑战提供持续自然语言理解任务；还有不同领域的持续强化学习基准、连续无人驾驶仿真环境等。这些基准推动算法从依赖任务ID的小规模实验，走向更贴近真实的情境。评测指标也越发丰富，除了遗忘率、累计精度外，开始考虑模型的<strong>计算效率、内存开销</strong>以及在长期学习中的<strong>稳定性</strong><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=trained%20models%20cannot%20effectively%20deal,implementations%20of%20continuous%20learning%20algorithms">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=learning%20algorithms%20that%20perform%20continuous,are%20extensively%20explored%20in%20this">link.springer.com</a>。这些综合指标对于具身智能系统尤为重要，因为实际应用中资源受限且需要持续运行。</li>
<li><strong>联邦持续学习与分布式学习:</strong> 在物联网和边缘计算兴起的背景下，<strong>联邦持续学习</strong>成为新方向<a href="https://arxiv.org/html/2411.13740v1#:~:text=Federated%20Continual%20Learning%20for%20Edge,AI">arxiv.org</a>。即多个分散设备（如一群机器人或智能传感器）在各自持续学习的同时，定期交流模型更新，从而在保证隐私下实现知识共享和共同进化。诸如 FedWeIT、FCL 等算法探索了如何在联邦场景下减少遗忘并高效通信<a href="https://arxiv.org/html/2411.13740v1#:~:text=Federated%20Continual%20Learning%20for%20Edge,AI">arxiv.org</a>。对具身智能来说，这意味着例如一队协作机器人的经验可以融合，使整体学习速度加快且每个体遗忘降低。该领域仍在起步，面临异步学习、设备差异等挑战，但前景值得期待。</li>
<li><strong>理论分析与可解释性:</strong> 持续学习理论方面，近年有人尝试从信息论和最优化角度给出遗忘的分析框架，如用Fisher信息界定参数迁移平衡<a href="https://ar5iv.org/pdf/1907.00182#:~:text=weights%20and%20produce%20an%20adapted,Fisher%20matrix%20using%20the%20Kronecker">ar5iv.org</a>。另外，对持续学习过程中的<strong>可解释性</strong>要求也在提高——在机器人应用中，理解模型为何遗忘某能力、何时需要触发新学习，对于建立用户信任很重要。一些研究利用可视化技术观察随着任务增添，网络内部表示如何演化，以寻找缓解遗忘的线索。还有工作将<strong>神经符号方法</strong>引入持续学习，以借助符号逻辑的约束保持旧知识。这些方向虽属于前沿探索，但表明社区已不仅满足于经验提升性能，也在寻求持续学习更深层的理论和可解释支撑。</li>
</ul>
<p>综上，持续学习作为迈向真正智能系统的关键一步，近年来在算法和应用上都取得了显著进展。从最初发现问题、提出启发式对策，到如今各种融合策略在复杂环境中落地，我们离“像人一样终身学习”的AI越来越近。在具身智能领域，实现持续学习将赋予机器人和自主代理长久的自主适应能力，使其能够随着环境和任务变化不断成长，而无需频繁人工干预。展望未来，持续学习研究需要进一步结合<strong>元学习、强化学习、因果推断</strong>等范式，研发更加通用高效的算法。同时，在真实世界大规模部署持续学习系统时，还需重视 <strong>安全机制</strong> （防止在学习过程中性能突然退化）、 <strong>伦理与隐私</strong> （学习过程中对用户数据的处理）等问题。可以预见，随着研究的深化，持续学习将在机器人自主导航、智能助理、自动驾驶乃至通用人工智能等领域扮演日益重要的角色，推动人工智能从“静态聪明”走向“动态成长”。</p>
<p><strong>参考文献：</strong></p>
<ul>
<li>McCloskey, M. &amp; Cohen, N. J. (1989).  <em>Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem</em> . In  <strong>Psychology of Learning and Motivation</strong> , vol. 24, pp. 109–165<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>. ( <em>首次揭示神经网络顺序学习遗忘问题</em> )</li>
<li>French, R. M. (1999).  <em>Catastrophic forgetting in connectionist networks</em> .  <strong>Trends in Cognitive Sciences, 3</strong> (4):128–135<a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=Unfortunately%2C%20though%2C%20catastrophic%20forgetting%20does,brain%20might%20have%20overcome%20this">cell.com</a>. ( <em>灾难性遗忘综述，分析原因并讨论可能解决方案</em> )</li>
<li>Robins, A. (1995).  <em>Catastrophic forgetting, rehearsal and pseudorehearsal</em> .  <strong>Connection Science, 7</strong> (2):123–146<a href="https://ar5iv.org/pdf/1907.00182#:~:text=,In%202016%20IEEE%2055th%20Conference">ar5iv.org</a>. ( <em>提出伪重演方法，用随机伪样本重放旧知识</em> )</li>
<li>Goodfellow, I. et al. (2014).  <em>An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</em> . In  <strong>ICLR 2014</strong> . ( <em>实证分析深度网络遗忘现象，评估基本缓解策略</em> )</li>
<li>Li, Z. &amp; Hoiem, D. (2016).  <em>Learning without Forgetting</em> . In <strong>ECCV 2016</strong><a href="https://arxiv.org/abs/1606.09282#:~:text=retraining%20on%20such%20data%20becomes,and%20new%20task%20datasets%20for">arxiv.org</a>. ( <em>知识蒸馏用于持续学习，只用新任务数据保持旧任务性能</em> )</li>
<li>Kirkpatrick, J. et al. (2017).  <em>Overcoming catastrophic forgetting in neural networks</em> .  <strong>PNAS, 114</strong> (13):3521–3526<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>. ( <em>提出EWC，通过弹性权重凝固保护重要参数<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a></em> )</li>
<li>Zenke, F. et al. (2017).  <em>Continual Learning Through Synaptic Intelligence</em> . In <strong>ICML 2017</strong><a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>. ( <em>提出SI算法，智能累积参数重要性减少遗忘<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a></em> )</li>
<li>Rebuffi, S.-A. et al. (2017).  <em>iCaRL: Incremental Classifier and Representation Learning</em> . In <strong>CVPR 2017</strong><a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail">arxiv.org</a>. ( <em>提出增量分类策略，结合样本保存和蒸馏避免遗忘<a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail">arxiv.org</a></em> )</li>
<li>Lopez-Paz, D. &amp; Ranzato, M. (2017).  <em>Gradient Episodic Memory for Continual Learning</em> . In <strong>NeurIPS 2017</strong><a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>. ( <em>提出GEM算法，用梯度约束保证新任务不增大旧任务损失<a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a></em> )</li>
<li>Shin, H. et al. (2017).  <em>Continual Learning with Deep Generative Replay</em> . In <strong>NeurIPS 2017</strong><a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks">arxiv.org</a>. ( <em>提出深度生成回放DGR，通过生成模型重现旧样本融合训练<a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks">arxiv.org</a></em> )</li>
<li>Rusu, A. A. et al. (2016).  <em>Progressive Neural Networks</em> . <strong>arXiv:1606.04671</strong><a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>. ( <em>提出渐进网络架构，扩展新列避免遗忘并实现知识迁移<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a></em> )</li>
<li>Mallya, A. &amp; Lazebnik, S. (2018).  <em>PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning</em> . In <strong>CVPR 2018</strong><a href="https://arxiv.org/abs/1711.05769#:~:text=,scale">arxiv.org</a>. ( <em>通过迭代剪枝为新任务腾出容量，实现单网络多任务无遗忘<a href="https://arxiv.org/abs/1711.05769#:~:text=pruning%20techniques%2C%20we%20exploit%20redundancies,grained">arxiv.org</a></em> )</li>
<li>Schwarz, J. et al. (2018).  <em>Progress &amp; Compress: A scalable framework for continual learning</em> . In <strong>ICML 2018</strong><a href="https://arxiv.org/abs/1805.06370#:~:text=problems,alphabets%20as%20well%20as%20two">arxiv.org</a>. ( <em>提出进展-压缩框架，结合渐进扩展和蒸馏压缩，实现无增长持续学习<a href="https://arxiv.org/abs/1805.06370#:~:text=problems,alphabets%20as%20well%20as%20two">arxiv.org</a></em> )</li>
<li>Aljundi, R. et al. (2019).  <em>Online Continual Learning with Maximal Interfered Retrieval</em> . In  <strong>NeurIPS 2019</strong> . ( <em>提出在线持续学习算法MIR，选择干扰最大的记忆样本回放，任务无关场景有效</em> )</li>
<li>Pellegrini, L. et al. (2020).  <em>Latent Replay for Real-Time Continual Learning</em> . In <strong>IROS 2020</strong><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=16,IEEE">link.springer.com</a>. ( <em>提出在特征空间进行重放，支持机器人实时持续学习，降低存储与计算需求</em> )</li>
<li>Kemker, R. &amp; Kanan, C. (2018).  <em>FearNet: Brain-Inspired Model for Incremental Learning</em> . In <strong>ICLR 2018</strong><a href="https://arxiv.org/abs/1711.10563#:~:text=for%20each%20class%2C%20making%20it,art%20performance%20at%20incremental%20class">arxiv.org</a>. ( <em>提出双内存脑启发模型，不存原始数据通过生物式记忆系统整合知识<a href="https://arxiv.org/abs/1711.10563#:~:text=for%20each%20class%2C%20making%20it,art%20performance%20at%20incremental%20class">arxiv.org</a></em> )</li>
<li>Hajizada, E. et al. (2024).  <em>Continually Learning Prototypes</em> . <strong>arXiv:2404.00418</strong><a href="https://arxiv.org/html/2404.00418v1#:~:text=require%20buffering%20and%20a%20balanced,free%2C%20hence%20does%20not">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=novelties%20and%20learning%20new%20items,simple%20version%20of%20CLP%20in">arxiv.org</a>. ( <em>提出原型持续学习方法，少样本在线学习并支持开放世界新类发现，无需存储回放</em> )</li>
<li>Shaheen, K. et al. (2022).  <em>Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks</em> .  <strong>Journal of Intelligent &amp; Robotic Systems, 105</strong> (9)<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Continual%20learning%20is%20essential%20for,implementations%20of%20continuous%20learning%20algorithms">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=learning%20algorithms%20that%20perform%20continuous,are%20extensively%20explored%20in%20this">link.springer.com</a>. ( <em>面向自主系统的持续学习综述，分析算法在无人车、无人机等中的性能和挑战</em> )</li>
<li>Lesort, T. et al. (2020).  <em>Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges</em> .  <strong>Information Fusion, 58</strong> :52–68<a href="https://arxiv.org/abs/1907.00182#:~:text=,learning%20is%20not%20necessarily%20finding">arxiv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Robotic%20agents%20have%20to%20learn,robotics%20approaches%20in%20a%20way">ar5iv.org</a>. ( <em>持续学习在机器人领域的综述，提出评测框架和跨领域方法借鉴思路</em> )</li>
</ul>
<p>引用</p>
<p><a href="https://arxiv.org/abs/1907.00182#:~:text=,learning%20is%20not%20necessarily%20finding"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=An%20important%20challenge%20for%20machine,of%20complex%20skills%20and%20knowledge"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=1%20Introduction"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=You%20want%20to%20learn%20from,not%20know%20how%20and%20when"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=cognitive%20science,not%20disrupted%20by%2C%20new%20information"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&sz=32">Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference</a><a href="https://arxiv.org/abs/2312.10549#:~:text=Catastrophic%20Forgetting%20%28CF%29,solutions%2C%20proposes%20a%20taxonomy%20to"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[2312.10549] Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomyhttps://arxiv.org/abs/2312.10549</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&sz=32">Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference</a><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=All%20natural%20cognitive%20systems%2C%20and%2C,generalize%2C%20to%20function%20in%20the">Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2</a><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=Unfortunately%2C%20though%2C%20catastrophic%20forgetting%20does,brain%20might%20have%20overcome%20this">Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2</a><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=catastrophic%20forgetting,solution%20for%20distributed%20connectionist%20networks">Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=phenomenon%20that%20reflects%20the%20trade,18%2C%203"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=McCloskey%20and%20Cohen%20,with%20backpropagation%20neural%20network%20modelling"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&sz=32">Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=each%20learning%20trial%20on%20the,Furthermore%2C%20the%20problems%202%2B1"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&sz=32">Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=,In%202016%20IEEE%2055th%20Conference"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Instead%20of%20modeling%20the%20past,based%20learning%2C%20where%20the%20model"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=the%20past%20is%20not%20forgotten,91%2C%2020%20%2C%20%20110"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf#:~:text=,Hinton%20et%20al.%2C"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://www.cs.uic.edu&sz=32">[PDF] Continual Learning and Catastrophic Forgettinghttps://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf</a><a href="https://arxiv.org/abs/1606.09282#:~:text=retraining%20on%20such%20data%20becomes,and%20new%20task%20datasets%20for"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282</a><a href="https://arxiv.org/abs/1606.09282#:~:text=for%20its%20existing%20capabilities%20are,for%20improved%20new%20task%20performance"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282</a><a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796</a><a href="https://arxiv.org/abs/1612.00796#:~:text=tasks%20by%20selectively%20slowing%20down,several%20Atari%202600%20games%20sequentially"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=weights%20and%20produce%20an%20adapted,Fisher%20matrix%20using%20the%20Kronecker"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200</a><a href="https://arxiv.org/abs/1703.04200#:~:text=intelligent%20synapses%20that%20bring%20some,forgetting%20while%20maintaining%20computational%20efficiency"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200</a><a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=also%20generative%20models%20,high%20regularization%2C%20and%20finding%20a"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Basic%20regularization%20techniques%20that%20could,More%20complex%20methods%20consist%20in"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=streams%20of%20data%20presented%20sequentially,speed%20during%20the%20learning%20process"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=One%20solution%20to%20Continual%20Learning,where%20continual%20learning%20is%20necessary"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725</a><a href="https://arxiv.org/abs/1611.07725#:~:text=only%20the%20training%20data%20for,where%20other%20strategies%20quickly%20fail"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725</a><a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1706.08840] Gradient Episodic Memory for Continual Learninghttps://arxiv.org/abs/1706.08840</a><a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=A%20classical%20method%20implementing%20a,model%20to%20learn%20next%20task"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1705.08690#:~:text=the%20problem%2C%20it%20requires%20large,sequential%20learning%20settings%20involving%20image"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690</a><a href="https://arxiv.org/abs/1705.08690#:~:text=dual%20model%20architecture%20consisting%20of,settings%20involving%20image%20classification%20tasks"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690</a><a href="https://arxiv.org/abs/1705.08690#:~:text=solving%20multiple%20tasks%20have%20been,those%20for%20a%20new%20task"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=While%20most%20of%20the%20Generative,62"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Lee%20et%20al.%20,%E2%9C%93%20%20%20%E2%9C%93"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Achille%20et%20al.%20,%E2%9C%93%20%20%20%E2%9C%93"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=%2A%20%20%5B20%5D%20H.%C2%A0Caselles,Di%C2%A0Stefano%2C%20and"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671</a><a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671</a><a href="https://arxiv.org/abs/1606.04671#:~:text=a%20step%20forward%20in%20this,layers%20of%20the%20learned%20policy"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671</a><a href="https://arxiv.org/abs/1711.05769#:~:text=,scale"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769</a><a href="https://arxiv.org/abs/1711.05769#:~:text=pruning%20techniques%2C%20we%20exploit%20redundancies,grained"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769</a><a href="https://arxiv.org/abs/1711.05769#:~:text=minimal%20storage%20overhead,available%20at%20this%20https%20URL"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769</a><a href="https://arxiv.org/abs/1711.10563#:~:text=for%20each%20class%2C%20making%20it,art%20performance%20at%20incremental%20class"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563</a><a href="https://arxiv.org/abs/1711.10563#:~:text=previous%20examples%2C%20making%20it%20memory,AudioSet%29%20benchmarks"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563</a><a href="https://arxiv.org/abs/1711.10563#:~:text=recent%20memories%20inspired%20by%20the,art%20performance%20at%20incremental%20class"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563</a><a href="https://arxiv.org/abs/1711.10563#:~:text=to%20suffer%20from%20catastrophic%20forgetting,FearNet%20also%20uses%20a%20module"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Moreover%2C%20most%20of%20continual%20learning,and%20the%20strategies%20they%20propose"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1805.06370#:~:text=problems,alphabets%20as%20well%20as%20two"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1805.06370] Progress &amp; Compress: A scalable framework for continual learninghttps://arxiv.org/abs/1805.06370</a><a href="https://arxiv.org/abs/1907.00182#:~:text=,most%20recent%20papers%20on%20continual"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Robotic%20agents%20have%20to%20learn,robotics%20approaches%20in%20a%20way"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322003004#:~:text=Towards%20lifelong%20object%20recognition%3A%20A,vision%20with%20quantifiable%20environmental%20factors"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://www.sciencedirect.com&sz=32">Towards lifelong object recognition: A dataset and benchmarkhttps://www.sciencedirect.com/science/article/abs/pii/S0031320322003004</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=16,IEEE"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://ieeexplore.ieee.org/document/9341460/#:~:text=Xplore%20ieeexplore,space%2C%20we%20store%20activations">Latent Replay for Real-Time Continual Learning - IEEE Xplorehttps://ieeexplore.ieee.org/document/9341460/</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=require%20buffering%20and%20a%20balanced,free%2C%20hence%20does%20not"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=novelties%20and%20learning%20new%20items,simple%20version%20of%20CLP%20in"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=scenarios%20where%20robots%20must%20learn,free%2C%20hence%20does%20not"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=Learning%20Prototypes%20%28CLP%29,In%20a%20low"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=networks%20and%20the%20more%20natural,towards%20realistic%20learning%20for%20robots"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=supervision%C2%A0%5B8%5D.%20Therefore%2C%20we%20extend%20FS,OWCL"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=forgetting%2C%20CLP%20utilizes%20a%20novel,In%20the%20open%20world"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=dynamic%20adaptation%20mechanism%20for%20the,part%20of%20the%20Lava%20software"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=7,6523%E2%80%936541%20%282020"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://dl.acm.org/doi/10.1145/3546790.3546791#:~:text=Interactive%20continual%20learning%20for%20robots%3A,footprint%20and%20interactive%20learning%20capability"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://dl.acm.org&sz=32">Interactive continual learning for robots: a neuromorphic approachhttps://dl.acm.org/doi/10.1145/3546790.3546791</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Continual%20learning%20is%20essential%20for,implementations%20of%20continuous%20learning%20algorithms"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=under%20three%20main%20autonomous%20systems%2C,extensively%20explored%20in%20this%20article"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=learning%20algorithms%20that%20perform%20continuous,are%20extensively%20explored%20in%20this"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=,Task%20and%20Lifelong%20Learning%2C%202019"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=18,2019"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=Yet%2C%20FS,as%20such%20a%20system%20should"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Google%20Scholar"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=15,11254%E2%80%9311263%20%282019"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://arxiv.org/pdf/2302.00487#:~:text=Realistic%20applications%20present%20particular%20challenges,other%20vision%20domains%20such%20as"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">https://arxiv.org/pdf/2302.00487</a><a href="https://arxiv.org/pdf/2302.00487#:~:text=latter%2C%20although%20current%20advances%20mainly,methods%20are%20adapted%20to%20them"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">https://arxiv.org/pdf/2302.00487</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=trained%20models%20cannot%20effectively%20deal,implementations%20of%20continuous%20learning%20algorithms"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://arxiv.org/html/2411.13740v1#:~:text=Federated%20Continual%20Learning%20for%20Edge,AI"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Federated Continual Learning for Edge-AI: A Comprehensive Surveyhttps://arxiv.org/html/2411.13740v1</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
