<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-06-16 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, Paper Summary">
<meta name="description" content="Avoid Catastrophy forget">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-06-16/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-06-16/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-06-16/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-06-16">
  <meta property="og:description" content="Avoid Catastrophy forget">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-16T14:14:45+08:00">
    <meta property="article:modified_time" content="2025-06-16T14:14:45+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-06-16">
<meta name="twitter:description" content="Avoid Catastrophy forget">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-06-16",
      "item": "https://tzj2006.github.io/bugjournal/2025-06-16/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-06-16",
  "name": "Bug Journal 2025-06-16",
  "description": "Avoid Catastrophy forget",
  "keywords": [
    "Bug Journal", "Paper Summary"
  ],
  "articleBody": "持续学习中避免灾难性遗忘：具身智能领域的研究进展综述 引言与问题背景 持续学习（Continual Learning，也称终身学习）指模型在数据分布和学习目标不断变化的情境下，能够连续学习新任务且不忘记已有知识的能力arxiv.orgar5iv.org。传统深度学习假设训练数据 i.i.d.且一次性可用，这在现实具身智能（如机器人、自主系统）中往往不成立ar5iv.orgar5iv.org。其中最大挑战之一是灾难性遗忘（catastrophic forgetting） ，即模型在顺序学习多个任务时，新知识的获取会导致旧知识被快速、大幅遗忘en.wikipedia.org。这一现象最早由 McCloskey 和 Cohen 于1989年在联结主义神经网络中发现arxiv.org，体现了所谓 稳定-可塑性权衡 ：模型既要对新信息足够可塑（plasticity），又要对既有知识保持稳定（stability）en.wikipedia.org。与生物神经系统相比，人工神经网络在顺序学习时更容易“灾难性”遗忘过去经验，而人类和动物通常表现出渐进、有选择的遗忘cell.comcell.com。如何在保持模型泛化能力的同时避免旧知识被破坏，成为持续学习研究的核心问题cell.com。为此，大量研究围绕不同技术路线展开，包括利用附加数据回放、正则化约束、动态模块化架构和外部记忆等方法来缓解遗忘arxiv.org。本文将按时间脉络梳理持续学习避免遗忘的关键进展，重点聚焦具身智能场景的应用，并比较不同方法在这些场景中的优劣差异。\n早期探索：灾难性遗忘的提出与初步对策 灾难性遗忘现象的提出（1980s–1990s）: 神经网络中的灾难性遗忘问题由 McCloskey 和 Cohen (1989)en.wikipedia.org以及 Ratcliff (1990) 首次严谨描述en.wikipedia.org。他们发现序列训练两个任务时，后学任务会显著干扰先前任务的记忆en.wikipedia.orgen.wikipedia.org。这一问题可视为Grossberg提出的稳定-可塑性两难的极端表现en.wikipedia.org。此后，研究者逐步意识到，若要让人工智能具备人类般持续学习的能力，必须解决神经网络的遗忘灾难。1990年代中期，一些学者开始探索初步对策。例如，Robins (1995) 提出了重演/伪重演 (rehearsal/pseudorehearsal) 方法，即在学习新任务时将旧任务样本或由模型生成的“伪样本”一并训练，以巩固旧知识ar5iv.org。这种方法模拟了生物大脑在睡眠中重放记忆的过程，缓解了遗忘问题，被视为后续生成式回放方法的雏形ar5iv.orgar5iv.org。与此同时，Grossberg 等人在稳定-塑性理论指导下发展**自适应共振理论 (ART)**网络，通过网络结构与记忆单元的设计减少旧记忆被覆盖的风险。这一时期的工作揭示了灾难性遗忘的严重性，并奠定了若干基本思想：通过保留过去经验（真实或模拟）或限制参数剧烈更新来保护已有知识cell.com。然而，由于当时神经网络规模和应用场景有限，这些早期方法未形成统一框架，但为后续深度学习时代的持续学习研究提供了宝贵思路。\n深度学习时代兴起前的持续学习理念: 在2000年前后，机器学习领域也出现了一些“终身学习”思想，例如 Thrun 和 Mitchell 等人在机器人领域讨论让机器人不断积累知识、自主适应新环境的算法。但受限于模型能力，这些工作多偏向理论构想或特定场景下的增量学习算法。值得一提的是，Silver et al. (2013) 等人在认知科学领域提出**“永不停歇学习”(Never-Ending Learning)的愿景，旨在构建能无限获取新知识的AI系统。这些理念上的探索进一步强调了持续学习的重要性，但真正有效的算法突破还要等待深度学习的成熟。进入2010年代，深度神经网络在图像、语音等任务上取得突破，但其遗忘现象依然明显**。Goodfellow et al. (2014) 的实证研究表明，即使现代深度网络在顺序学习多任务（如不同MNIST变换）时，仍然发生严重的性能遗忘，他们尝试用Dropout等正则策略略微缓解遗忘cs.uic.edu。这一阶段的研究重新量化了深度模型遗忘的程度，引发了学界对持续学习的关注，也为随后的关键方法发明做好了铺垫。\n深度学习时代的方法演进（2016–2020） 进入深度学习时代后，大量持续学习算法被提出。总体而言，这些方法可分为以下几类： 参数正则化 、 经验回放 （或生成回放）、 参数隔离/模块化 、以及外部记忆等arxiv.org。各类方法都有经典代表工作，我们按时间演进介绍主要方法及其贡献。\n参数正则化方法 Learning without Forgetting (LwF, 2016): Li 和 Hoiem 提出“学习不遗忘”算法arxiv.org。他们假设只能获取新任务数据，无法重温旧任务数据的现实情况，通过让模型在学习新任务时蒸馏(distillation)旧任务模型的输出分布来正则化模型参数变化arxiv.org。具体而言，在训练新任务时对旧任务模型的预测进行保持，使新模型尽可能产生与旧模型相似的输出，从而保护原有能力。LwF是知识蒸馏用于持续学习的开创性工作，实现了只用新任务数据也能较好保留旧任务性能arxiv.org。该方法在2016年ECCV发表，此后蒸馏正则化成为持续学习的重要手段之一。\nElastic Weight Consolidation (EWC, 2017): Kirkpatrick 等人（DeepMind）在PNAS 2017发表了著名的EWC算法arxiv.org。EWC通过近似计算每个参数对旧任务的重要程度（利用费舍尔信息矩阵对参数敏感度进行估计），在学习新任务的损失中添加项，惩罚对重要参数的大幅更新arxiv.org。直观来说，模型会“放慢”那些对旧任务重要参数的学习速率，以免遗忘旧知识arxiv.org。EWC在经典分类任务（如顺序MNIST）和强化学习任务（顺序Atari游戏）上验证了有效性arxiv.org。作为持续学习领域里程碑，EWC证明通过软约束参数更新，可以在一定程度上同时保持先前任务性能和新任务学习能力arxiv.org。其思路后来衍生出许多变体，例如利用更精细近似二阶信息的方法等ar5iv.org。\nSynaptic Intelligence (SI, 2017): Zenke 等人在ICML 2017提出了另一本质类似EWC的正则化方法SIarxiv.org。不同于EWC预先计算参数重要度，SI在训练过程中实时累积每个参数对损失的贡献度，结束当前任务时将其视为该参数的重要性arxiv.org。这种“智能突触”机制借鉴了生物突触强度调节的复杂性，每个突触（参数）在多个任务中累积“任务相关信息”，新任务来时利用这些信息调整学习率arxiv.org。SI在多个连续分类任务上显著降低了遗忘，同时保持了计算高效arxiv.org。与EWC相比，SI无需存储旧任务样本，同样不增加模型容量，对于资源受限的设备具有吸引力arxiv.org。但正如多数正则化方法的局限，当任务数量增多或差异较大时，单纯靠增加惩罚会导致模型学习新任务受限，需要在稳定与塑性间权衡ar5iv.orgar5iv.org。\n其他正则化进展: 除上述，2017年前后还出现了多种参数正则化方案。例如 Li et al. (2017) 的增量时刻匹配（IMM）方法通过匹配参数分布的方式融合旧新任务模型；Aljundi et al. (2018) 提出的MAS（Memory Aware Synapses）利用输出敏感度评估参数重要性。这些方法本质均为在损失函数中添加某种形式的正则项，使模型避免过度调整关键参数以保留旧知识ar5iv.orgar5iv.org。正则化方法的优点是 无需存储旧样本，内存占用低 ，易于在嵌入式/机器人等设备上实现ar5iv.org。它们的不足在于当任务间差异巨大、参数冲突严重时效果受限，而且累积过多任务后模型可能进入“过度稳定”状态难以学习新任务ar5iv.org。在具身智能场景中，由于设备算力和存储有限，正则化方法仍是常用选择之一。例如 EWC 被用于机器人连续控制任务以保护低层政策参数不被遗忘arxiv.org。但如果机器人遇到全新领域任务，正则化可能限制其适应新技能的能力，这是后续方法力图解决的问题。\n经验重放与生成式回放方法 显式经验重放 (Experience Replay): 针对持续学习，最直接的思路是 在学习新任务时重温部分旧任务的数据 ，仿佛让模型“复习”以前的知识ar5iv.org。Rebuffi 等人在CVPR 2017提出的 iCaRL 方法将这一思想与深度学习结合arxiv.org。iCaRL在每学新类别时， 保存每个旧类别少量代表样本（记忆库） ，训练时将这些旧样本与新数据一起用于更新模型，并对模型输出进行知识蒸馏以防决策边界偏移arxiv.org。通过同时学习分类新类别和回顾旧类别，iCaRL实现了深度网络在长时间增量学习许多类时，比仅新数据训练的策略遗忘显著减少arxiv.org。iCaRL开创了样本记忆回放+蒸馏结合的范式。之后许多增量学习方法沿用了“小样本记忆”思路，如 Hou et al. (2019) 的UCIR、Wu et al. (2019) 的BiC等，都在如何精选和高效利用少量旧样本上下功夫。经验重放策略也直接应用于强化学习领域——在非平稳环境中，智能体可将过去经历的轨迹保存一部分，在策略更新时混入重放，以避免策略完全偏离先前成功经验。这与深度Q网络（DQN）的经验回放缓冲理念一脉相承，只是这里目的是防遗忘而不仅是破除相关性。在机器人学习中，经验重放意味着机器人在学习新技能时定期练习已掌握的技能（通过模拟旧技能的传感器输入等），这在实践中提高了多技能机器人系统的鲁棒性。\nGradient Episodic Memory (GEM, 2017): Lopez-Paz \u0026 Ranzato 在 NeurIPS 2017 提出的 GEM 则进一步创新了重放的使用方式arxiv.org。与直接将旧样本混入训练不同，GEM把少量旧任务样本存入 episodic memory ，在每次参数更新时，通过约束新梯度与旧样本梯度的内积为非负，确保新任务训练不会增加旧任务损失arxiv.org。这种基于优化约束的方法保证了模型对记忆样本性能不下降，实现了一定的“向后迁移”能力：在学习新任务的同时还有可能改进旧任务表现arxiv.org。GEM开创了利用回放样本的梯度信息指导优化的思路，其后续简化版A-GEM (Chaudhry et al. 2018)降低了计算成本。对于具身智能而言，GEM这类方法的优势在于即使少量记忆也能通过优化约束起效，而且不需要明确任务边界（可以对任意过去经验施加约束）。不过其劣势是需要实时计算并存储梯度，复杂度较高，且仍需维护一个小型记忆库。\n生成式回放 (Generative Replay): 当直接存储原始旧样本受限时，另一策略是训练生成模型来产生日前学过的数据，从而实现回放。Shin et al. 在 NeurIPS 2017 提出的 Deep Generative Replay (DGR) 是该思路的里程碑arxiv.org。DGR构建了一个生成模型（如GAN或VAE）作为“记忆仿真器”，在学习新任务时利用生成模型产生日前各任务的合成样本，并与新数据混合训练Solver模型arxiv.org。在每完成一个任务后，Solver的参数固定，然后训练生成模型去拟合更新后的Solver分布，以便下次产生更新的数据分布ar5iv.org。这种双模型协同框架受到大脑“海马-新皮层”互作机制的启发，即利用快速变化的“海马体”生成回忆来训练慢更新的“皮层”网络arxiv.org。DGR实验证明，即使不保存任何真实旧样本，模型仍能通过生成模拟数据达到与有存储时相近的效果ar5iv.orgarxiv.org。生成回放的优势是 不直接占用存储真实数据 ，在隐私敏感或内存极小的设备上尤为有用arxiv.org。其缺点在于生成模型本身也面临持续学习问题（如何不忘记早期的数据分布），且训练开销较大ar5iv.org。后续不少工作改进了生成回放，如 Wu et al. (2018) 将GAN与变分特征结合（MeRGAN），Rios et al. (2018) 用生成对抗网络生成特征而非像素，提高效率等ar5iv.orgar5iv.org。生成回放还被应用到强化学习的状态生成中，例如 Caselles-Dupré et al. (2019) 提出的自触发生成回放（S-TRIGGER）用于连续学习环境状态表示ar5iv.org。总的来看，回放类方法（包括经验重放和生成回放）在各种基准上往往表现突出，被认为是目前抗遗忘最有效的范式之一。然而它们对存储或生成能力有要求，在具身智能中需权衡内存/算力和性能：对于机器人等设备，存储少量关键经验（如图像片段、关键帧）进行回放在实践中较常用，而实时训练复杂生成模型则相对少见。\n模块化架构与参数隔离方法 Progressive Neural Networks (PNN, 2016): Rusu 等人提出的渐进神经网络是持续学习的架构派代表arxiv.org。PNN在每遇到新任务时 冻结已有网络 ，并侧旁新增一组“列”网络用于学习新任务arxiv.org。同时，通过旁路连接让新任务列能够利用之前各列学到的特征（实现知识迁移）arxiv.org。这种架构确保旧任务的参数永不修改，从而彻底避免遗忘arxiv.org；而新增模块可以专门学习新任务，有充足的模型容量。PNN在Atari游戏和3D迷宫导航等一系列强化学习任务上取得优于微调的成绩，并显示出显著的前向迁移能力arxiv.org。其缺点也很明显：每增加一个任务网络规模就线性增长，在任务数很多时不切实际arxiv.org。尽管如此，PNN证明了模块隔离在避免遗忘上的有效性，许多后续方法受此启发引入可扩展或可选择激活的架构。\nPathNet (2017) 与 PackNet (2018): 为了缓解PNN网络爆炸的问题，Fernando 等人 (2017) 提出的 PathNet 利用进化算法在固定网络中为每个任务选择一条互不干扰的子网络路径，相当于在共享参数的前提下实现参数隔离。Mallya 和 Lazebnik (2018) 则提出 PackNet ，通过反复剪枝和重训练来为新任务腾出参数空间arxiv.org。具体来说，PackNet先训练初始任务模型，然后剪除一定比例不重要的参数（权重置零但保留位置），学习第二个任务时仅利用空闲参数；如此迭代，将多个任务“打包”进单个网络中arxiv.org。实验表明，在ImageNet等大型数据上，PackNet可在一个VGG模型中连续容纳多个细粒度分类任务，性能接近于单独训练arxiv.org。PackNet无需存储旧数据，也不引入新参数，因此相比PNN更高效arxiv.org。但PackNet需要预先设定剪枝比例，且剪枝过多可能损害旧任务性能，过少则限制新任务空间。后来一些变体如 “Piggyback” (Mallya, 2018) 则改为学习任务特定的掩码，更灵活地实现参数复用。总体而言，参数隔离类方法（含动态扩张和网络剪枝）通过结构上的硬约束避免了遗忘，其优势是旧知识完全保留、无干扰arxiv.org。在机器人等具身智能中，如果任务集是离散且有限的，这类方法可考虑使用。例如在多任务机器人控制中，可为每个任务分配专属网络模块或参数子集，新任务加入时扩展网络并冻结旧模块，从而保持以往技能arxiv.org。然而，在开放环境下任务可能连续涌现且无法预知数量，单纯无限扩展网络不切实际。因此近期一些工作尝试结合元学习或 条件网络 ，自动决定何时复用旧参数、何时增加新参数，以兼顾模型规模和遗忘防护。比如 Serra et al. (2018) 提出的 HAT 方法对每层参数学习可训练门控，通过门控向量的稀疏化实现在相同网络中隔离不同任务的激活区域，从而在不显著增加参数的情况下减少干扰。\n脑启发的双记忆体系: 值得注意的是，一些方法从神经科学的双重内存理论汲取灵感，将快速学习模块和稳定长时模块结合起来应对遗忘。例如 Kemker 和 Kanan (2018) 提出的 FearNet 模型采用“大脑 海马-新皮层 ”的架构arxiv.org：用一个类似海马体的小网络专门快速学习当前任务，并在适当时机（模拟睡眠）将新知识整合（consolidate）到另一个类似皮层的大网络中做长期存储arxiv.org。同时还有一个类似杏仁核的模块，根据输入判断应该用哪套记忆系统回答arxiv.org。FearNet不需存储旧样本，依赖生成式机制回忆旧类数据，达到与iCaRL相当的性能arxiv.org。这类方法实质上属于架构+回放的混合策略（因为短期网本身可看作一种内生记忆生成器）。双记忆策略对具身智能有自然的意义：机器人或代理可以配置一个“小而快”的在线学习器来及时适应新变化，同时定期将知识固化到“大而稳”的长期模型中，从而两全其美。不过如何确定巩固频率以及双网络的容量匹配仍在探索中。\n方法对比与小结 不同持续学习方法各有优劣，在具身智能场景下需要平衡选择ar5iv.org。正则化方法不需保存样本、开销低，适合嵌入式设备在线更新，但在任务变化剧烈时可能束缚新知识获取ar5iv.org。回放方法往往效果最佳，即便少量样本重放也能显著降低遗忘arxiv.org；对于机器人这种可反复与环境交互的场景，还可通过自主采样过去环境状态进行重演。然而存储真实数据可能受限于隐私或容量，而训练生成模型又对计算资源有较高要求arxiv.org。模块化/参数隔离方法彻底杜绝了遗忘，在多任务机器人系统（任务有限且可拆分）中很有价值，但在开放任务中扩展性受限arxiv.org。外部记忆和双重内存策略提供了一种折中：通过引入专门的记忆模块，模型可以在不反复调整主要网络权重的情况下查询和更新知识。例如在多人对话交互机器人中，引入一个可读写的记忆单元存储历史对话要点，有助于长期一致的对话理解。但引入记忆也增大了系统复杂度，需要设计高效的检索和写入机制。\n此外，许多先进方法不再局限于单一策略，而是混合多种机制以取长补短ar5iv.org。例如 Schwarz 等人 (2018) 提出的 Progress \u0026 Compress 框架将动态架构与蒸馏结合：使用Progressive Network扩展新任务列，然后通过蒸馏将新列知识压缩回主干网络，从而既避免遗忘又控制模型规模arxiv.org。再如 von Oswald et al. (2019) 的 MER 方法将元学习思想融入记忆重放，通过元训练提高模型表示对新旧任务的解耦，从而辅助减少干扰。这些综合方法在近年不断涌现，说明持续学习领域正朝着多策略融合与自动适应方向发展。\n具身智能场景中的持续学习应用 具身智能领域（如机器人、自主车辆、智能代理）为持续学习提供了最实际也最具挑战的用武之地arxiv.org。与静态数据集不同，具身智能体在物理世界中连续感知和行动，环境非平稳且任务边界往往不明确ar5iv.orgar5iv.org。以下我们重点考察持续学习方法在几个具身场景的应用进展：\n机器人视觉与物体识别： 服务机器人需要在不断变化的环境中识别新对象、适应新场景，这正是开放域的持续学习问题。为评测算法，2020年提出了OpenLORIS-Object机器人视觉数据集，包含随时间推移环境光照、视角、物距等变化的数据流sciencedirect.com。在该数据集上，Lomonaco 等人组织了持续学习挑战，促进了算法在真实机器人感知条件下的比较。一系列方法被测试：如 iCaRL 的小样本存储结合知识蒸馏策略在这种增量物体识别中取得稳健表现；又如 IROS 2020 的 Latent Replay 方法，Pellegrini et al. 提出只在特征空间保存和重放旧数据link.springer.com。具体而言，机器人摄像头图像经卷积网络得到中间表示，将这些低维激活缓存代替原始高维图像，可大幅减少存储并实现实时回放ieeexplore.ieee.org。实验表明，在OpenLORIS这种持续视觉任务中，Latent Replay比直接存图像几乎不降性能，却更高效满足机器人实时性需求ieeexplore.ieee.org。另一最新进展是 Hajizada et al. (2024) 提出的 Continually Learning Prototypes (CLP) 算法arxiv.orgarxiv.org。CLP针对机器人少样本在线学习和开放世界场景设计：它采用原型向量表征每类知识，并通过元可塑性机制动态调整每个原型的学习速率来平衡新旧知识稳定性arxiv.orgarxiv.org。同时CLP具备新类别自我检测与无监督学习能力（即机器人遇到未知物体时可判断新类别并自主创建原型学习）arxiv.orgarxiv.org。重要的是，CLP不使用任何显式回放数据且兼容神经形态芯片，实现了超低能耗下的持续学习arxiv.orgarxiv.org。这对于内存和电池有限的移动机器人具有现实意义。总的来看，在机器人视觉领域，混合使用 小样本记忆 、 特征回放 、适应性学习率等技术已取得显著效果，使机器人能逐步扩展认知能力且遗忘受控。\n人机交互与多模态学习： 具身智能体常涉及多模态感知（视觉、听觉、语言）和人机交互，这带来了持续学习的新课题。例如社交机器人需要持续学习新的对话内容、新的手势动作等。NLP领域已有针对增量学习的综述（如 Biesialska et al. , 2020link.springer.com），其中提到自然语言处理任务在持续学习中面临词汇和语义随时间演变的问题。一些方法通过动态扩充词典或嵌入空间缓解了“遗忘”早期语义的现象。对于多模态交互，Kulkarni et al. (2019) 提出在对话系统中使用弹性权重约束来保留模型早期对话技能，同时新增新领域对话意图。交互学习中一个重要方面是 用户在环（human-in-the-loop） ：机器人可通过用户反馈实时修正知识。近期有工作探索 交互式持续学习 ，如 Hazifa et al. (2022) 结合神经形态计算，利用片上在线学习快速吸收用户教授的新知识，同时通过正则保护已有知识dl.acm.org。虽然具体算法仍在早期，但这些尝试指出了方向——未来的具身智能体应能通过持续人机交互 自我进化 ，并且做到“学而不忘”。\n连续控制与强化学习： 在自主驾驶、机器人控制等连续决策场景，持续学习同样关键。例如自动驾驶车辆遇到新道路场景，需要学习新策略而不忘记基本驾驶技能。Shaheen et al. (2022) 的综述link.springer.comlink.springer.com总结了三类自主系统（无人车、无人机和移动机器人）中的持续学习挑战：模型需在在线方式从大量顺序数据中学习，且资源受限、须保障安全稳定link.springer.comlink.springer.com。一些研究采用策略蒸馏或迁移学习避免遗忘旧任务策略。如 Rusu et al. 在DeepMind的机器人实验中，用渐进网络将仿真训练的技能迁移到现实机器人上，同时保持仿真技能不丢失arxiv.org。又如 Traoré et al. (2019) 提出的 DiscoRL 框架，将旧策略压缩为策略库，再用Policy Distillation（策略蒸馏）技术在新环境中融合旧策略以加速学习，同时旧策略作为教师防止遗忘ar5iv.org。在连续控制中，策略往往以神经网络表示，类似分类任务的遗忘也会发生：新环境下调整策略网络，会导致旧环境下性能下降。为此 Rolnick et al. (2019) 提出的 CLEAR 方法，将off-policy经验重放引入强化学习的策略梯度训练，既提高新任务样本效率又维持旧任务价值函数不变。该方法在Atari游戏顺序学习中取得好结果，被视为强化学习领域对抗遗忘的有效方案之一。需要强调的是，强化学习场景中任务界限往往模糊，甚至代理可能在一个不断演变的环境中持续学习（如运营多年的家庭服务机器人，会不断遇到新任务）。这接近无任务标签 (task-agnostic)的持续学习。Aljundi et al. (2019) 针对此提出了在线持续学习方案：通过检测网络对新数据的干扰程度动态触发记忆重放（MIR）link.springer.com，以及使用梯度稀疏化挑选对旧任务干扰最大的记忆样本来更新，从而在无明确任务边界下也能抑制遗忘。此类方法在机器人持续感知与导航中具有潜力，因为现实中机器人很难知道自己何时“切换了任务”，只能根据环境变化连续调整。\n开放世界和自主适应: 具身智能体经常处于开放世界，可能遇到训练时未见过的全新情况。持续学习的终极目标是在这种开放环境中实现持续适应而不崩溃。Open-world持续学习需要综合上述技术，还涉及新知识的自主发现和 主动学习 。比如前述CLP方法引入了新类检测机制，让机器人在开放世界下识别何时需要学习新对象arxiv.orgarxiv.org。又如 Mundt et al. (2020) 探讨了结合异常检测和持续学习，使模型在检测到输入分布偏移时能触发新任务学习流程。对自主车而言，面对从未见过的道路情况（极端天气、新施工区域），如果能自动检测出“新情境”并调用持续学习模块更新模型，将大幅提高安全性。当然，这也带来安全约束下的学习稳定性问题，需要确保新学习不会在尚未充分验证时投入决策。近期一些研究主张引入 不确定性估计 （如Bayesian NN）判断模型何时需要学习新任务，以及学习后的性能变化link.springer.com。这些探索尚属前沿，但对于真正长期自主运作的智能体至关重要。\n近年新进展（2020–2025）与展望 过去五年中，持续学习领域涌现了一系列新趋势和方法，进一步提高了模型在复杂环境中的持续适应能力：\n任务无关持续学习: 越来越多工作关注在无明确任务边界、数据连续流动场景下的学习link.springer.comlink.springer.com。这更贴近现实中的机器人/代理感知流。为此，方法上强调在线更新、有限内存和即时评估。例如 2020 年的 GDumb 方法提出一种极端简单但强大的baseline：始终只训练当前模型在收到的全部数据上（存储一定量最近数据），每次新数据到来直接从头训练。这种方法虽谈不上高明，却在一些线上学习赛道表现接近更复杂的方法，提示我们需要重新审视评价指标。在可预见的将来， 线上持续学习 （Single-Pass Continual Learning）将成为研究热点，它要求算法一次遍历数据且不泄露未来信息，在边学习边推理的同时抗遗忘arxiv.orgarxiv.org。具身智能如实时视频流分析、持续语音识别都属于这种场景。 持续学习评测基准丰富化: 近年构建了许多新数据集和基准来评测持续学习算法在更复杂任务上的性能。如 CORe50、OpenLORIS 等视觉序列数据集用于评测在线物体识别link.springer.com；ACL 2021 Lifelong NLP挑战提供持续自然语言理解任务；还有不同领域的持续强化学习基准、连续无人驾驶仿真环境等。这些基准推动算法从依赖任务ID的小规模实验，走向更贴近真实的情境。评测指标也越发丰富，除了遗忘率、累计精度外，开始考虑模型的计算效率、内存开销以及在长期学习中的稳定性link.springer.comlink.springer.com。这些综合指标对于具身智能系统尤为重要，因为实际应用中资源受限且需要持续运行。 联邦持续学习与分布式学习: 在物联网和边缘计算兴起的背景下，联邦持续学习成为新方向arxiv.org。即多个分散设备（如一群机器人或智能传感器）在各自持续学习的同时，定期交流模型更新，从而在保证隐私下实现知识共享和共同进化。诸如 FedWeIT、FCL 等算法探索了如何在联邦场景下减少遗忘并高效通信arxiv.org。对具身智能来说，这意味着例如一队协作机器人的经验可以融合，使整体学习速度加快且每个体遗忘降低。该领域仍在起步，面临异步学习、设备差异等挑战，但前景值得期待。 理论分析与可解释性: 持续学习理论方面，近年有人尝试从信息论和最优化角度给出遗忘的分析框架，如用Fisher信息界定参数迁移平衡ar5iv.org。另外，对持续学习过程中的可解释性要求也在提高——在机器人应用中，理解模型为何遗忘某能力、何时需要触发新学习，对于建立用户信任很重要。一些研究利用可视化技术观察随着任务增添，网络内部表示如何演化，以寻找缓解遗忘的线索。还有工作将神经符号方法引入持续学习，以借助符号逻辑的约束保持旧知识。这些方向虽属于前沿探索，但表明社区已不仅满足于经验提升性能，也在寻求持续学习更深层的理论和可解释支撑。 综上，持续学习作为迈向真正智能系统的关键一步，近年来在算法和应用上都取得了显著进展。从最初发现问题、提出启发式对策，到如今各种融合策略在复杂环境中落地，我们离“像人一样终身学习”的AI越来越近。在具身智能领域，实现持续学习将赋予机器人和自主代理长久的自主适应能力，使其能够随着环境和任务变化不断成长，而无需频繁人工干预。展望未来，持续学习研究需要进一步结合元学习、强化学习、因果推断等范式，研发更加通用高效的算法。同时，在真实世界大规模部署持续学习系统时，还需重视 安全机制 （防止在学习过程中性能突然退化）、 伦理与隐私 （学习过程中对用户数据的处理）等问题。可以预见，随着研究的深化，持续学习将在机器人自主导航、智能助理、自动驾驶乃至通用人工智能等领域扮演日益重要的角色，推动人工智能从“静态聪明”走向“动态成长”。\n参考文献：\nMcCloskey, M. \u0026 Cohen, N. J. (1989). Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem . In Psychology of Learning and Motivation , vol. 24, pp. 109–165en.wikipedia.org. ( 首次揭示神经网络顺序学习遗忘问题 ) French, R. M. (1999). Catastrophic forgetting in connectionist networks . Trends in Cognitive Sciences, 3 (4):128–135cell.com. ( 灾难性遗忘综述，分析原因并讨论可能解决方案 ) Robins, A. (1995). Catastrophic forgetting, rehearsal and pseudorehearsal . Connection Science, 7 (2):123–146ar5iv.org. ( 提出伪重演方法，用随机伪样本重放旧知识 ) Goodfellow, I. et al. (2014). An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks . In ICLR 2014 . ( 实证分析深度网络遗忘现象，评估基本缓解策略 ) Li, Z. \u0026 Hoiem, D. (2016). Learning without Forgetting . In ECCV 2016arxiv.org. ( 知识蒸馏用于持续学习，只用新任务数据保持旧任务性能 ) Kirkpatrick, J. et al. (2017). Overcoming catastrophic forgetting in neural networks . PNAS, 114 (13):3521–3526arxiv.org. ( 提出EWC，通过弹性权重凝固保护重要参数arxiv.org ) Zenke, F. et al. (2017). Continual Learning Through Synaptic Intelligence . In ICML 2017arxiv.org. ( 提出SI算法，智能累积参数重要性减少遗忘arxiv.org ) Rebuffi, S.-A. et al. (2017). iCaRL: Incremental Classifier and Representation Learning . In CVPR 2017arxiv.org. ( 提出增量分类策略，结合样本保存和蒸馏避免遗忘arxiv.org ) Lopez-Paz, D. \u0026 Ranzato, M. (2017). Gradient Episodic Memory for Continual Learning . In NeurIPS 2017arxiv.org. ( 提出GEM算法，用梯度约束保证新任务不增大旧任务损失arxiv.org ) Shin, H. et al. (2017). Continual Learning with Deep Generative Replay . In NeurIPS 2017arxiv.org. ( 提出深度生成回放DGR，通过生成模型重现旧样本融合训练arxiv.org ) Rusu, A. A. et al. (2016). Progressive Neural Networks . arXiv:1606.04671arxiv.org. ( 提出渐进网络架构，扩展新列避免遗忘并实现知识迁移arxiv.org ) Mallya, A. \u0026 Lazebnik, S. (2018). PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning . In CVPR 2018arxiv.org. ( 通过迭代剪枝为新任务腾出容量，实现单网络多任务无遗忘arxiv.org ) Schwarz, J. et al. (2018). Progress \u0026 Compress: A scalable framework for continual learning . In ICML 2018arxiv.org. ( 提出进展-压缩框架，结合渐进扩展和蒸馏压缩，实现无增长持续学习arxiv.org ) Aljundi, R. et al. (2019). Online Continual Learning with Maximal Interfered Retrieval . In NeurIPS 2019 . ( 提出在线持续学习算法MIR，选择干扰最大的记忆样本回放，任务无关场景有效 ) Pellegrini, L. et al. (2020). Latent Replay for Real-Time Continual Learning . In IROS 2020link.springer.com. ( 提出在特征空间进行重放，支持机器人实时持续学习，降低存储与计算需求 ) Kemker, R. \u0026 Kanan, C. (2018). FearNet: Brain-Inspired Model for Incremental Learning . In ICLR 2018arxiv.org. ( 提出双内存脑启发模型，不存原始数据通过生物式记忆系统整合知识arxiv.org ) Hajizada, E. et al. (2024). Continually Learning Prototypes . arXiv:2404.00418arxiv.orgarxiv.org. ( 提出原型持续学习方法，少样本在线学习并支持开放世界新类发现，无需存储回放 ) Shaheen, K. et al. (2022). Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks . Journal of Intelligent \u0026 Robotic Systems, 105 (9)link.springer.comlink.springer.com. ( 面向自主系统的持续学习综述，分析算法在无人车、无人机等中的性能和挑战 ) Lesort, T. et al. (2020). Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges . Information Fusion, 58 :52–68arxiv.orgar5iv.org. ( 持续学习在机器人领域的综述，提出评测框架和跨领域方法借鉴思路 ) 引用\n[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[2312.10549] Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomyhttps://arxiv.org/abs/2312.10549Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interferenceCatastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[PDF] Continual Learning and Catastrophic Forgettinghttps://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725[1706.08840] Gradient Episodic Memory for Continual Learninghttps://arxiv.org/abs/1706.08840[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182[1805.06370] Progress \u0026 Compress: A scalable framework for continual learninghttps://arxiv.org/abs/1805.06370[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Towards lifelong object recognition: A dataset and benchmarkhttps://www.sciencedirect.com/science/article/abs/pii/S0031320322003004Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccLatent Replay for Real-Time Continual Learning - IEEE Xplorehttps://ieeexplore.ieee.org/document/9341460/Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccInteractive continual learning for robots: a neuromorphic approachhttps://dl.acm.org/doi/10.1145/3546790.3546791Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfcc[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccContinual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfcchttps://arxiv.org/pdf/2302.00487https://arxiv.org/pdf/2302.00487Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent \u0026 Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported\u0026code=2588747a-8932-4197-a391-b846298fdfccFederated Continual Learning for Edge-AI: A Comprehensive Surveyhttps://arxiv.org/html/2411.13740v1\n2024–2025 年具身智能持续学习新方法综述：缓解灾难性遗忘的创新策略 持续学习（Continual Learning）旨在让智能体能顺序学习多个任务而不遗忘已学知识。然而传统方法（如 EWC、iCaRL、GEM、PackNet、DGR 等）主要在静态数据上验证，在具身智能场景中效果有限。具身智能中的持续学习面临真实环境的挑战：任务顺序模糊、样本稀少、实时交互、高维感知等。这要求新机制在稳定-可塑性间取得更佳平衡，避免旧知识被覆盖（灾难性遗忘）同时高效习得新技能。以下我们综述 2024–2025 年出现的多篇新论文，每篇提供方法简介、核心创新、与传统方法的区别及其在具身场景中的优势分析。 Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation (Yuanqi Yao 等, 2025 年, arXiv) 方法简介：这项工作提出了PSPL（Primitive-level Skill Prompt Learning）方法，用于机器人操作的无重温持续学习。PSPL 将技能解构为可复用的“原语”（primitives），通过前缀提示（prefix prompts）来表示，并在两阶段训练方案中不断扩展技能库。首先进行多技能联合预训练，学习一组运动感知的技能提示（motion-aware skill prompts），提取不同技能间的共性语义与运动原语。随后在增量学习新技能时，为每个新技能添加新的前缀提示，通过与旧技能提示的跨注意力机制共享知识，从而加速新技能学习。这一提示学习策略使机器人能够在无需重放旧任务数据的情况下连续获取新技能。 **核心创新：PSPL 将提示学习（prompt learning）引入机器人持续学习领域，创新性地使用“技能前缀提示”作为可扩展的模块化单元来表示和存储技能知识。相比以往为每个新任务增添适配器的方式，PSPL 的提示可以在技能之间共享和重用，显式捕捉技能间的共性原语（包括语义和运动层面的共性）。此外提出的文本+光流联合提示查询（text-flow query）机制，将语言指令与视觉运动信息融合，用于检索适当的原语提示，从而关联语义截然不同但运动模式相似的技能。这一机制解决了仅靠文本嵌入查询时，不同技能间缺乏关联的问题。 与传统方法区别：相较于 EWC 等对网络权重施加正则的方式，PSPL 并不限制参数更新，而是通过前缀提示的模块化扩展来避免遗忘：旧技能的提示向量固定，新技能通过新增提示获取模型容量，从而防止旧知识被覆盖。这类似 PackNet 的“逐任务分配参数”思想，但PSPL的提示可以通过跨注意力与旧提示交互，实现旧新知识的共享迁移，弥补 PackNet 类方法模块隔离导致的无法迁移问题。与 iCaRL、GEM 等需要存储旧样本回放不同，PSPL 不需要任何经验回放——模型通过提示机制保留旧技能，因此在训练新任务时无需重温旧数据便可避免遗忘。相比于 LwF 这类基于知识蒸馏的方法，PSPL直接在模型内部保留了原技能的“软提示”，避免了仅凭日志概率约束可能出现的信息不足。 具身场景下的优势：首先，无重放需求降低了实际机器人系统的存储和隐私负担（无需记录旧任务视频或演示数据），非常适合那些无法无限存储过去经验的场景。其次，PSPL 的原语提示捕捉了跨任务的共性动作模式，使机器人能在少样本情况下将已学技能迁移到新任务上，例如仅观察少量演示也能通过提示召回相关原语来加速学习。再次，该方法对每个技能只新增少量提示向量，属于参数高效的扩展方式，不像全面微调那样消耗大量新参数，适合机器人有限算力的约束。最后，在 LIBERO 基准的模拟和真实操作实验中，PSPL 在前向迁移（FWT）和背向保留（BWT）指标上均达到当前最好水平，验证了其在长期任务序列中的遗忘防御和跨技能知识共享能力。 Preserving and Combining Knowledge in Robotic Lifelong Reinforcement Learning (Yuan Meng 等, 2025 年, Nature Machine Intelligence) 方法简介：该研究提出了名为LEGION的机器人终身强化学习框架 nature.com 。LEGION 构建了一个非参数贝叶斯知识空间（knowledge space），用以逐步积累和保存机器人的技能知识。具体而言，作者使用Dirichlet 过程混合模型（DPMM）来根据任务生成的表征不断拓展知识空间的簇，以容纳新任务知识，同时保留旧任务的簇不被覆盖。每当机器人从连续到来的一次性任务流中学习完一个任务，DPMM 就在知识空间中产生或调整相应的知识组分，实现对该任务知识的分离存储与渐进扩充。此外，LEGION 将语言嵌入引入任务表征，使智能体理解任务语义，从而在学习和推理中利用语言指导任务推断。在执行阶段，机器人能够将知识空间中多个任务的技能组合重新应用于长序列复合任务（例如依次执行多步操作完成“清理桌子”等目标）。 核心创新：LEGION 的突出创新在于引入贝叶斯非参模型管理知识：不同于传统固定架构，DPMM 知识空间可以动态增加新的知识组分，无需预先限定任务数量。这解决了过去结构模块化方法在任务数未知时难以扩展的问题 nature.com 。知识空间中的每个簇本质上代表一类技能或策略的“知识单元”，新任务到来时，如果其知识无法用现有簇解释，就会自动生成新簇存储，避免了旧知识的遗忘与冲突。同时，每个任务的知识以概率混合模型形式保存在连续的先验空间中，便于在需要时被识别和重新调用。另一个创新是融合语言描述提升任务区分和推理能力：语言嵌入提供了任务的高层语义提示，帮助知识空间进行任务归属推断和知识检索，使得在执行长视距任务时，智能体可以更准确地推理需要调用哪些已有技能。 与传统方法区别：LEGION 综合了多种持续学习理念但又有所突破。相较于 EWC 等正则化方法，LEGION 不直接作用于网络参数，而是在参数输出的高层空间保存知识，因而不存在权重被覆盖的问题。相比 PackNet 这类结构隔离方法，LEGION 的知识空间无需预设模块数且能持续增长，解决了传统结构方法难以应对未知数量任务的局限 nature.com 。同时，有别于纯经验回放策略（如 GEM）必须存储大量过往数据，LEGION 通过贝叶斯记忆高效概括了过去任务的精华，不需要完整保留旧样本即可实现类似回放的效果。虽然训练中仍使用有限缓冲作为强化学习的 replay（SAC 算法自身需要）, 但由于知识已存于非参空间，LEGION 即使在不增加缓冲容量的情况下依然能逐任务提高成功率。另外，LEGION 与以往假定任务明确分布范围的多任务学习不同，它能够适应非参数任务变化，处理现实中出现的新规则或新交互形式——这些情景下老方法（例如基于高斯混合的元学习）因需预设任务数量而难以胜任。 具身场景下的优势：LEGION 在真实机器人持续学习中展示出多项优势。首先，其知识保存与扩展机制使机器人几乎无遗忘地累积技能——实验表明，即使经过长时间训练再次回访早期任务，成功率仍接近初始掌握水平，证明旧技能通过知识空间得到有效保留。其次，LEGION 可以将已学知识用于新任务的快速推理和组合，体现了少样本快速迁移能力：作者展示了机器人在学习一系列基本任务后，能够在一次提示下将相关技能串联完成复杂长任务，例如“制作咖啡”，这源于知识空间支持对已学技能的自由组合。再次，LEGION 引入语言降低了感知与任务推断难度，使机器人在多模态指令下表现出色——通过语言描述任务，机器人无需大量探索即可定位对应知识簇，从而更快适应新任务语境 nature.com 。最后，在对比实验中，LEGION 在顺序任务中稳定提升成功率，显著优于“完美记忆”（无限重放）、“reservoir采样回放”和 A-GEM 等方法。这表明在数据受限、任务未知的真实环境中，LEGION 的知识空间机制比传统持续学习策略更适合维持和重用机器人知识。 LEAGUE++: Empowering Continual Robot Learning through Guided Skill Acquisition with Language Models (Zhaoyi Li 等, 2024 年, VLMNM 2024 研讨会) 方法简介：LEAGUE++ 提出了一种利用大语言模型（LLM）引导机器人持续学习技能的框架。该方法将任务与运动规划（TAMP）和深度强化学习（DRL）相结合：先由预训练的大语言模型将复杂长任务自动分解为一系列子任务操作序列（即生成操作符及步骤规划），并产生日志级别的稠密奖励信号指导每个子任务的强化学习训练。机器人通过 DRL 学习各子步骤的低层技能，同时维护一个符号化技能库记录已学的技能。当新任务到来时，LEAGUE++ 使用已有技能库中的技能作为Warm-Start（预热），加速学习新技能。整个系统在四个模拟任务域上进行了验证，包括家务操作等复杂任务场景。结果表明，LEAGUE++ 相比基线有更快的学习速度和更高的任务成功率，并能通过技能重用在新域中显著缩短收敛时间。 核心创新：LEAGUE++ 最大的创新在于将大模型的规划能力和传统强化学习有机结合，实现“边规划、边学习”。具体来说，它利用 LLM 强大的语义理解和分解能力，自动将高层指令分解为低层动作序列（解决了RL直接面对长远稀疏奖励任务时的困难），从而大幅降低了长视距任务的学习难度。其次，它提出了符号技能库概念：已学技能以可调用的符号模块形式存储，使机器人在新任务时可以检索并复用相关技能，而非从零学习。这种技能库机制相当于一种渐进专家网络：随着任务增多，库中专家技能增多，策略可以根据需要调用对应子政策，类似人类在新任务中调用过往经验。最后，LEAGUE++ 将现有预训练模型用于策略初始化（warm-start），例如视觉模型用于感知、行为克隆模型用于初始策略等。这减少了新任务的探索开销，使持续学习更高效稳定。 与传统方法区别：传统持续学习多半依赖网络自身调节参数来避免遗忘，而 LEAGUE++ 则更像一种“学习+规划”双轨方案。它没有采用 EWC/GEM 等直接在RL算法中增加遗忘惩罚，而是通过任务分解将复杂问题拆解，降低每阶段学习干扰。这种方式避免了以往RL持续学习中，不同任务策略存于同一网络导致的梯度干扰，从根本上缓解了遗忘。此外，与 PackNet 等为每任务固定网络模块不同，LEAGUE++ 通过符号调用机制在同一策略网络中执行不同技能，相当于软模块化——技能库中的技能相当于可切换的策略段，无需冻结参数就能隔离任务干扰。相比纯回放或迁移学习，LEAGUE++ 依赖 LLM 提供额外知识（如何解任务、设计奖励），扩充了信息来源：这类似人类导师提供提示，加速了学习而非仅靠算法克服遗忘。可以说，LEAGUE++ 将知识提炼（通过LLM）与技能留存（通过库）结合，超出了传统持续学习框架。 具身场景下的优势：LEAGUE++ 非常契合具身智能体执行复杂任务的需求。首先，LLM 提供的分层规划使机器人哪怕面对长且模糊的指令（例如“准备早餐”）也能一步步拆解执行，避免在长时间任务中遗忘目标或迷失——这在人类复杂指令场景尤为关键。其次，技能库的重用大幅提升了数据效率：当机器人进入新环境或任务，只要能从库中找到类似子任务，就无需从头练习，少样本即可适应。这特别适合真实机器人训练昂贵、无法大量试错的情况。第三，由于采用符号层指导和技能初始化，LEAGUE++ 在每个任务的学习过程都更快、更稳定，降低了资源消耗和试错成本。作者的实验表明，在多个复杂模拟环境（如家庭清洁、装配任务）中，LEAGUE++ 全面优于纯RL的持续学习基线。例如在新域任务上，因复用了旧域技能，学习速度显著快于从零开始的对照。这证明在真实机器人需要反复学习不同任务的长期部署中，引入LLM指导和技能库的框架能够实现效率和稳健性的兼顾，这是传统方法难以同时达到的。 Scalable and Efficient Continual Learning from Demonstration via a Hypernetwork-generated Stable Dynamics Model (Sayantan Auddy 等, 2024 年, arXiv) 方法简介：该工作针对机器人示教学习（LfD）场景提出了一个持续学习方法，利用超网络（Hypernetwork）和稳定动力学模型来保持多技能学习过程中的稳定性和不遗忘。核心思想是训练一个超网络，生成两个子网络：一个用于学习每项示范任务的轨迹动力学模型，另一个生成与之配套的李雅普诺夫稳定性函数。通过在每次引入新示范任务时，让超网络输出相应的轨迹模型并辅以稳定性约束，该方法确保机器人复现的轨迹是收敛稳定的，不会在插值或续航过程中发散。更重要的是，这种引入稳定性的方式还显著提升了持续学习性能：作者发现，相较不考虑稳定性的基线，引入李雅普诺夫函数后，机器人在序列学习多个技能时的遗忘显著减少。此外，为了提高效率，作者提出了分块超网络（chunked hypernetwork）和随机正则化策略，将训练多个任务的总时间从原先的 $O(N^2)$ 降低到 $O(N)$。整个方法在实时的机器人绘画轨迹学习任务（包括 LASA 数据集扩展到高维、和作者自建的 RoboTasks9 实验）上验证，结果显示无论在轨迹精度还是持续学习指标上都优于现有方法。 核心创新：本方法的创新点在于将鲁棒控制中的稳定性保障引入持续示教学习。通过让超网络同时学轨迹模型和Lyapunov 函数，每新增一项技能，系统不仅学会了模仿轨迹本身，还获得了对该技能的稳定约束，使其在整个状态空间具有吸引域。这解决了过去很多 LfD 方法关注模仿误差、却忽视了轨迹泛化稳定性的问题——即使没有回放旧示范，稳定性保障使旧技能的轨迹在训练新技能时不易被扰乱。其次，使用超网络作为核心架构，使得多任务知识统一存储在超网络的权重中，通过不同输入（如任务ID或上下文）生成相应任务的模型。相比传统逐任务微调网络，超网络天然具备参数共享和隔离的双重特性：共享是指不同任务的共性部分自动在超网络中得到整合，隔离是指超网络输出不同任务模型，相当于为每个任务保留了一套隐含的参数。这种方式巧妙地避免了不同任务参数直接冲突覆盖。再次，提出的随机正则项训练方法，通过每次训练仅对一个随机选取的旧任务施加约束，代替以往对所有旧任务都正则化的做法，大幅降低了训练复杂度。这一技巧保持性能不变却将训练开销线性化，提升了持续学习的可扩展性。 与传统方法区别：相比 EWC 等对参数层面的稳定约束，本方法将稳定性提升到行为层面：EWC关心参数不剧烈变化，而该方法直接确保每个技能的输出轨迹在引入新技能后依然收敛不发散，因而对遗忘的防御更直接有效。与 iCaRL/GEM 等需要存储和重放示范不同，该方法不需要对旧示范数据回放：旧技能知识蕴含在超网络权重中，加之稳定性函数的限制，新任务训练不会破坏旧技能轨迹，无需样本重温也能保护旧技能。同时，相较 PackNet 逐任务固定部分网络的硬隔离，超网络属于软隔离：所有任务共享同一个生成网络，但通过任务索引输出不同技能模型，相当于用函数生成参数，既避免干扰又保持容量节省。与之前一项采用超网络+神经ODE方法做 continual LfD 的工作相比（作者提及的最近方法），本方法增加了严格的稳定性保障，因此旧技能保真度和新技能学习效果都有明显提升。总的来说，新方法在理念上融合了控制理论和持续学习，提供了一种不同于以往任何单一范式的框架。 具身场景下的优势：在实际机器人示教应用中，该方法具有显著优势。首先，稳定性约束确保机器人在执行已学轨迹时不会因为学习了新技能而出现意外失稳或偏差，这对物理系统尤为重要——避免了因遗忘造成的运行危险，提高了安全性和可靠性。其次，由于无需反复重训旧示范，机器人能更高效地学习新技能：作者在真实机器人轨迹跟踪任务中展示，新技能加入后旧技能仍能零-shot 执行，表现几乎不下降，这意味着机器人随时可用, 不需要频繁校准旧技能。第三，该方法在高维技能扩展上表现良好，证明其可应用于涉及多自由度的复杂操作（例如机械臂同时控制位置和姿态的轨迹）。传统方法在高维连续控制上的持续学习往往不稳定，而本方法借助Lyapunov函数确保了每个技能在高维空间都是吸引子的，因此具备更强的鲁棒性。最后，在资源受限的机器人上，训练效率的线性提升非常有价值：每新增技能的训练开销不会随着技能数量指数增长，使得机器人可以规模化地持续学习几十上百个技能而不会因为训练时间过长变得不可行。综上，该方法为现实中机器人持续学习大量示教任务提供了安全、高效、稳健的解决方案。 Online Continual Learning for Interactive Instruction Following Agents (Byeonghwi Kim 等, 2024 年, ICLR 2024) 方法简介：这项工作定义了更贴近现实的交互式指令跟随持续学习场景，并提出了信心感知滑动平均（CAMA）算法来缓解遗忘。作者指出，过去大多假设智能体一开始就能获取所有训练任务的数据，而不符合机器人应持续探索、持续学习的现实。为此论文提出两个设置：(1) 行为增量学习（Behavior-IL）：机器人不断学习新指令行为；(2) 环境增量学习（Environment-IL）：在新环境中执行之前学过的指令。困难在于任务边界可能不明确，传统“数据先验”方法（如基于旧任务输出日志概率的蒸馏）需要明确任务边界和缓存旧任务信息。CAMA 则不需要任务边界：智能体在训练过程中对每个观察到的指令计算模型输出的置信度，并以滑动平均的方式更新其对旧任务的输出分布估计。当模型学习新行为时，如果对某类以前学过的指令置信度下降，CAMA 会根据之前维护的滑动平均分布对模型进行轻微调整，以拉回对旧知识的信心，从而达到持续无边界学习的目的。实验证明，在作者设计的新基准上（包含连续新增指令和环境的条件），CAMA 相较现有方法取得了显著更好的表现。 核心创新：CAMA 的创新在于引入任务无关的置信度追踪机制：传统方法如 LwF 或 iCaRL 需要存储每个旧任务的模型输出分布或实例样本，当新任务到来时通过知识蒸馏或重放维持旧任务性能。这实际隐含了已知的任务边界和任务身份。而 CAMA 不存储具体样本或明确任务ID，而是对模型输出信心做持续监控。它维护一个滑动平均估计，可视作模型对过去经验的“模糊记忆”，随着时间衰减地记录旧任务的输出倾向。当模型学新任务时，如果这种倾向发生显著漂移，表明遗忘在发生，那么CAMA依据滑动平均的差异，对模型参数进行微调修正（如调整输出层偏置等），无需知道具体哪个任务受影响，只基于置信度变化即可纠正。这种方法跳出了任务级别的框架，实现了任务无关（task-free）**的持续自稳训练。另外，CAMA 所需存储的信息量极小，仅为每个输出类别一个滑动均值，计算与存储开销极低，非常简洁却有效。 与传统方法区别：与 EWC 等正则方法需要在参数变化上加全局约束不同，CAMA 工作在输出空间：它不直接限制权重，而是看模型对以前输出的信心水平是否下降。这有点类似知识蒸馏（LwF）的思想，但不需要旧模型保存——蒸馏通常要保存旧模型输出作为固定目标，而CAMA持续更新的滑动平均本质上充当了“旧模型记忆”，避免了存储多个旧模型或大量旧样本。相比 iCaRL 维护每类样本代表来近似旧任务分布，CAMA 维护的是压缩的统计量（置信度均值），因而不需要样本库也没有额外模型，只需少量内存。与 GEM 这类基于梯度投影的方法相比，CAMA 算法更简单，不需要复杂的二次规划求解，只以移动平均做调整，在线即可执行。另外，CAMA 属于任务无边界的方法，解决了传统方法大多假定任务切换已知、或者需要在检测到切换后才能应用策略的问题。在现实机器人持续学习中，任务变化往往连续发生且未必有明显分界，CAMA 正是面向这种情况设计的。 具身场景下的优势：CAMA 所针对的交互式指令跟随场景极具代表性：机器人在不断遇到新指令、新环境的过程中持续学习，这对服务机器人等非常实际。CAMA 能在无监督任务切换的前提下，让机器人保持之前学会的指令技能。例如，一个家庭助理机器人在学会一系列语音指令后，被主人带到新房间教它新任务，CAMA 确保机器人在学新任务时不会忘记旧房间的指令执行方法。由于不需要存储大量过往数据，CAMA 也适合长期部署：机器人可以不断学习数十数百种指令而不必担心存储开销或隐私问题（无需录音或录像存档）。同时，CAMA 的实时性很高，它在模型训练时即可动态调整参数，无需离线阶段，适合机器人在线学习的需求。作者报告该方法在提出的 Behavior-IL 和 Environment-IL 基准上显著优于已有的持续学习算法，这表明它成功地解决了任务模糊情况下的遗忘问题，让机器人在变化的环境和任务中保持稳健性能。总之，CAMA 为具身智能体提供了一种轻量级但有效的持续学习机制，使其能像人一样一边执行指令、一边随着环境改变不断积累本领。 ICAL: In-Context Abstraction Learning for Multimodal Agents (Gabriel Sarch 等, 2024 年, arXiv) 方法简介：ICAL 提出了一种利用大模型自身的生成与内省能力，持续提升多模态智能体决策的方案。该方法关注这样的问题：大型语言或视觉-语言模型（LLM/VLM）虽具有强大的Few-shot能力，但需要高质量范例作提示。人为提供高质量示例既昂贵又不具通用性。ICAL 的解决方案是：让大模型从次优的示范轨迹和人类反馈中自动生成可用于提示的抽象示例。具体流程包括：给定一个全新领域的嘈杂演示（例如一个任务的视频示范，可能包含低效动作或错误），首先由视觉-语言模型对该轨迹进行解析抽象。模型会产出一个通用的程序化描述，修正了示范中的低效步骤，并添加认知注释，标注出任务涉及的关系、对象状态变化、时间子目标和关键细节等。接下来，让真实机器人/代理尝试执行这份抽象计划，在执行过程中人类可以提供自然语言反馈纠正模型的错误理解或补充知识。模型根据反馈交互式地细化之前产生的抽象。最终得到的这些抽象示例（带有语言注释和优化后的步骤），被存储起来作为内存范例。在后续决策中，大模型可以将这些示例作为提示的一部分，从而显著提升对于类似任务的决策能力。ICAL 在三个具有挑战的环境中验证：TEACh对话式指令跟随、VisualWeb互联网多模态代理、Ego4D第一人称视频动作预测，都取得了新的SOTA性能。 核心创新：ICAL 将持续学习转化为持续积累提示示例的过程，而非传统的持续调权过程。这是一个范式转变：与其反复梯度更新模型参数，ICAL 让模型自己“理解反思”示范并生成抽象的知识总结，逐步丰富模型的提示库。这样的内省式示教机制是首次提出。具体创新点包括：(1) 自动抽象：利用预训练VLM从视觉示范中提取高层语义——不像以往只关注低级动作序列，ICAL 提取了任务因果关系、对象状态变化、时间逻辑、任务构造等四类认知抽象，这些抽象比原始示范更精炼、更具概括性。(2) 人机交互细化：ICAL 不仅依赖模型自我生成，还引入人类在模型执行时给出自然语言反馈，让模型迭代改进其抽象。这种循环使得次优示范（含错误或低效部分）也能通过反馈纠正而变得有价值，突破了以往示范学习要求专家示范的限制。(3) 提示记忆库：将生成的抽象示例存入一个不断扩展的库，并使用检索增强的大模型决策——模型在推理新任务时，会从库中取出相关示例放入提示，提高决策准确率。ICAL 相当于在持续学习过程中渐进地提升模型的上下文学习能力：模型见到的新任务越多，就自行总结出越多可泛化的知识片段，下次遇到类似任务时即使不调权也能通过提示完成。实验显示，ICAL 随着学习示例增多，模型性能持续提升，呈现出真正的“持续改进”特性。 与传统方法区别：传统持续学习多在参数空间操作，如EWC约束权重变化、经验回放存数据样本等。而 ICAL 完全绕开了参数遗忘问题：模型主体参数基本保持不变（或只在最后有小幅微调作为增强），因此谈不上灾难性遗忘。取而代之，ICAL 通过不断加入新的知识提示，让模型具备解决更多任务的能力。这种基于大模型提示的持续学习不同于以往任何持续学习范式。与知识蒸馏类方法相比，ICAL 并非让模型去逼近旧模型输出，而是生成更好的知识为己所用；与模块化或正则方法相比，它无需设计模型结构拆分或损失项，利用的是LLM/VLM自身强大的Few-shot学习能力和生成能力来“吸收”经验。在需要多模态理解和指令执行的任务中，ICAL 运用了视觉和语言的结合，这比单纯依赖视觉记忆（如CV领域一些记忆网络）要丰富，也比仅语言的持续学习（如对LLM增量训练新知识）更直观：ICAL 让模型直接看视频学，也听取人类语言，得到的是跨模态的知识。这些知识用自然语言+视觉标记来存储，具有很强的可解释性和可移植性。传统持续学习很难在不调整参数情况下大幅提高性能，而ICAL 展示了另一种可能：通过增强模型输入（上下文）而非改变模型本身来实现持续学习。 具身场景下的优势：ICAL 针对的场景包括家庭机器人任务（TEACh）、网络操作代理以及视频预测，都与具身智能密切相关。它的优势在于：对于复杂开放环境，很难人工定义明确的任务边界或提供足够训练数据，而ICAL 利用了现有大型模型，只需较少示范和交互即可让模型适应新任务。因此在具身智能典型的少样本、多样任务情况下，ICAL 能快速扩展技能。此外，它不要求持续占用机器人反复训练（除了必要的人机对话反馈），大部分“学习”都在模型的推理过程中完成（让模型生成抽象并评估）——这意味着机器人可以在线学习新任务而不中断服务，通过对话方式边干边学，正如人类新手在导师指导下学习新技能一样。对于任务模糊或长时间任务，ICAL 的抽象总结能力尤为关键：机器人可以从冗长的示范中提炼要点，避免执行时被次要细节干扰。这提高了机器人在复杂任务下的鲁棒性和泛化。实验证实，ICAL 在 TEACh 基准上将先前SOTA的目标达成率提升了12.6%，在VisualWebArena中成功率从14.3%提高到22.7%，在Ego4D动作预测上击败了Few-shot GPT-4V。更重要的是，性能提升是持续的：每当学习更多示例，模型表现就有所提高。因此在长期来看，ICAL 提供了一个持续增长智能体能力的途径，而无须反复训练模型参数，大大降低了具身人工智能系统维护的成本和风险（例如不会出现传统持续学习不慎遗忘必须人工纠正的问题）。这表明，充分利用大模型强大的自监督和内省能力，或许是具身智能持续学习的一条有效新路。 MLLM-CL: Continual Learning for Multimodal Large Language Models (Hongbo Zhao 等, 2025 年, arXiv) 方法简介：MLLM-CL 提出了一个多模态大模型（视觉-语言模型）持续学习的基准和方法。该工作将持续学习划分为两种情形：(1) 领域持续学习（Domain CL）：模型依次学习遥感、医学、科学、自动驾驶、金融等不同视觉问答领域的知识；(2) 能力持续学习（Ability CL）：模型顺序学习OCR识别、数学逻辑、视觉感知、GUI操作代理等不同能力。作者发现，传统增量学习范式（每次在上一个模型参数基础上微调新任务）不适用于多模态大模型：直接用上一任务的权重初始化下一任务，会损害模型对新任务的可塑性，导致次优结果。为解决这一任务冲突问题，MLLM-CL提出了MR-LoRA方法。具体来说，在每个新任务到来时，并不在原模型权重上继续微调，而是为该任务新建一个LoRA低秩适配器，从零随机初始化。这样每个领域/能力都有自己独立的LoRA模块，避免了继承上个任务权重带来的冲突，同时LoRA只引入极少参数，保证高效。在推理时，为了自动选择对应任务的LoRA，作者设计了一个基于大模型的路由器：利用多模态大模型本身处理复杂输入的能力，输入待测试样本后，由模型生成一个路由指令，选择最合适的专家LoRA来回答。这个路由器通过在持续学习每阶段后收集少量任务样本，对大模型进行轻量few-shot微调得到。总体而言，MR-LoRA包含多LoRA专家 + 大模型路由两部分。实验结果显示，在上述多个领域和能力的持续学习任务序列中，该方法在所有任务上的平均性能和最终性能都超越了现有方法，如直接微调、参数隔离、基于提示的方法等。 核心创新：MLLM-CL 的创新在于针对大模型的持续学习提出了避免遗忘的新范式。其一，每任务新LoRA的策略突破了以往串行微调“一脉相承”导致性能下降的窘境。这种做法类似“进渐网络（Progressive Networks）”，但由于LoRA参数规模小，可以在不剧增参数的情况下无限扩展到更多任务。各任务的LoRA彼此独立，保证旧任务参数不受新任务影响，从根本上杜绝了遗忘。同时，所有LoRA附加在同一个大模型上，共享其通用表示能力，又实现了知识迁移（因为基础模型权重保留了多任务共性表示）。其二，引入大模型驱动的路由机制，这是此前持续学习中少有探索的。传统任务识别通常依赖样本的特征相似度或任务ID，而该方法让多模态大模型读入输入后自主生成路由决策。由于大模型本身掌握高层语义和复杂推理，它可以比简单特征距离更精准地选出对应领域的LoRA专家。这保证了在推理阶段对于复杂多模态输入也能正确地匹配到相应能力模块。其三，该方法还建立了系统化的多模态持续学习基准（MLLM-CL基准），涵盖不同类型迁移（领域和能力）并提供了评价指标和数据集构建流程。这填补了多模态大模型持续学习缺少评测标准的空白，为后续研究提供了平台。 与传统方法区别：MLLM-CL 在设计上融合了参数高效微调和专家路由思想。与 EWC 等全模型正则不同，它不直接约束原模型参数，而是固定主干模型，只对每任务引入少量新参数，这避免了原模型权重冲突累积。与 PackNet 类似的方法相比，MR-LoRA 也采用了“每任务额外参数”的思路，但PackNet通过剪枝固定网络容量，MR-LoRA 则用可增添的LoRA实现弹性容量，理论上任务越多仅线性增加参数，无需预留容量。和提示学习（L2P、ModalPrompt 等）相比，提示方法在多模态大模型上效果有限，而且往往需要固定提示长度、可能仍有干扰；MR-LoRA 通过独立LoRA完全隔离任务，比软提示更干净利落。还有，许多多头网络或专家混合模型需要已知任务ID才能选专家，MR-LoRA 则通过路由器实现任务自动判别，不需要人工指示任务类别。可以说，它将模块化与任务识别两个问题一起解决了。在多模态场景下（例如同时应对图片文本、多任务问答），这种方法比单纯视觉或单纯语言的持续学习方法更复杂但也更全面地考虑了输入多样性和任务多样性。 具身场景下的优势：虽然 MLLM-CL 的实验主要是多模态问答和 GUI 任务，但其思想对许多具身智能应用同样有益。比如，一个家庭服务机器人具备多个视觉语言能力（识物、对话、读屏、算术等），MR-LoRA 可以让其持续添加新能力而旧能力不衰减：每项新技能加一个LoRA，机器人就掌握了新本领，又不会遗忘以前学过的（因为以前的LoRA保留原样）。这样的能力库扩展非常符合具身AI逐步进化的需求。其次，由于LoRA模块小，机器人可以在资源受限设备上部署多个技能专家，而不会像扩增整个模型那样内存爆炸。例如针对移动设备，把不同领域的视觉问答能力拆成多个LoRA加载，按需调用，远比加载多个大模型高效。再次，路由器机制使机器人在遇到新感知输入时能自动判断调用哪种能力，这对于多模态交互场景很关键——现实中用户不会明确告知机器人“这是一道医学问题”或“现在开始OCR任务”，机器人必须自适应切换内部技能。MR-LoRA 的路由方案正是训练机器人根据输入自行选择专家。作者实验也表明，在跨领域问答中，MR-LoRA 能正确选择相应LoRA模块，最终在所有领域上同时取得高精度，相比其它方法在后期任务训练后前期任务精度大幅下降，MR-LoRA 几乎零遗忘且新任务精度也高。这证明了其稳定性和塑造性兼顾的能力，非常适合需要终身学习的多才多艺型机器人。总之，MLLM-CL 为具身智能体集成多模态大模型指明了一条持续进化的道路：通过低秩模块化扩展和智能路由，实现持续学习众多技能而性能不减。 Task-Unaware Lifelong Robot Learning with Retrieval-based Weighted Local Adaptation (Pengzhi Yang 等, 2024 年, arXiv) 方法简介：此工作面向机器人开放环境，提出了无任务标识的终身学习框架，结合检索式记忆和局部微调来提升持续学习效果。在真实世界中，机器人遇到的新任务往往没有清晰的边界或ID，且无法存储海量以往数据。为此作者的方法在机器人学习过程中维护一个情景记忆（Episodic Memory），存储每个任务的一小部分关键示例。当机器人在序列学习新技能时，主要模型仍采用常规的基于经验回放的训练（比如采用一部分记忆样本进行练习，以降低遗忘）。然而仅靠有限回放仍难免遗忘一些早期技能。因此在测试执行阶段，如果机器人遇到类似以前学过却已部分遗忘的情境，该方法会从记忆中检索最相关的过往示例，并对当前策略进行快速局部微调（local fine-tuning），以恢复在该情境下的性能。为了提高这种局部微调的效率，作者引入选择性加权机制：首先让机器人在当前情境下执行若干回合，记录其性能（例如哪些步骤出错），然后将这些失败轨迹与记忆中检索到的示范进行比对，自动衡量哪部分旧示范对当前情况帮助最大，给予这些片段更高权重来指导微调 arxiv.org 。简言之，该框架模仿人类温习知识的过程：在需要时重点复习遗忘的难点，从而高效恢复技能。整个方法适用于没有明确任务划分的开放式场景，作者在 LIBERO等操作任务基准上测试了该方法，结果表明即使任务无明显边界，机器人也能持续学到新技能并在需要时重新找回旧技能，大幅优于不采用检索适应的基线。 核心创新：该方法的创新之处在于提出了任务无关的回顾与适应机制。传统持续学习通常假定任务边界明确（如任务开始和结束）以便采取对应策略，比如任务后冻结部分网络或保存样本。然而现实中机器人可能连续不断遇到各种挑战，无法提前知道哪些属于同一任务。本文通过检索式记忆绕过了显式的任务划分：无论当前遇到的情况是否是以前练过的任务，机器人都可以基于当前观测从记忆库检索类似经验。这赋予了模型一种情景感知能力，让它能够自行判定何时需要参考旧经验。其次，提出的Weighted Local Adaptation将元学习思想引入了持续学习测试阶段：不像传统只在训练阶段防遗忘，这里在测试阶段也运行一次短暂微调，相当于在机器人执行时刻临时提高旧技能专门性。这种做法以往少见，因为通常假设模型定型后就执行不变，但在终身学习背景下，适度的测试微调可以极大提升旧技能复现效果。通过限制微调只针对检索到的示例且有选择地重点学习难点，既保证了微调幅度受控不会破坏模型总体性能，又有针对性地弥补遗忘。第三，任务无关意味着模型无需任务标签输入或边界信号，这对开放环境学习非常关键。作者利用视觉和语言嵌入的一致性作为检索键值 arxiv.org ，避免了因持续学习造成的表征漂移（通过预训练模型确保不同阶段embedding空间一致）。这保证了即使模型学了新东西，仍能用统一标准去查找旧记忆，提高了检索可靠性。 与传统方法区别：相比 EWC等在训练时防遗忘的方法，该框架将遗忘补救延伸到了测试时刻：传统模型一旦训练完部署，如果遗忘了旧技能往往束手无策；而此方法允许机器人在执行某任务前“温习一遍”相关经验，相当于给模型一个自行恢复的机会。与 GEM 等在训练时用小样本回放不同，这里的记忆检索主要服务于执行阶段的微调，而非整个训练过程持续混入，因而不会显著增加训练难度，同时又充分利用了记忆在关键时刻的价值。和 iCaRL 直接根据记忆做最近邻推断不同，该方法选择的是微调模型，因此能适应当前状况的细微差别，而不是简单模式匹配，效果更强。在没有任务ID的情况下，很多传统方法如多头网络就无法应用，而本方法完全不依赖任务标签或边界，通过检索+微调实现了隐式的任务处理。可以将其视作结合了经验回放和快速自适应：既保留少量记忆（回放理念），又在需要时通过微调快速适应（元学习理念），融合了二者优点。 具身场景下的优势：对于在动态未知环境中工作的机器人来说，该方法提供了极高的灵活性。机器人无需预先知道将面对哪些任务，也不用在模型结构上做固定分配，它可以不断遇新仍保持从容：每当遗忘可能影响当前任务时，就从记忆中找回相关信息补强自己。这很像人类在现场快速翻阅笔记确认知识点，从而表现出色。尤其在长期部署中，机器人可能几天甚至几月不执行某项技能，这种情况下直接执行可能失败，但如果允许机器人在执行前复习一下过去案例，就能大幅提升成功率。作者的方法正是提供了这种快速热身功能，让机器人在现实使用中更可靠。其次，由于只存储挑选的一小部分示范数据，记忆库很轻便，不会像保存所有数据那样不现实。并且由于采用预训练embedding统一表示，不同时间学习的经验可以共同检索，这意味着机器人可以在跨环境、跨时间的情况下整合经验，非常符合具身智能需要持续集成多方信息的特性。最后，实验中该方法在没有任务边界信息的情况下，其性能超出了使用明确任务边界且大量回放的策略，体现出开放场景适应性。总之，此方法赋予机器人一种人类般的学习策略：即使偶尔遗忘细节，通过快速翻阅过往经历又能想起来，从而在不断变化的任务挑战中保持整体能力不退化，为具身智能的长期自主学习提供了新的思路。 方法演化趋势与对比 综上所述，2024–2025 年关于具身智能持续学习的方法展现出一些共同的发展趋势与对过去方法的根本区别： 从参数保护到知识重组：过去方法（EWC 等）通过阻止参数改动来防遗忘，但在机器人复杂任务中效果不佳。新方法更强调对知识的表达与重组，如通过前缀提示、技能库、知识空间等将知识单元化，避免直接在同一参数上博弈。这些机制允许共享与隔离并存：共享的是不同任务的共性（如PSPL原语、LEGION语言嵌入），隔离的是各任务独特部分（如每任务LoRA、每技能提示）。因此新方法能在不牺牲塑性前提下保持稳定性。 从任务清晰到任务模糊：许多新工作针对任务边界不明的现实。比如任务无关检索、CAMA 的置信度调整都不需要预先知道任务什么时候切换。相比之下，传统方法大多假定任务切换明确并提供信号，现实中难以满足。新方法通过连续监测模型行为或环境，相当于赋予模型自适应觉察能力，真正做到持续学习“终身运行”而非分阶段训练。 从被动防御到主动利用：以前方法把旧知识当需要保护的内容，新方法则倾向主动利用旧知识来帮助新学习。例如PSPL利用旧技能提示加速新技能习得、LEGION 用旧知识簇推断长序列任务、LEAGUE++ 复用已学技能解决新问题。这种正向迁移思路解决了传统方法虽然防遗忘但也不擅长迁移的问题，新方法在防止遗忘的同时显著提高了前向迁移性能，使机器人越学技能越多，解决新任务反而越快。 减少对存储和重放的依赖：传统方法如经验重放在机器人上往往不可行（存储无限增长且隐私风险）。许多新方法都强调“不依赖回放”：PSPL 无需回放旧经验、CAMA 无需存样本只存统计、LEGION 只以有限回放辅以知识空间、Hypernetwork 方法不重训旧示范。即便需要，也以小规模记忆或生成器代替全量存储（如Task-Unaware方法的小记忆+检索、本质上非常有限）。这使持续学习更能适应机器人存储和计算受限的条件。 引入大模型与多模态：与以往专注单一模型训练不同，新方法勇于将预训练大模型纳入持续学习框架，如利用LLM规划子任务（LEAGUE++）、用VLM自我生成知识（ICAL）、大模型本身作为路由器和骨干（MLLM-CL）。多模态信息（语言、视觉、触觉）也被融入，如LEGION用语言辅助任务编码、PSPL用文本和光流提示、ICAL用视频+语言反馈。这些让机器人更好地理解任务和环境，也提供了额外手段缓解遗忘（例如语言描述可作为语义锚点，帮助模型回忆对应技能）。这是持续学习与大模型、多模态技术的交叉融合，标志着具身智能持续学习进入一个更智能更复杂的阶段。 小结：传统持续学习方法在具身智能情境下面临诸多挑战，如参数共用导致遗忘、任务未知导致方法失效、资源有限导致回放不可行等。2024–2025 年的一系列新方法通过全新的机制——提示网络、非参知识库、超网络稳定、符号技能库、任务无关调整、大模型自监督等，成功地解决或缓解了这些问题。在机器人连续学习复杂技能、跨模态理解指令的任务中，这些方法展现出显著优于旧方法的性能，有的甚至实现了遗忘几乎为零且持续正迁移的效果。可以预见，未来具身智能持续学习将沿着模块化+共享、智能检索+自适应、融合大模型知识的路线继续发展，使智能体更接近人类的终身学习水平，在不断变化的世界中保持学习新知识的同时不忘记旧本领。\n",
  "wordCount" : "1827",
  "inLanguage": "en",
  "datePublished": "2025-06-16T14:14:45+08:00",
  "dateModified": "2025-06-16T14:14:45+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-06-16/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-06-16
    </h1>
    <div class="post-meta"><span title='2025-06-16 14:14:45 +0800 +0800'>June 16, 2025</span>&nbsp;·&nbsp;9 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h1 id="持续学习中避免灾难性遗忘具身智能领域的研究进展综述">持续学习中避免灾难性遗忘：具身智能领域的研究进展综述<a hidden class="anchor" aria-hidden="true" href="#持续学习中避免灾难性遗忘具身智能领域的研究进展综述">#</a></h1>
<h2 id="引言与问题背景">引言与问题背景<a hidden class="anchor" aria-hidden="true" href="#引言与问题背景">#</a></h2>
<p>持续学习（Continual Learning，也称终身学习）指模型在数据分布和学习目标不断变化的情境下，能够连续学习新任务且不忘记已有知识的能力<a href="https://arxiv.org/abs/1907.00182#:~:text=,learning%20is%20not%20necessarily%20finding">arxiv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=An%20important%20challenge%20for%20machine,of%20complex%20skills%20and%20knowledge">ar5iv.org</a>。传统深度学习假设训练数据 <em>i.i.d.<em>且一次性可用，这在现实具身智能（如机器人、自主系统）中往往不成立<a href="https://ar5iv.org/pdf/1907.00182#:~:text=1%20Introduction">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=You%20want%20to%20learn%20from,not%20know%20how%20and%20when">ar5iv.org</a>。其中最大挑战之一是</em>灾难性遗忘（catastrophic forgetting）</em> ，即模型在顺序学习多个任务时，新知识的获取会导致旧知识被<strong>快速、大幅</strong>遗忘<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=cognitive%20science,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>。这一现象最早由 McCloskey 和 Cohen 于1989年在联结主义神经网络中发现<a href="https://arxiv.org/abs/2312.10549#:~:text=Catastrophic%20Forgetting%20%28CF%29,solutions%2C%20proposes%20a%20taxonomy%20to">arxiv.org</a>，体现了所谓 <em>稳定-可塑性权衡</em> ：模型既要对新信息足够可塑（plasticity），又要对既有知识保持稳定（stability）<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>。与生物神经系统相比，人工神经网络在顺序学习时更容易“灾难性”遗忘过去经验，而人类和动物通常表现出渐进、有选择的遗忘<a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=All%20natural%20cognitive%20systems%2C%20and%2C,generalize%2C%20to%20function%20in%20the">cell.com</a><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=Unfortunately%2C%20though%2C%20catastrophic%20forgetting%20does,brain%20might%20have%20overcome%20this">cell.com</a>。如何在保持模型泛化能力的同时避免旧知识被破坏，成为持续学习研究的核心问题<a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=catastrophic%20forgetting,solution%20for%20distributed%20connectionist%20networks">cell.com</a>。为此，大量研究围绕不同技术路线展开，包括利用附加数据回放、正则化约束、动态模块化架构和外部记忆等方法来缓解遗忘<a href="https://arxiv.org/html/2404.00418v1#:~:text=phenomenon%20that%20reflects%20the%20trade,18%2C%203">arxiv.org</a>。本文将按时间脉络梳理持续学习避免遗忘的关键进展，重点聚焦具身智能场景的应用，并比较不同方法在这些场景中的优劣差异。</p>
<h2 id="早期探索灾难性遗忘的提出与初步对策">早期探索：灾难性遗忘的提出与初步对策<a hidden class="anchor" aria-hidden="true" href="#早期探索灾难性遗忘的提出与初步对策">#</a></h2>
<p><strong>灾难性遗忘现象的提出（1980s–1990s）:</strong> 神经网络中的灾难性遗忘问题由 McCloskey 和 Cohen (1989)<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=cognitive%20science,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>以及 Ratcliff (1990) 首次严谨描述<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>。他们发现序列训练两个任务时，后学任务会显著干扰先前任务的记忆<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=McCloskey%20and%20Cohen%20,with%20backpropagation%20neural%20network%20modelling">en.wikipedia.org</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=each%20learning%20trial%20on%20the,Furthermore%2C%20the%20problems%202%2B1">en.wikipedia.org</a>。这一问题可视为Grossberg提出的稳定-可塑性两难的极端表现<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>。此后，研究者逐步意识到，若要让人工智能具备人类般持续学习的能力，必须解决神经网络的遗忘灾难。1990年代中期，一些学者开始探索初步对策。例如，Robins (1995) 提出了<strong>重演/伪重演 (rehearsal/pseudorehearsal)</strong> 方法，即在学习新任务时将旧任务样本或由模型生成的“伪样本”一并训练，以巩固旧知识<a href="https://ar5iv.org/pdf/1907.00182#:~:text=,In%202016%20IEEE%2055th%20Conference">ar5iv.org</a>。这种方法模拟了生物大脑在睡眠中<strong>重放记忆</strong>的过程，缓解了遗忘问题，被视为后续生成式回放方法的雏形<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Instead%20of%20modeling%20the%20past,based%20learning%2C%20where%20the%20model">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=the%20past%20is%20not%20forgotten,91%2C%2020%20%2C%20%20110">ar5iv.org</a>。与此同时，Grossberg 等人在稳定-塑性理论指导下发展**自适应共振理论 (ART)**网络，通过网络结构与记忆单元的设计减少旧记忆被覆盖的风险。这一时期的工作揭示了灾难性遗忘的严重性，并奠定了若干基本思想：通过保留过去经验（真实或模拟）或限制参数剧烈更新来保护已有知识<a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=Unfortunately%2C%20though%2C%20catastrophic%20forgetting%20does,brain%20might%20have%20overcome%20this">cell.com</a>。然而，由于当时神经网络规模和应用场景有限，这些早期方法未形成统一框架，但为后续深度学习时代的持续学习研究提供了宝贵思路。</p>
<p><strong>深度学习时代兴起前的持续学习理念:</strong> 在2000年前后，机器学习领域也出现了一些“终身学习”思想，例如 Thrun 和 Mitchell 等人在机器人领域讨论让机器人不断积累知识、自主适应新环境的算法。但受限于模型能力，这些工作多偏向理论构想或特定场景下的增量学习算法。值得一提的是，Silver <em>et al.</em> (2013) 等人在认知科学领域提出**“永不停歇学习”<strong>(Never-Ending Learning)的愿景，旨在构建能无限获取新知识的AI系统。这些理念上的探索进一步强调了持续学习的重要性，但真正有效的算法突破还要等待深度学习的成熟。进入2010年代，深度神经网络在图像、语音等任务上取得突破，但其</strong>遗忘现象依然明显**。Goodfellow <em>et al.</em> (2014) 的实证研究表明，即使现代深度网络在顺序学习多任务（如不同MNIST变换）时，仍然发生严重的性能遗忘，他们尝试用<strong>Dropout</strong>等正则策略略微缓解遗忘<a href="https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf#:~:text=,Hinton%20et%20al.%2C">cs.uic.edu</a>。这一阶段的研究重新量化了深度模型遗忘的程度，引发了学界对持续学习的关注，也为随后的关键方法发明做好了铺垫。</p>
<h2 id="深度学习时代的方法演进20162020">深度学习时代的方法演进（2016–2020）<a hidden class="anchor" aria-hidden="true" href="#深度学习时代的方法演进20162020">#</a></h2>
<p>进入深度学习时代后，大量持续学习算法被提出。总体而言，这些方法可分为以下几类： <strong>参数正则化</strong> 、 <strong>经验回放</strong> （或生成回放）、 <strong>参数隔离/模块化</strong> 、以及<strong>外部记忆</strong>等<a href="https://arxiv.org/html/2404.00418v1#:~:text=phenomenon%20that%20reflects%20the%20trade,18%2C%203">arxiv.org</a>。各类方法都有经典代表工作，我们按时间演进介绍主要方法及其贡献。</p>
<h3 id="参数正则化方法">参数正则化方法<a hidden class="anchor" aria-hidden="true" href="#参数正则化方法">#</a></h3>
<p><strong>Learning without Forgetting (LwF, 2016):</strong> Li 和 Hoiem 提出“学习不遗忘”算法<a href="https://arxiv.org/abs/1606.09282#:~:text=retraining%20on%20such%20data%20becomes,and%20new%20task%20datasets%20for">arxiv.org</a>。他们假设只能获取新任务数据，无法重温旧任务数据的现实情况，通过让模型在学习新任务时<strong>蒸馏(distillation)<strong>旧任务模型的输出分布来</strong>正则化</strong>模型参数变化<a href="https://arxiv.org/abs/1606.09282#:~:text=retraining%20on%20such%20data%20becomes,and%20new%20task%20datasets%20for">arxiv.org</a>。具体而言，在训练新任务时对旧任务模型的预测进行保持，使新模型尽可能产生与旧模型相似的输出，从而保护原有能力。LwF是<strong>知识蒸馏用于持续学习</strong>的开创性工作，实现了只用新任务数据也能较好保留旧任务性能<a href="https://arxiv.org/abs/1606.09282#:~:text=for%20its%20existing%20capabilities%20are,for%20improved%20new%20task%20performance">arxiv.org</a>。该方法在2016年ECCV发表，此后蒸馏正则化成为持续学习的重要手段之一。</p>
<p><strong>Elastic Weight Consolidation (EWC, 2017):</strong> Kirkpatrick 等人（DeepMind）在PNAS 2017发表了著名的EWC算法<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。EWC通过近似计算每个参数对旧任务的重要程度（利用费舍尔信息矩阵对参数敏感度进行估计），在学习新任务的损失中添加项，<strong>惩罚对重要参数的大幅更新</strong><a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。直观来说，模型会“放慢”那些对旧任务重要参数的学习速率，以免遗忘旧知识<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。EWC在经典分类任务（如顺序MNIST）和强化学习任务（顺序Atari游戏）上验证了有效性<a href="https://arxiv.org/abs/1612.00796#:~:text=tasks%20by%20selectively%20slowing%20down,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。作为持续学习领域里程碑，EWC证明通过软约束参数更新，可以在一定程度上同时保持先前任务性能和新任务学习能力<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。其思路后来衍生出许多变体，例如利用更精细近似二阶信息的方法等<a href="https://ar5iv.org/pdf/1907.00182#:~:text=weights%20and%20produce%20an%20adapted,Fisher%20matrix%20using%20the%20Kronecker">ar5iv.org</a>。</p>
<p><strong>Synaptic Intelligence (SI, 2017):</strong> Zenke 等人在ICML 2017提出了另一本质类似EWC的正则化方法SI<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。不同于EWC预先计算参数重要度，SI在训练过程中<strong>实时累积</strong>每个参数对损失的贡献度，结束当前任务时将其视为该参数的重要性<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。这种“智能突触”机制借鉴了生物突触强度调节的复杂性，每个突触（参数）在多个任务中累积“任务相关信息”，新任务来时利用这些信息调整学习率<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。SI在多个连续分类任务上显著降低了遗忘，同时保持了计算高效<a href="https://arxiv.org/abs/1703.04200#:~:text=intelligent%20synapses%20that%20bring%20some,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。与EWC相比，SI无需存储旧任务样本，同样不增加模型容量，对于资源受限的设备具有吸引力<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>。但正如多数正则化方法的局限，当任务数量增多或差异较大时，单纯靠增加惩罚会导致模型学习新任务受限，需要在稳定与塑性间权衡<a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=also%20generative%20models%20,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a>。</p>
<p><strong>其他正则化进展:</strong> 除上述，2017年前后还出现了多种参数正则化方案。例如 Li <em>et al.</em> (2017) 的增量时刻匹配（IMM）方法通过匹配参数分布的方式融合旧新任务模型；Aljundi <em>et al.</em> (2018) 提出的MAS（Memory Aware Synapses）利用输出敏感度评估参数重要性。这些方法本质均为在损失函数中添加某种形式的正则项，使模型<strong>避免过度调整关键参数</strong>以保留旧知识<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Basic%20regularization%20techniques%20that%20could,More%20complex%20methods%20consist%20in">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a>。正则化方法的优点是 <strong>无需存储旧样本，内存占用低</strong> ，易于在嵌入式/机器人等设备上实现<a href="https://ar5iv.org/pdf/1907.00182#:~:text=streams%20of%20data%20presented%20sequentially,speed%20during%20the%20learning%20process">ar5iv.org</a>。它们的不足在于当任务间差异巨大、参数冲突严重时效果受限，而且累积过多任务后模型可能进入“过度稳定”状态难以学习新任务<a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a>。在具身智能场景中，由于设备算力和存储有限，正则化方法仍是常用选择之一。例如 EWC 被用于机器人连续控制任务以保护低层政策参数不被遗忘<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>。但如果机器人遇到全新领域任务，正则化可能限制其适应新技能的能力，这是后续方法力图解决的问题。</p>
<h3 id="经验重放与生成式回放方法">经验重放与生成式回放方法<a hidden class="anchor" aria-hidden="true" href="#经验重放与生成式回放方法">#</a></h3>
<p><strong>显式经验重放 (Experience Replay):</strong> 针对持续学习，最直接的思路是 <strong>在学习新任务时重温部分旧任务的数据</strong> ，仿佛让模型“复习”以前的知识<a href="https://ar5iv.org/pdf/1907.00182#:~:text=One%20solution%20to%20Continual%20Learning,where%20continual%20learning%20is%20necessary">ar5iv.org</a>。Rebuffi 等人在CVPR 2017提出的 <strong>iCaRL</strong> 方法将这一思想与深度学习结合<a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail">arxiv.org</a>。iCaRL在每学新类别时， <strong>保存每个旧类别少量代表样本（记忆库）</strong> ，训练时将这些旧样本与新数据一起用于更新模型，并对模型输出进行知识蒸馏以防决策边界偏移<a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail">arxiv.org</a>。通过同时学习分类新类别和回顾旧类别，iCaRL实现了深度网络在长时间增量学习许多类时，比仅新数据训练的策略遗忘显著减少<a href="https://arxiv.org/abs/1611.07725#:~:text=only%20the%20training%20data%20for,where%20other%20strategies%20quickly%20fail">arxiv.org</a>。iCaRL开创了<strong>样本记忆回放+蒸馏</strong>结合的范式。之后许多增量学习方法沿用了“小样本记忆”思路，如 Hou <em>et al.</em> (2019) 的UCIR、Wu <em>et al.</em> (2019) 的BiC等，都在如何精选和高效利用少量旧样本上下功夫。经验重放策略也直接应用于强化学习领域——在非平稳环境中，智能体可将过去经历的<strong>轨迹</strong>保存一部分，在策略更新时混入重放，以避免策略完全偏离先前成功经验。这与深度Q网络（DQN）的经验回放缓冲理念一脉相承，只是这里目的是防遗忘而不仅是破除相关性。在机器人学习中，经验重放意味着机器人在学习新技能时定期练习已掌握的技能（通过模拟旧技能的传感器输入等），这在实践中提高了多技能机器人系统的鲁棒性。</p>
<p><strong>Gradient Episodic Memory (GEM, 2017):</strong> Lopez-Paz &amp; Ranzato 在 NeurIPS 2017 提出的 GEM 则进一步创新了重放的使用方式<a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>。与直接将旧样本混入训练不同，GEM把少量旧任务样本存入 <em>episodic memory</em> ，在每次参数更新时，通过约束新梯度与旧样本梯度的内积为非负，<strong>确保新任务训练不会增加旧任务损失</strong><a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>。这种基于<em>优化约束</em>的方法保证了模型对记忆样本性能不下降，实现了一定的“向后迁移”能力：在学习新任务的同时还有可能改进旧任务表现<a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>。GEM开创了利用<strong>回放样本的梯度信息</strong>指导优化的思路，其后续简化版A-GEM (Chaudhry et al. 2018)降低了计算成本。对于具身智能而言，GEM这类方法的优势在于即使少量记忆也能通过优化约束起效，而且不需要明确任务边界（可以对任意过去经验施加约束）。不过其劣势是需要实时计算并存储梯度，复杂度较高，且仍需维护一个小型记忆库。</p>
<p><strong>生成式回放 (Generative Replay):</strong> 当直接存储原始旧样本受限时，另一策略是训练<strong>生成模型</strong>来产生日前学过的数据，从而实现回放。Shin <em>et al.</em> 在 NeurIPS 2017 提出的 <strong>Deep Generative Replay (DGR)</strong> 是该思路的里程碑<a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks">arxiv.org</a>。DGR构建了一个生成模型（如GAN或VAE）作为“记忆仿真器”，在学习新任务时利用生成模型产生日前各任务的合成样本，并与新数据混合训练<strong>Solver</strong>模型<a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks">arxiv.org</a>。在每完成一个任务后，Solver的参数固定，然后训练生成模型去拟合更新后的Solver分布，以便下次产生更新的数据分布<a href="https://ar5iv.org/pdf/1907.00182#:~:text=A%20classical%20method%20implementing%20a,model%20to%20learn%20next%20task">ar5iv.org</a>。这种<strong>双模型协同</strong>框架受到大脑“海马-新皮层”互作机制的启发，即利用快速变化的“海马体”生成回忆来训练慢更新的“皮层”网络<a href="https://arxiv.org/abs/1705.08690#:~:text=the%20problem%2C%20it%20requires%20large,sequential%20learning%20settings%20involving%20image">arxiv.org</a>。DGR实验证明，即使不保存任何真实旧样本，模型仍能通过生成模拟数据达到与有存储时相近的效果<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Instead%20of%20modeling%20the%20past,based%20learning%2C%20where%20the%20model">ar5iv.org</a><a href="https://arxiv.org/abs/1705.08690#:~:text=dual%20model%20architecture%20consisting%20of,settings%20involving%20image%20classification%20tasks">arxiv.org</a>。生成回放的优势是 <strong>不直接占用存储真实数据</strong> ，在隐私敏感或内存极小的设备上尤为有用<a href="https://arxiv.org/abs/1705.08690#:~:text=solving%20multiple%20tasks%20have%20been,those%20for%20a%20new%20task">arxiv.org</a>。其缺点在于生成模型本身也面临持续学习问题（如何不忘记早期的数据分布），且训练开销较大<a href="https://ar5iv.org/pdf/1907.00182#:~:text=While%20most%20of%20the%20Generative,62">ar5iv.org</a>。后续不少工作改进了生成回放，如 Wu <em>et al.</em> (2018) 将GAN与变分特征结合（MeRGAN），Rios <em>et al.</em> (2018) 用生成对抗网络生成特征而非像素，提高效率等<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Lee%20et%20al.%20,%E2%9C%93%20%20%20%E2%9C%93">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Achille%20et%20al.%20,%E2%9C%93%20%20%20%E2%9C%93">ar5iv.org</a>。生成回放还被应用到强化学习的状态生成中，例如 Caselles-Dupré <em>et al.</em> (2019) 提出的自触发生成回放（S-TRIGGER）用于连续学习环境状态表示<a href="https://ar5iv.org/pdf/1907.00182#:~:text=%2A%20%20%5B20%5D%20H.%C2%A0Caselles,Di%C2%A0Stefano%2C%20and">ar5iv.org</a>。总的来看，回放类方法（包括经验重放和生成回放）在各种基准上往往表现突出，被认为是目前<strong>抗遗忘最有效</strong>的范式之一。然而它们对存储或生成能力有要求，在具身智能中需权衡内存/算力和性能：对于机器人等设备，存储少量关键经验（如图像片段、关键帧）进行回放在实践中较常用，而实时训练复杂生成模型则相对少见。</p>
<h3 id="模块化架构与参数隔离方法">模块化架构与参数隔离方法<a hidden class="anchor" aria-hidden="true" href="#模块化架构与参数隔离方法">#</a></h3>
<p><strong>Progressive Neural Networks (PNN, 2016):</strong> Rusu 等人提出的渐进神经网络是持续学习的架构派代表<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。PNN在每遇到新任务时 <strong>冻结已有网络</strong> ，并侧旁新增一组“列”网络用于学习新任务<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。同时，通过旁路连接让新任务列能够利用之前各列学到的特征（实现知识迁移）<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a>。这种架构确保旧任务的参数永不修改，从而<strong>彻底避免遗忘</strong><a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a>；而新增模块可以专门学习新任务，有充足的模型容量。PNN在Atari游戏和3D迷宫导航等一系列强化学习任务上取得优于微调的成绩，并显示出显著的<strong>前向迁移</strong>能力<a href="https://arxiv.org/abs/1606.04671#:~:text=a%20step%20forward%20in%20this,layers%20of%20the%20learned%20policy">arxiv.org</a>。其缺点也很明显：每增加一个任务网络规模就线性增长，在任务数很多时不切实际<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。尽管如此，PNN证明了模块隔离在避免遗忘上的有效性，许多后续方法受此启发引入<em>可扩展</em>或<em>可选择激活</em>的架构。</p>
<p><strong>PathNet (2017) 与 PackNet (2018):</strong> 为了缓解PNN网络爆炸的问题，Fernando 等人 (2017) 提出的 PathNet 利用进化算法在固定网络中为每个任务选择一条互不干扰的子网络路径，相当于在共享参数的前提下实现参数隔离。Mallya 和 Lazebnik (2018) 则提出  <strong>PackNet</strong> ，通过反复<strong>剪枝</strong>和<strong>重训练</strong>来为新任务腾出参数空间<a href="https://arxiv.org/abs/1711.05769#:~:text=,scale">arxiv.org</a>。具体来说，PackNet先训练初始任务模型，然后剪除一定比例不重要的参数（权重置零但保留位置），学习第二个任务时仅利用空闲参数；如此迭代，将多个任务“打包”进单个网络中<a href="https://arxiv.org/abs/1711.05769#:~:text=pruning%20techniques%2C%20we%20exploit%20redundancies,grained">arxiv.org</a>。实验表明，在ImageNet等大型数据上，PackNet可在一个VGG模型中连续容纳多个细粒度分类任务，性能接近于单独训练<a href="https://arxiv.org/abs/1711.05769#:~:text=minimal%20storage%20overhead,available%20at%20this%20https%20URL">arxiv.org</a>。PackNet无需存储旧数据，也不引入新参数，因此相比PNN更高效<a href="https://arxiv.org/abs/1711.05769#:~:text=,scale">arxiv.org</a>。但PackNet需要预先设定剪枝比例，且剪枝过多可能损害旧任务性能，过少则限制新任务空间。后来一些变体如 “Piggyback” (Mallya, 2018) 则改为学习任务特定的掩码，更灵活地实现参数复用。总体而言，<em>参数隔离</em>类方法（含动态扩张和网络剪枝）通过<strong>结构上的硬约束</strong>避免了遗忘，其优势是旧知识完全保留、无干扰<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a>。在机器人等具身智能中，如果任务集是离散且有限的，这类方法可考虑使用。例如在多任务机器人控制中，可为每个任务分配专属网络模块或参数子集，新任务加入时扩展网络并冻结旧模块，从而保持以往技能<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a>。然而，在开放环境下任务可能连续涌现且无法预知数量，单纯无限扩展网络不切实际。因此近期一些工作尝试结合元学习或 <strong>条件网络</strong> ，自动决定何时复用旧参数、何时增加新参数，以兼顾模型规模和遗忘防护。比如 Serra <em>et al.</em> (2018) 提出的 HAT 方法对每层参数学习可训练门控，通过门控向量的稀疏化实现在相同网络中隔离不同任务的激活区域，从而在不显著增加参数的情况下减少干扰。</p>
<p><strong>脑启发的双记忆体系:</strong> 值得注意的是，一些方法从神经科学的<strong>双重内存</strong>理论汲取灵感，将快速学习模块和稳定长时模块结合起来应对遗忘。例如 Kemker 和 Kanan (2018) 提出的 <strong>FearNet</strong> 模型采用“大脑 <strong>海马-新皮层</strong> ”的架构<a href="https://arxiv.org/abs/1711.10563#:~:text=for%20each%20class%2C%20making%20it,art%20performance%20at%20incremental%20class">arxiv.org</a>：用一个类似海马体的小网络专门快速学习当前任务，并在适当时机（模拟睡眠）将新知识整合（consolidate）到另一个类似皮层的大网络中做长期存储<a href="https://arxiv.org/abs/1711.10563#:~:text=previous%20examples%2C%20making%20it%20memory,AudioSet%29%20benchmarks">arxiv.org</a>。同时还有一个类似杏仁核的模块，根据输入判断应该用哪套记忆系统回答<a href="https://arxiv.org/abs/1711.10563#:~:text=recent%20memories%20inspired%20by%20the,art%20performance%20at%20incremental%20class">arxiv.org</a>。FearNet不需存储旧样本，依赖<strong>生成式</strong>机制回忆旧类数据，达到与iCaRL相当的性能<a href="https://arxiv.org/abs/1711.10563#:~:text=to%20suffer%20from%20catastrophic%20forgetting,FearNet%20also%20uses%20a%20module">arxiv.org</a>。这类方法实质上属于架构+回放的混合策略（因为短期网本身可看作一种内生记忆生成器）。双记忆策略对具身智能有自然的意义：机器人或代理可以配置一个“小而快”的在线学习器来及时适应新变化，同时定期将知识固化到“大而稳”的长期模型中，从而两全其美。不过如何确定巩固频率以及双网络的容量匹配仍在探索中。</p>
<h3 id="方法对比与小结">方法对比与小结<a hidden class="anchor" aria-hidden="true" href="#方法对比与小结">#</a></h3>
<p>不同持续学习方法各有优劣，在具身智能场景下需要平衡选择<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Moreover%2C%20most%20of%20continual%20learning,and%20the%20strategies%20they%20propose">ar5iv.org</a>。<strong>正则化方法</strong>不需保存样本、开销低，适合嵌入式设备在线更新，但在任务变化剧烈时可能束缚新知识获取<a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a">ar5iv.org</a>。<strong>回放方法</strong>往往效果最佳，即便少量样本重放也能显著降低遗忘<a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>；对于机器人这种可<strong>反复与环境交互</strong>的场景，还可通过自主采样过去环境状态进行重演。然而存储真实数据可能受限于隐私或容量，而训练生成模型又对计算资源有较高要求<a href="https://arxiv.org/abs/1705.08690#:~:text=solving%20multiple%20tasks%20have%20been,those%20for%20a%20new%20task">arxiv.org</a>。<strong>模块化/参数隔离方法</strong>彻底杜绝了遗忘，在多任务机器人系统（任务有限且可拆分）中很有价值，但在开放任务中扩展性受限<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。<strong>外部记忆和双重内存</strong>策略提供了一种折中：通过引入专门的记忆模块，模型可以在不反复调整主要网络权重的情况下查询和更新知识。例如在多人对话交互机器人中，引入一个可读写的记忆单元存储历史对话要点，有助于长期一致的对话理解。但引入记忆也增大了系统复杂度，需要设计高效的检索和写入机制。</p>
<p>此外，许多先进方法不再局限于单一策略，而是<strong>混合多种机制</strong>以取长补短<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Moreover%2C%20most%20of%20continual%20learning,and%20the%20strategies%20they%20propose">ar5iv.org</a>。例如 Schwarz 等人 (2018) 提出的 <strong>Progress &amp; Compress</strong> 框架将<strong>动态架构</strong>与<strong>蒸馏</strong>结合：使用Progressive Network扩展新任务列，然后通过蒸馏将新列知识压缩回主干网络，从而既避免遗忘又控制模型规模<a href="https://arxiv.org/abs/1805.06370#:~:text=problems,alphabets%20as%20well%20as%20two">arxiv.org</a>。再如 von Oswald <em>et al.</em> (2019) 的 <strong>MER</strong> 方法将元学习思想融入记忆重放，通过元训练提高模型表示对新旧任务的解耦，从而辅助减少干扰。这些综合方法在近年不断涌现，说明持续学习领域正朝着<strong>多策略融合</strong>与<strong>自动适应</strong>方向发展。</p>
<h2 id="具身智能场景中的持续学习应用">具身智能场景中的持续学习应用<a hidden class="anchor" aria-hidden="true" href="#具身智能场景中的持续学习应用">#</a></h2>
<p>具身智能领域（如机器人、自主车辆、智能代理）为持续学习提供了最实际也最具挑战的用武之地<a href="https://arxiv.org/abs/1907.00182#:~:text=,most%20recent%20papers%20on%20continual">arxiv.org</a>。与静态数据集不同，具身智能体在物理世界中连续感知和行动，环境非平稳且任务边界往往不明确<a href="https://ar5iv.org/pdf/1907.00182#:~:text=Robotic%20agents%20have%20to%20learn,robotics%20approaches%20in%20a%20way">ar5iv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=You%20want%20to%20learn%20from,not%20know%20how%20and%20when">ar5iv.org</a>。以下我们重点考察持续学习方法在几个具身场景的应用进展：</p>
<p><strong>机器人视觉与物体识别：</strong> 服务机器人需要在不断变化的环境中识别新对象、适应新场景，这正是开放域的持续学习问题。为评测算法，2020年提出了<strong>OpenLORIS-Object</strong>机器人视觉数据集，包含随时间推移环境光照、视角、物距等变化的数据流<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322003004#:~:text=Towards%20lifelong%20object%20recognition%3A%20A,vision%20with%20quantifiable%20environmental%20factors">sciencedirect.com</a>。在该数据集上，Lomonaco 等人组织了持续学习挑战，促进了算法在真实机器人感知条件下的比较。一系列方法被测试：如 iCaRL 的小样本存储结合知识蒸馏策略在这种增量物体识别中取得稳健表现；又如 IROS 2020 的 <strong>Latent Replay</strong> 方法，Pellegrini <em>et al.</em> 提出<strong>只在特征空间保存和重放</strong>旧数据<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=16,IEEE">link.springer.com</a>。具体而言，机器人摄像头图像经卷积网络得到中间表示，将这些<strong>低维激活</strong>缓存代替原始高维图像，可大幅减少存储并实现实时回放<a href="https://ieeexplore.ieee.org/document/9341460/#:~:text=Xplore%20ieeexplore,space%2C%20we%20store%20activations">ieeexplore.ieee.org</a>。实验表明，在OpenLORIS这种持续视觉任务中，Latent Replay比直接存图像几乎不降性能，却更高效满足机器人实时性需求<a href="https://ieeexplore.ieee.org/document/9341460/#:~:text=Xplore%20ieeexplore,space%2C%20we%20store%20activations">ieeexplore.ieee.org</a>。另一最新进展是 Hajizada <em>et al.</em> (2024) 提出的 <strong>Continually Learning Prototypes (CLP)</strong> 算法<a href="https://arxiv.org/html/2404.00418v1#:~:text=require%20buffering%20and%20a%20balanced,free%2C%20hence%20does%20not">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=novelties%20and%20learning%20new%20items,simple%20version%20of%20CLP%20in">arxiv.org</a>。CLP针对机器人<strong>少样本在线学习</strong>和<strong>开放世界</strong>场景设计：它采用<em>原型</em>向量表征每类知识，并通过<strong>元可塑性</strong>机制动态调整每个原型的学习速率来平衡新旧知识稳定性<a href="https://arxiv.org/html/2404.00418v1#:~:text=scenarios%20where%20robots%20must%20learn,free%2C%20hence%20does%20not">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=Learning%20Prototypes%20%28CLP%29,In%20a%20low">arxiv.org</a>。同时CLP具备新类别<strong>自我检测与无监督学习</strong>能力（即机器人遇到未知物体时可判断新类别并自主创建原型学习）<a href="https://arxiv.org/html/2404.00418v1#:~:text=networks%20and%20the%20more%20natural,towards%20realistic%20learning%20for%20robots">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=supervision%C2%A0%5B8%5D.%20Therefore%2C%20we%20extend%20FS,OWCL">arxiv.org</a>。重要的是，CLP<strong>不使用任何显式回放数据</strong>且兼容神经形态芯片，实现了超低能耗下的持续学习<a href="https://arxiv.org/html/2404.00418v1#:~:text=forgetting%2C%20CLP%20utilizes%20a%20novel,In%20the%20open%20world">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=dynamic%20adaptation%20mechanism%20for%20the,part%20of%20the%20Lava%20software">arxiv.org</a>。这对于内存和电池有限的移动机器人具有现实意义。总的来看，在机器人视觉领域，混合使用 <strong>小样本记忆</strong> 、 <strong>特征回放</strong> 、<strong>适应性学习率</strong>等技术已取得显著效果，使机器人能逐步扩展认知能力且遗忘受控。</p>
<p><strong>人机交互与多模态学习：</strong> 具身智能体常涉及多模态感知（视觉、听觉、语言）和人机交互，这带来了持续学习的新课题。例如社交机器人需要持续学习新的对话内容、新的手势动作等。NLP领域已有针对增量学习的综述（如 Biesialska  <em>et al.</em> , 2020<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=7,6523%E2%80%936541%20%282020">link.springer.com</a>），其中提到自然语言处理任务在持续学习中面临词汇和语义随时间演变的问题。一些方法通过动态扩充词典或嵌入空间缓解了“遗忘”早期语义的现象。对于多模态交互，Kulkarni <em>et al.</em> (2019) 提出在对话系统中使用<strong>弹性权重约束</strong>来保留模型早期对话技能，同时新增新领域对话意图。交互学习中一个重要方面是 <strong>用户在环（human-in-the-loop）</strong> ：机器人可通过用户反馈实时修正知识。近期有工作探索 <strong>交互式持续学习</strong> ，如 Hazifa <em>et al.</em> (2022) 结合神经形态计算，利用片上在线学习快速吸收用户教授的新知识，同时通过正则保护已有知识<a href="https://dl.acm.org/doi/10.1145/3546790.3546791#:~:text=Interactive%20continual%20learning%20for%20robots%3A,footprint%20and%20interactive%20learning%20capability">dl.acm.org</a>。虽然具体算法仍在早期，但这些尝试指出了方向——未来的具身智能体应能通过持续人机交互 <strong>自我进化</strong> ，并且做到“学而不忘”。</p>
<p><strong>连续控制与强化学习：</strong> 在自主驾驶、机器人控制等连续决策场景，持续学习同样关键。例如自动驾驶车辆遇到新道路场景，需要学习新策略而不忘记基本驾驶技能。Shaheen <em>et al.</em> (2022) 的综述<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Continual%20learning%20is%20essential%20for,implementations%20of%20continuous%20learning%20algorithms">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=under%20three%20main%20autonomous%20systems%2C,extensively%20explored%20in%20this%20article">link.springer.com</a>总结了三类自主系统（无人车、无人机和移动机器人）中的持续学习挑战：模型需在<strong>在线方式</strong>从大量顺序数据中学习，且资源受限、须保障安全稳定<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Continual%20learning%20is%20essential%20for,implementations%20of%20continuous%20learning%20algorithms">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=learning%20algorithms%20that%20perform%20continuous,are%20extensively%20explored%20in%20this">link.springer.com</a>。一些研究采用<strong>策略蒸馏</strong>或<strong>迁移学习</strong>避免遗忘旧任务策略。如 Rusu <em>et al.</em> 在DeepMind的机器人实验中，用<strong>渐进网络</strong>将仿真训练的技能迁移到现实机器人上，同时保持仿真技能不丢失<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>。又如 Traoré <em>et al.</em> (2019) 提出的 <strong>DiscoRL</strong> 框架，将旧策略压缩为策略库，再用<strong>Policy Distillation（策略蒸馏）<strong>技术在新环境中融合旧策略以加速学习，同时旧策略作为教师防止遗忘<a href="https://ar5iv.org/pdf/1907.00182#:~:text=,Task%20and%20Lifelong%20Learning%2C%202019">ar5iv.org</a>。在连续控制中，策略往往以神经网络表示，类似分类任务的遗忘也会发生：新环境下调整策略网络，会导致旧环境下性能下降。为此 Rolnick <em>et al.</em> (2019) 提出的 <strong>CLEAR</strong> 方法，将</strong>off-policy经验重放</strong>引入强化学习的策略梯度训练，既提高新任务样本效率又维持旧任务价值函数不变。该方法在Atari游戏顺序学习中取得好结果，被视为强化学习领域对抗遗忘的有效方案之一。需要强调的是，强化学习场景中任务界限往往模糊，甚至代理可能在<em>一个</em>不断演变的环境中持续学习（如运营多年的家庭服务机器人，会不断遇到新任务）。这接近<strong>无任务标签 (task-agnostic)<strong>的持续学习。Aljundi <em>et al.</em> (2019) 针对此提出了</strong>在线持续学习</strong>方案：通过检测网络对新数据的<strong>干扰程度</strong>动态触发记忆重放（MIR）<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=18,2019">link.springer.com</a>，以及使用<strong>梯度稀疏化</strong>挑选对旧任务干扰最大的记忆样本来更新，从而在无明确任务边界下也能抑制遗忘。此类方法在机器人持续感知与导航中具有潜力，因为现实中机器人很难知道自己何时“切换了任务”，只能根据环境变化连续调整。</p>
<p><strong>开放世界和自主适应:</strong> 具身智能体经常处于开放世界，可能遇到训练时未见过的全新情况。持续学习的终极目标是在这种开放环境中实现持续适应而不崩溃。Open-world持续学习需要综合上述技术，还涉及<strong>新知识的自主发现</strong>和 <strong>主动学习</strong> 。比如前述CLP方法引入了<strong>新类检测</strong>机制，让机器人在开放世界下识别何时需要学习新对象<a href="https://arxiv.org/html/2404.00418v1#:~:text=Yet%2C%20FS,as%20such%20a%20system%20should">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=supervision%C2%A0%5B8%5D.%20Therefore%2C%20we%20extend%20FS,OWCL">arxiv.org</a>。又如 Mundt <em>et al.</em> (2020) 探讨了结合<strong>异常检测</strong>和持续学习，使模型在检测到输入分布偏移时能触发新任务学习流程。对自主车而言，面对从未见过的道路情况（极端天气、新施工区域），如果能自动检测出“新情境”并调用持续学习模块更新模型，将大幅提高安全性。当然，这也带来安全约束下的学习稳定性问题，需要确保新学习不会在尚未充分验证时投入决策。近期一些研究主张引入 <strong>不确定性估计</strong> （如Bayesian NN）判断模型何时需要学习新任务，以及学习后的性能变化<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Google%20Scholar">link.springer.com</a>。这些探索尚属前沿，但对于真正长期自主运作的智能体至关重要。</p>
<h2 id="近年新进展20202025与展望">近年新进展（2020–2025）与展望<a hidden class="anchor" aria-hidden="true" href="#近年新进展20202025与展望">#</a></h2>
<p>过去五年中，持续学习领域涌现了一系列新趋势和方法，进一步提高了模型在复杂环境中的持续适应能力：</p>
<ul>
<li><strong>任务无关持续学习:</strong> 越来越多工作关注在无明确任务边界、数据连续流动场景下的学习<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=15,11254%E2%80%9311263%20%282019">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=18,2019">link.springer.com</a>。这更贴近现实中的机器人/代理感知流。为此，方法上强调在线更新、有限内存和即时评估。例如 2020 年的 GDumb 方法提出一种极端简单但强大的baseline：始终只训练当前模型在收到的全部数据上（存储一定量最近数据），每次新数据到来直接从头训练。这种方法虽谈不上高明，却在一些线上学习赛道表现接近更复杂的方法，提示我们需要重新审视评价指标。在可预见的将来， <strong>线上持续学习</strong> （Single-Pass Continual Learning）将成为研究热点，它要求算法一次遍历数据且不泄露未来信息，在边学习边推理的同时抗遗忘<a href="https://arxiv.org/pdf/2302.00487#:~:text=Realistic%20applications%20present%20particular%20challenges,other%20vision%20domains%20such%20as">arxiv.org</a><a href="https://arxiv.org/pdf/2302.00487#:~:text=latter%2C%20although%20current%20advances%20mainly,methods%20are%20adapted%20to%20them">arxiv.org</a>。具身智能如实时视频流分析、持续语音识别都属于这种场景。</li>
<li><strong>持续学习评测基准丰富化:</strong> 近年构建了许多新数据集和基准来评测持续学习算法在更复杂任务上的性能。如 CORe50、OpenLORIS 等视觉序列数据集用于评测<strong>在线物体识别</strong><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=16,IEEE">link.springer.com</a>；ACL 2021 Lifelong NLP挑战提供持续自然语言理解任务；还有不同领域的持续强化学习基准、连续无人驾驶仿真环境等。这些基准推动算法从依赖任务ID的小规模实验，走向更贴近真实的情境。评测指标也越发丰富，除了遗忘率、累计精度外，开始考虑模型的<strong>计算效率、内存开销</strong>以及在长期学习中的<strong>稳定性</strong><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=trained%20models%20cannot%20effectively%20deal,implementations%20of%20continuous%20learning%20algorithms">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=learning%20algorithms%20that%20perform%20continuous,are%20extensively%20explored%20in%20this">link.springer.com</a>。这些综合指标对于具身智能系统尤为重要，因为实际应用中资源受限且需要持续运行。</li>
<li><strong>联邦持续学习与分布式学习:</strong> 在物联网和边缘计算兴起的背景下，<strong>联邦持续学习</strong>成为新方向<a href="https://arxiv.org/html/2411.13740v1#:~:text=Federated%20Continual%20Learning%20for%20Edge,AI">arxiv.org</a>。即多个分散设备（如一群机器人或智能传感器）在各自持续学习的同时，定期交流模型更新，从而在保证隐私下实现知识共享和共同进化。诸如 FedWeIT、FCL 等算法探索了如何在联邦场景下减少遗忘并高效通信<a href="https://arxiv.org/html/2411.13740v1#:~:text=Federated%20Continual%20Learning%20for%20Edge,AI">arxiv.org</a>。对具身智能来说，这意味着例如一队协作机器人的经验可以融合，使整体学习速度加快且每个体遗忘降低。该领域仍在起步，面临异步学习、设备差异等挑战，但前景值得期待。</li>
<li><strong>理论分析与可解释性:</strong> 持续学习理论方面，近年有人尝试从信息论和最优化角度给出遗忘的分析框架，如用Fisher信息界定参数迁移平衡<a href="https://ar5iv.org/pdf/1907.00182#:~:text=weights%20and%20produce%20an%20adapted,Fisher%20matrix%20using%20the%20Kronecker">ar5iv.org</a>。另外，对持续学习过程中的<strong>可解释性</strong>要求也在提高——在机器人应用中，理解模型为何遗忘某能力、何时需要触发新学习，对于建立用户信任很重要。一些研究利用可视化技术观察随着任务增添，网络内部表示如何演化，以寻找缓解遗忘的线索。还有工作将<strong>神经符号方法</strong>引入持续学习，以借助符号逻辑的约束保持旧知识。这些方向虽属于前沿探索，但表明社区已不仅满足于经验提升性能，也在寻求持续学习更深层的理论和可解释支撑。</li>
</ul>
<p>综上，持续学习作为迈向真正智能系统的关键一步，近年来在算法和应用上都取得了显著进展。从最初发现问题、提出启发式对策，到如今各种融合策略在复杂环境中落地，我们离“像人一样终身学习”的AI越来越近。在具身智能领域，实现持续学习将赋予机器人和自主代理长久的自主适应能力，使其能够随着环境和任务变化不断成长，而无需频繁人工干预。展望未来，持续学习研究需要进一步结合<strong>元学习、强化学习、因果推断</strong>等范式，研发更加通用高效的算法。同时，在真实世界大规模部署持续学习系统时，还需重视 <strong>安全机制</strong> （防止在学习过程中性能突然退化）、 <strong>伦理与隐私</strong> （学习过程中对用户数据的处理）等问题。可以预见，随着研究的深化，持续学习将在机器人自主导航、智能助理、自动驾驶乃至通用人工智能等领域扮演日益重要的角色，推动人工智能从“静态聪明”走向“动态成长”。</p>
<p><strong>参考文献：</strong></p>
<ul>
<li>McCloskey, M. &amp; Cohen, N. J. (1989).  <em>Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem</em> . In  <strong>Psychology of Learning and Motivation</strong> , vol. 24, pp. 109–165<a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information">en.wikipedia.org</a>. ( <em>首次揭示神经网络顺序学习遗忘问题</em> )</li>
<li>French, R. M. (1999).  <em>Catastrophic forgetting in connectionist networks</em> .  <strong>Trends in Cognitive Sciences, 3</strong> (4):128–135<a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=Unfortunately%2C%20though%2C%20catastrophic%20forgetting%20does,brain%20might%20have%20overcome%20this">cell.com</a>. ( <em>灾难性遗忘综述，分析原因并讨论可能解决方案</em> )</li>
<li>Robins, A. (1995).  <em>Catastrophic forgetting, rehearsal and pseudorehearsal</em> .  <strong>Connection Science, 7</strong> (2):123–146<a href="https://ar5iv.org/pdf/1907.00182#:~:text=,In%202016%20IEEE%2055th%20Conference">ar5iv.org</a>. ( <em>提出伪重演方法，用随机伪样本重放旧知识</em> )</li>
<li>Goodfellow, I. et al. (2014).  <em>An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</em> . In  <strong>ICLR 2014</strong> . ( <em>实证分析深度网络遗忘现象，评估基本缓解策略</em> )</li>
<li>Li, Z. &amp; Hoiem, D. (2016).  <em>Learning without Forgetting</em> . In <strong>ECCV 2016</strong><a href="https://arxiv.org/abs/1606.09282#:~:text=retraining%20on%20such%20data%20becomes,and%20new%20task%20datasets%20for">arxiv.org</a>. ( <em>知识蒸馏用于持续学习，只用新任务数据保持旧任务性能</em> )</li>
<li>Kirkpatrick, J. et al. (2017).  <em>Overcoming catastrophic forgetting in neural networks</em> .  <strong>PNAS, 114</strong> (13):3521–3526<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a>. ( <em>提出EWC，通过弹性权重凝固保护重要参数<a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially">arxiv.org</a></em> )</li>
<li>Zenke, F. et al. (2017).  <em>Continual Learning Through Synaptic Intelligence</em> . In <strong>ICML 2017</strong><a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a>. ( <em>提出SI算法，智能累积参数重要性减少遗忘<a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency">arxiv.org</a></em> )</li>
<li>Rebuffi, S.-A. et al. (2017).  <em>iCaRL: Incremental Classifier and Representation Learning</em> . In <strong>CVPR 2017</strong><a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail">arxiv.org</a>. ( <em>提出增量分类策略，结合样本保存和蒸馏避免遗忘<a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail">arxiv.org</a></em> )</li>
<li>Lopez-Paz, D. &amp; Ranzato, M. (2017).  <em>Gradient Episodic Memory for Continual Learning</em> . In <strong>NeurIPS 2017</strong><a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a>. ( <em>提出GEM算法，用梯度约束保证新任务不增大旧任务损失<a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to">arxiv.org</a></em> )</li>
<li>Shin, H. et al. (2017).  <em>Continual Learning with Deep Generative Replay</em> . In <strong>NeurIPS 2017</strong><a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks">arxiv.org</a>. ( <em>提出深度生成回放DGR，通过生成模型重现旧样本融合训练<a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks">arxiv.org</a></em> )</li>
<li>Rusu, A. A. et al. (2016).  <em>Progressive Neural Networks</em> . <strong>arXiv:1606.04671</strong><a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy">arxiv.org</a>. ( <em>提出渐进网络架构，扩展新列避免遗忘并实现知识迁移<a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high">arxiv.org</a></em> )</li>
<li>Mallya, A. &amp; Lazebnik, S. (2018).  <em>PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning</em> . In <strong>CVPR 2018</strong><a href="https://arxiv.org/abs/1711.05769#:~:text=,scale">arxiv.org</a>. ( <em>通过迭代剪枝为新任务腾出容量，实现单网络多任务无遗忘<a href="https://arxiv.org/abs/1711.05769#:~:text=pruning%20techniques%2C%20we%20exploit%20redundancies,grained">arxiv.org</a></em> )</li>
<li>Schwarz, J. et al. (2018).  <em>Progress &amp; Compress: A scalable framework for continual learning</em> . In <strong>ICML 2018</strong><a href="https://arxiv.org/abs/1805.06370#:~:text=problems,alphabets%20as%20well%20as%20two">arxiv.org</a>. ( <em>提出进展-压缩框架，结合渐进扩展和蒸馏压缩，实现无增长持续学习<a href="https://arxiv.org/abs/1805.06370#:~:text=problems,alphabets%20as%20well%20as%20two">arxiv.org</a></em> )</li>
<li>Aljundi, R. et al. (2019).  <em>Online Continual Learning with Maximal Interfered Retrieval</em> . In  <strong>NeurIPS 2019</strong> . ( <em>提出在线持续学习算法MIR，选择干扰最大的记忆样本回放，任务无关场景有效</em> )</li>
<li>Pellegrini, L. et al. (2020).  <em>Latent Replay for Real-Time Continual Learning</em> . In <strong>IROS 2020</strong><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=16,IEEE">link.springer.com</a>. ( <em>提出在特征空间进行重放，支持机器人实时持续学习，降低存储与计算需求</em> )</li>
<li>Kemker, R. &amp; Kanan, C. (2018).  <em>FearNet: Brain-Inspired Model for Incremental Learning</em> . In <strong>ICLR 2018</strong><a href="https://arxiv.org/abs/1711.10563#:~:text=for%20each%20class%2C%20making%20it,art%20performance%20at%20incremental%20class">arxiv.org</a>. ( <em>提出双内存脑启发模型，不存原始数据通过生物式记忆系统整合知识<a href="https://arxiv.org/abs/1711.10563#:~:text=for%20each%20class%2C%20making%20it,art%20performance%20at%20incremental%20class">arxiv.org</a></em> )</li>
<li>Hajizada, E. et al. (2024).  <em>Continually Learning Prototypes</em> . <strong>arXiv:2404.00418</strong><a href="https://arxiv.org/html/2404.00418v1#:~:text=require%20buffering%20and%20a%20balanced,free%2C%20hence%20does%20not">arxiv.org</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=novelties%20and%20learning%20new%20items,simple%20version%20of%20CLP%20in">arxiv.org</a>. ( <em>提出原型持续学习方法，少样本在线学习并支持开放世界新类发现，无需存储回放</em> )</li>
<li>Shaheen, K. et al. (2022).  <em>Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks</em> .  <strong>Journal of Intelligent &amp; Robotic Systems, 105</strong> (9)<a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Continual%20learning%20is%20essential%20for,implementations%20of%20continuous%20learning%20algorithms">link.springer.com</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=learning%20algorithms%20that%20perform%20continuous,are%20extensively%20explored%20in%20this">link.springer.com</a>. ( <em>面向自主系统的持续学习综述，分析算法在无人车、无人机等中的性能和挑战</em> )</li>
<li>Lesort, T. et al. (2020).  <em>Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges</em> .  <strong>Information Fusion, 58</strong> :52–68<a href="https://arxiv.org/abs/1907.00182#:~:text=,learning%20is%20not%20necessarily%20finding">arxiv.org</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Robotic%20agents%20have%20to%20learn,robotics%20approaches%20in%20a%20way">ar5iv.org</a>. ( <em>持续学习在机器人领域的综述，提出评测框架和跨领域方法借鉴思路</em> )</li>
</ul>
<p>引用</p>
<p><a href="https://arxiv.org/abs/1907.00182#:~:text=,learning%20is%20not%20necessarily%20finding"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=An%20important%20challenge%20for%20machine,of%20complex%20skills%20and%20knowledge"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=1%20Introduction"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=You%20want%20to%20learn%20from,not%20know%20how%20and%20when"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=cognitive%20science,not%20disrupted%20by%2C%20new%20information"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&sz=32">Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference</a><a href="https://arxiv.org/abs/2312.10549#:~:text=Catastrophic%20Forgetting%20%28CF%29,solutions%2C%20proposes%20a%20taxonomy%20to"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[2312.10549] Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomyhttps://arxiv.org/abs/2312.10549</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=of%20the%20scientific%20community%20by,not%20disrupted%20by%2C%20new%20information"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&sz=32">Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference</a><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=All%20natural%20cognitive%20systems%2C%20and%2C,generalize%2C%20to%20function%20in%20the">Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2</a><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=Unfortunately%2C%20though%2C%20catastrophic%20forgetting%20does,brain%20might%20have%20overcome%20this">Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2</a><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2#:~:text=catastrophic%20forgetting,solution%20for%20distributed%20connectionist%20networks">Catastrophic forgetting in connectionist networks: Trends in Cognitive Scienceshttps://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=phenomenon%20that%20reflects%20the%20trade,18%2C%203"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=McCloskey%20and%20Cohen%20,with%20backpropagation%20neural%20network%20modelling"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&sz=32">Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference</a><a href="https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=each%20learning%20trial%20on%20the,Furthermore%2C%20the%20problems%202%2B1"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&sz=32">Catastrophic interference - Wikipediahttps://en.wikipedia.org/wiki/Catastrophic_interference</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=,In%202016%20IEEE%2055th%20Conference"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Instead%20of%20modeling%20the%20past,based%20learning%2C%20where%20the%20model"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=the%20past%20is%20not%20forgotten,91%2C%2020%20%2C%20%20110"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf#:~:text=,Hinton%20et%20al.%2C"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://www.cs.uic.edu&sz=32">[PDF] Continual Learning and Catastrophic Forgettinghttps://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf</a><a href="https://arxiv.org/abs/1606.09282#:~:text=retraining%20on%20such%20data%20becomes,and%20new%20task%20datasets%20for"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282</a><a href="https://arxiv.org/abs/1606.09282#:~:text=for%20its%20existing%20capabilities%20are,for%20improved%20new%20task%20performance"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.09282] Learning without Forgettinghttps://arxiv.org/abs/1606.09282</a><a href="https://arxiv.org/abs/1612.00796#:~:text=overcome%20this%20limitation%20and%20train,several%20Atari%202600%20games%20sequentially"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796</a><a href="https://arxiv.org/abs/1612.00796#:~:text=tasks%20by%20selectively%20slowing%20down,several%20Atari%202600%20games%20sequentially"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1612.00796] Overcoming catastrophic forgetting in neural networkshttps://arxiv.org/abs/1612.00796</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=weights%20and%20produce%20an%20adapted,Fisher%20matrix%20using%20the%20Kronecker"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200</a><a href="https://arxiv.org/abs/1703.04200#:~:text=intelligent%20synapses%20that%20bring%20some,forgetting%20while%20maintaining%20computational%20efficiency"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200</a><a href="https://arxiv.org/abs/1703.04200#:~:text=continually%20adapt%20to%20changing%20domains%2C,forgetting%20while%20maintaining%20computational%20efficiency"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1703.04200] Continual Learning Through Synaptic Intelligencehttps://arxiv.org/abs/1703.04200</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=The%20regularization%20methods%20have%20been,high%20regularization%2C%20and%20finding%20a"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=also%20generative%20models%20,high%20regularization%2C%20and%20finding%20a"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Basic%20regularization%20techniques%20that%20could,More%20complex%20methods%20consist%20in"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=streams%20of%20data%20presented%20sequentially,speed%20during%20the%20learning%20process"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=One%20solution%20to%20Continual%20Learning,where%20continual%20learning%20is%20necessary"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1611.07725#:~:text=concepts%20over%20time%20from%20a,where%20other%20strategies%20quickly%20fail"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725</a><a href="https://arxiv.org/abs/1611.07725#:~:text=only%20the%20training%20data%20for,where%20other%20strategies%20quickly%20fail"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1611.07725] iCaRL: Incremental Classifier and Representation Learninghttps://arxiv.org/abs/1611.07725</a><a href="https://arxiv.org/abs/1706.08840#:~:text=Second%2C%20we%20propose%20a%20model,of%20GEM%20when%20compared%20to"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1706.08840] Gradient Episodic Memory for Continual Learninghttps://arxiv.org/abs/1706.08840</a><a href="https://arxiv.org/abs/1705.08690#:~:text=generative%20nature%20of%20hippocampus%20as,settings%20involving%20image%20classification%20tasks"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=A%20classical%20method%20implementing%20a,model%20to%20learn%20next%20task"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1705.08690#:~:text=the%20problem%2C%20it%20requires%20large,sequential%20learning%20settings%20involving%20image"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690</a><a href="https://arxiv.org/abs/1705.08690#:~:text=dual%20model%20architecture%20consisting%20of,settings%20involving%20image%20classification%20tasks"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690</a><a href="https://arxiv.org/abs/1705.08690#:~:text=solving%20multiple%20tasks%20have%20been,those%20for%20a%20new%20task"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1705.08690] Continual Learning with Deep Generative Replayhttps://arxiv.org/abs/1705.08690</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=While%20most%20of%20the%20Generative,62"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Lee%20et%20al.%20,%E2%9C%93%20%20%20%E2%9C%93"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Achille%20et%20al.%20,%E2%9C%93%20%20%20%E2%9C%93"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=%2A%20%20%5B20%5D%20H.%C2%A0Caselles,Di%C2%A0Stefano%2C%20and"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,layers%20of%20the%20learned%20policy"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671</a><a href="https://arxiv.org/abs/1606.04671#:~:text=transfer%20and%20avoiding%20catastrophic%20forgetting,level%20sensory%20and%20high"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671</a><a href="https://arxiv.org/abs/1606.04671#:~:text=a%20step%20forward%20in%20this,layers%20of%20the%20learned%20policy"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1606.04671] Progressive Neural Networkshttps://arxiv.org/abs/1606.04671</a><a href="https://arxiv.org/abs/1711.05769#:~:text=,scale"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769</a><a href="https://arxiv.org/abs/1711.05769#:~:text=pruning%20techniques%2C%20we%20exploit%20redundancies,grained"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769</a><a href="https://arxiv.org/abs/1711.05769#:~:text=minimal%20storage%20overhead,available%20at%20this%20https%20URL"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.05769] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruninghttps://arxiv.org/abs/1711.05769</a><a href="https://arxiv.org/abs/1711.10563#:~:text=for%20each%20class%2C%20making%20it,art%20performance%20at%20incremental%20class"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563</a><a href="https://arxiv.org/abs/1711.10563#:~:text=previous%20examples%2C%20making%20it%20memory,AudioSet%29%20benchmarks"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563</a><a href="https://arxiv.org/abs/1711.10563#:~:text=recent%20memories%20inspired%20by%20the,art%20performance%20at%20incremental%20class"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563</a><a href="https://arxiv.org/abs/1711.10563#:~:text=to%20suffer%20from%20catastrophic%20forgetting,FearNet%20also%20uses%20a%20module"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1711.10563] FearNet: Brain-Inspired Model for Incremental Learninghttps://arxiv.org/abs/1711.10563</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Moreover%2C%20most%20of%20continual%20learning,and%20the%20strategies%20they%20propose"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://arxiv.org/abs/1805.06370#:~:text=problems,alphabets%20as%20well%20as%20two"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1805.06370] Progress &amp; Compress: A scalable framework for continual learninghttps://arxiv.org/abs/1805.06370</a><a href="https://arxiv.org/abs/1907.00182#:~:text=,most%20recent%20papers%20on%20continual"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://arxiv.org/abs/1907.00182</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=Robotic%20agents%20have%20to%20learn,robotics%20approaches%20in%20a%20way"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322003004#:~:text=Towards%20lifelong%20object%20recognition%3A%20A,vision%20with%20quantifiable%20environmental%20factors"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://www.sciencedirect.com&sz=32">Towards lifelong object recognition: A dataset and benchmarkhttps://www.sciencedirect.com/science/article/abs/pii/S0031320322003004</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=16,IEEE"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://ieeexplore.ieee.org/document/9341460/#:~:text=Xplore%20ieeexplore,space%2C%20we%20store%20activations">Latent Replay for Real-Time Continual Learning - IEEE Xplorehttps://ieeexplore.ieee.org/document/9341460/</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=require%20buffering%20and%20a%20balanced,free%2C%20hence%20does%20not"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=novelties%20and%20learning%20new%20items,simple%20version%20of%20CLP%20in"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=scenarios%20where%20robots%20must%20learn,free%2C%20hence%20does%20not"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=Learning%20Prototypes%20%28CLP%29,In%20a%20low"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=networks%20and%20the%20more%20natural,towards%20realistic%20learning%20for%20robots"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=supervision%C2%A0%5B8%5D.%20Therefore%2C%20we%20extend%20FS,OWCL"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=forgetting%2C%20CLP%20utilizes%20a%20novel,In%20the%20open%20world"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=dynamic%20adaptation%20mechanism%20for%20the,part%20of%20the%20Lava%20software"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=7,6523%E2%80%936541%20%282020"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://dl.acm.org/doi/10.1145/3546790.3546791#:~:text=Interactive%20continual%20learning%20for%20robots%3A,footprint%20and%20interactive%20learning%20capability"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://dl.acm.org&sz=32">Interactive continual learning for robots: a neuromorphic approachhttps://dl.acm.org/doi/10.1145/3546790.3546791</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Continual%20learning%20is%20essential%20for,implementations%20of%20continuous%20learning%20algorithms"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=under%20three%20main%20autonomous%20systems%2C,extensively%20explored%20in%20this%20article"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=learning%20algorithms%20that%20perform%20continuous,are%20extensively%20explored%20in%20this"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://ar5iv.org/pdf/1907.00182#:~:text=,Task%20and%20Lifelong%20Learning%2C%202019"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://ar5iv.org&sz=32">[1907.00182] Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challengeshttps://ar5iv.org/pdf/1907.00182</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=18,2019"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://arxiv.org/html/2404.00418v1#:~:text=Yet%2C%20FS,as%20such%20a%20system%20should"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Continual Learning for Autonomous Robots: A Prototype-based Approachhttps://arxiv.org/html/2404.00418v1</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=Google%20Scholar"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=15,11254%E2%80%9311263%20%282019"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://arxiv.org/pdf/2302.00487#:~:text=Realistic%20applications%20present%20particular%20challenges,other%20vision%20domains%20such%20as"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">https://arxiv.org/pdf/2302.00487</a><a href="https://arxiv.org/pdf/2302.00487#:~:text=latter%2C%20although%20current%20advances%20mainly,methods%20are%20adapted%20to%20them"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">https://arxiv.org/pdf/2302.00487</a><a href="https://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc#:~:text=trained%20models%20cannot%20effectively%20deal,implementations%20of%20continuous%20learning%20algorithms"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://link.springer.com&sz=32">Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks | Journal of Intelligent &amp; Robotic Systemshttps://link.springer.com/article/10.1007/s10846-022-01603-6?error=cookies_not_supported&amp;code=2588747a-8932-4197-a391-b846298fdfcc</a><a href="https://arxiv.org/html/2411.13740v1#:~:text=Federated%20Continual%20Learning%20for%20Edge,AI"><img alt="Favicon" loading="lazy" src="https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32">Federated Continual Learning for Edge-AI: A Comprehensive Surveyhttps://arxiv.org/html/2411.13740v1</a></p>
<p>2024–2025 年具身智能持续学习新方法综述：缓解灾难性遗忘的创新策略
持续学习（Continual Learning）旨在让智能体能顺序学习多个任务而不遗忘已学知识。然而传统方法（如 EWC、iCaRL、GEM、PackNet、DGR 等）主要在静态数据上验证，在具身智能场景中效果有限。具身智能中的持续学习面临真实环境的挑战：任务顺序模糊、样本稀少、实时交互、高维感知等。这要求新机制在稳定-可塑性间取得更佳平衡，避免旧知识被覆盖（灾难性遗忘）同时高效习得新技能。以下我们综述 2024–2025 年出现的多篇新论文，每篇提供方法简介、核心创新、与传统方法的区别及其在具身场景中的优势分析。
Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation (Yuanqi Yao 等, 2025 年, arXiv)
方法简介：这项工作提出了PSPL（Primitive-level Skill Prompt Learning）方法，用于机器人操作的无重温持续学习。PSPL 将技能解构为可复用的“原语”（primitives），通过前缀提示（prefix prompts）来表示，并在两阶段训练方案中不断扩展技能库。首先进行多技能联合预训练，学习一组运动感知的技能提示（motion-aware skill prompts），提取不同技能间的共性语义与运动原语。随后在增量学习新技能时，为每个新技能添加新的前缀提示，通过与旧技能提示的跨注意力机制共享知识，从而加速新技能学习。这一提示学习策略使机器人能够在无需重放旧任务数据的情况下连续获取新技能。 **核心创新：PSPL 将提示学习（prompt learning）引入机器人持续学习领域，创新性地使用“技能前缀提示”作为可扩展的模块化单元来表示和存储技能知识。相比以往为每个新任务增添适配器的方式，PSPL 的提示可以在技能之间共享和重用，显式捕捉技能间的共性原语（包括语义和运动层面的共性）。此外提出的文本+光流联合提示查询（text-flow query）<strong>机制，将语言指令与视觉运动信息融合，用于检索适当的原语提示，从而关联语义截然不同但运动模式相似的技能。这一机制解决了仅靠文本嵌入查询时，不同技能间缺乏关联的问题。 与传统方法区别：相较于 EWC 等对网络权重施加正则的方式，PSPL 并不限制参数更新，而是通过前缀提示的模块化扩展来避免遗忘：旧技能的提示向量固定，新技能通过新增提示获取模型容量，从而防止旧知识被覆盖。这类似 PackNet 的“逐任务分配参数”思想，但PSPL的提示可以通过跨注意力与旧提示交互，实现旧新知识的共享迁移，弥补 PackNet 类方法模块隔离导致的无法迁移问题。与 iCaRL、GEM 等需要存储旧样本回放不同，PSPL 不需要任何经验回放——模型通过提示机制保留旧技能，因此在训练新任务时无需重温旧数据便可避免遗忘。相比于 LwF 这类基于知识蒸馏的方法，PSPL直接在模型内部保留了原技能的“软提示”，避免了仅凭日志概率约束可能出现的信息不足。 具身场景下的优势：首先，无重放需求降低了实际机器人系统的存储和隐私负担（无需记录旧任务视频或演示数据），非常适合那些无法无限存储过去经验的场景。其次，PSPL 的原语提示捕捉了跨任务的共性动作模式，使机器人能在少样本情况下将已学技能迁移到新任务上，例如仅观察少量演示也能通过提示召回相关原语来加速学习。再次，该方法对每个技能只新增少量提示向量，属于参数高效的扩展方式，不像全面微调那样消耗大量新参数，适合机器人有限算力的约束。最后，在 LIBERO 基准的模拟和真实操作实验中，PSPL 在前向迁移（FWT）和背向保留（BWT）指标上均达到当前最好水平，验证了其在长期任务序列中的遗忘防御和跨技能知识共享能力。
Preserving and Combining Knowledge in Robotic Lifelong Reinforcement Learning (Yuan Meng 等, 2025 年, Nature Machine Intelligence)
方法简介：该研究提出了名为LEGION的机器人终身强化学习框架
nature.com
。LEGION 构建了一个非参数贝叶斯知识空间（knowledge space），用以逐步积累和保存机器人的技能知识。具体而言，作者使用Dirichlet 过程混合模型（DPMM）来根据任务生成的表征不断拓展知识空间的簇，以容纳新任务知识，同时保留旧任务的簇不被覆盖。每当机器人从连续到来的一次性任务流中学习完一个任务，DPMM 就在知识空间中产生或调整相应的知识组分，实现对该任务知识的分离存储与渐进扩充。此外，LEGION 将语言嵌入引入任务表征，使智能体理解任务语义，从而在学习和推理中利用语言指导任务推断。在执行阶段，机器人能够将知识空间中多个任务的技能组合重新应用于长序列复合任务（例如依次执行多步操作完成“清理桌子”等目标）。 核心创新：LEGION 的突出创新在于引入贝叶斯非参模型管理知识：不同于传统固定架构，DPMM 知识空间可以动态增加新的知识组分，无需预先限定任务数量。这解决了过去结构模块化方法在任务数未知时难以扩展的问题
nature.com
。知识空间中的每个簇本质上代表一类技能或策略的“知识单元”，新任务到来时，如果其知识无法用现有簇解释，就会自动生成新簇存储，避免了旧知识的遗忘与冲突。同时，每个任务的知识以概率混合模型形式保存在连续的先验空间中，便于在需要时被识别和重新调用。另一个创新是融合语言描述提升任务区分和推理能力：语言嵌入提供了任务的高层语义提示，帮助知识空间进行任务归属推断和知识检索，使得在执行长视距任务时，智能体可以更准确地推理需要调用哪些已有技能。 与传统方法区别：LEGION 综合了多种持续学习理念但又有所突破。相较于 EWC 等正则化方法，LEGION 不直接作用于网络参数，而是在参数输出的高层空间保存知识，因而不存在权重被覆盖的问题。相比 PackNet 这类结构隔离方法，LEGION 的知识空间无需预设模块数且能持续增长，解决了传统结构方法难以应对未知数量任务的局限
nature.com
。同时，有别于纯经验回放策略（如 GEM）必须存储大量过往数据，LEGION 通过贝叶斯记忆高效概括了过去任务的精华，不需要完整保留旧样本即可实现类似回放的效果。虽然训练中仍使用有限缓冲作为强化学习的 replay（SAC 算法自身需要）, 但由于知识已存于非参空间，LEGION 即使在不增加缓冲容量的情况下依然能逐任务提高成功率。另外，LEGION 与以往假定任务明确分布范围的多任务学习不同，它能够适应非参数任务变化，处理现实中出现的新规则或新交互形式——这些情景下老方法（例如基于高斯混合的元学习）因需预设任务数量而难以胜任。 具身场景下的优势：LEGION 在真实机器人持续学习中展示出多项优势。首先，其知识保存与扩展机制使机器人几乎无遗忘地累积技能——实验表明，即使经过长时间训练再次回访早期任务，成功率仍接近初始掌握水平，证明旧技能通过知识空间得到有效保留。其次，LEGION 可以将已学知识用于新任务的快速推理和组合，体现了少样本快速迁移能力：作者展示了机器人在学习一系列基本任务后，能够在一次提示下将相关技能串联完成复杂长任务，例如“制作咖啡”，这源于知识空间支持对已学技能的自由组合。再次，LEGION 引入语言降低了感知与任务推断难度，使机器人在多模态指令下表现出色——通过语言描述任务，机器人无需大量探索即可定位对应知识簇，从而更快适应新任务语境
nature.com
。最后，在对比实验中，LEGION 在顺序任务中稳定提升成功率，显著优于“完美记忆”（无限重放）、“reservoir采样回放”和 A-GEM 等方法。这表明在数据受限、任务未知的真实环境中，LEGION 的知识空间机制比传统持续学习策略更适合维持和重用机器人知识。
LEAGUE++: Empowering Continual Robot Learning through Guided Skill Acquisition with Language Models (Zhaoyi Li 等, 2024 年, VLMNM 2024 研讨会)
方法简介：LEAGUE++ 提出了一种利用大语言模型（LLM）引导机器人持续学习技能的框架。该方法将任务与运动规划（TAMP）和深度强化学习（DRL）相结合：先由预训练的大语言模型将复杂长任务自动分解为一系列子任务操作序列（即生成操作符及步骤规划），并产生日志级别的稠密奖励信号指导每个子任务的强化学习训练。机器人通过 DRL 学习各子步骤的低层技能，同时维护一个符号化技能库记录已学的技能。当新任务到来时，LEAGUE++ 使用已有技能库中的技能作为Warm-Start（预热），加速学习新技能。整个系统在四个模拟任务域上进行了验证，包括家务操作等复杂任务场景。结果表明，LEAGUE++ 相比基线有更快的学习速度和更高的任务成功率，并能通过技能重用在新域中显著缩短收敛时间。 核心创新：LEAGUE++ 最大的创新在于将大模型的规划能力和传统强化学习有机结合，实现“边规划、边学习”。具体来说，它利用 LLM 强大的语义理解和分解能力，自动将高层指令分解为低层动作序列（解决了RL直接面对长远稀疏奖励任务时的困难），从而大幅降低了长视距任务的学习难度。其次，它提出了符号技能库概念：已学技能以可调用的符号模块形式存储，使机器人在新任务时可以检索并复用相关技能，而非从零学习。这种技能库机制相当于一种渐进专家网络：随着任务增多，库中专家技能增多，策略可以根据需要调用对应子政策，类似人类在新任务中调用过往经验。最后，LEAGUE++ 将现有预训练模型用于策略初始化（warm-start），例如视觉模型用于感知、行为克隆模型用于初始策略等。这减少了新任务的探索开销，使持续学习更高效稳定。 与传统方法区别：传统持续学习多半依赖网络自身调节参数来避免遗忘，而 LEAGUE++ 则更像一种“学习+规划”双轨方案。它没有采用 EWC/GEM 等直接在RL算法中增加遗忘惩罚，而是通过任务分解将复杂问题拆解，降低每阶段学习干扰。这种方式避免了以往RL持续学习中，不同任务策略存于同一网络导致的梯度干扰，从根本上缓解了遗忘。此外，与 PackNet 等为每任务固定网络模块不同，LEAGUE++ 通过符号调用机制在同一策略网络中执行不同技能，相当于软模块化——技能库中的技能相当于可切换的策略段，无需冻结参数就能隔离任务干扰。相比纯回放或迁移学习，LEAGUE++ 依赖 LLM 提供额外知识（如何解任务、设计奖励），扩充了信息来源：这类似人类导师提供提示，加速了学习而非仅靠算法克服遗忘。可以说，LEAGUE++ 将知识提炼（通过LLM）与技能留存（通过库）结合，超出了传统持续学习框架。 具身场景下的优势：LEAGUE++ 非常契合具身智能体执行复杂任务的需求。首先，LLM 提供的分层规划使机器人哪怕面对长且模糊的指令（例如“准备早餐”）也能一步步拆解执行，避免在长时间任务中遗忘目标或迷失——这在人类复杂指令场景尤为关键。其次，技能库的重用大幅提升了数据效率：当机器人进入新环境或任务，只要能从库中找到类似子任务，就无需从头练习，少样本即可适应。这特别适合真实机器人训练昂贵、无法大量试错的情况。第三，由于采用符号层指导和技能初始化，LEAGUE++ 在每个任务的学习过程都更快、更稳定，降低了资源消耗和试错成本。作者的实验表明，在多个复杂模拟环境（如家庭清洁、装配任务）中，LEAGUE++ 全面优于纯RL的持续学习基线。例如在新域任务上，因复用了旧域技能，学习速度显著快于从零开始的对照。这证明在真实机器人需要反复学习不同任务的长期部署中，引入LLM指导和技能库的框架能够实现效率和稳健性的兼顾，这是传统方法难以同时达到的。
Scalable and Efficient Continual Learning from Demonstration via a Hypernetwork-generated Stable Dynamics Model (Sayantan Auddy 等, 2024 年, arXiv)
方法简介：该工作针对机器人示教学习（LfD）场景提出了一个持续学习方法，利用超网络（Hypernetwork）和稳定动力学模型来保持多技能学习过程中的稳定性和不遗忘。核心思想是训练一个超网络，生成两个子网络：一个用于学习每项示范任务的轨迹动力学模型，另一个生成与之配套的李雅普诺夫稳定性函数。通过在每次引入新示范任务时，让超网络输出相应的轨迹模型并辅以稳定性约束，该方法确保机器人复现的轨迹是收敛稳定的，不会在插值或续航过程中发散。更重要的是，这种引入稳定性的方式还显著提升了持续学习性能：作者发现，相较不考虑稳定性的基线，引入李雅普诺夫函数后，机器人在序列学习多个技能时的遗忘显著减少。此外，为了提高效率，作者提出了分块超网络（chunked hypernetwork）和随机正则化策略，将训练多个任务的总时间从原先的 $O(N^2)$ 降低到 $O(N)$。整个方法在实时的机器人绘画轨迹学习任务（包括 LASA 数据集扩展到高维、和作者自建的 RoboTasks9 实验）上验证，结果显示无论在轨迹精度还是持续学习指标上都优于现有方法。 核心创新：本方法的创新点在于将鲁棒控制中的稳定性保障引入持续示教学习。通过让超网络同时学轨迹模型和Lyapunov 函数，每新增一项技能，系统不仅学会了模仿轨迹本身，还获得了对该技能的稳定约束，使其在整个状态空间具有吸引域。这解决了过去很多 LfD 方法关注模仿误差、却忽视了轨迹泛化稳定性的问题——即使没有回放旧示范，稳定性保障使旧技能的轨迹在训练新技能时不易被扰乱。其次，使用超网络作为核心架构，使得多任务知识统一存储在超网络的权重中，通过不同输入（如任务ID或上下文）生成相应任务的模型。相比传统逐任务微调网络，超网络天然具备参数共享和隔离的双重特性：共享是指不同任务的共性部分自动在超网络中得到整合，隔离是指超网络输出不同任务模型，相当于为每个任务保留了一套隐含的参数。这种方式巧妙地避免了不同任务参数直接冲突覆盖。再次，提出的随机正则项训练方法，通过每次训练仅对一个随机选取的旧任务施加约束，代替以往对所有旧任务都正则化的做法，大幅降低了训练复杂度。这一技巧保持性能不变却将训练开销线性化，提升了持续学习的可扩展性。 与传统方法区别：相比 EWC 等对参数层面的稳定约束，本方法将稳定性提升到行为层面：EWC关心参数不剧烈变化，而该方法直接确保每个技能的输出轨迹在引入新技能后依然收敛不发散，因而对遗忘的防御更直接有效。与 iCaRL/GEM 等需要存储和重放示范不同，该方法不需要对旧示范数据回放：旧技能知识蕴含在超网络权重中，加之稳定性函数的限制，新任务训练不会破坏旧技能轨迹，无需样本重温也能保护旧技能。同时，相较 PackNet 逐任务固定部分网络的硬隔离，超网络属于软隔离：所有任务共享同一个生成网络，但通过任务索引输出不同技能模型，相当于用函数生成参数，既避免干扰又保持容量节省。与之前一项采用超网络+神经ODE方法做 continual LfD 的工作相比（作者提及的最近方法），本方法增加了严格的稳定性保障，因此旧技能保真度和新技能学习效果都有明显提升。总的来说，新方法在理念上融合了控制理论和持续学习，提供了一种不同于以往任何单一范式的框架。 具身场景下的优势：在实际机器人示教应用中，该方法具有显著优势。首先，稳定性约束确保机器人在执行已学轨迹时不会因为学习了新技能而出现意外失稳或偏差，这对物理系统尤为重要——避免了因遗忘造成的运行危险，提高了安全性和可靠性。其次，由于无需反复重训旧示范，机器人能更高效地学习新技能：作者在真实机器人轨迹跟踪任务中展示，新技能加入后旧技能仍能零-shot 执行，表现几乎不下降，这意味着机器人随时可用, 不需要频繁校准旧技能。第三，该方法在高维技能扩展上表现良好，证明其可应用于涉及多自由度的复杂操作（例如机械臂同时控制位置和姿态的轨迹）。传统方法在高维连续控制上的持续学习往往不稳定，而本方法借助Lyapunov函数确保了每个技能在高维空间都是吸引子的，因此具备更强的鲁棒性。最后，在资源受限的机器人上，训练效率的线性提升非常有价值：每新增技能的训练开销不会随着技能数量指数增长，使得机器人可以规模化地持续学习几十上百个技能而不会因为训练时间过长变得不可行。综上，该方法为现实中机器人持续学习大量示教任务提供了安全、高效、稳健的解决方案。
Online Continual Learning for Interactive Instruction Following Agents (Byeonghwi Kim 等, 2024 年, ICLR 2024)
方法简介：这项工作定义了更贴近现实的交互式指令跟随持续学习场景，并提出了信心感知滑动平均（CAMA）算法来缓解遗忘。作者指出，过去大多假设智能体一开始就能获取所有训练任务的数据，而不符合机器人应持续探索、持续学习的现实。为此论文提出两个设置：(1) 行为增量学习（Behavior-IL）：机器人不断学习新指令行为；(2) 环境增量学习（Environment-IL）：在新环境中执行之前学过的指令。困难在于任务边界可能不明确，传统“数据先验”方法（如基于旧任务输出日志概率的蒸馏）需要明确任务边界和缓存旧任务信息。CAMA 则不需要任务边界：智能体在训练过程中对每个观察到的指令计算模型输出的置信度，并以滑动平均的方式更新其对旧任务的输出分布估计。当模型学习新行为时，如果对某类以前学过的指令置信度下降，CAMA 会根据之前维护的滑动平均分布对模型进行轻微调整，以拉回对旧知识的信心，从而达到持续无边界学习的目的。实验证明，在作者设计的新基准上（包含连续新增指令和环境的条件），CAMA 相较现有方法取得了显著更好的表现。 核心创新：CAMA 的创新在于引入任务无关的置信度追踪机制：传统方法如 LwF 或 iCaRL 需要存储每个旧任务的模型输出分布或实例样本，当新任务到来时通过知识蒸馏或重放维持旧任务性能。这实际隐含了已知的任务边界和任务身份。而 CAMA 不存储具体样本或明确任务ID，而是对模型输出信心做持续监控。它维护一个滑动平均估计，可视作模型对过去经验的“模糊记忆”，随着时间衰减地记录旧任务的输出倾向。当模型学新任务时，如果这种倾向发生显著漂移，表明遗忘在发生，那么CAMA依据滑动平均的差异，对模型参数进行微调修正（如调整输出层偏置等），无需知道具体哪个任务受影响，只基于置信度变化即可纠正。这种方法跳出了任务级别的框架，实现了</strong>任务无关（task-free）**的持续自稳训练。另外，CAMA 所需存储的信息量极小，仅为每个输出类别一个滑动均值，计算与存储开销极低，非常简洁却有效。 与传统方法区别：与 EWC 等正则方法需要在参数变化上加全局约束不同，CAMA 工作在输出空间：它不直接限制权重，而是看模型对以前输出的信心水平是否下降。这有点类似知识蒸馏（LwF）的思想，但不需要旧模型保存——蒸馏通常要保存旧模型输出作为固定目标，而CAMA持续更新的滑动平均本质上充当了“旧模型记忆”，避免了存储多个旧模型或大量旧样本。相比 iCaRL 维护每类样本代表来近似旧任务分布，CAMA 维护的是压缩的统计量（置信度均值），因而不需要样本库也没有额外模型，只需少量内存。与 GEM 这类基于梯度投影的方法相比，CAMA 算法更简单，不需要复杂的二次规划求解，只以移动平均做调整，在线即可执行。另外，CAMA 属于任务无边界的方法，解决了传统方法大多假定任务切换已知、或者需要在检测到切换后才能应用策略的问题。在现实机器人持续学习中，任务变化往往连续发生且未必有明显分界，CAMA 正是面向这种情况设计的。 具身场景下的优势：CAMA 所针对的交互式指令跟随场景极具代表性：机器人在不断遇到新指令、新环境的过程中持续学习，这对服务机器人等非常实际。CAMA 能在无监督任务切换的前提下，让机器人保持之前学会的指令技能。例如，一个家庭助理机器人在学会一系列语音指令后，被主人带到新房间教它新任务，CAMA 确保机器人在学新任务时不会忘记旧房间的指令执行方法。由于不需要存储大量过往数据，CAMA 也适合长期部署：机器人可以不断学习数十数百种指令而不必担心存储开销或隐私问题（无需录音或录像存档）。同时，CAMA 的实时性很高，它在模型训练时即可动态调整参数，无需离线阶段，适合机器人在线学习的需求。作者报告该方法在提出的 Behavior-IL 和 Environment-IL 基准上显著优于已有的持续学习算法，这表明它成功地解决了任务模糊情况下的遗忘问题，让机器人在变化的环境和任务中保持稳健性能。总之，CAMA 为具身智能体提供了一种轻量级但有效的持续学习机制，使其能像人一样一边执行指令、一边随着环境改变不断积累本领。
ICAL: In-Context Abstraction Learning for Multimodal Agents (Gabriel Sarch 等, 2024 年, arXiv)
方法简介：ICAL 提出了一种利用大模型自身的生成与内省能力，持续提升多模态智能体决策的方案。该方法关注这样的问题：大型语言或视觉-语言模型（LLM/VLM）虽具有强大的Few-shot能力，但需要高质量范例作提示。人为提供高质量示例既昂贵又不具通用性。ICAL 的解决方案是：让大模型从次优的示范轨迹和人类反馈中自动生成可用于提示的抽象示例。具体流程包括：给定一个全新领域的嘈杂演示（例如一个任务的视频示范，可能包含低效动作或错误），首先由视觉-语言模型对该轨迹进行解析抽象。模型会产出一个通用的程序化描述，修正了示范中的低效步骤，并添加认知注释，标注出任务涉及的关系、对象状态变化、时间子目标和关键细节等。接下来，让真实机器人/代理尝试执行这份抽象计划，在执行过程中人类可以提供自然语言反馈纠正模型的错误理解或补充知识。模型根据反馈交互式地细化之前产生的抽象。最终得到的这些抽象示例（带有语言注释和优化后的步骤），被存储起来作为内存范例。在后续决策中，大模型可以将这些示例作为提示的一部分，从而显著提升对于类似任务的决策能力。ICAL 在三个具有挑战的环境中验证：TEACh对话式指令跟随、VisualWeb互联网多模态代理、Ego4D第一人称视频动作预测，都取得了新的SOTA性能。 核心创新：ICAL 将持续学习转化为持续积累提示示例的过程，而非传统的持续调权过程。这是一个范式转变：与其反复梯度更新模型参数，ICAL 让模型自己“理解反思”示范并生成抽象的知识总结，逐步丰富模型的提示库。这样的内省式示教机制是首次提出。具体创新点包括：(1) 自动抽象：利用预训练VLM从视觉示范中提取高层语义——不像以往只关注低级动作序列，ICAL 提取了任务因果关系、对象状态变化、时间逻辑、任务构造等四类认知抽象，这些抽象比原始示范更精炼、更具概括性。(2) 人机交互细化：ICAL 不仅依赖模型自我生成，还引入人类在模型执行时给出自然语言反馈，让模型迭代改进其抽象。这种循环使得次优示范（含错误或低效部分）也能通过反馈纠正而变得有价值，突破了以往示范学习要求专家示范的限制。(3) 提示记忆库：将生成的抽象示例存入一个不断扩展的库，并使用检索增强的大模型决策——模型在推理新任务时，会从库中取出相关示例放入提示，提高决策准确率。ICAL 相当于在持续学习过程中渐进地提升模型的上下文学习能力：模型见到的新任务越多，就自行总结出越多可泛化的知识片段，下次遇到类似任务时即使不调权也能通过提示完成。实验显示，ICAL 随着学习示例增多，模型性能持续提升，呈现出真正的“持续改进”特性。 与传统方法区别：传统持续学习多在参数空间操作，如EWC约束权重变化、经验回放存数据样本等。而 ICAL 完全绕开了参数遗忘问题：模型主体参数基本保持不变（或只在最后有小幅微调作为增强），因此谈不上灾难性遗忘。取而代之，ICAL 通过不断加入新的知识提示，让模型具备解决更多任务的能力。这种基于大模型提示的持续学习不同于以往任何持续学习范式。与知识蒸馏类方法相比，ICAL 并非让模型去逼近旧模型输出，而是生成更好的知识为己所用；与模块化或正则方法相比，它无需设计模型结构拆分或损失项，利用的是LLM/VLM自身强大的Few-shot学习能力和生成能力来“吸收”经验。在需要多模态理解和指令执行的任务中，ICAL 运用了视觉和语言的结合，这比单纯依赖视觉记忆（如CV领域一些记忆网络）要丰富，也比仅语言的持续学习（如对LLM增量训练新知识）更直观：ICAL 让模型直接看视频学，也听取人类语言，得到的是跨模态的知识。这些知识用自然语言+视觉标记来存储，具有很强的可解释性和可移植性。传统持续学习很难在不调整参数情况下大幅提高性能，而ICAL 展示了另一种可能：通过增强模型输入（上下文）而非改变模型本身来实现持续学习。 具身场景下的优势：ICAL 针对的场景包括家庭机器人任务（TEACh）、网络操作代理以及视频预测，都与具身智能密切相关。它的优势在于：对于复杂开放环境，很难人工定义明确的任务边界或提供足够训练数据，而ICAL 利用了现有大型模型，只需较少示范和交互即可让模型适应新任务。因此在具身智能典型的少样本、多样任务情况下，ICAL 能快速扩展技能。此外，它不要求持续占用机器人反复训练（除了必要的人机对话反馈），大部分“学习”都在模型的推理过程中完成（让模型生成抽象并评估）——这意味着机器人可以在线学习新任务而不中断服务，通过对话方式边干边学，正如人类新手在导师指导下学习新技能一样。对于任务模糊或长时间任务，ICAL 的抽象总结能力尤为关键：机器人可以从冗长的示范中提炼要点，避免执行时被次要细节干扰。这提高了机器人在复杂任务下的鲁棒性和泛化。实验证实，ICAL 在 TEACh 基准上将先前SOTA的目标达成率提升了12.6%，在VisualWebArena中成功率从14.3%提高到22.7%，在Ego4D动作预测上击败了Few-shot GPT-4V。更重要的是，性能提升是持续的：每当学习更多示例，模型表现就有所提高。因此在长期来看，ICAL 提供了一个持续增长智能体能力的途径，而无须反复训练模型参数，大大降低了具身人工智能系统维护的成本和风险（例如不会出现传统持续学习不慎遗忘必须人工纠正的问题）。这表明，充分利用大模型强大的自监督和内省能力，或许是具身智能持续学习的一条有效新路。
MLLM-CL: Continual Learning for Multimodal Large Language Models (Hongbo Zhao 等, 2025 年, arXiv)
方法简介：MLLM-CL 提出了一个多模态大模型（视觉-语言模型）持续学习的基准和方法。该工作将持续学习划分为两种情形：(1) 领域持续学习（Domain CL）：模型依次学习遥感、医学、科学、自动驾驶、金融等不同视觉问答领域的知识；(2) 能力持续学习（Ability CL）：模型顺序学习OCR识别、数学逻辑、视觉感知、GUI操作代理等不同能力。作者发现，传统增量学习范式（每次在上一个模型参数基础上微调新任务）不适用于多模态大模型：直接用上一任务的权重初始化下一任务，会损害模型对新任务的可塑性，导致次优结果。为解决这一任务冲突问题，MLLM-CL提出了MR-LoRA方法。具体来说，在每个新任务到来时，并不在原模型权重上继续微调，而是为该任务新建一个LoRA低秩适配器，从零随机初始化。这样每个领域/能力都有自己独立的LoRA模块，避免了继承上个任务权重带来的冲突，同时LoRA只引入极少参数，保证高效。在推理时，为了自动选择对应任务的LoRA，作者设计了一个基于大模型的路由器：利用多模态大模型本身处理复杂输入的能力，输入待测试样本后，由模型生成一个路由指令，选择最合适的专家LoRA来回答。这个路由器通过在持续学习每阶段后收集少量任务样本，对大模型进行轻量few-shot微调得到。总体而言，MR-LoRA包含多LoRA专家 + 大模型路由两部分。实验结果显示，在上述多个领域和能力的持续学习任务序列中，该方法在所有任务上的平均性能和最终性能都超越了现有方法，如直接微调、参数隔离、基于提示的方法等。 核心创新：MLLM-CL 的创新在于针对大模型的持续学习提出了避免遗忘的新范式。其一，每任务新LoRA的策略突破了以往串行微调“一脉相承”导致性能下降的窘境。这种做法类似“进渐网络（Progressive Networks）”，但由于LoRA参数规模小，可以在不剧增参数的情况下无限扩展到更多任务。各任务的LoRA彼此独立，保证旧任务参数不受新任务影响，从根本上杜绝了遗忘。同时，所有LoRA附加在同一个大模型上，共享其通用表示能力，又实现了知识迁移（因为基础模型权重保留了多任务共性表示）。其二，引入大模型驱动的路由机制，这是此前持续学习中少有探索的。传统任务识别通常依赖样本的特征相似度或任务ID，而该方法让多模态大模型读入输入后自主生成路由决策。由于大模型本身掌握高层语义和复杂推理，它可以比简单特征距离更精准地选出对应领域的LoRA专家。这保证了在推理阶段对于复杂多模态输入也能正确地匹配到相应能力模块。其三，该方法还建立了系统化的多模态持续学习基准（MLLM-CL基准），涵盖不同类型迁移（领域和能力）并提供了评价指标和数据集构建流程。这填补了多模态大模型持续学习缺少评测标准的空白，为后续研究提供了平台。 与传统方法区别：MLLM-CL 在设计上融合了参数高效微调和专家路由思想。与 EWC 等全模型正则不同，它不直接约束原模型参数，而是固定主干模型，只对每任务引入少量新参数，这避免了原模型权重冲突累积。与 PackNet 类似的方法相比，MR-LoRA 也采用了“每任务额外参数”的思路，但PackNet通过剪枝固定网络容量，MR-LoRA 则用可增添的LoRA实现弹性容量，理论上任务越多仅线性增加参数，无需预留容量。和提示学习（L2P、ModalPrompt 等）相比，提示方法在多模态大模型上效果有限，而且往往需要固定提示长度、可能仍有干扰；MR-LoRA 通过独立LoRA完全隔离任务，比软提示更干净利落。还有，许多多头网络或专家混合模型需要已知任务ID才能选专家，MR-LoRA 则通过路由器实现任务自动判别，不需要人工指示任务类别。可以说，它将模块化与任务识别两个问题一起解决了。在多模态场景下（例如同时应对图片文本、多任务问答），这种方法比单纯视觉或单纯语言的持续学习方法更复杂但也更全面地考虑了输入多样性和任务多样性。 具身场景下的优势：虽然 MLLM-CL 的实验主要是多模态问答和 GUI 任务，但其思想对许多具身智能应用同样有益。比如，一个家庭服务机器人具备多个视觉语言能力（识物、对话、读屏、算术等），MR-LoRA 可以让其持续添加新能力而旧能力不衰减：每项新技能加一个LoRA，机器人就掌握了新本领，又不会遗忘以前学过的（因为以前的LoRA保留原样）。这样的能力库扩展非常符合具身AI逐步进化的需求。其次，由于LoRA模块小，机器人可以在资源受限设备上部署多个技能专家，而不会像扩增整个模型那样内存爆炸。例如针对移动设备，把不同领域的视觉问答能力拆成多个LoRA加载，按需调用，远比加载多个大模型高效。再次，路由器机制使机器人在遇到新感知输入时能自动判断调用哪种能力，这对于多模态交互场景很关键——现实中用户不会明确告知机器人“这是一道医学问题”或“现在开始OCR任务”，机器人必须自适应切换内部技能。MR-LoRA 的路由方案正是训练机器人根据输入自行选择专家。作者实验也表明，在跨领域问答中，MR-LoRA 能正确选择相应LoRA模块，最终在所有领域上同时取得高精度，相比其它方法在后期任务训练后前期任务精度大幅下降，MR-LoRA 几乎零遗忘且新任务精度也高。这证明了其稳定性和塑造性兼顾的能力，非常适合需要终身学习的多才多艺型机器人。总之，MLLM-CL 为具身智能体集成多模态大模型指明了一条持续进化的道路：通过低秩模块化扩展和智能路由，实现持续学习众多技能而性能不减。
Task-Unaware Lifelong Robot Learning with Retrieval-based Weighted Local Adaptation (Pengzhi Yang 等, 2024 年, arXiv)
方法简介：此工作面向机器人开放环境，提出了无任务标识的终身学习框架，结合检索式记忆和局部微调来提升持续学习效果。在真实世界中，机器人遇到的新任务往往没有清晰的边界或ID，且无法存储海量以往数据。为此作者的方法在机器人学习过程中维护一个情景记忆（Episodic Memory），存储每个任务的一小部分关键示例。当机器人在序列学习新技能时，主要模型仍采用常规的基于经验回放的训练（比如采用一部分记忆样本进行练习，以降低遗忘）。然而仅靠有限回放仍难免遗忘一些早期技能。因此在测试执行阶段，如果机器人遇到类似以前学过却已部分遗忘的情境，该方法会从记忆中检索最相关的过往示例，并对当前策略进行快速局部微调（local fine-tuning），以恢复在该情境下的性能。为了提高这种局部微调的效率，作者引入选择性加权机制：首先让机器人在当前情境下执行若干回合，记录其性能（例如哪些步骤出错），然后将这些失败轨迹与记忆中检索到的示范进行比对，自动衡量哪部分旧示范对当前情况帮助最大，给予这些片段更高权重来指导微调
arxiv.org
。简言之，该框架模仿人类温习知识的过程：在需要时重点复习遗忘的难点，从而高效恢复技能。整个方法适用于没有明确任务划分的开放式场景，作者在 LIBERO等操作任务基准上测试了该方法，结果表明即使任务无明显边界，机器人也能持续学到新技能并在需要时重新找回旧技能，大幅优于不采用检索适应的基线。 核心创新：该方法的创新之处在于提出了任务无关的回顾与适应机制。传统持续学习通常假定任务边界明确（如任务开始和结束）以便采取对应策略，比如任务后冻结部分网络或保存样本。然而现实中机器人可能连续不断遇到各种挑战，无法提前知道哪些属于同一任务。本文通过检索式记忆绕过了显式的任务划分：无论当前遇到的情况是否是以前练过的任务，机器人都可以基于当前观测从记忆库检索类似经验。这赋予了模型一种情景感知能力，让它能够自行判定何时需要参考旧经验。其次，提出的Weighted Local Adaptation将元学习思想引入了持续学习测试阶段：不像传统只在训练阶段防遗忘，这里在测试阶段也运行一次短暂微调，相当于在机器人执行时刻临时提高旧技能专门性。这种做法以往少见，因为通常假设模型定型后就执行不变，但在终身学习背景下，适度的测试微调可以极大提升旧技能复现效果。通过限制微调只针对检索到的示例且有选择地重点学习难点，既保证了微调幅度受控不会破坏模型总体性能，又有针对性地弥补遗忘。第三，任务无关意味着模型无需任务标签输入或边界信号，这对开放环境学习非常关键。作者利用视觉和语言嵌入的一致性作为检索键值
arxiv.org
，避免了因持续学习造成的表征漂移（通过预训练模型确保不同阶段embedding空间一致）。这保证了即使模型学了新东西，仍能用统一标准去查找旧记忆，提高了检索可靠性。 与传统方法区别：相比 EWC等在训练时防遗忘的方法，该框架将遗忘补救延伸到了测试时刻：传统模型一旦训练完部署，如果遗忘了旧技能往往束手无策；而此方法允许机器人在执行某任务前“温习一遍”相关经验，相当于给模型一个自行恢复的机会。与 GEM 等在训练时用小样本回放不同，这里的记忆检索主要服务于执行阶段的微调，而非整个训练过程持续混入，因而不会显著增加训练难度，同时又充分利用了记忆在关键时刻的价值。和 iCaRL 直接根据记忆做最近邻推断不同，该方法选择的是微调模型，因此能适应当前状况的细微差别，而不是简单模式匹配，效果更强。在没有任务ID的情况下，很多传统方法如多头网络就无法应用，而本方法完全不依赖任务标签或边界，通过检索+微调实现了隐式的任务处理。可以将其视作结合了经验回放和快速自适应：既保留少量记忆（回放理念），又在需要时通过微调快速适应（元学习理念），融合了二者优点。 具身场景下的优势：对于在动态未知环境中工作的机器人来说，该方法提供了极高的灵活性。机器人无需预先知道将面对哪些任务，也不用在模型结构上做固定分配，它可以不断遇新仍保持从容：每当遗忘可能影响当前任务时，就从记忆中找回相关信息补强自己。这很像人类在现场快速翻阅笔记确认知识点，从而表现出色。尤其在长期部署中，机器人可能几天甚至几月不执行某项技能，这种情况下直接执行可能失败，但如果允许机器人在执行前复习一下过去案例，就能大幅提升成功率。作者的方法正是提供了这种快速热身功能，让机器人在现实使用中更可靠。其次，由于只存储挑选的一小部分示范数据，记忆库很轻便，不会像保存所有数据那样不现实。并且由于采用预训练embedding统一表示，不同时间学习的经验可以共同检索，这意味着机器人可以在跨环境、跨时间的情况下整合经验，非常符合具身智能需要持续集成多方信息的特性。最后，实验中该方法在没有任务边界信息的情况下，其性能超出了使用明确任务边界且大量回放的策略，体现出开放场景适应性。总之，此方法赋予机器人一种人类般的学习策略：即使偶尔遗忘细节，通过快速翻阅过往经历又能想起来，从而在不断变化的任务挑战中保持整体能力不退化，为具身智能的长期自主学习提供了新的思路。
方法演化趋势与对比
综上所述，2024–2025 年关于具身智能持续学习的方法展现出一些共同的发展趋势与对过去方法的根本区别：
从参数保护到知识重组：过去方法（EWC 等）通过阻止参数改动来防遗忘，但在机器人复杂任务中效果不佳。新方法更强调对知识的表达与重组，如通过前缀提示、技能库、知识空间等将知识单元化，避免直接在同一参数上博弈。这些机制允许共享与隔离并存：共享的是不同任务的共性（如PSPL原语、LEGION语言嵌入），隔离的是各任务独特部分（如每任务LoRA、每技能提示）。因此新方法能在不牺牲塑性前提下保持稳定性。
从任务清晰到任务模糊：许多新工作针对任务边界不明的现实。比如任务无关检索、CAMA 的置信度调整都不需要预先知道任务什么时候切换。相比之下，传统方法大多假定任务切换明确并提供信号，现实中难以满足。新方法通过连续监测模型行为或环境，相当于赋予模型自适应觉察能力，真正做到持续学习“终身运行”而非分阶段训练。
从被动防御到主动利用：以前方法把旧知识当需要保护的内容，新方法则倾向主动利用旧知识来帮助新学习。例如PSPL利用旧技能提示加速新技能习得、LEGION 用旧知识簇推断长序列任务、LEAGUE++ 复用已学技能解决新问题。这种正向迁移思路解决了传统方法虽然防遗忘但也不擅长迁移的问题，新方法在防止遗忘的同时显著提高了前向迁移性能，使机器人越学技能越多，解决新任务反而越快。
减少对存储和重放的依赖：传统方法如经验重放在机器人上往往不可行（存储无限增长且隐私风险）。许多新方法都强调“不依赖回放”：PSPL 无需回放旧经验、CAMA 无需存样本只存统计、LEGION 只以有限回放辅以知识空间、Hypernetwork 方法不重训旧示范。即便需要，也以小规模记忆或生成器代替全量存储（如Task-Unaware方法的小记忆+检索、本质上非常有限）。这使持续学习更能适应机器人存储和计算受限的条件。
引入大模型与多模态：与以往专注单一模型训练不同，新方法勇于将预训练大模型纳入持续学习框架，如利用LLM规划子任务（LEAGUE++）、用VLM自我生成知识（ICAL）、大模型本身作为路由器和骨干（MLLM-CL）。多模态信息（语言、视觉、触觉）也被融入，如LEGION用语言辅助任务编码、PSPL用文本和光流提示、ICAL用视频+语言反馈。这些让机器人更好地理解任务和环境，也提供了额外手段缓解遗忘（例如语言描述可作为语义锚点，帮助模型回忆对应技能）。这是持续学习与大模型、多模态技术的交叉融合，标志着具身智能持续学习进入一个更智能更复杂的阶段。
小结：传统持续学习方法在具身智能情境下面临诸多挑战，如参数共用导致遗忘、任务未知导致方法失效、资源有限导致回放不可行等。2024–2025 年的一系列新方法通过全新的机制——提示网络、非参知识库、超网络稳定、符号技能库、任务无关调整、大模型自监督等，成功地解决或缓解了这些问题。在机器人连续学习复杂技能、跨模态理解指令的任务中，这些方法展现出显著优于旧方法的性能，有的甚至实现了遗忘几乎为零且持续正迁移的效果。可以预见，未来具身智能持续学习将沿着模块化+共享、智能检索+自适应、融合大模型知识的路线继续发展，使智能体更接近人类的终身学习水平，在不断变化的世界中保持学习新知识的同时不忘记旧本领。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
