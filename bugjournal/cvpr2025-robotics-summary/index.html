<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal CVPR2025-Summary | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, CVPR 2025, Robotics">
<meta name="description" content="CVPR 2025 Robotics Paper Summary">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal CVPR2025-Summary">
  <meta property="og:description" content="CVPR 2025 Robotics Paper Summary">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-13T13:50:37+08:00">
    <meta property="article:modified_time" content="2025-06-13T13:50:37+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal CVPR2025-Summary">
<meta name="twitter:description" content="CVPR 2025 Robotics Paper Summary">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal CVPR2025-Summary",
      "item": "https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal CVPR2025-Summary",
  "name": "Bug Journal CVPR2025-Summary",
  "description": "CVPR 2025 Robotics Paper Summary",
  "keywords": [
    "Bug Journal", "CVPR 2025", "Robotics"
  ],
  "articleBody": "一、Vision-Language-Action (VLA) 模型 论文 目标 创新 平台 Code 算力 总结 UniAct 将 28 种机器人异构动作映射到统一“通用动作空间”，提升跨形态迁移 动作离散化＋对比约束学习，0.5 B 参数模型优于 7 B 基线 Open-X Embodiment、Libero、Droid；64×A100 训练 GitHub 64 × A100，10 天训练 UniAct 针对不同机器人动作空间异构的问题，提出“Universal Action Space”，把 28 种平台的演示映射到一组离散原子行为，使跨形态学习成为可能。该框架通过语义对比与动作重构双重约束，让 0.5 B 参数模型在多任务操控上超越 7 B 基线。实验表明，在 OXE 与 LIBERO 任务上显著提高数据效率与泛化性。 MoManipVLA 把固定底座 VLA 策略快速迁移到移动底座 双退火搜索选基座位姿＋SLSQP 精细优化 OVMM benchmark；Hexman Echo Plus + RM65 TBD 4×RTX 3090（作者建议） MoManipVLA 设计“双退火 + SLSQP”策略，把固定底座 VLA 生成的末端轨迹快速迁移到移动底座，实现零样本导航-操控一体。方法不改动原 VLA，只在推理阶段搜索最优基座位姿并微调手臂解算器。仿真与 Hexman + RM65 实验展示跨房间场景的 70 %→89 % 成功率提升。 CoT-VLA 为 VLA 引入显式visual chain-of-thought时序规划 先自回归预测未来视觉帧，再输出短动作序列 PyBullet 6DoF 手臂模拟 TBD 8×A100（7 B 模型预训练） CoT-VLA 将“视觉链式思考”引入 VLA：模型先自回归预测未来图像帧，作为显式视觉目标，再生成短动作序列。这样的分两步推理显著提升复杂操作的时间规划能力，并在 RT-X benchmark 刷新成绩。其 7 B 版本在无需额外模态标签下超越同规模基线。 SOLAMI 3D 虚拟角色“看-说-动”社交交互 合成 SynMSI 多模态对话 + 双塔解耦发声/动作 Oculus Quest 3 VR 前端；2×H800 推理 GitHub 32×V100 预训，16×V100 指令微调 SOLAMI 是首个端到端“社交” VLA 框架，可同时生成语音与动作驱动 3D 角色与人沉浸式互动。作者合成 6.3 K 多轮多模态对话数据 SynMSI 并预训练双塔模型，显著提升动作-语音一致性。用户 VR 研究验证，其角色在情感贴合度上优于 GPT-4o + 动画基线。 PVM Revisit 系统评测 PVM 预训练策略在机器人任务中的效果 提出 SlotMIM：在非对象中心数据上也保留 object-centric 表征 Franka Kitchen / Meta-World / Habitat GitHub 8×A100 训练 论文系统比较 DINO、iBOT、MAE 等预训练方法对机器人任务影响，发现“对象中心”特征是关键。为解决非对象中心数据劣化，提出 SlotMIM：语义瓶颈 + 跨视图一致约束诱导对象显式槽位。在 Franka Kitchen、Habitat 等 8 项任务全面超越现有 PVM。 Think Small, Act Big (PPL) 终身学习中复用“动作基元”避免遗忘 Primitive Prompt+Motion-Aware Query与 Diffusion Transformer MimicGen + LIBERO (sim)；Franka Panda TBD 4×A100（预训），1×A40（增量学习） Primitive Prompt Learning 首先在多技能库中学习可重用“动作基元”提示，再在终身阶段冻结旧提示、增量插入新提示，缓解灾难遗忘。模型利用光流-文本查询选择最相关基元，引导条件扩散 Transformer 产出动作。仿真与 Franka 实测均较 PEFT + 经验回放提升 20 % +。 Phoenix 失败后自我反思并细粒度修正动作 LLM 生成“语义-动作”双反思 → Diffusion Policy 校正 RoboMimic 仿真 + UR5 实机 GitHub 单张 RTX 4070 即可微调 Phoenix 通过 MLLM 生成“语义反思+动作反思”文本，先粗调运动指令，再用条件扩散策略做高频微调，实现失败后的自我纠正。框架把泛化压力转移到 LLM 层，低阶策略仅需少量新数据迭代。RoboMimic \u0026 UR5 显示跨任务鲁棒性提升 25 %。 OmniManip 物体中心交互基元 + VLM 推理，实现开箱即用操控 定义 canonical 空间 + 方向约束；零样本泛化 Franka Panda + RealSense GitHub TBD 4×A6000 训练 OmniManip 引入 “Object-Centric Interaction Primitive” 作为中间空间约束，让 VLM 只需输出 3D 关键点与方向即可驱动精密操控。通过零样本 IK + 轨迹优化，在 Franka 上完成 12 项复杂任务，平均成功率 78 %。方法避免昂贵 robot-Finetune，兼具成本与泛化优势。 Domain Discrepancy Mitigation 减小人演示与机器人视觉差 Vision Adapter 对齐嵌入；双塔 CLIP 训练 RLBench；xArm 7 GitHub 4×A6000 论文揭示人演示视频与机器人视觉存在形态与尺度差异，导致直接迁移失效。作者在视觉编码器尾部插入可学习适配器，并用跨域对比损失同时拉近语义与几何分布。RLBench 与 xArm7 实验显示在 unseen-object 设定下成功率翻倍。 Object-Centric Prompt-Driven (CrayonRobo) 用彩线标注关键方向减少冗余 四色线标 + GPT 选择最相关 3 条 SAPIEN + Franka GitHub \u003e40 GB VRAM 研究用四色线条在图像中显式标注接触点与位姿方向，减少语言与视觉冗余。GPT 自动从 32 候选线中挑 3 条最相关提示，送入 CLIP-LLM 预测 SE(3) 接触位姿后由 IK 执行。模拟与 Franka 真实机结果优于端到端 VLM 8–12 pp。 Robotic Visual Instruction 用箭头/圆圈视觉语言代替冗余文本 手工 15 K 图像标注 RoVI 语言；VLM 直接生成代码 UR5 / XArm6 + SAPIEN GitHub TBD 1×A40 RoVI 提出箭头、圆圈、颜色、数字四元素的视觉指令语言，解决纯语音交互空间精度不足与嘈杂场景受限问题。作者手工标注 15 K 图像并用 VLM 生成任务代码，机器人可直接按图索骥。UR5 /X-Arm6 实验展示在无语音环境中完成多步装配。 RoboGround 结合语言先验做抓取/推动等 统一 Vision-Language-Action Prompt SAPIEN；Franka Panda GitHub 8×A100（训练） RoboGround 以 grounding-mask 作为视觉中介，将对象定位与策略网络分离；同时自动合成大规模仿真数据扩展训练域。结果在 see/unseen 物体抓取、推拉任务上均超越 End-to-End Diffusion Policy。 二、Policy / Diffusion 控制 论文 目标 创新 平台 Code 算力 总结 KStar Diffuser 双臂协作轨迹生成 物理关节动态图 + 可微前向运动学 Bimanual Franka (sim \u0026 real) TBD 8×A100 KStar Diffuser 用机器人双臂物理拓扑构造动态时空图并融入可微正运动学，引导扩散过程生成协调关节动作。此设计在 bimanual 夹取与对接任务上比 baseline Policy Diffusion 提升 17 %。引入 kinematic loss 使收敛速度加快 35 %。 RoboPEPP 视觉-关节姿态预训练 时序预测任务 + 姿态编码器 Isaac Gym 7DoF GitHub 4×A6000 RoboPEPP 把“遮盖-预测”自监督移植到机器人图像，强迫编码器重建被 Mask 关节嵌入，从而学到物理结构感知。微调后在多数据集姿态估计误差降低 30 %，对遮挡鲁棒性最强且推理耗时最低。 Lift3D Policy 将 2D 基础模型迁移到 3D 抓取 深度 Lift 模块 + Domain Adapt - GitHub 8×A100 Lift3D 先用任务感知 MAE 为 2D 基础模型注入深度重建能力，再通过坐标映射“抬升”到点云编码，构建显式 3D 表征。方法在 ManiSkill 等 3D 任务大幅超过纯 2D 预训和显式 3D CNN 基线。 PDFactor 三视角扩散场统一多任务策略 Tri-Perspective 视图条件扩散 Meta-World GitHub 8×A6000 PDFactor 以“鸟瞰-第一视角-自由视角”三视图为条件，学习统一 Policy Diffusion Field，同步处理抓取、转动、插配多任务。多视角条件显著提高迁移，CVPR 实验成功率相较单视角提升 22 %。 Two by Two 跨任务配对装配 Pairwise Object Assembly + Transformer SAPIEN GitHub 4×A100 作者发布含 517 物体对、18 类日常装配任务的大型 2BY2 数据集，并提出等变 SE(3) 两步姿态估计方法。新方法在所有装配任务上刷新 SOTA，并在真实机器人验证兼容性。 FlowRAM Region-Aware Mamba + Flow Matching 局部注意 + OT 监督 RLBench TBD 4×A40 FlowRAM 结合 Region-Aware Mamba 感知器与 Flow-Matching 生成器，统一视觉编码与动作扩散，提高采样效率。测试显示对 occlusion 与 multi-object 情况鲁棒性显著提升。 G3Flow 3D 语义流生成多场景抓放 TSDF + Diffusion Flow Habitat GitHub 8×A100 G3Flow 用 TSDF 构建稠密场景，再以生成式 3D 语义流预测目标-手序列，实现跨场景抓-放一体。Pose-aware 设计让训练迭代减少 40 %，零样本任务成功率提升 18 %。 DexHandDiff 自适应灵巧手规划 接触感知 diff + 物理约束 Shadow Hand TBD 8×A100 DexHandDiff 引入手-物接触显式编码与能量约束到扩散规划中，避免“漂浮抓取”幽灵态。框架在 Shadow Hand 抓转与重定位七项基准刷新记录。 Tra-MoE 多域轨迹预测条件策略 动态专家 gate + BC 初始化 ManiSkill2 TBD 4×RTX 3090 Tra-MoE 通过稀疏门控专家网络吸收多域外数据，预测任意目标轨迹，并以可学习 2D mask 对齐视觉观测指导策略。实验证明在 DomainGap20 % 情况下仍维持高成功率。 AffordDP 利用可迁移 affordance 的泛化策略 Affordance Mask + Diffusion Policy LIBERO TBD 4×A6000 AffordDP 把 3D 接触点 + 轨迹定义为可迁移 affordance，并在扩散采样中注入 6D 变换引导，支持 unseen 类别操控。与原 Diffusion Policy 比，新架构在真实实验 unseen 物体上提升 30 % 成功率。 三、Grasp 论文 目标 创新 平台 Code 算力 总结 UniGraspTransformer 通用抓取蒸馏 Transformer-Distil 模型简化训练 Shadow Hand (sim) GitHub 4×A40 提出通用 Transformer 抓取网络及简化蒸馏流程，既减低训练成本，又扩展到海量手型。实验证实缩短 40 % 训练时间同时保持高成功率。 DexGrasp Anything 面向任意物体的物理感知抓取 物理引擎约束 Embedding Franka + Mujoco GitHub 8×A100 DexGrasp Anything 在训练和采样阶段显式加入物理约束，并发布 3.4 M 抓姿-15 K 物体数据集。方法在多 benchmark 抓取精度全线领先。 ZeroGrasp 零样本形状重建驱动抓取 SDF ↔ Partial Depth BP Isaac Gym GitHub 4×A6000 ZeroGrasp 联合实时 3D 重建与抓姿预测，实现 zero-shot 抓取；采用 SDF 先验避免碰撞。近实时推理 (\u003c 50 ms) 在模拟和 Franka 验证效果稳定。 四、Humanoid 论文 目标 创新 平台 Code 算力 总结 Humanoid Hiking 复杂崎岖地形步行 多技能 Curriculum + Terrain Adapt Unitree H1 TBD 8×A100 LEGO-H 框架通过层次 Transformer 预测未来局部目标，并以特权学习将视觉导航与步态控制整合，使模拟 humanoid 能走崎岖山路。跨多机器人形态验证了迁移与鲁棒性。 MobileH2R 合成数据学手递物 Unity-based Synthetic Human→Robot 数据 Clearpath Ridgeback + UR5 GitHub 4×A40 MobileH2R 用 Unity 合成高质量人-机器人交接数据，使移动底座在大空间内可靠收物。Sim-to-Real 迁移到 Ridgeback + UR5，达到 90 % 交接成功率。 五、3D Vision \u0026 Perception 论文 目标 创新 平台 Code 算力 总结 3D-MVP 用多视图 MAE 预训 3D 表征 分离视图编码 + Objaverse 预训 RVT + ManiSkill TBD 8×A100 3D-MVP 把 MAE 扩展到多视图 point-cloud，利用 RVT 对齐 voxel-level 特征；在多任务上全面提升 3D 理解与操控性能。 VidBot 2D 野外视频 → 零样本 3D 轨迹 SfM + 粗-细阶段扩散 Hexman Echo Plus TBD 4×A6000 VidBot 从网络 2D 视频自动恢复手轨迹与目标点，再通过粗-细扩散生成 3D 交互轨迹，实现零样本部署；在 13 项家庭任务上显著超越 SOTA。 Touch2Shape 触觉条件 3D 形状扩散 Vision-Touch 融合扩散 GelSight + iCub 手 TBD 4×A100 Touch2Shape 将 GelSight 触觉嵌入 3D Diffusion，结合 RL 引导探索，实现高保真局部细节重建；在触觉+视觉形状重建基准取得最佳分数。 六、Planning \u0026 Reasoning 论文 目标 创新 平台 Code 算力 总结 RoboBrain 从抽象到具体统一规划 层级 Brain 模型 + 递归搜索 SAPIEN (多机器人) GitHub 8×A100 RoboBrain 融合机器人与通用多模态数据，多阶段训练 MLLM，能处理长视频 + 高分辨率图像并输出操控计划，在多任务刷新成绩。 PhysVLM 让 VLM 知道“够不着” S-P Reachability Map 注入视觉编码 PyBullet + UR3/XArm6 GitHub 8×A800 × 48h PhysVLM 预先计算 Reachability Map 并注入 SigLip + Qwen 语义空间，LLM 可回答“到不了哪里”并辅助 VoxPoser 规划；零样本实机验证成功率最高。 RoboSpatial 训练 VLM 空间推理三视角 1 M 图 + 5 K 扫描数据集 Kinova Jaco + cuRobo GitHub 8×H100 × 20–40h ROBOSPATIAL 构造百万图像 + 5 K 3D 扫描数据集，以三视角问答方式训练 VLM 空间理解；Kinova 抓放实验超越 GPT-4o。 Tartan IMU 轻量惯导定位 F-model Transformer-Tiny + IMU 重加权 Tartan Drive Hugging Face TBD 单 RTX 2080 提出轻量级 Transformer 模型仅用 IMU 即可做定位，统一导航与姿态估计；在 TartanDrive 数据集上达成 SOTA。 Code-as-Monitor 视觉编程自动生成约束检测 LLM-to-Code + 生命周期触发 Amazon AWS 码实验室 TBD CPU-only CaM 将开集故障判定统一为约束求解问题，用 VLM 生成 Python 代码实时监控。系统在三模拟器+真实场景把成功率提高 28.7 %。 七、Video \u0026 Representation 论文 目标 创新 平台 Code 算力 总结 TASTE-Rob 任务导向手-物交互视频生成 三阶段：DynamiCrafter → MDM → SD-Adapter SAPIEN (sim) GitHub 8×A100 三阶段视频生成管线（DynamiCrafter→MDM→SD-Adapter）让语言-场景-手姿一致，可直接用作模仿学习数据；SAPIEN 验证动作可复现。 GraphMimic 视频 → Graph-to-Graph 生成策略 Skeleton-Graph VAE RoboMimic TBD 4×A6000 GraphMimic 把视频抽象为时序场景-动作图，并训练 Graph-to-Graphs 生成器预训练策略网络，在少量下游数据时显著提升。 八、Sim2Real \u0026 机器人模型 论文 目标 创新 平台 Code 算力 总结 Prof. Robot 无自碰 \u0026 无静态穿模可微渲染 带 Signed Distance 渲染器 Blender + PyTorch3D GitHub 1×A40 在可微渲染中加入 Eikonal 正则化学习碰撞分类器，实现无静态 / 自碰的梯度优化动作；对比 Dr.Robot 成本相当但碰撞率降低一半。 AutoURDF 无监督点云→URDF 模型 Cluster Registration + IK 估计 RealSense 点云 GitHub 1×RTX 3090 AutoURDF 通过点云聚类配准，无监督推断关节拓扑与参数，自动生成符合主流模拟器的 URDF；在多机器人扫描数据集精度领先。 九、基准与数据集 论文 目标 创新 规模 Code / 数据 算力 总结 RoboTwin 双臂数字孪生基准 生成式 Digital Twin 105 场景 × 10 K 轨迹 GitHub 数据预处理需 8×CPU RoboTwin 使用 3D 生成与 LLM 产出多样专家示范，建立双臂数字孪生评测；提供 105 场景、10 K 轨迹，填补双臂基准空白。 Pixel-aligned RGB-NIR Stereo 近红外+RGB 对齐立体数据 同轴 NIR Stereo 标定 50 K 对 TBD 无 提出同轴 RGB-NIR 立体系统并发布多光照数据集，示范两种融合方法显著改进暗光下深度与语义性能。 RoboSense 拥挤非结构环境 egocentric 感知 3D 相机 + 激光混合注释 150 K 帧，900 min 视频 GitHub 无 RoboSense 搭建 360° 相机-LiDAR 采集平台，发布 133 K 帧、1.4 M 3D box 的拥挤非结构环境数据，并定义近场匹配指标，覆盖 KITTI 270× 标注量。 ",
  "wordCount" : "1081",
  "inLanguage": "en",
  "datePublished": "2025-06-13T13:50:37+08:00",
  "dateModified": "2025-06-13T13:50:37+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/cvpr2025-robotics-summary/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal CVPR2025-Summary
    </h1>
    <div class="post-meta"><span title='2025-06-13 13:50:37 +0800 +0800'>June 13, 2025</span>&nbsp;·&nbsp;6 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h2 id="一vision-language-action-vla-模型">一、Vision-Language-Action (VLA) 模型<a hidden class="anchor" aria-hidden="true" href="#一vision-language-action-vla-模型">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>平台</th>
          <th>Code</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2501.10105">UniAct</a></strong></td>
          <td>将 28 种机器人异构动作映射到统一“通用动作空间”，提升跨形态迁移</td>
          <td>动作离散化＋对比约束学习，0.5 B 参数模型优于 7 B 基线</td>
          <td>Open-X Embodiment、Libero、Droid；64×A100 训练</td>
          <td><a href="https://github.com/2toinf/UniAct">GitHub</a></td>
          <td>64 × A100，10 天训练</td>
          <td>UniAct 针对不同机器人动作空间异构的问题，提出“Universal Action Space”，把 28 种平台的演示映射到一组离散原子行为，使跨形态学习成为可能。该框架通过语义对比与动作重构双重约束，让 0.5 B 参数模型在多任务操控上超越 7 B 基线。实验表明，在 OXE 与 LIBERO 任务上显著提高数据效率与泛化性。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.13446">MoManipVLA</a></strong></td>
          <td>把固定底座 VLA 策略快速迁移到移动底座</td>
          <td>双退火搜索选基座位姿＋SLSQP 精细优化</td>
          <td>OVMM benchmark；Hexman Echo Plus + RM65</td>
          <td>TBD</td>
          <td>4×RTX 3090（作者建议）</td>
          <td>MoManipVLA 设计“双退火 + SLSQP”策略，把固定底座 VLA 生成的末端轨迹快速迁移到移动底座，实现零样本导航-操控一体。方法不改动原 VLA，只在推理阶段搜索最优基座位姿并微调手臂解算器。仿真与 Hexman + RM65 实验展示跨房间场景的 70 %→89 % 成功率提升。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.22020">CoT-VLA</a></strong></td>
          <td>为 VLA 引入显式<em>visual chain-of-thought</em>时序规划</td>
          <td>先自回归预测未来视觉帧，再输出短动作序列</td>
          <td>PyBullet 6DoF 手臂模拟</td>
          <td>TBD</td>
          <td>8×A100（7 B 模型预训练）</td>
          <td>CoT-VLA 将“视觉链式思考”引入 VLA：模型先自回归预测未来图像帧，作为显式视觉目标，再生成短动作序列。这样的分两步推理显著提升复杂操作的时间规划能力，并在 RT-X benchmark 刷新成绩。其 7 B 版本在无需额外模态标签下超越同规模基线。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2412.00174">SOLAMI</a></strong></td>
          <td>3D 虚拟角色“看-说-动”社交交互</td>
          <td>合成 SynMSI 多模态对话 + 双塔解耦发声/动作</td>
          <td>Oculus Quest 3 VR 前端；2×H800 推理</td>
          <td><a href="https://github.com/AlanJiang98/SOLAMI">GitHub</a></td>
          <td>32×V100 预训，16×V100 指令微调</td>
          <td>SOLAMI 是首个端到端“社交” VLA 框架，可同时生成语音与动作驱动 3D 角色与人沉浸式互动。作者合成 6.3 K 多轮多模态对话数据 SynMSI 并预训练双塔模型，显著提升动作-语音一致性。用户 VR 研究验证，其角色在情感贴合度上优于 GPT-4o + 动画基线。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.06960">PVM Revisit</a></strong></td>
          <td>系统评测 PVM 预训练策略在机器人任务中的效果</td>
          <td>提出 SlotMIM：在非对象中心数据上也保留 object-centric 表征</td>
          <td>Franka Kitchen / Meta-World / Habitat</td>
          <td><a href="https://github.com/CVMI-Lab/SlotMIM">GitHub</a></td>
          <td>8×A100 训练</td>
          <td>论文系统比较 DINO、iBOT、MAE 等预训练方法对机器人任务影响，发现“对象中心”特征是关键。为解决非对象中心数据劣化，提出 SlotMIM：语义瓶颈 + 跨视图一致约束诱导对象显式槽位。在 Franka Kitchen、Habitat 等 8 项任务全面超越现有 PVM。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2504.00420">Think Small, Act Big (PPL)</a></strong></td>
          <td>终身学习中复用“动作基元”避免遗忘</td>
          <td><em>Primitive Prompt</em>+<em>Motion-Aware Query</em>与 Diffusion Transformer</td>
          <td>MimicGen + LIBERO (sim)；Franka Panda</td>
          <td>TBD</td>
          <td>4×A100（预训），1×A40（增量学习）</td>
          <td>Primitive Prompt Learning 首先在多技能库中学习可重用“动作基元”提示，再在终身阶段冻结旧提示、增量插入新提示，缓解灾难遗忘。模型利用光流-文本查询选择最相关基元，引导条件扩散 Transformer 产出动作。仿真与 Franka 实测均较 PEFT + 经验回放提升 20 % +。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2504.14588">Phoenix</a></strong></td>
          <td>失败后自我反思并细粒度修正动作</td>
          <td>LLM 生成“语义-动作”双反思 → Diffusion Policy 校正</td>
          <td>RoboMimic 仿真 + UR5 实机</td>
          <td><a href="https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework">GitHub</a></td>
          <td>单张 RTX 4070 即可微调</td>
          <td>Phoenix 通过 MLLM 生成“语义反思+动作反思”文本，先粗调运动指令，再用条件扩散策略做高频微调，实现失败后的自我纠正。框架把泛化压力转移到 LLM 层，低阶策略仅需少量新数据迭代。RoboMimic &amp; UR5 显示跨任务鲁棒性提升 25 %。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2501.03841">OmniManip</a></strong></td>
          <td>物体中心交互基元 + VLM 推理，实现开箱即用操控</td>
          <td>定义 canonical 空间 + 方向约束；零样本泛化</td>
          <td>Franka Panda + RealSense</td>
          <td><a href="https://github.com/pmj110119/OmniManip">GitHub</a> TBD</td>
          <td>4×A6000 训练</td>
          <td>OmniManip 引入 “Object-Centric Interaction Primitive” 作为中间空间约束，让 VLM 只需输出 3D 关键点与方向即可驱动精密操控。通过零样本 IK + 轨迹优化，在 Franka 上完成 12 项复杂任务，平均成功率 78 %。方法避免昂贵 robot-Finetune，兼具成本与泛化优势。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2406.14235">Domain Discrepancy Mitigation</a></strong></td>
          <td>减小人演示与机器人视觉差</td>
          <td>Vision Adapter 对齐嵌入；双塔 CLIP 训练</td>
          <td>RLBench；xArm 7</td>
          <td><a href="https://github.com/jiaming-zhou/HumanRobotAlign">GitHub</a></td>
          <td>4×A6000</td>
          <td>论文揭示人演示视频与机器人视觉存在形态与尺度差异，导致直接迁移失效。作者在视觉编码器尾部插入可学习适配器，并用跨域对比损失同时拉近语义与几何分布。RLBench 与 xArm7 实验显示在 unseen-object 设定下成功率翻倍。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2505.02166">Object-Centric Prompt-Driven (CrayonRobo)</a></strong></td>
          <td>用彩线标注关键方向减少冗余</td>
          <td>四色线标 + GPT 选择最相关 3 条</td>
          <td>SAPIEN + Franka</td>
          <td><a href="https://github.com/clorislili/CrayonRobo">GitHub</a></td>
          <td>&gt;40 GB VRAM</td>
          <td>研究用四色线条在图像中显式标注接触点与位姿方向，减少语言与视觉冗余。GPT 自动从 32 候选线中挑 3 条最相关提示，送入 CLIP-LLM 预测 SE(3) 接触位姿后由 IK 执行。模拟与 Franka 真实机结果优于端到端 VLM 8–12 pp。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2505.00693">Robotic Visual Instruction</a></strong></td>
          <td>用箭头/圆圈视觉语言代替冗余文本</td>
          <td>手工 15 K 图像标注 RoVI 语言；VLM 直接生成代码</td>
          <td>UR5 / XArm6 + SAPIEN</td>
          <td><a href="https://robotic-visual-instruction.github.io/#">GitHub</a> TBD</td>
          <td>1×A40</td>
          <td>RoVI 提出箭头、圆圈、颜色、数字四元素的视觉指令语言，解决纯语音交互空间精度不足与嘈杂场景受限问题。作者手工标注 15 K 图像并用 VLM 生成任务代码，机器人可直接按图索骥。UR5 /X-Arm6 实验展示在无语音环境中完成多步装配。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2504.21530">RoboGround</a></strong></td>
          <td>结合语言先验做抓取/推动等</td>
          <td>统一 Vision-Language-Action Prompt</td>
          <td>SAPIEN；Franka Panda</td>
          <td><a href="https://github.com/ZzZZCHS/RoboGround">GitHub</a></td>
          <td>8×A100（训练）</td>
          <td>RoboGround 以 grounding-mask 作为视觉中介，将对象定位与策略网络分离；同时自动合成大规模仿真数据扩展训练域。结果在 see/unseen 物体抓取、推拉任务上均超越 End-to-End Diffusion Policy。</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="二policy--diffusion-控制">二、Policy / Diffusion 控制<a hidden class="anchor" aria-hidden="true" href="#二policy--diffusion-控制">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>平台</th>
          <th>Code</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.10743">KStar Diffuser</a></strong></td>
          <td>双臂协作轨迹生成</td>
          <td>物理关节动态图 + 可微前向运动学</td>
          <td>Bimanual Franka (sim &amp; real)</td>
          <td>TBD</td>
          <td>8×A100</td>
          <td>KStar Diffuser 用机器人双臂物理拓扑构造动态时空图并融入可微正运动学，引导扩散过程生成协调关节动作。此设计在 bimanual 夹取与对接任务上比 baseline Policy Diffusion 提升 17 %。引入 kinematic loss 使收敛速度加快 35 %。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2411.17662">RoboPEPP</a></strong></td>
          <td>视觉-关节姿态预训练</td>
          <td>时序预测任务 + 姿态编码器</td>
          <td>Isaac Gym 7DoF</td>
          <td><a href="https://github.com/raktimgg/RoboPEPP">GitHub</a></td>
          <td>4×A6000</td>
          <td>RoboPEPP 把“遮盖-预测”自监督移植到机器人图像，强迫编码器重建被 Mask 关节嵌入，从而学到物理结构感知。微调后在多数据集姿态估计误差降低 30 %，对遮挡鲁棒性最强且推理耗时最低。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2411.18623">Lift3D Policy</a></strong></td>
          <td>将 2D 基础模型迁移到 3D 抓取</td>
          <td>深度 Lift 模块 + Domain Adapt</td>
          <td>-</td>
          <td><a href="https://github.com/PKU-HMI-Lab/LIFT3D">GitHub</a></td>
          <td>8×A100</td>
          <td>Lift3D 先用任务感知 MAE 为 2D 基础模型注入深度重建能力，再通过坐标映射“抬升”到点云编码，构建显式 3D 表征。方法在 ManiSkill 等 3D 任务大幅超过纯 2D 预训和显式 3D CNN 基线。</td>
      </tr>
      <tr>
          <td><strong><a href="https://cvpr.thecvf.com/virtual/2025/poster/33943">PDFactor</a></strong></td>
          <td>三视角扩散场统一多任务策略</td>
          <td>Tri-Perspective 视图条件扩散</td>
          <td>Meta-World</td>
          <td><a href="">GitHub</a></td>
          <td>8×A6000</td>
          <td>PDFactor 以“鸟瞰-第一视角-自由视角”三视图为条件，学习统一 Policy Diffusion Field，同步处理抓取、转动、插配多任务。多视角条件显著提高迁移，CVPR 实验成功率相较单视角提升 22 %。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2504.06961">Two by Two</a></strong></td>
          <td>跨任务配对装配</td>
          <td>Pairwise Object Assembly + Transformer</td>
          <td>SAPIEN</td>
          <td><a href="https://github.com/TEA-Lab/TwoByTWo">GitHub</a></td>
          <td>4×A100</td>
          <td>作者发布含 517 物体对、18 类日常装配任务的大型 2BY2 数据集，并提出等变 SE(3) 两步姿态估计方法。新方法在所有装配任务上刷新 SOTA，并在真实机器人验证兼容性。</td>
      </tr>
      <tr>
          <td><strong><a href="https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_FlowRAM_Grounding_Flow_CVPR_2025_supplemental.pdf">FlowRAM</a></strong></td>
          <td>Region-Aware Mamba + Flow Matching</td>
          <td>局部注意 + OT 监督</td>
          <td>RLBench</td>
          <td>TBD</td>
          <td>4×A40</td>
          <td>FlowRAM 结合 Region-Aware Mamba 感知器与 Flow-Matching 生成器，统一视觉编码与动作扩散，提高采样效率。测试显示对 occlusion 与 multi-object 情况鲁棒性显著提升。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2411.18369">G3Flow</a></strong></td>
          <td>3D 语义流生成多场景抓放</td>
          <td>TSDF + Diffusion Flow</td>
          <td>Habitat</td>
          <td><a href="https://github.com/TianxingChen/G3Flow">GitHub</a></td>
          <td>8×A100</td>
          <td>G3Flow 用 TSDF 构建稠密场景，再以生成式 3D 语义流预测目标-手序列，实现跨场景抓-放一体。Pose-aware 设计让训练迭代减少 40 %，零样本任务成功率提升 18 %。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2411.18562">DexHandDiff</a></strong></td>
          <td>自适应灵巧手规划</td>
          <td>接触感知 diff + 物理约束</td>
          <td>Shadow Hand</td>
          <td>TBD</td>
          <td>8×A100</td>
          <td>DexHandDiff 引入手-物接触显式编码与能量约束到扩散规划中，避免“漂浮抓取”幽灵态。框架在 Shadow Hand 抓转与重定位七项基准刷新记录。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2411.14519">Tra-MoE</a></strong></td>
          <td>多域轨迹预测条件策略</td>
          <td>动态专家 gate + BC 初始化</td>
          <td>ManiSkill2</td>
          <td>TBD</td>
          <td>4×RTX 3090</td>
          <td>Tra-MoE 通过稀疏门控专家网络吸收多域外数据，预测任意目标轨迹，并以可学习 2D mask 对齐视觉观测指导策略。实验证明在 DomainGap20 % 情况下仍维持高成功率。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2412.03142">AffordDP</a></strong></td>
          <td>利用可迁移 affordance 的泛化策略</td>
          <td>Affordance Mask + Diffusion Policy</td>
          <td>LIBERO</td>
          <td>TBD</td>
          <td>4×A6000</td>
          <td>AffordDP 把 3D 接触点 + 轨迹定义为可迁移 affordance，并在扩散采样中注入 6D 变换引导，支持 unseen 类别操控。与原 Diffusion Policy 比，新架构在真实实验 unseen 物体上提升 30 % 成功率。</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="三grasp">三、Grasp<a hidden class="anchor" aria-hidden="true" href="#三grasp">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>平台</th>
          <th>Code</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2412.02699">UniGraspTransformer</a></strong></td>
          <td>通用抓取蒸馏</td>
          <td>Transformer-Distil 模型简化训练</td>
          <td>Shadow Hand (sim)</td>
          <td><a href="https://github.com/microsoft/UniGraspTransformer">GitHub</a></td>
          <td>4×A40</td>
          <td>提出通用 Transformer 抓取网络及简化蒸馏流程，既减低训练成本，又扩展到海量手型。实验证实缩短 40 % 训练时间同时保持高成功率。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.08257">DexGrasp Anything</a></strong></td>
          <td>面向任意物体的物理感知抓取</td>
          <td>物理引擎约束 Embedding</td>
          <td>Franka + Mujoco</td>
          <td><a href="https://github.com/4DVLab/DexGrasp-Anything">GitHub</a></td>
          <td>8×A100</td>
          <td>DexGrasp Anything 在训练和采样阶段显式加入物理约束，并发布 3.4 M 抓姿-15 K 物体数据集。方法在多 benchmark 抓取精度全线领先。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2504.10857">ZeroGrasp</a></strong></td>
          <td>零样本形状重建驱动抓取</td>
          <td>SDF ↔ Partial Depth BP</td>
          <td>Isaac Gym</td>
          <td><a href="https://github.com/sh8/ZeroGrasp">GitHub</a></td>
          <td>4×A6000</td>
          <td>ZeroGrasp 联合实时 3D 重建与抓姿预测，实现 zero-shot 抓取；采用 SDF 先验避免碰撞。近实时推理 (&lt; 50 ms) 在模拟和 Franka 验证效果稳定。</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="四humanoid">四、Humanoid<a hidden class="anchor" aria-hidden="true" href="#四humanoid">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>平台</th>
          <th>Code</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2505.06218">Humanoid Hiking</a></strong></td>
          <td>复杂崎岖地形步行</td>
          <td>多技能 Curriculum + Terrain Adapt</td>
          <td>Unitree H1</td>
          <td>TBD</td>
          <td>8×A100</td>
          <td>LEGO-H 框架通过层次 Transformer 预测未来局部目标，并以特权学习将视觉导航与步态控制整合，使模拟 humanoid 能走崎岖山路。跨多机器人形态验证了迁移与鲁棒性。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2501.04595">MobileH2R</a></strong></td>
          <td>合成数据学手递物</td>
          <td>Unity-based Synthetic Human→Robot 数据</td>
          <td>Clearpath Ridgeback + UR5</td>
          <td><a href="https://github.com/chenjy2003/genh2r">GitHub</a></td>
          <td>4×A40</td>
          <td>MobileH2R 用 Unity 合成高质量人-机器人交接数据，使移动底座在大空间内可靠收物。Sim-to-Real 迁移到 Ridgeback + UR5，达到 90 % 交接成功率。</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="五3d-vision--perception">五、3D Vision &amp; Perception<a hidden class="anchor" aria-hidden="true" href="#五3d-vision--perception">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>平台</th>
          <th>Code</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2406.18158">3D-MVP</a></strong></td>
          <td>用多视图 MAE 预训 3D 表征</td>
          <td>分离视图编码 + Objaverse 预训</td>
          <td>RVT + ManiSkill</td>
          <td>TBD</td>
          <td>8×A100</td>
          <td>3D-MVP 把 MAE 扩展到多视图 point-cloud，利用 RVT 对齐 voxel-level 特征；在多任务上全面提升 3D 理解与操控性能。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.07135">VidBot</a></strong></td>
          <td>2D 野外视频 → 零样本 3D 轨迹</td>
          <td>SfM + 粗-细阶段扩散</td>
          <td>Hexman Echo Plus</td>
          <td>TBD</td>
          <td>4×A6000</td>
          <td>VidBot 从网络 2D 视频自动恢复手轨迹与目标点，再通过粗-细扩散生成 3D 交互轨迹，实现零样本部署；在 13 项家庭任务上显著超越 SOTA。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2505.13091">Touch2Shape</a></strong></td>
          <td>触觉条件 3D 形状扩散</td>
          <td>Vision-Touch 融合扩散</td>
          <td>GelSight + iCub 手</td>
          <td>TBD</td>
          <td>4×A100</td>
          <td>Touch2Shape 将 GelSight 触觉嵌入 3D Diffusion，结合 RL 引导探索，实现高保真局部细节重建；在触觉+视觉形状重建基准取得最佳分数。</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="六planning--reasoning">六、Planning &amp; Reasoning<a hidden class="anchor" aria-hidden="true" href="#六planning--reasoning">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>平台</th>
          <th>Code</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2502.21257">RoboBrain</a></strong></td>
          <td>从抽象到具体统一规划</td>
          <td>层级 Brain 模型 + 递归搜索</td>
          <td>SAPIEN (多机器人)</td>
          <td><a href="https://github.com/FlagOpen/RoboBrain">GitHub</a></td>
          <td>8×A100</td>
          <td>RoboBrain 融合机器人与通用多模态数据，多阶段训练 MLLM，能处理长视频 + 高分辨率图像并输出操控计划，在多任务刷新成绩。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.08481">PhysVLM</a></strong></td>
          <td>让 VLM 知道“够不着”</td>
          <td>S-P Reachability Map 注入视觉编码</td>
          <td>PyBullet + UR3/XArm6</td>
          <td><a href="https://github.com/unira-zwj/PhysVLM">GitHub</a></td>
          <td>8×A800 × 48h</td>
          <td>PhysVLM 预先计算 Reachability Map 并注入 SigLip + Qwen 语义空间，LLM 可回答“到不了哪里”并辅助 VoxPoser 规划；零样本实机验证成功率最高。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2411.16537">RoboSpatial</a></strong></td>
          <td>训练 VLM 空间推理三视角</td>
          <td>1 M 图 + 5 K 扫描数据集</td>
          <td>Kinova Jaco + cuRobo</td>
          <td><a href="https://github.com/NVlabs/RoboSpatial">GitHub</a></td>
          <td>8×H100 × 20–40h</td>
          <td>ROBOSPATIAL 构造百万图像 + 5 K 3D 扫描数据集，以三视角问答方式训练 VLM 空间理解；Kinova 抓放实验超越 GPT-4o。</td>
      </tr>
      <tr>
          <td><strong><a href="https://cvpr.thecvf.com/virtual/2025/poster/33873">Tartan IMU</a></strong></td>
          <td>轻量惯导定位 F-model</td>
          <td>Transformer-Tiny + IMU 重加权</td>
          <td>Tartan Drive</td>
          <td><a href="https://huggingface.co/datasets/raphael-blanchard/TartanIMU/tree/main">Hugging Face</a> TBD</td>
          <td>单 RTX 2080</td>
          <td>提出轻量级 Transformer 模型仅用 IMU 即可做定位，统一导航与姿态估计；在 TartanDrive 数据集上达成 SOTA。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2412.04455">Code-as-Monitor</a></strong></td>
          <td>视觉编程自动生成约束检测</td>
          <td>LLM-to-Code + 生命周期触发</td>
          <td>Amazon AWS 码实验室</td>
          <td>TBD</td>
          <td>CPU-only</td>
          <td>CaM 将开集故障判定统一为约束求解问题，用 VLM 生成 Python 代码实时监控。系统在三模拟器+真实场景把成功率提高 28.7 %。</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="七video--representation">七、Video &amp; Representation<a hidden class="anchor" aria-hidden="true" href="#七video--representation">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>平台</th>
          <th>Code</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.11423">TASTE-Rob</a></strong></td>
          <td>任务导向手-物交互视频生成</td>
          <td>三阶段：DynamiCrafter → MDM → SD-Adapter</td>
          <td>SAPIEN (sim)</td>
          <td><a href="https://github.com/GAP-LAB-CUHK-SZ/TASTE-Rob">GitHub</a></td>
          <td>8×A100</td>
          <td>三阶段视频生成管线（DynamiCrafter→MDM→SD-Adapter）让语言-场景-手姿一致，可直接用作模仿学习数据；SAPIEN 验证动作可复现。</td>
      </tr>
      <tr>
          <td><strong><a href="https://cvpr.thecvf.com/virtual/2025/poster/34942">GraphMimic</a></strong></td>
          <td>视频 → Graph-to-Graph 生成策略</td>
          <td>Skeleton-Graph VAE</td>
          <td>RoboMimic</td>
          <td>TBD</td>
          <td>4×A6000</td>
          <td>GraphMimic 把视频抽象为时序场景-动作图，并训练 Graph-to-Graphs 生成器预训练策略网络，在少量下游数据时显著提升。</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="八sim2real--机器人模型">八、Sim2Real &amp; 机器人模型<a hidden class="anchor" aria-hidden="true" href="#八sim2real--机器人模型">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>平台</th>
          <th>Code</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2503.11269">Prof. Robot</a></strong></td>
          <td>无自碰 &amp; 无静态穿模可微渲染</td>
          <td>带 Signed Distance 渲染器</td>
          <td>Blender + PyTorch3D</td>
          <td><a href="https://github.com/qrcat/prof.robot/">GitHub</a></td>
          <td>1×A40</td>
          <td>在可微渲染中加入 Eikonal 正则化学习碰撞分类器，实现无静态 / 自碰的梯度优化动作；对比 Dr.Robot 成本相当但碰撞率降低一半。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2412.05507">AutoURDF</a></strong></td>
          <td>无监督点云→URDF 模型</td>
          <td>Cluster Registration + IK 估计</td>
          <td>RealSense 点云</td>
          <td><a href="https://github.com/jl6017/AutoURDF">GitHub</a></td>
          <td>1×RTX 3090</td>
          <td>AutoURDF 通过点云聚类配准，无监督推断关节拓扑与参数，自动生成符合主流模拟器的 URDF；在多机器人扫描数据集精度领先。</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="九基准与数据集">九、基准与数据集<a hidden class="anchor" aria-hidden="true" href="#九基准与数据集">#</a></h2>
<table>
  <thead>
      <tr>
          <th>论文</th>
          <th>目标</th>
          <th>创新</th>
          <th>规模</th>
          <th>Code / 数据</th>
          <th>算力</th>
          <th>总结</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2409.02920">RoboTwin</a></strong></td>
          <td>双臂数字孪生基准</td>
          <td>生成式 Digital Twin</td>
          <td>105 场景 × 10 K 轨迹</td>
          <td><a href="https://github.com/TianxingChen/RoboTwin">GitHub</a></td>
          <td>数据预处理需 8×CPU</td>
          <td>RoboTwin 使用 3D 生成与 LLM 产出多样专家示范，建立双臂数字孪生评测；提供 105 场景、10 K 轨迹，填补双臂基准空白。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2411.18025">Pixel-aligned RGB-NIR Stereo</a></strong></td>
          <td>近红外+RGB 对齐立体数据</td>
          <td>同轴 NIR Stereo 标定</td>
          <td>50 K 对</td>
          <td>TBD</td>
          <td>无</td>
          <td>提出同轴 RGB-NIR 立体系统并发布多光照数据集，示范两种融合方法显著改进暗光下深度与语义性能。</td>
      </tr>
      <tr>
          <td><strong><a href="https://alphaxiv.org/abs/2408.15503">RoboSense</a></strong></td>
          <td>拥挤非结构环境 egocentric 感知</td>
          <td>3D 相机 + 激光混合注释</td>
          <td>150 K 帧，900 min 视频</td>
          <td><a href="https://github.com/suhaisheng/RoboSense">GitHub</a></td>
          <td>无</td>
          <td>RoboSense 搭建 360° 相机-LiDAR 采集平台，发布 133 K 帧、1.4 M 3D box 的拥挤非结构环境数据，并定义近场匹配指标，覆盖 KITTI 270× 标注量。</td>
      </tr>
  </tbody>
</table>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
