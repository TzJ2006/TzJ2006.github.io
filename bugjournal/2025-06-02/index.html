<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-06-02 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, CLIP, BLIP, SigLIP, ResNet, DenseNet, AlexNet">
<meta name="description" content="Review of Three base VLA models and three basic CNNs.">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-06-02/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-06-02/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-06-02/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-06-02">
  <meta property="og:description" content="Review of Three base VLA models and three basic CNNs.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-02T11:24:12+08:00">
    <meta property="article:modified_time" content="2025-06-02T11:24:12+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-06-02">
<meta name="twitter:description" content="Review of Three base VLA models and three basic CNNs.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-06-02",
      "item": "https://tzj2006.github.io/bugjournal/2025-06-02/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-06-02",
  "name": "Bug Journal 2025-06-02",
  "description": "Review of Three base VLA models and three basic CNNs.",
  "keywords": [
    "Bug Journal", "CLIP", "BLIP", "SigLIP", "ResNet", "DenseNet", "AlexNet"
  ],
  "articleBody": "CLIP 这是 CLIP 的结构，主要是两个部分，一个是 text encoder, 用于得到 text embedding, 一个是 image encoder, 用于得到 image encodding.\nText encoder 是一个 text transformer, Image encoder 是一个 ResNet50 / ViT.\n输入数据是一张图片和它的 alternative text.\n训练的逻辑也不难理解：\n现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text, 现在我对这 N 个 pair 做两两配对。\n如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近 如果不属于同一个 pair, 那么我希望他们的 embedding 更远\n同时，我希望这个embedding的距离是有意义的，越相近的离得越近，越不同的离得越远。\n这时候我们就可以用 Cosine Similarity Loss 来比较两个 embedding 之间的距离。\nThat’s it.\n这里有个 assumption, 就是说，虽然我的数据质量不怎么样，但是我有很好的数量。\n对于每一个patch, 我有整整 32k 个图文 pair, 加起来一共 1B 个 True/False pair, 那我一定是可以学到一些东西的。\n但是这里也就是它的局限所在：32k 个图文 pair 要放到非常大量的 GPU 中才能 work, 这时设备之间的通信就成为了最大的效率瓶颈。\n但是虽然说这个模型 train 起来非常复杂，但是其实这个模型不算太大，单 GPU 就足以 inference.\n代码也非常简洁，一个简单的实现如下:\nimport clip import torch from PIL import Image device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model, preprocess = clip.load(\"ViT-B/32\", device=device) image = preprocess(Image.open(\"test.png\")).unsqueeze(0).to(device) # CLIP.png为本文中图一，即CLIP的流程图 text = clip.tokenize([\"car\", \"apple\", \"leaf\"]).to(device) # 将这三句话向量化 with torch.no_grad(): logits_per_image, logits_per_text = model(image, text) probs = logits_per_image.softmax(dim=-1).cpu().numpy() print(\"Label probs:\", probs) BLIP 这是 BLIP，大部分和之前一样：\n现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text, 现在我对这 N 个 pair 做两两配对。\n如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近 如果不属于同一个 pair, 那么我希望他们的 embedding 更远\n但是，现在我增加了一个部分：除了原本的 Contrastive learning 之外，我还要做一个图片和文字之间的 cross-attention.\n另外，原本的数据里有很多噪音。\n现在我已经初步 train 好一个图文匹配模型了。\n那我们默认在这个模型中图文匹配比较好的，就是数据中“高质量”的部分。\n这时我们再引入一个图生文模型，让模型自己学习这些“高质量”数据，然后覆盖“低质量数据”。\n这样就可以提高数据的整体质量。\nQ-Former 是轻量、任务相关、可控制的视觉语义提取器。\nSigLIP AlexNet AlexNet就是最开始的 CNN 网络\nResNet ResNet引入残差的概念，不再让 CNN 学习原始表示，而是让 CNN 学习不同层之间的差 同时，有些层的结果可以越过中间某些层直接去往更深的层。 这让更深的网络成为了可能。\nDenseNet DenseNet 则是再进一步，DenseNet 会让每层之间形成两两连接，使得网络效果更好。\nTake Away 为什么要用 Cosine Similarity:\n可大可小，既可以拉进，又可以推远；重要的是大小是有意义的，越大代表越不相近，越小代表越相近。\n单塔模型和双塔模型：\n单塔模型就是一个输入的模型；双塔输出就是有两个输入的模型。 或者说，如果一个 embedding 只过一遍模型，那就是双塔模型。 如果一个 embedding 可能要过多遍一个模型，那就是单塔模型。\n",
  "wordCount" : "210",
  "inLanguage": "en",
  "datePublished": "2025-06-02T11:24:12+08:00",
  "dateModified": "2025-06-02T11:24:12+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-06-02/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-06-02
    </h1>
    <div class="post-meta"><span title='2025-06-02 11:24:12 +0800 +0800'>June 2, 2025</span>&nbsp;·&nbsp;1 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h3 id="clip">CLIP<a hidden class="anchor" aria-hidden="true" href="#clip">#</a></h3>
<p><img alt="1748860373179" loading="lazy" src="https://tzj2006.github.io/images/2025-06-02/1748860373179.png"></p>
<p>这是 CLIP 的结构，主要是两个部分，一个是 text encoder, 用于得到 text embedding, 一个是 image encoder, 用于得到 image encodding.</p>
<p>Text encoder 是一个 text transformer, Image encoder 是一个 ResNet50 / ViT.</p>
<p>输入数据是一张图片和它的 alternative text.</p>
<p>训练的逻辑也不难理解：</p>
<p>现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text,
现在我对这 N 个 pair 做两两配对。</p>
<p>如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近
如果不属于同一个 pair, 那么我希望他们的 embedding 更远</p>
<p>同时，我希望这个embedding的距离是有意义的，越相近的离得越近，越不同的离得越远。</p>
<p>这时候我们就可以用 Cosine Similarity Loss 来比较两个 embedding 之间的距离。</p>
<p>That&rsquo;s it.</p>
<p>这里有个 assumption, 就是说，虽然我的数据质量不怎么样，但是我有很好的数量。</p>
<p>对于每一个patch, 我有整整 32k 个图文 pair, 加起来一共 1B 个 True/False pair,
那我一定是可以学到一些东西的。</p>
<p>但是这里也就是它的局限所在：32k 个图文 pair 要放到非常大量的 GPU 中才能 work,
这时设备之间的通信就成为了最大的效率瓶颈。</p>
<p>但是虽然说这个模型 train 起来非常复杂，但是其实这个模型不算太大，单 GPU 就足以 inference.</p>
<p>代码也非常简洁，一个简单的实现如下:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl"><span class="n">import</span> <span class="n">clip</span>
</span></span><span class="line"><span class="cl"><span class="n">import</span> <span class="n">torch</span>
</span></span><span class="line"><span class="cl"><span class="n">from</span> <span class="n">PIL</span> <span class="n">import</span> <span class="ne">Image</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="n">clip</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;ViT-B/32&#34;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="ne">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&#34;test.png&#34;</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># CLIP.png为本文中图一，即CLIP的流程图</span>
</span></span><span class="line"><span class="cl"><span class="n">text</span> <span class="o">=</span> <span class="n">clip</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&#34;car&#34;</span><span class="p">,</span> <span class="s2">&#34;apple&#34;</span><span class="p">,</span> <span class="s2">&#34;leaf&#34;</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># 将这三句话向量化</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits_per_image</span><span class="p">,</span> <span class="n">logits_per_text</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">probs</span> <span class="o">=</span> <span class="n">logits_per_image</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Label probs:&#34;</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="blip">BLIP<a hidden class="anchor" aria-hidden="true" href="#blip">#</a></h3>
<p><img alt="1748860350648" loading="lazy" src="https://tzj2006.github.io/images/2025-06-02/1748860350648.png"></p>
<p>这是 BLIP，大部分和之前一样：</p>
<p>现在有一个 patch, 里面包含了 N 张图片和 N 个 alternative text,
现在我对这 N 个 pair 做两两配对。</p>
<p>如果他们属于同一个 pair, 那么我希望他们的 embedding 更接近
如果不属于同一个 pair, 那么我希望他们的 embedding 更远</p>
<p>但是，现在我增加了一个部分：除了原本的 Contrastive learning 之外，我还要做一个图片和文字之间的 cross-attention.</p>
<p>另外，原本的数据里有很多噪音。</p>
<p>现在我已经初步 train 好一个图文匹配模型了。</p>
<p>那我们默认在这个模型中图文匹配比较好的，就是数据中“高质量”的部分。</p>
<p>这时我们再引入一个图生文模型，让模型自己学习这些“高质量”数据，然后覆盖“低质量数据”。</p>
<p>这样就可以提高数据的整体质量。</p>
<p>Q-Former 是轻量、任务相关、可控制的视觉语义提取器。</p>
<h3 id="siglip">SigLIP<a hidden class="anchor" aria-hidden="true" href="#siglip">#</a></h3>
<h3 id="alexnet">AlexNet<a hidden class="anchor" aria-hidden="true" href="#alexnet">#</a></h3>
<p>AlexNet就是最开始的 CNN 网络</p>
<h3 id="resnet">ResNet<a hidden class="anchor" aria-hidden="true" href="#resnet">#</a></h3>
<p>ResNet引入残差的概念，不再让 CNN 学习原始表示，而是让 CNN 学习不同层之间的差
同时，有些层的结果可以越过中间某些层直接去往更深的层。
这让更深的网络成为了可能。</p>
<h3 id="densenet">DenseNet<a hidden class="anchor" aria-hidden="true" href="#densenet">#</a></h3>
<p>DenseNet 则是再进一步，DenseNet 会让每层之间形成两两连接，使得网络效果更好。</p>
<h3 id="take-away">Take Away<a hidden class="anchor" aria-hidden="true" href="#take-away">#</a></h3>
<p>为什么要用 Cosine Similarity:</p>
<p>可大可小，既可以拉进，又可以推远；重要的是大小是有意义的，越大代表越不相近，越小代表越相近。</p>
<p>单塔模型和双塔模型：</p>
<p>单塔模型就是一个输入的模型；双塔输出就是有两个输入的模型。
或者说，如果一个 embedding 只过一遍模型，那就是双塔模型。
如果一个 embedding 可能要过多遍一个模型，那就是单塔模型。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
