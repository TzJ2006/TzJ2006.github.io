<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-06-03 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, Paper Review">
<meta name="description" content="Catch It! Learning to Catch in Flight with Mobile Dexterous Hands
发表时间: 16 Sep 2024
动机
Basically, 这是之前那篇 DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。
模型流程图

这是真机器人的样子：
一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。

训练的过程分为两步：
第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。
第二步是微调灵巧手让手抓住这个物体。
最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。
解决的难点

部署到真机器人上
一步到位 end-to-end 效果没那么好
抓不住从未见过的物体

还需要解决的难点

从未见过的物体还是不好抓
仍然没有考虑材质之类的问题
还是无法在当物体在空中时就判断物体的形状

Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own
发布时间: 4 Oct. 2023">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-06-03/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-06-03/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-06-03/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-06-03">
  <meta property="og:description" content="Catch It! Learning to Catch in Flight with Mobile Dexterous Hands 发表时间: 16 Sep 2024
动机 Basically, 这是之前那篇 DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。
模型流程图 这是真机器人的样子： 一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。
训练的过程分为两步：
第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。 第二步是微调灵巧手让手抓住这个物体。
最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。
解决的难点 部署到真机器人上 一步到位 end-to-end 效果没那么好 抓不住从未见过的物体 还需要解决的难点 从未见过的物体还是不好抓 仍然没有考虑材质之类的问题 还是无法在当物体在空中时就判断物体的形状 Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own 发布时间: 4 Oct. 2023">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-03T11:50:32+08:00">
    <meta property="article:modified_time" content="2025-06-03T11:50:32+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-06-03">
<meta name="twitter:description" content="Catch It! Learning to Catch in Flight with Mobile Dexterous Hands
发表时间: 16 Sep 2024
动机
Basically, 这是之前那篇 DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。
模型流程图

这是真机器人的样子：
一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。

训练的过程分为两步：
第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。
第二步是微调灵巧手让手抓住这个物体。
最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。
解决的难点

部署到真机器人上
一步到位 end-to-end 效果没那么好
抓不住从未见过的物体

还需要解决的难点

从未见过的物体还是不好抓
仍然没有考虑材质之类的问题
还是无法在当物体在空中时就判断物体的形状

Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own
发布时间: 4 Oct. 2023">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-06-03",
      "item": "https://tzj2006.github.io/bugjournal/2025-06-03/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-06-03",
  "name": "Bug Journal 2025-06-03",
  "description": "Catch It! Learning to Catch in Flight with Mobile Dexterous Hands 发表时间: 16 Sep 2024\n动机 Basically, 这是之前那篇 DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。\n模型流程图 这是真机器人的样子： 一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。\n训练的过程分为两步：\n第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。 第二步是微调灵巧手让手抓住这个物体。\n最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。\n解决的难点 部署到真机器人上 一步到位 end-to-end 效果没那么好 抓不住从未见过的物体 还需要解决的难点 从未见过的物体还是不好抓 仍然没有考虑材质之类的问题 还是无法在当物体在空中时就判断物体的形状 Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own 发布时间: 4 Oct. 2023\n",
  "keywords": [
    "Bug Journal", "Paper Review"
  ],
  "articleBody": "Catch It! Learning to Catch in Flight with Mobile Dexterous Hands 发表时间: 16 Sep 2024\n动机 Basically, 这是之前那篇 DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。\n模型流程图 这是真机器人的样子： 一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。\n训练的过程分为两步：\n第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。 第二步是微调灵巧手让手抓住这个物体。\n最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。\n解决的难点 部署到真机器人上 一步到位 end-to-end 效果没那么好 抓不住从未见过的物体 还需要解决的难点 从未见过的物体还是不好抓 仍然没有考虑材质之类的问题 还是无法在当物体在空中时就判断物体的形状 Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own 发布时间: 4 Oct. 2023\n动机 现在机器人训练要 $10^6$ 级别的数据，这要的时间太长了。 反观人类，人类不需要这么多数据。 这可能是因为人类在训练之前就知道什么能做什么不能做。\n那么，我们能不能输入一个 policy 给机器人让它也知道什么能做什么不能做呢？\n主要论点 训练方式：Reinforcement Learning\n并非模仿学习\n作者提出了一个新框架：\nRLFP（Reinforcement Learning with Foundation Priors），其中融合三种先验： 策略先验（Policy Prior）：告诉 agent “该怎么做”。 价值先验（Value Prior）：评估当前状态“是否更接近成功”。 成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 他们在此框架下构建了具体算法 FAC（Foundation-guided Actor-Critic），把三类先验引入到 Actor-Critic 的学习流程中，并在真实机器人与模拟任务中验证了该方法的有效性。\n模型流程图 策略先验（Policy Prior）：告诉 agent “该怎么做”。 这会生成一个策略分布。 这个策略分布会作为一个 KL 正则项。 希望 Actor 生成的策略和这个策略分布不会差太远\n价值先验（Value Prior）：评估当前状态“是否更接近成功”。 这会作为 Reward 的一部分。 告诉 Robot 它是不是更接近成功了。\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 这也会作为 Reward 的一部分。 告诉 Rotbot 它是不是成功了。\n实现细节 在现实中： 策略先验（Policy Prior）：告诉 agent “该怎么做”。 本文使用了 GPT-4V 来实现这个功能：\n对于每一个单独的任务，需要重新写一个这样的 prompt, 但是模板都是一样的。\n模板如下：\nYou are a helpful robot programming assistant. Here is an image of the current environment, which includes: \u003cTask Instruction\u003e Your job: \u003cTask Instruction\u003e You may only use the following primitive skills: \u003cPrimitive Skills List\u003e \u003cImage Input\u003e Please write a Python code to solve this task, however, you can only write code in this format: \u003cCode Format\u003e e.g. (根据文章推测的 example prompt):\nYou are a helpful robot programming assistant. Here is an image of the current environment, which includes: - A plastic bottle with a green cap (the bottle is fixed to the table) - A pink plate nearby Your job: Help a robot arm **unscrew the bottle cap** and **place it on the pink plate**. You cannot lift the bottle. You must rotate the cap **anticlockwise** to unscrew it. You may only use the following primitive skills: # Primitive Skills: # 1. move_to x y z —— move the gripper to position (x, y, z) # 2. grasp —— close the gripper to grasp # 3. release —— open the gripper # 4. rotate_anticlockwise —— rotate the gripper anticlockwise (90°) # 5. rotate_clockwise —— rotate the gripper clockwise (90°) # 6. reset —— move back to the home position \u003cinput image\u003e Please write a Python function `code_policy()` that returns a plan list using the above skills. Be sure to: - Estimate the coordinates from the image (roughly) - Include comments to explain each step - Output only the code block and nothing else Your format should be like this: def code_policy(): plans = [ 'move_to 0.5 0.0 0.26', 'grasp', 'rotate_anticlockwise', 'move_to 0.75 0.0 0.06', 'release' ] return plans Now write the code: 价值先验（Value Prior）：评估当前状态“是否更接近成功”。 使用了一个 Pretrain LLM 来 “判断好不好” VIP: Universal Visual Reward and Representation via Value-Implicit Pretraining\nVIP 是一个使用大规模离线机器人/视频数据集，目标是 通过一个 image 得到一个方程 $V(O_{t_i})$, 越大表示越成功。\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 在现实中使用 GPT-4V 来判断这个任务是否完成。\n这有浇花的时候没判断成功 (3 success in all 4 tasks)。\n浇花和狡猾脚滑谐音，做不对是正常的\n在虚拟环境中 策略先验（Policy Prior）：告诉 agent “该怎么做”。 为了证明 Policy 不需要固定的形式\n使用了 “a diffusion-based policy prior, following the UniPi [25] pipeline”\n先用扩散模型生成一个完成任务的视频，再通过一个逆动力学模型把视频帧之间的状态变化转化为动作。\n为了效率起见，使用了 开源视频扩散模型 Seer [26] 预生成视频，然后离线训练（distill）出一个策略模型（policy network）\n然而，因为模拟环境图像质量比现实差，所以生成的视频效果也不好。\n所以用了 10个视频 fine-tune 了一下。\n价值先验（Value Prior）：评估当前状态“是否更接近成功”。 Same set up.\n成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。 这里有 Ground Truth 了，就不用 GPT-4V 了 但是，为了模拟现实中 GPT-4V 的情况，加入了一些噪声。 加入方法如下： 训练一个模型，从状态 + label 预测是否成功 这个模型不是 100% 准确。\n结果 现实世界一个小时后： 模拟世界:\nAblation study\nTake away run on 3090 GPU\nGeneral Flow as Foundation Affordance for Scalable Robot Learning 发表时间: 21 Jan 2024\n动机 一言以蔽之：机器人如何从感知（图像）中知道：“我该操作哪儿”和“怎么操作”？\n当前机器人操作学习普遍依赖：\n大量手工收集的数据； 手动定义的 affordance； 复杂的模仿学习或强化学习流程 而现在的数据：\n泛化能力差 没有统一，可以拓展的，自动的，包含语义的 embedding 主要论点 General Flow（GF） —— 一种结构化、密集的视觉场表示，表征“像素应如何在操作中流动”。\n这个和 NVIDIA DLSS 中的 OPTICAL FLOW 很类似，只不过加入了语义信息。\n模型流程图 ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors 发表时间: 30 Apr 2025\n动机 机器人操作策略泛化能力差, 能不能用 mask 的办法让机器人操作策略的泛化能力更强。\n主要论点 构建一个自动化数据生成流水线，合成高复杂度、多样化语言指令的数据集（112K 指令，24K 演示）;\n利用 GLaMM 模型和 SAM 架构生成目标对象与放置区域的精细分割 mask; 将这些 masks 融合进策略网络\n模型流程图 Part 1 数据集 现在的数据集不够好，我要弄一个新的数据集。 我有一个虚拟环境，这个环境里面有一些物体。\n那我能不能写一个脚本来自动设计一个数据集。\n为了给测试用的模型增加难度，我要在环境中添加一些相似的物体。\n那就可以在图片中找一些相似的物体出来，最好是有一项特征(如颜色)完全一致\n这时候 GPT 可以帮忙\nGPT 对这些物体有一定理解 (3视图，材质，颜色)\nGPT 的这些理解也可以加入进来帮我挑选要放入那些物体。\n有了环境信息还不够，我还要一个 Language Instruction.\n首先，我可以根据位置信息自动生成一些 rule-based Instruction: 比如把 A 移动到 B的右边…\n那如果要一些更 abstract 的 Instruction, 那我可以用 GPT 生成一个 Language Instruction.\n比如说 水果 -\u003e making jam.\nPart 2 New method 左边的部分和那天的 SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation 很像，都是通过一个 LLM + SAM 获取起始点和终点的信息。\n而右边的部分就是通过 强调起始点和终点 的 attention 来增强起点和终点权重。\n更详细地：通过增加了两个 Query, 分别只和起点和终点做 attention 来增强。\n最后通过一个 transformer decoder 输出离散的 action。\n训练是通过模仿学习，最小化和样本之间的差距。\n效果 计算复杂度 8 * 4090 GPU Approx. 5 days.\n",
  "wordCount" : "677",
  "inLanguage": "en",
  "datePublished": "2025-06-03T11:50:32+08:00",
  "dateModified": "2025-06-03T11:50:32+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-06-03/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-06-03
    </h1>
    <div class="post-meta"><span title='2025-06-03 11:50:32 +0800 +0800'>June 3, 2025</span>&nbsp;·&nbsp;4 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h2 id="catch-it-learning-to-catch-in-flight-with-mobile-dexterous-hands">Catch It! Learning to Catch in Flight with Mobile Dexterous Hands<a hidden class="anchor" aria-hidden="true" href="#catch-it-learning-to-catch-in-flight-with-mobile-dexterous-hands">#</a></h2>
<p><a href="https://arxiv.org/abs/2409.10319">发表时间: 16 Sep 2024</a></p>
<h4 id="动机">动机<a hidden class="anchor" aria-hidden="true" href="#动机">#</a></h4>
<p>Basically, 这是之前那篇 <em><a href="https://arxiv.org/abs/2310.08809">DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands</a></em> 的 follow up. 讲述了 Shanghai Qi Zhi Institute 和 Shanghai AI Lab 的人如何把上一篇文章的工作部署到真机器人上。</p>
<h4 id="模型流程图">模型流程图<a hidden class="anchor" aria-hidden="true" href="#模型流程图">#</a></h4>
<p><img alt="1748922875088" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748922875088.png"></p>
<p>这是真机器人的样子：
一个双目摄像头，一个 6 DoF 的机械臂，一个 16 DoF 的灵巧手，还有一个可以移动的平台。</p>
<p><img alt="1748922862160" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748922862160.png"></p>
<p>训练的过程分为两步：</p>
<p>第一步是不管灵巧手，先尝试移动整个机器人让物体砸到灵巧手上。
第二步是微调灵巧手让手抓住这个物体。</p>
<p>最后在控制上加了一个速度优化器解决了无法抓到从未见过的物体的问题。</p>
<h4 id="解决的难点">解决的难点<a hidden class="anchor" aria-hidden="true" href="#解决的难点">#</a></h4>
<ol>
<li>部署到真机器人上</li>
<li>一步到位 end-to-end 效果没那么好</li>
<li>抓不住从未见过的物体</li>
</ol>
<h4 id="还需要解决的难点">还需要解决的难点<a hidden class="anchor" aria-hidden="true" href="#还需要解决的难点">#</a></h4>
<ol>
<li>从未见过的物体还是不好抓</li>
<li>仍然没有考虑材质之类的问题</li>
<li>还是无法在当物体在空中时就判断物体的形状</li>
</ol>
<h2 id="reinforcement-learning-with-foundation-priors-let-the-embodied-agent-efficiently-learn-on-its-own">Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning-with-foundation-priors-let-the-embodied-agent-efficiently-learn-on-its-own">#</a></h2>
<p><a href="https://arxiv.org/abs/2310.02635">发布时间: 4 Oct. 2023</a></p>
<h4 id="动机-1">动机<a hidden class="anchor" aria-hidden="true" href="#动机-1">#</a></h4>
<p>现在机器人训练要 $10^6$ 级别的数据，这要的时间太长了。
反观人类，人类不需要这么多数据。
这可能是因为人类在训练之前就<strong>知道什么能做什么不能做</strong>。</p>
<p>那么，我们能不能输入一个 policy 给机器人让它也知道什么能做什么不能做呢？</p>
<h4 id="主要论点">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点">#</a></h4>
<p>训练方式：Reinforcement Learning</p>
<p><strong>并非模仿学习</strong></p>
<p>作者提出了一个新框架：</p>
<ul>
<li>RLFP（Reinforcement Learning with Foundation Priors），其中融合三种先验：</li>
<li>策略先验（Policy Prior）：告诉 agent “该怎么做”。</li>
<li>价值先验（Value Prior）：评估当前状态“是否更接近成功”。</li>
<li>成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。</li>
</ul>
<p>他们在此框架下构建了具体算法 FAC（Foundation-guided Actor-Critic），把三类先验引入到 Actor-Critic 的学习流程中，并在真实机器人与模拟任务中验证了该方法的有效性。</p>
<h4 id="模型流程图-1">模型流程图<a hidden class="anchor" aria-hidden="true" href="#模型流程图-1">#</a></h4>
<p><img alt="1748931143529" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748931143529.png"></p>
<ul>
<li>策略先验（Policy Prior）：告诉 agent “该怎么做”。</li>
</ul>
<p>这会生成一个策略分布。
这个策略分布会作为一个 KL 正则项。
希望 Actor 生成的策略和这个策略分布不会差太远</p>
<ul>
<li>价值先验（Value Prior）：评估当前状态“是否更接近成功”。</li>
</ul>
<p>这会作为 Reward 的一部分。
告诉 Robot 它是不是更接近成功了。</p>
<ul>
<li>成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。</li>
</ul>
<p>这也会作为 Reward 的一部分。
告诉 Rotbot 它是不是成功了。</p>
<h4 id="实现细节">实现细节<a hidden class="anchor" aria-hidden="true" href="#实现细节">#</a></h4>
<h5 id="在现实中">在现实中：<a hidden class="anchor" aria-hidden="true" href="#在现实中">#</a></h5>
<ul>
<li>策略先验（Policy Prior）：告诉 agent “该怎么做”。</li>
</ul>
<p>本文使用了 GPT-4V 来实现这个功能：</p>
<p>对于每一个单独的任务，需要重新写一个这样的 prompt, 但是模板都是一样的。</p>
<p>模板如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown"><span class="line"><span class="cl">You are a helpful robot programming assistant.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Here is an image of the current environment, which includes:
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">Task</span> <span class="na">Instruction</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Your job:
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">Task</span> <span class="na">Instruction</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">You may only use the following primitive skills:
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">Primitive</span> <span class="na">Skills</span> <span class="na">List</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">Image</span> <span class="na">Input</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Please write a Python code to solve this task, however, you can only write code in this format:
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">Code</span> <span class="na">Format</span><span class="p">&gt;</span>
</span></span></code></pre></div><p>e.g. (根据文章推测的 example prompt):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown"><span class="line"><span class="cl">You are a helpful robot programming assistant.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Here is an image of the current environment, which includes:
</span></span><span class="line"><span class="cl"><span class="k">-</span> A plastic bottle with a green cap (the bottle is fixed to the table)
</span></span><span class="line"><span class="cl"><span class="k">-</span> A pink plate nearby
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Your job: Help a robot arm <span class="gs">**unscrew the bottle cap**</span> and <span class="gs">**place it on the pink plate**</span>.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">You cannot lift the bottle. You must rotate the cap <span class="gs">**anticlockwise**</span> to unscrew it.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">You may only use the following primitive skills:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="gh"># Primitive Skills:
</span></span></span><span class="line"><span class="cl"><span class="gh"># 1. move_to x y z —— move the gripper to position (x, y, z)
</span></span></span><span class="line"><span class="cl"><span class="gh"># 2. grasp —— close the gripper to grasp
</span></span></span><span class="line"><span class="cl"><span class="gh"># 3. release —— open the gripper
</span></span></span><span class="line"><span class="cl"><span class="gh"># 4. rotate_anticlockwise —— rotate the gripper anticlockwise (90°)
</span></span></span><span class="line"><span class="cl"><span class="gh"># 5. rotate_clockwise —— rotate the gripper clockwise (90°)
</span></span></span><span class="line"><span class="cl"><span class="gh"># 6. reset —— move back to the home position
</span></span></span><span class="line"><span class="cl"><span class="gh"></span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">input</span> <span class="na">image</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Please write a Python function <span class="sb">`code_policy()`</span> that returns a plan list using the above skills. Be sure to:
</span></span><span class="line"><span class="cl"><span class="k">-</span> Estimate the coordinates from the image (roughly)
</span></span><span class="line"><span class="cl"><span class="k">-</span> Include comments to explain each step
</span></span><span class="line"><span class="cl"><span class="k">-</span> Output only the code block and nothing else
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Your format should be like this:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">def code_policy():
</span></span><span class="line"><span class="cl">    plans = [
</span></span><span class="line"><span class="cl">        &#39;move_to 0.5 0.0 0.26&#39;,
</span></span><span class="line"><span class="cl">        &#39;grasp&#39;,
</span></span><span class="line"><span class="cl">        &#39;rotate_anticlockwise&#39;,
</span></span><span class="line"><span class="cl">        &#39;move_to 0.75 0.0 0.06&#39;,
</span></span><span class="line"><span class="cl">        &#39;release&#39;
</span></span><span class="line"><span class="cl">    ]
</span></span><span class="line"><span class="cl">    return plans
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Now write the code:
</span></span></code></pre></div><ul>
<li>价值先验（Value Prior）：评估当前状态“是否更接近成功”。</li>
</ul>
<p>使用了一个 Pretrain LLM 来 “判断好不好”
VIP: Universal Visual Reward and Representation via Value-Implicit Pretraining</p>
<p>VIP 是一个使用大规模离线机器人/视频数据集，目标是 通过一个 image 得到一个方程 $V(O_{t_i})$, 越大表示越成功。</p>
<ul>
<li>成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。</li>
</ul>
<p>在现实中使用 GPT-4V 来判断这个任务是否完成。</p>
<p>这有浇花的时候没判断成功 (3 success in all 4 tasks)。</p>
<p><del>浇花和狡猾脚滑谐音，做不对是正常的</del></p>
<h5 id="在虚拟环境中">在虚拟环境中<a hidden class="anchor" aria-hidden="true" href="#在虚拟环境中">#</a></h5>
<ul>
<li>策略先验（Policy Prior）：告诉 agent “该怎么做”。</li>
</ul>
<p>为了证明 Policy 不需要固定的形式</p>
<p>使用了 &ldquo;a diffusion-based policy prior, following the UniPi [25] pipeline&rdquo;</p>
<p>先用扩散模型生成一个完成任务的视频，再通过一个逆动力学模型把视频帧之间的状态变化转化为动作。</p>
<p>为了效率起见，使用了 开源视频扩散模型 Seer [26] 预生成视频，然后离线训练（distill）出一个策略模型（policy network）</p>
<p>然而，因为模拟环境图像质量比现实差，所以生成的视频效果也不好。</p>
<p>所以用了 10个视频 fine-tune 了一下。</p>
<ul>
<li>价值先验（Value Prior）：评估当前状态“是否更接近成功”。</li>
</ul>
<p>Same set up.</p>
<ul>
<li>成功判定先验（Success-Reward Prior）：判断任务是否完成（0-1）。</li>
</ul>
<p>这里有 Ground Truth 了，就不用 GPT-4V 了
但是，为了模拟现实中 GPT-4V 的情况，加入了一些噪声。
加入方法如下：
训练一个模型，从状态 + label 预测是否成功
这个模型不是 100% 准确。</p>
<h4 id="结果">结果<a hidden class="anchor" aria-hidden="true" href="#结果">#</a></h4>
<p>现实世界一个小时后：
<img alt="1748933431046" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748933431046.png"></p>
<p>模拟世界:</p>
<p><img alt="1748934186964" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748934186964.png"></p>
<p>Ablation study</p>
<p><img alt="1748934226518" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748934226518.png"></p>
<h4 id="take-away">Take away<a hidden class="anchor" aria-hidden="true" href="#take-away">#</a></h4>
<p>run on 3090 GPU</p>
<h2 id="general-flow-as-foundation-affordance-for-scalable-robot-learning">General Flow as Foundation Affordance for Scalable Robot Learning<a hidden class="anchor" aria-hidden="true" href="#general-flow-as-foundation-affordance-for-scalable-robot-learning">#</a></h2>
<p><a href="https://arxiv.org/abs/2401.11439">发表时间: 21 Jan 2024</a></p>
<h4 id="动机-2">动机<a hidden class="anchor" aria-hidden="true" href="#动机-2">#</a></h4>
<p>一言以蔽之：机器人如何从感知（图像）中知道：“我该操作哪儿”和“怎么操作”？</p>
<p>当前机器人操作学习普遍依赖：</p>
<ul>
<li>大量手工收集的数据；</li>
<li>手动定义的 affordance；</li>
<li>复杂的模仿学习或强化学习流程</li>
</ul>
<p>而现在的数据：</p>
<ul>
<li>泛化能力差</li>
<li>没有统一，可以拓展的，自动的，包含语义的 embedding</li>
</ul>
<h4 id="主要论点-1">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-1">#</a></h4>
<p>General Flow（GF） —— 一种结构化、密集的视觉场表示，表征“像素应如何在操作中流动”。</p>
<p>这个和 NVIDIA DLSS 中的 OPTICAL FLOW 很类似，只不过加入了语义信息。</p>
<h4 id="模型流程图-2">模型流程图<a hidden class="anchor" aria-hidden="true" href="#模型流程图-2">#</a></h4>
<p><img alt="1748944822289" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748944822289.png"></p>
<h2 id="roboground-robotic-manipulation-with-grounded-vision-language-priors">ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors<a hidden class="anchor" aria-hidden="true" href="#roboground-robotic-manipulation-with-grounded-vision-language-priors">#</a></h2>
<p><a href="https://arxiv.org/abs/2504.21530">发表时间: 30 Apr 2025</a></p>
<h4 id="动机-3">动机<a hidden class="anchor" aria-hidden="true" href="#动机-3">#</a></h4>
<p>机器人操作策略泛化能力差, 能不能用 mask 的办法让机器人操作策略的泛化能力更强。</p>
<h4 id="主要论点-2">主要论点<a hidden class="anchor" aria-hidden="true" href="#主要论点-2">#</a></h4>
<p>构建一个自动化数据生成流水线，合成高复杂度、多样化语言指令的数据集（112K 指令，24K 演示）;</p>
<p>利用 GLaMM 模型和 SAM 架构生成目标对象与放置区域的精细分割 mask;
将这些 masks 融合进策略网络</p>
<h4 id="模型流程图-3">模型流程图<a hidden class="anchor" aria-hidden="true" href="#模型流程图-3">#</a></h4>
<h5 id="part-1-数据集">Part 1 数据集<a hidden class="anchor" aria-hidden="true" href="#part-1-数据集">#</a></h5>
<p>现在的数据集不够好，我要弄一个新的数据集。
<img alt="1748941633766" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748941633766.png"></p>
<p>我有一个虚拟环境，这个环境里面有一些物体。</p>
<p>那我能不能写一个脚本来自动设计一个数据集。</p>
<p>为了给测试用的模型增加难度，我要在环境中添加一些相似的物体。</p>
<p>那就可以在图片中找一些相似的物体出来，最好是有一项特征(如颜色)完全一致</p>
<p>这时候 GPT 可以帮忙</p>
<p>GPT 对这些物体有一定理解 (3视图，材质，颜色)</p>
<p>GPT 的这些理解也可以加入进来帮我挑选要放入那些物体。</p>
<p>有了环境信息还不够，我还要一个 Language Instruction.</p>
<p>首先，我可以根据位置信息自动生成一些 rule-based Instruction:
比如把 A 移动到 B的右边&hellip;</p>
<p>那如果要一些更 abstract 的 Instruction,
那我可以用 GPT 生成一个 Language Instruction.</p>
<p>比如说 水果 -&gt; making jam.</p>
<h5 id="part-2-new-method">Part 2 New method<a hidden class="anchor" aria-hidden="true" href="#part-2-new-method">#</a></h5>
<p><img alt="1748942193763" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748942193763.png"></p>
<p>左边的部分和那天的 <a href="https://tzj2006.github.io/bugjournal/2025-05-28/#skil-semantic-keypoint-imitation-learning-for-generalizable-data-efficient-manipulation">SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation</a> 很像，都是通过一个 LLM + SAM 获取起始点和终点的信息。</p>
<p>而右边的部分就是通过 <strong>强调起始点和终点</strong> 的 attention 来增强起点和终点权重。</p>
<p>更详细地：通过增加了两个 Query, 分别只和起点和终点做 attention 来增强。</p>
<p>最后通过一个 transformer decoder 输出离散的 action。</p>
<p>训练是通过模仿学习，最小化和样本之间的差距。</p>
<h4 id="效果">效果<a hidden class="anchor" aria-hidden="true" href="#效果">#</a></h4>
<p><img alt="1748942699425" loading="lazy" src="https://tzj2006.github.io/images/2025-06-03/1748942699425.png"></p>
<h4 id="计算复杂度">计算复杂度<a hidden class="anchor" aria-hidden="true" href="#计算复杂度">#</a></h4>
<p>8 * 4090 GPU Approx. 5 days.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
