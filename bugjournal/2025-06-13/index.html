<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bug Journal 2025-06-13 | TzJ&#39;s Net</title>
<meta name="keywords" content="Bug Journal, CVPR 2025, Robotics, Paper">
<meta name="description" content="Paper review of CVPR 2025">
<meta name="author" content="">
<link rel="canonical" href="https://tzj2006.github.io/bugjournal/2025-06-13/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="https://tzj2006.github.io/assets/css/stylesheet.af858c2feef42adc7846f815c3e21de9982d82f8fc4f65879451b2686859975a.css" integrity="sha256-r4WML&#43;70Ktx4RvgVw&#43;Id6Zgtgvj8T2WHlFGyaGhZl1o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tzj2006.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tzj2006.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tzj2006.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tzj2006.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://tzj2006.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tzj2006.github.io/bugjournal/2025-06-13/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script src="https://tzj2006.github.io/js/checkbox-state.min.481208bf28be32dd7419d90065130144ba9a464a94857de0dc07fd19d3f2f6f3.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://tzj2006.github.io/bugjournal/2025-06-13/">
  <meta property="og:site_name" content="TzJ&#39;s Net">
  <meta property="og:title" content="Bug Journal 2025-06-13">
  <meta property="og:description" content="Paper review of CVPR 2025">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="bugjournal">
    <meta property="article:published_time" content="2025-06-13T10:24:57+08:00">
    <meta property="article:modified_time" content="2025-06-13T10:24:57+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bug Journal 2025-06-13">
<meta name="twitter:description" content="Paper review of CVPR 2025">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "BugJournals",
      "item": "https://tzj2006.github.io/bugjournal/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bug Journal 2025-06-13",
      "item": "https://tzj2006.github.io/bugjournal/2025-06-13/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bug Journal 2025-06-13",
  "name": "Bug Journal 2025-06-13",
  "description": "Paper review of CVPR 2025",
  "keywords": [
    "Bug Journal", "CVPR 2025", "Robotics", "Paper"
  ],
  "articleBody": "PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability CVPR 2025\nFrom 北京交通大学 \u0026 广东技术师范大学\n目标： 告诉机器人什么位置它到不了\n动机： 有时候机器人不知道一个位置到不到得了，结果把自己搞坏了\n模型流程：\n首先离线计算什么位置是机械臂能达到的。 形成一个点云 (S-P Map)\n然后用 SigLip-400M 提取图像和点云的特征\n然后把这个 embedding 和文字的 embedding 混合之后\n通过 Qwen-2.5-Instruct-3B，生成一个回答给 VoxPoser 做。\n实验设计：\n仿真：基于 CUDA 11.x 的 PyBullet 环境，六种机器人（UR5、FR5、CR5、FRANKA、UR3、XArm6），共 1.3K 问答，零样本评测。\n实机：在 UR3、XArm6 真实平台上各 10 次零样本任务，评估任务成功率。\n评估指标：\nEQA-phys：基于 LLM 评分（5 分正确、1 分错误）计算平均分； RoboVQA-val / OpenEQA：标准 BLEU-4 与准确率； 任务规划：成功率。\n结果：\nS-P Map 在很多 LLM 上都有用\nPhysVLM-3B 效果平均最好\n数据集： Zero-shot\n算力要求：\n\u003c 48h * 8 * A800\n代码：\n开源\nObject-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation CVPR 2025\nFrom PKU Agibot Lab\n目标： 帮助机器人排除冗余信息干扰\n动机： 语言和视频中冗余信息过多\n模型流程：\n在图片上加一些标记 分别是：\n接触点（蓝色） 末端执行器在接触时的 z 轴方向（红色） y 轴方向（绿色） 接触后移动方向（黄色） 这些标记可能来自 Grounding-Dino + SAM 或者 ChatGPT-4o 或者 手动标记\n标记方式如下：\n均匀生成 N(32) 条线，让 GPT 选择 3 条标上颜色\n然后把这个“增强”过后的信息输入给 CLIP,然后再让 LLM 输出\"应该在哪里，以什么角度接触\"\n对于这个信息，我们可以和GT 做 train\n最后，我们得到了这个位置和姿态的信息，我们就可以用IK求解路径了。\n结果：\n数据集：\n模拟环境：SAPIEN + PartNet-Mobility •\t平台：使用 SAPIEN（一个支持刚体与关节物体的物理模拟器），其提供高保真动力学与渲染接口，适合零样本机器人操控研究 ￼ ￼。 •\t资产集：加载 PartNet-Mobility 中的 1,500 多个关节化 CAD 模型，涵盖抽屉、门、笔记本电脑盖等常见家电与家具 ￼。 •\t飞行夹持器（Flying Gripper）：采用 SAPIEN 中的“飞行”版 Franka Panda Gripper，使得末端执行器可在自由空间中无碰撞地移动，以便专注测试模型对提示的理解能力 ￼。 •\t摄像机随机化：在每个场景随机采样摄像机视角（水平±45°、俯仰30°–60°），并在单帧中记录 RGB 图像与 3D 末端执行器位姿，用于提示生成与训练 ￼。 •\t数据采集：共采集 ~10,000 条训练样本，过程耗时约 6–8 小时；测试集与训练集按照固定比例拆分，模拟了 Seen/Unseen 物体类别的泛化能力评估 ￼ ￼。\n现实机器人平台 •\t硬件平台：使用真实的 Franka Emika Panda 机械臂（7 自由度，集成高精度力矩传感器），配合标准的 RGB-D 摄像头（如 Intel RealSense D415）采集视觉输入 ￼。 •\t执行流程：将 CrayonRobo 在模拟环境中输出的 SE(3) 接触位姿与移动方向，通过 ROS + MoveIt! 的 IK 求解与笛卡尔轨迹规划一键下发真实机器人执行 ￼。 •\t测试任务：与模拟中一致，包括“拉抽屉”“开门”“掀笔记本盖”等单步原子操作，每个任务在 5–10 个不同初始姿态下重复试验。 •\t评价指标：以二值成功率衡量：当末端执行器按照预测方向将物体部件移动超过阈值（如 5 cm）即记为成功；平均成功率达到 74%（Seen）与 72%（Unseen） ￼。\n算力要求： 未知 建议 \u003e 40 GB VRAM\n代码： 开源\nCheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation CVPR 2025\nFrom PKU Agibot lab\n目标： 让机器人读取说明书之后根据说明书做出正确操作\n动机： 阅读说明书\n电子产品有时候视觉信息不够，很多有着相似外观的东西可能有着不同的功能。\n所以要读说明书\n模型流程：\nOCR → GPT-4o 提取图文信息 → GPT‑4o 生成操作步骤 → SoM、Grounding‑DINO、SAM 实现视觉对齐 → FoundationPose 匹配 CAD\n最后输出{部件，操作} pair, 最后使用 VoxPoser 进行操作。\n实验设置 模拟平台：SAPIEN 0.8 + Franka Panda 飞行夹具 ￼\n数据集： PartNet-Mobility CAD 模型； CheckManual 合成说明书（已公开，可下载使用） ￼\n评估指标： 任务完成率\n现实验证： Franka + RealSense 摄像头，完成单个用例的实物测试\n算力要求： 未知 建议 \u003e 40 GB VRAM\nCode availability: 开源\n结果：\n总之有 manual 效果更好\nTASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation CVPR 2025\nFrom Xiaoguang Han’s Lab at 港中深\n目标： 优化对齐视频-人手数据集\n动机： 现在有这些问题：\n视角不一致 动作语义无法对齐 手部姿态稳定性不高 这个模型想要解决这些问题 模型流程：\n数据集构建（Sec. 3）： •\t100,856 条 1–8 秒单动作视频，静态 1080p 视角，语义指令一一对应。 粗视频生成（Stage I – Coarse Action Planner）： •\t基于 DynamiCrafter（512 × 512 分辨率），语言指令＋静态环境图→粗交互视频； •\t微调参数：batch=16, lr=5×10⁻⁵, 30K steps。 姿态优化（Stage II – MDM Refinement）： •\t使用 Motion Diffusion Model 以 60 帧手部关键点序列为条件，保证握姿一致性； •\t训练设置：batch=64, lr=1×10⁻⁴, 500K steps; 推理 10-step DDIM denoise。 最终生成（Stage III – Frame-wise Adapter）： •\t将优化后手部姿态图像与环境图和语言指令一起接入 Stable Diffusion 2 Adapter，生成最终视频； •\t训练设置：batch=32, lr=5×10⁻⁵, 30K steps。 实验设置 •\t仿真验证：在 SAPIEN 模拟平台上，用 DynamiCrafter＋MDM + Adapter 生成视频，并通过 Figure 17 示范在虚拟机器人上的模仿效果。\n模型流程：\n第一阶段：Coarse Action Planner（粗动作生成） •\t目标：在给定语言指令（如“拿起杯子并倒入水”）和静态环境图（512×512 分辨率）条件下，生成一段粗略的、合理的手–物体交互视频序列（通常 16 帧）。 •\t模型架构：基于 Video Diffusion Model（VDM），如 DynamiCrafter，使用文本–图像条件扩散网络。 •\t训练细节： •\tBatch size = 16，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t推理时使用 50-step DDIM 采样，平衡生成质量与速度。 •\t输出：一段低分辨率、含大致手部运动轨迹和物体交互的粗视频，用于后续姿态细化。\n⸻\n第二阶段：Motion Diffusion Model (MDM) Refinement（姿态细化） •\t目标：针对第一阶段生成的粗视频，从中提取手部的 3D 关键点序列（60 帧），并利用扩散模型细化运动轨迹，提升抓握姿态的连贯性与自然度。 •\t模型架构：1D 时序扩散网络，对帧间手部关键点做去噪与重建。 •\t训练细节： •\tBatch size = 64，学习率 1×10⁻⁴； •\t训练步数 500K steps； •\t推理时使用 10-step DDIM，快速得到精细关键点序列。 •\t输出：一组平滑、符合物理约束的 3D 手部关键点轨迹，用以指导下一阶段的视频生成。\n⸻\n第三阶段：Frame-wise Adapter（帧级最终生成） •\t目标：将姿态细化后的关键点与原环境图、语言指令结合，生成最终高质量、手部姿态稳定的交互视频。 •\t模型架构：基于 Stable Diffusion 2，通过 Adapter 模块将手部姿态（以可视化关节点或姿态图形式）作为条件，连同环境图与文本，一起输入扩散模型。 •\t训练细节： •\tBatch size = 32，学习率 5×10⁻⁵； •\t训练步数 30K steps； •\t采用与 Stage I 相似的 DDIM 采样流程，保证画质与动作一致性。 •\t输出：分辨率可达 512×512 的连续视频帧序列，手–物体交互清晰、握姿自然，可直接用于机器人模仿学习。\nVidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic CVPR 2025\nFrom Technical University of Munich\n目标： 大规模网络视频人类样本学习 训练 家务机器人 模型\n动机： 机器人依赖实例教学，但是做家务没那么多教学\n模型流程：\n模型概览\nVidBot 通过三大模块——3D 可交互性提取、粗阶段 affordance 预测、细阶段轨迹生成——实现从“野外”RGB 视频到机器人可执行动作的零样本迁移。首先，它利用 Structure-from-Motion 和度量深度模型，从单目视频中恢复一致的 3D 手部轨迹与接触/目标点；然后，因式分解成粗预测网络 πc（提取高层次接触点与目标点）与细预测网络 πf（基于扩散生成精细轨迹），并在推理时引入多目标、法线与避碰等可微成本进行采样指导；最后，将生成的 3D 交互轨迹直接部署于多种机器人平台，实现开“即用”的零样本操控能力 ￼ ￼。\n⸻\n3D 可交互性提取管道 1.1 数据准备 •\t视频与语言输入：给定原始 RGB 图像序列 {Ĩ0,…ĨT} 及指令 l。 •\tSfM 与深度预测：使用 SfM 系统估计相机内参 K、无尺度位姿 {TWC} 及稀疏地标，再调用度量深度基模型（如 ZoeDepth）生成密集深度 {D̂t}，实现时序一致且度量尺度的重建 ￼； •\t手-物体分割与填充：借助手—物体检测（如 [72]）与分割模型（如 [94]）提取手部与接触物体掩码，再通过视频修复生成无手帧，以消除动态遮挡对优化的影响 ￼。\n1.2 姿态与尺度优化 •\t全局尺度校正：优化全帧尺度 sg，使稀疏地标深度与预测深度对齐； •\t位姿细化：联合优化每帧位姿 TWCi 与局部尺度 si，补偿 SfM 在手—物体动态区域的误差，实现一致的 3D 重建 ￼。\n1.3 交互表示提取 •\t手部中心轨迹：将优化后各帧手部中心点恢复至首帧坐标系，插值形成平滑交互轨迹 τ̂； •\t接触与目标点采样：在首帧均匀下采样手部中心生成接触点 ĉ，在末帧提取目标点 ĝ，用于监督模型的中间预测； •\t表示定义：最终得到的 3D affordance a = {c, τ}，其中 c∈ℝNc×3 为接触点集，τ∈ℝH×3 为轨迹序列 ￼ ￼。\n⸻\n粗—细分级 Affordance 学习 2.1 模型结构因式分解\n将 affordance 模型 π({Ĩ, D̃},l) 分解为： 1.\t粗阶段 πc：从 RGB-D 图像 Ĩ→D̃ 及指令 l 中预测目标点 g 和接触点 c； 2.\t细阶段 πf：在粗阶段输出 {g,c} 及场景上下文指导下，通过扩散模型生成精细轨迹 τ ； 整个流程利用测试时可微成本（多目标到达、碰撞避让等）对采样进行引导，增强与新场景/新形体的适应性 ￼ ￼。\n2.2 粗阶段：目标与接触点预测 •\t输入预处理：使用开集物体检测器裁剪关注物体区域的 RGB-D 图像； •\t网络架构：πc 由两支网络 πgoalc 与 πcontc 组成，分别输出目标和接触热图及（目标点）深度。 •\t融合视觉特征、RoI 池化后得到的物体嵌入、语言嵌入（CLIP 特征）和位置编码，通过 Perceiver 与 Transformer 生成每像素概率分布； •\t3D 投影：根据相机内参与热图深度，将像素坐标提升到三维空间，得到 g∈ℝNg×3 与 c∈ℝNc×3 ￼。\n2.3 细阶段：基于扩散的轨迹生成 •\t条件扩散模型：πf 采用 1D U-Net 架构，输入包含轨迹状态 τk、TSDF 空间特征（由 3D U-Net 从 RGB-D 构建的体素化 TSDF 地图提取）、语言与物体嵌入等； •\t正向/反向过程：遵循扩散概率模型，逐步加入高斯噪声，再由网络学习去噪，直接回归未加噪轨迹 τ̄0； •\t测试时引导：在每个去噪步骤中加入可微成本函数——多目标到达、法线一致、碰撞避让——通过扰动引导采样，提高交互轨迹在新场景/新形体下的合理性与可执行性 ￼ ￼。\n⸻\n输入与输出 •\t输入：首帧的“修复”RGB 图像 Ĩ̃0、对应度量深度 D̃0、裁剪的物体图像 Ĩ̃0o 及语言指令 l ￼。 •\t输出：3D 接触点集 c 和交互轨迹 τ ，直接可用于机器人控制模块执行操作 ￼。 算力要求： 没说\nCode availability: 暂时没有 (2025-06-11)\nPhoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction CVPR 2025\nFrom 人大 \u0026 上海AI lab\n要达成的事情：\n让机器人能够自我反思到底是哪里做得不好，然后自我调整\n动机：\n人类可以很自然地反思：为什么失败了，为什么机器人不行呢？\n模型实现方式：\n首先，这是一个“语言指导”的RL方法。\n对于这个方法，首先由 LLM 生成一个文字指令：\n比如：现在我要移动一个杯子，我要怎么做\n然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。\n如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。\n现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL\n最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。\n对于修正指令这个步骤，模型一共会输出两条语句，分别是 “语义上的反思” (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)\n最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。\n那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。\n结果：\n更强的学习能力，更强的泛化能力。\n计算要求：\n仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行\n虚拟环境为 RoboMimic 模拟器\n使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs\nRobotic Visual Instruction: A New Paradigm for Human-Robot Communication CVPR 2025\nFrom IC + 上海 AI lab + UCSD …\n目标：\n更好的人机交互\n动机：\n语言有很多冗余信息，那在图片中增加信息不就行了？\n模型实现方式：\n机器人视觉指令 (RoVI) RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：\n箭头：指示运动方向和轨迹 圆圈：标记感兴趣的对象或动作目标 颜色：区分多个指令或动作步骤 数字：指示操作顺序 这种视觉语言具有以下几个优点：\n空间精确性：视觉标记精确地指示3D空间中的位置和路径 时间清晰性：顺序步骤被清晰地划分 直观设计：这些符号易于人类理解和创建 跨文化实用性：视觉指令超越语言障碍 RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。\n靠手动标记了 15K 图片。。。 工作量爆炸💥 而且以后也要人手标。。。\n🤔： 为什么不能自动标，难道作者没有想过这一点吗\n但总之，现在在这个图像的基础上，VLM 会帮忙生成：\n任务的文字描述 可以执行这些任务的代码 代码中包含：\n路径 起点，终点，过程点\n然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取\n算力要求：\nNvidia A40\n现实实验设置：\nUFACTORY X-Arm 6和UR5 两台机械臂 两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。 两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。\n模拟环境设置：\n使用了SAPIEN 作为模拟器。 SIMPLER 作为基础环境。\nMitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation CVPR 2025\nFrom 港科广\n目标：\n缩小人机之间的 Gap\n动机：\n人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap\n问题：\n现在有这些 Gap:\n视觉环境差异 人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。\n形态差异： 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。\n尺度和视角： 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。\n解决这一差距的传统方法分为两大类，每类都有显著的局限性：\n预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。\n在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。\n模型实现方式：\nbasically, 就是说希望用一个 Adaptor 来 fill in the gap.\n把从 human demo pretrain embedding 转换成 robot demo embedding.\n对于任务感知也是如此。\n用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。\n结果：\nAlign 一下效果变好了\n算力要求：\n4 * Nvidia A6000\n模拟环境为 RLBench\n真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头\nMoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation CVPR 2025\nFrom 北邮 + 南阳理工 + 清华\n目标：\n导航 + 空间操作\n动机：\n虽然静态的操作已经没问题了，但是若是平台移动就不太好办。\n实现细节：\n这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分\nMoManipVLA使用以下方法实现这些优化问题：\n双退火搜索算法用于基座位置寻找优化 序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案 该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：\nRGB相机进行视觉感知 关节编码器进行本体感知（感知机器人自身位置） (optional) 深度感知以增强障碍物避免\n结果：\n模拟环境：\n模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。\n真实环境（机械臂）：\n在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。\n算力：\n4 * RTX 3090\nROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics CVPR 2025\nFrom OSU and NVIDIA\n目标：\n引导 VLM 2D \u0026 3D 视觉，理解空间结构。\n动机：\nVLM 目前无法理解空间结构。 原因并非 VLM 不行，而是数据不够\"spatial\"\n模型实现细节：\n首先是数据收集：\n输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”\n同时，生成一个俯视图，来看看哪里适合放置一个物体。\n最后在看看这个物体是否适合被放在这个地方。\n对于物体的方位，每一次会从三个角度问问题：\n以机器人为中心的视角 (第一视角) 以物体为中心的视角 (第三视角) 以世界为中心的视角 (fix-cam)\n自我中心：“从您的视角看，书在电脑的左边吗？” 以物体为中心：“从电脑的视角看，书在电脑的左边吗？” 以世界为中心：“从海拔高度看，书在电脑的上方吗？” 这样的好处是可以让 VLM 有更强的空间理解\n结果：\n在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o\n算力要求：\n20-40h * 8 * H100\n模拟环境：\nROBOSPATIAL 数据集，这包括 ScanNet, Matterport3D, 3RScan, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。\n真实环境：\nKinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。 机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。\nThink Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation CVPR 2025\nFrom 上交，复旦，上海 AI lab\n目标：\n如何避免灾难性遗忘\n动机：\n有些动作有相似之处，比如递筷子和递镊子有相似之处 那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？\n模型实现细节：\nPPL 框架的核心组件包括：\n输入编码器： 本体感觉编码器：处理机器人的关节状态和夹爪姿势 视觉编码器：处理场景的 RGB 图像 光流编码器：处理光流信息以捕获运动模式 文本编码器：处理任务的语言指令\n基元提示： 跨任务共享的基本运动模式的学习表示 注入到多头自注意力层的键和值中\n终身提示 (Lifelong Prompts)： 在终身学习期间为新任务学习的特定于任务的提示 与原始提示连接以自定义模型的行为\n运动感知提示查询 (Motion-Aware Prompt Query)： 结合光流和文本指令信息 用于确定不同原始提示的相关性\n扩散Transformer (Diffusion Transformer)： 基于条件输入和提示生成机器人动作\n算力：\n论文中没有直接说明具体的GPU类型和训练时间。\n模拟环境：\n模拟实验是在基于 MimicGen 和 LIBERO 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。\n机械臂：\n真实世界的实验是在 Franka Panda 机械臂上进行的。\n数据集：\n论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。\n获取方式：\n这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。\nGenerating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision CVPR 2025\nFrom Kyoto University\n这篇论文的标题是“从自我中心视角下的动作描述生成6自由度物体操作轨迹”，其核心目标是根据文本描述和初始视觉输入来生成物体在3D空间中的操作轨迹序列。\n目标： 从自我中心视频中提取6自由度（6DoF）物体操作轨迹。 基于这些提取的轨迹和相关的动作描述，开发能够根据文本动作描述生成物体操作轨迹的模型。\n动机： 开发能够协助人类活动的交互式机器人，其中一个关键能力是让机器人能够按人类指令操作工具和物体。 训练生成此类操作轨迹的模型需要大量多样且详细的物体操作演示，但这在实际中很难大规模收集。 利用日常工作场景中的视频来提取人类演示中的各种物体操作，是一种很有前景的解决方案。 现有的3D物体操作轨迹预测方法受限于昂贵的3D注释成本，难以处理多样化的操作动作。 现有数据集（如HOT3D）中的人类运动轨迹数据有限，不足以开发能够生成物体操作轨迹的模型。\n数据来源： 训练数据： 论文使用Ego-Exo4D数据集 [30] 来构建大规模训练数据集，包含6DoF物体操作轨迹以及图像、深度图和动作描述。这个过程是自动化的，不依赖于预设的相机外部参数。\n评估数据： 论文使用现有的HOT3D [3] 数据集进行评估。HOT3D是一个用于3D手和物体跟踪的自我中心视角数据集，通过光学标记和多红外OptiTrack相机记录，提供了精确的手和物体6DoF信息。\n算力要求： 论文提到模型训练了30个epoch，批处理大小为8。优化器使用了AdamW，基础学习率为2e-5（对于LLMs）和2e-4（对于其他参数）。 为了处理旋转连续性问题，每个角度（roll, pitch, yaw）都用[cos(θ), sin(θ)]表示。 虽然论文没有直接给出具体的算力消耗数字（例如所需的GPU数量或训练时间），但从训练epoch和批处理大小来看，以及使用了BLIP-2、VILA、PointLLM和MiniGPT-3D等大型视觉语言模型作为骨干网络，可以推断出这需要相当大的计算资源。这些模型本身就比较大，并且训练涉及处理大规模视频数据。\n公开代码： 论文在摘要中明确提到了数据集和代码的链接：https://biscue5.github.io/egoscaler-project-page/。\n模拟环境和现实环境平台： 论文没有提到使用了特定的模拟环境平台。 在现实环境方面，论文主要使用了自我中心视频数据。HOT3D [3] 数据集是通过Project Aria眼镜 [21] 和Quest 3 [65] 记录的，这些是现实世界中的可穿戴设备，用于捕捉自我中心视角下的活动。这表明他们的研究是基于真实世界中的人类活动数据。\nUniversal Actions for Enhanced Embodied Foundation Models CVPR 2025 From 清华\n这篇文章介绍的 UniAct 框架，目标是解决具身基础模型在处理异构动作数据时面临的挑战，并构建一个能够在通用动作空间中操作的框架。\n主要目标：\n构建通用动作空间： 学习一种能够捕捉不同机器人通用原子行为的动作空间，从而消除机器人之间因物理形态和控制接口差异造成的动作异构性。\n实现跨形态泛化： 使得具身基础模型能够有效利用跨领域数据，并在不同的机器人形态之间实现更好的泛化控制和适应能力。\n提高模型效率： 训练一个相对较小（0.5B 参数）但性能优于更大（14倍）现有模型的具身基础模型，证明通用动作的优势。\n动机： 数据异构性挑战： 现有的大型基础模型在自然语言处理和计算机视觉领域取得了巨大成功，主要得益于海量的、多样化的互联网数据。然而，将同样的方法应用于具身智能体时，面临一个显著的挑战：不同机器人收集的动作数据存在显著的异构性，因为它们有不同的物理形态和控制接口。这种异构性严重阻碍了跨领域数据共享和通用具身基础模型的发展。\n现有解决方案的局限性： 大多数现有方法要么强制性地将不同动作空间视为等效，采用统一的离散化或归一化技术，但这可能导致动作编码的物理意义冲突；要么试图设计一个适用于各种机器人系统的物理可解释动作空间，但这需要大量人工工程，且未能充分利用不同具身动作空间之间的内在联系。\n对通用代理的需求： 开发能够处理跨任务、跨环境和跨形态泛化的通用具身基础模型，是构建通用具身智能体的一个有前景的途径。 数据来源：\nUniAct-0.5B 模型在训练时整合了来自多个开源机器人数据集的示范数据。这些数据集包括：\nOpen-X Embodiment (OXE) Libero Droid 这些数据被标准化，以包含第三人称视角观察和语言指令，同时保留了动作的异构性。总共使用了来自 28 种不同机器人形态的约 100 万个示范数据进行训练。\n算力要求： UniAct-0.5B 的训练是在 64 块 A100 GPU 上进行的，并使用了 DeepSpeed 进行优化，持续了 10 天。\n公开代码： 是的，文章中提到了项目的项目页面，通常这意味着代码是公开的： 项目页面\nSOLAMI: Social Vision‑Language‑Action Modeling for Immersive Interaction with 3D Autonomous Characters CVPR 2025\nSOLAMI 这篇文章旨在介绍一个端到端的社交视觉-语言-动作（VLA）建模框架，用于与 3D 自动角色进行沉浸式交互。\n文章的目标是构建能够感知、理解并与人类互动的 3D 自动角色，使其具备类似于人类的社交智能，通过多模态响应（语音和动作）驱动角色进行社交互动。\n这项研究的动机在于，目前的字符代理在与用户交互时，主要限于文本或语音交互，缺乏更丰富的模态。在社交互动中，沉浸感越深，人类体验越好。因此，研究人员希望构建具有更丰富模态的 3D 自动角色。此外，多模态交互数据非常稀缺，难以获取，这也促使他们开发了数据合成方法。\n数据主要来源于以下几个方面：\n交互式多模态数据（SynMSI）： 这是一个合成的多模态社交互动数据集，通过自动化流程生成，利用了现有的文本-动作数据集、基于文本的角色扮演模型和语音合成方法。SynMSI 数据集包含 6.3K 多轮多模态对话项。 运动数据：为了进行预训练阶段的运动与文本对齐，以及生成多模态数据用于指令微调，研究人员收集了包含丰富社交动作的现有数据集，例如 HumanML3D (24K 运动-文本对)、Inter-X (20K 运动-文本对和 10K 两人运动对)，以及 DLP-MoCap (2K 运动-文本对)。\n语音数据： 用于预训练阶段的语音-文本对齐，使用了 CommonVoice (150K 语音-文本对)、AnyInstruct (200K 语音-文本对和 100K 语音到语音项)，以及通过文本到语音方法（Azure TTS 和 XTTS_v2）生成的合成语音数据 (60K 语音-文本对)。\n算力要求： 在预训练阶段，SOLAMI 使用了 32 块 V100 GPU 来训练模型，批处理大小为 256。 在指令微调阶段，SOLAMI 使用了 16 块 V100 GPU，批处理大小为 48。推理时，所有模型都部署在 2 块 H800 GPU 上，并采用 vLLM 框架和异步机制来提高性能并保持公平性。\n代码： 项目的 GitHub 链接\n模拟环境在文章中没有明确提及，但实验中提到了使用 VR 界面进行用户研究，其中用户可以与各种 3D 角色进行沉浸式交互。\n现实环境指的是 VR 界面，研究人员开发了一个基于 Oculus Quest 3 前端和后端服务的 VR 界面。前端实现用户与 3D 自动角色的沉浸式交互，后端由 2 块 H800 GPU 提供算力支持。在实际使用中，VR 头显捕获用户的语音和身体动作，并将其发送到后端计算节点。\nA Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning CVPR 2025 from HKU\n这篇文章主要研究了预训练视觉模型（PVMs）在机器人学习任务中的应用，特别是视觉运动控制和感知任务。文章的目标是找出最优的预训练方法和数据来源，以提高PVMs在机器人学习任务中的表现。\n动机 文章的动机源于当前PVMs在机器人学习任务中的应用存在一些问题和局限性。尽管PVMs在传统视觉任务中表现出色，但它们在机器人学习任务中的最优配置仍不清楚。文章通过系统性的评估发现，虽然某些PVMs（如DINO和iBOT）在视觉运动控制和感知任务中表现出色，但它们在非对象中心（NOC）数据上的表现会显著下降。这种下降与它们学习对象中心表示的能力减弱密切相关。\n数据来源 文章中使用的数据集包括：\n对象中心数据集：ImageNet 场景中心数据集：COCO 网络爬取数据：CC12M 以自我为中心的数据：Ego4D 这些数据集被用来评估PVMs在不同类型的数据上的表现。\n算力要求 由于PVMs的训练和评估需要大量的计算资源，文章中提到使用了8个A100 GPU进行训练。对于某些任务，如导航任务，需要大约400M到500M步的训练和512到320个并行环境，这对计算资源提出了极高的要求。\n代码公开情况 文章中提到，他们的代码和模型是公开可用的，链接\n模拟环境 文章中使用了多个模拟环境平台进行评估，包括：\nFranka Kitchen Meta-World Habitat（用于导航任务，包含HM3D和Gibson环境） 现实环境 虽然文章主要关注模拟环境中的评估，但提到PVMs在现实环境中的应用潜力。现实环境中的平台并未在文章中具体提及，但提到了多个现实环境中的机器人学习任务和应用。\n总结 总的来说，这篇文章通过系统性的评估和实验，提出了SlotMIM方法，以有效地从NOC数据中学习对象中心的表示，并在多个任务中取得了优于现有方法的性能。文章的研究为PVMs在机器人学习任务中的应用提供了新的见解和方法。\nOmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints 这篇文章介绍了一个名为 OmniManip 的新方法，旨在实现通用机器人操作。\n这篇文章要做什么，目标是什么？ 这篇文章提出了 OmniManip，这是一种开放词汇的操控方法，旨在弥合视觉-语言模型 (VLM) 的高层推理能力与低层精确操控之间的差距。其核心目标是开发一个通用的机器人操控系统，能够通过物体中心交互基元作为空间约束，在非结构化环境中执行各种操控任务，并具有强大的零样本泛化能力。\n动机是什么？ 开发能够在非结构化环境中进行通用机器人操控的系统是一个重大挑战。虽然 VLM 在高层常识推理方面表现出色，但它们缺乏精确操控任务所需的精细 3D 空间理解能力。现有的解决方案，例如在机器人数据集上微调 VLM，面临数据收集成本高昂和泛化性差的问题。通过将机器人动作抽象为交互基元并利用 VLM 定义空间约束，是解决这些挑战的动机。\n数据是从哪里来的？ 文章中提到，为了评估 OmniManip 在真实世界场景中的操控能力，他们设计了 12 个任务来评估模型的操控能力，这些任务涵盖了各种对象和复杂环境。虽然没有明确说明具体的数据集来源，但实验部分提到了通过 OmniManip 自动生成演示数据，并收集了每项任务 150 条轨迹用于训练行为克隆策略。\n算力要求多少？ 文章中没有直接给出具体的算力要求，但提及了多个 VLM 调用会带来计算挑战，即使进行了并行处理也是如此。这暗示了该系统可能需要较高的计算资源。\n公开代码吗？ 文章中没有明确提到代码是否公开。\n模拟环境用的是什么平台？ 文章中没有提到具体使用了哪个模拟环境平台。\n现实环境用的是什么平台？ 现实环境实验平台是基于 Franka Emika Panda 机器人臂搭建的，并配备了 UMI 机械手。感知方面，使用了两台 Intel RealSense D415 深度相机，一台安装在机械手上提供第一人称视角，另一台则放置在机器人对面提供第三人称视角。\n",
  "wordCount" : "1485",
  "inLanguage": "en",
  "datePublished": "2025-06-13T10:24:57+08:00",
  "dateModified": "2025-06-13T10:24:57+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tzj2006.github.io/bugjournal/2025-06-13/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TzJ's Net",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tzj2006.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tzj2006.github.io/" accesskey="h" title="TzJ&#39;s Net (Alt + H)">TzJ&#39;s Net</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tzj2006.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/bugjournal/" title="bugJournal">
                    <span>bugJournal</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/leetcode/" title="leetcode">
                    <span>leetcode</span>
                </a>
            </li>
            <li>
                <a href="https://tzj2006.github.io/posts/" title="posts &amp; notes">
                    <span>posts &amp; notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tzj2006.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://tzj2006.github.io/bugjournal/">BugJournals</a></div>
    <h1 class="post-title entry-hint-parent">
      Bug Journal 2025-06-13
    </h1>
    <div class="post-meta"><span title='2025-06-13 10:24:57 +0800 CST'>June 13, 2025</span>&nbsp;·&nbsp;7 min


      
      <div class="meta-item">
        <span id="busuanzi_container_page_pv">
           &nbsp; People Read: <span id="busuanzi_value_page_pv"></span>
        </span>
     </div>

    </div>
  </header> 
  <div class="post-content"><h2 id="physvlm-enabling-visual-language-models-to-understand-robotic-physical-reachability">PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability<a hidden class="anchor" aria-hidden="true" href="#physvlm-enabling-visual-language-models-to-understand-robotic-physical-reachability">#</a></h2>
<p>CVPR 2025</p>
<p>From 北京交通大学 &amp; 广东技术师范大学</p>
<p>目标：
告诉机器人什么位置它到不了</p>
<p>动机：
有时候机器人不知道一个位置到不到得了，结果把自己搞坏了</p>
<p>模型流程：</p>
<p><img alt="1749625628269" loading="lazy" src="https://tzj2006.github.io/images/2025-06-11/1749625628269.png"></p>
<p>首先离线计算什么位置是机械臂能达到的。
形成一个点云 (S-P Map)</p>
<p>然后用 SigLip-400M 提取图像和点云的特征</p>
<p>然后把这个 embedding 和文字的 embedding 混合之后</p>
<p>通过 Qwen-2.5-Instruct-3B，生成一个回答给 VoxPoser 做。</p>
<p>实验设计：</p>
<p>仿真：基于 CUDA 11.x 的 PyBullet 环境，六种机器人（UR5、FR5、CR5、FRANKA、UR3、XArm6），共 1.3K 问答，零样本评测。</p>
<p>实机：在 UR3、XArm6 真实平台上各 10 次零样本任务，评估任务成功率。</p>
<p>评估指标：</p>
<p>EQA-phys：基于 LLM 评分（5 分正确、1 分错误）计算平均分；
RoboVQA-val / OpenEQA：标准 BLEU-4 与准确率；
任务规划：成功率。</p>
<p>结果：</p>
<p><img alt="1749626091281" loading="lazy" src="https://tzj2006.github.io/images/2025-06-11/1749626091281.png"></p>
<p>S-P Map 在很多 LLM 上都有用</p>
<p>PhysVLM-3B 效果平均最好</p>
<p>数据集：
Zero-shot</p>
<p>算力要求：</p>
<p>&lt; 48h * 8 * A800</p>
<p>代码：</p>
<p><a href="https://github.com/unira-zwj/PhysVLM">开源</a></p>
<hr>
<h2 id="object-centric-prompt-driven-vision-language-action-model-for-robotic-manipulation">Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation<a hidden class="anchor" aria-hidden="true" href="#object-centric-prompt-driven-vision-language-action-model-for-robotic-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From PKU Agibot Lab</p>
<p>目标：
帮助机器人排除冗余信息干扰</p>
<p>动机：
语言和视频中冗余信息过多</p>
<p>模型流程：</p>
<p><img alt="1749626781693" loading="lazy" src="https://tzj2006.github.io/images/2025-06-11/1749626781693.png"></p>
<p>在图片上加一些标记
分别是：</p>
<ol>
<li>接触点（蓝色）</li>
<li>末端执行器在接触时的 z 轴方向（红色）</li>
<li>y 轴方向（绿色）</li>
<li>接触后移动方向（黄色）</li>
</ol>
<p>这些标记可能来自 Grounding-Dino + SAM 或者 ChatGPT-4o 或者 手动标记</p>
<p>标记方式如下：</p>
<p>均匀生成 N(32) 条线，让 GPT 选择 3 条标上颜色</p>
<p>然后把这个“增强”过后的信息输入给 CLIP,然后再让 LLM 输出&quot;应该在哪里，以什么角度接触&quot;</p>
<p>对于这个信息，我们可以和GT 做 train</p>
<p>最后，我们得到了这个位置和姿态的信息，我们就可以用IK求解路径了。</p>
<p>结果：</p>
<p>数据集：</p>
<ol>
<li>
<p>模拟环境：SAPIEN + PartNet-Mobility
•	平台：使用 SAPIEN（一个支持刚体与关节物体的物理模拟器），其提供高保真动力学与渲染接口，适合零样本机器人操控研究  ￼ ￼。
•	资产集：加载 PartNet-Mobility 中的 1,500 多个关节化 CAD 模型，涵盖抽屉、门、笔记本电脑盖等常见家电与家具  ￼。
•	飞行夹持器（Flying Gripper）：采用 SAPIEN 中的“飞行”版 Franka Panda Gripper，使得末端执行器可在自由空间中无碰撞地移动，以便专注测试模型对提示的理解能力  ￼。
•	摄像机随机化：在每个场景随机采样摄像机视角（水平±45°、俯仰30°–60°），并在单帧中记录 RGB 图像与 3D 末端执行器位姿，用于提示生成与训练  ￼。
•	数据采集：共采集 ~10,000 条训练样本，过程耗时约 6–8 小时；测试集与训练集按照固定比例拆分，模拟了 Seen/Unseen 物体类别的泛化能力评估  ￼ ￼。</p>
</li>
<li>
<p>现实机器人平台
•	硬件平台：使用真实的 Franka Emika Panda 机械臂（7 自由度，集成高精度力矩传感器），配合标准的 RGB-D 摄像头（如 Intel RealSense D415）采集视觉输入  ￼。
•	执行流程：将 CrayonRobo 在模拟环境中输出的 SE(3) 接触位姿与移动方向，通过 ROS + MoveIt! 的 IK 求解与笛卡尔轨迹规划一键下发真实机器人执行  ￼。
•	测试任务：与模拟中一致，包括“拉抽屉”“开门”“掀笔记本盖”等单步原子操作，每个任务在 5–10 个不同初始姿态下重复试验。
•	评价指标：以二值成功率衡量：当末端执行器按照预测方向将物体部件移动超过阈值（如 5 cm）即记为成功；平均成功率达到 74%（Seen）与 72%（Unseen）  ￼。</p>
</li>
</ol>
<p>算力要求：
未知
建议 &gt; 40 GB VRAM</p>
<p>代码：
<a href="https://github.com/clorislili/CrayonRobo">开源</a></p>
<hr>
<h2 id="checkmanual-a-new-challenge-and-benchmark-for-manual-based-appliance-manipulation">CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation<a hidden class="anchor" aria-hidden="true" href="#checkmanual-a-new-challenge-and-benchmark-for-manual-based-appliance-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From PKU Agibot lab</p>
<p>目标：
让机器人读取说明书之后根据说明书做出正确操作</p>
<p>动机：
阅读说明书</p>
<p>电子产品有时候视觉信息不够，很多有着相似外观的东西可能有着不同的功能。</p>
<p>所以要读说明书</p>
<p>模型流程：</p>
<p><img alt="1749623536451" loading="lazy" src="https://tzj2006.github.io/images/2025-06-11/1749623536451.png"></p>
<p>OCR → GPT-4o 提取图文信息 → GPT‑4o 生成操作步骤 → SoM、Grounding‑DINO、SAM 实现视觉对齐 → FoundationPose 匹配 CAD</p>
<p>最后输出{部件，操作} pair, 最后使用 VoxPoser 进行操作。</p>
<p><img alt="1749625591808" loading="lazy" src="https://tzj2006.github.io/images/2025-06-11/1749625591808.png"></p>
<p>实验设置
模拟平台：SAPIEN 0.8 + Franka Panda 飞行夹具  ￼</p>
<p>数据集：
PartNet-Mobility CAD 模型；
CheckManual 合成说明书（已公开，可下载使用）  ￼</p>
<p>评估指标：
任务完成率</p>
<p>现实验证：
Franka + RealSense 摄像头，完成单个用例的实物测试</p>
<p>算力要求：
未知
建议 &gt; 40 GB VRAM</p>
<p>Code availability:
<a href="https://github.com/LYX0501/CheckManual">开源</a></p>
<p>结果：</p>
<p><img alt="1749623757952" loading="lazy" src="https://tzj2006.github.io/images/2025-06-11/1749623757952.png"></p>
<p>总之有 manual 效果更好</p>
<hr>
<h2 id="taste-rob-advancing-video-generation-of-task-oriented-hand-object-interaction-for-generalizable-robotic-manipulation">TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation<a hidden class="anchor" aria-hidden="true" href="#taste-rob-advancing-video-generation-of-task-oriented-hand-object-interaction-for-generalizable-robotic-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From Xiaoguang Han&rsquo;s Lab at 港中深</p>
<p>目标：
优化对齐视频-人手数据集</p>
<p>动机：
现在有这些问题：</p>
<ol>
<li>视角不一致</li>
<li>动作语义无法对齐</li>
<li>手部姿态稳定性不高
这个模型想要解决这些问题</li>
</ol>
<p>模型流程：</p>
<ol>
<li>数据集构建（Sec. 3）：
•	100,856 条 1–8 秒单动作视频，静态 1080p 视角，语义指令一一对应。</li>
<li>粗视频生成（Stage I – Coarse Action Planner）：
•	基于 DynamiCrafter（512 × 512 分辨率），语言指令＋静态环境图→粗交互视频；
•	微调参数：batch=16, lr=5×10⁻⁵, 30K steps。</li>
<li>姿态优化（Stage II – MDM Refinement）：
•	使用 Motion Diffusion Model 以 60 帧手部关键点序列为条件，保证握姿一致性；
•	训练设置：batch=64, lr=1×10⁻⁴, 500K steps; 推理 10-step DDIM denoise。</li>
<li>最终生成（Stage III – Frame-wise Adapter）：
•	将优化后手部姿态图像与环境图和语言指令一起接入 Stable Diffusion 2 Adapter，生成最终视频；
•	训练设置：batch=32, lr=5×10⁻⁵, 30K steps。</li>
</ol>
<p>实验设置
•	仿真验证：在 SAPIEN 模拟平台上，用 DynamiCrafter＋MDM + Adapter 生成视频，并通过 Figure 17 示范在虚拟机器人上的模仿效果。</p>
<p>模型流程：</p>
<p>第一阶段：Coarse Action Planner（粗动作生成）
•	目标：在给定语言指令（如“拿起杯子并倒入水”）和静态环境图（512×512 分辨率）条件下，生成一段粗略的、合理的手–物体交互视频序列（通常 16 帧）。
•	模型架构：基于 Video Diffusion Model（VDM），如 DynamiCrafter，使用文本–图像条件扩散网络。
•	训练细节：
•	Batch size = 16，学习率 5×10⁻⁵；
•	训练步数 30K steps；
•	推理时使用 50-step DDIM 采样，平衡生成质量与速度。
•	输出：一段低分辨率、含大致手部运动轨迹和物体交互的粗视频，用于后续姿态细化。</p>
<p>⸻</p>
<p>第二阶段：Motion Diffusion Model (MDM) Refinement（姿态细化）
•	目标：针对第一阶段生成的粗视频，从中提取手部的 3D 关键点序列（60 帧），并利用扩散模型细化运动轨迹，提升抓握姿态的连贯性与自然度。
•	模型架构：1D 时序扩散网络，对帧间手部关键点做去噪与重建。
•	训练细节：
•	Batch size = 64，学习率 1×10⁻⁴；
•	训练步数 500K steps；
•	推理时使用 10-step DDIM，快速得到精细关键点序列。
•	输出：一组平滑、符合物理约束的 3D 手部关键点轨迹，用以指导下一阶段的视频生成。</p>
<p>⸻</p>
<p>第三阶段：Frame-wise Adapter（帧级最终生成）
•	目标：将姿态细化后的关键点与原环境图、语言指令结合，生成最终高质量、手部姿态稳定的交互视频。
•	模型架构：基于 Stable Diffusion 2，通过 Adapter 模块将手部姿态（以可视化关节点或姿态图形式）作为条件，连同环境图与文本，一起输入扩散模型。
•	训练细节：
•	Batch size = 32，学习率 5×10⁻⁵；
•	训练步数 30K steps；
•	采用与 Stage I 相似的 DDIM 采样流程，保证画质与动作一致性。
•	输出：分辨率可达 512×512 的连续视频帧序列，手–物体交互清晰、握姿自然，可直接用于机器人模仿学习。</p>
<hr>
<h2 id="vidbot-learning-generalizable-3d-actions-from-in-the-wild-2d-human-videos-for-zero-shot-robotic">VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic<a hidden class="anchor" aria-hidden="true" href="#vidbot-learning-generalizable-3d-actions-from-in-the-wild-2d-human-videos-for-zero-shot-robotic">#</a></h2>
<p>CVPR 2025</p>
<p>From Technical University of Munich</p>
<p>目标：
大规模网络视频人类样本学习 训练 家务机器人 模型</p>
<p>动机：
机器人依赖实例教学，但是做家务没那么多教学</p>
<p>模型流程：</p>
<p><img alt="1749624107681" loading="lazy" src="https://tzj2006.github.io/images/2025-06-11/1749624107681.png"></p>
<p>模型概览</p>
<p>VidBot 通过三大模块——3D 可交互性提取、粗阶段 affordance 预测、细阶段轨迹生成——实现从“野外”RGB 视频到机器人可执行动作的零样本迁移。首先，它利用 Structure-from-Motion 和度量深度模型，从单目视频中恢复一致的 3D 手部轨迹与接触/目标点；然后，因式分解成粗预测网络 πc（提取高层次接触点与目标点）与细预测网络 πf（基于扩散生成精细轨迹），并在推理时引入多目标、法线与避碰等可微成本进行采样指导；最后，将生成的 3D 交互轨迹直接部署于多种机器人平台，实现开“即用”的零样本操控能力 ￼ ￼。</p>
<p>⸻</p>
<ol>
<li>3D 可交互性提取管道</li>
</ol>
<p>1.1 数据准备
•	视频与语言输入：给定原始 RGB 图像序列 {Ĩ0,…ĨT} 及指令 l。
•	SfM 与深度预测：使用 SfM 系统估计相机内参 K、无尺度位姿 {TWC} 及稀疏地标，再调用度量深度基模型（如 ZoeDepth）生成密集深度 {D̂t}，实现时序一致且度量尺度的重建 ￼；
•	手-物体分割与填充：借助手—物体检测（如 [72]）与分割模型（如 [94]）提取手部与接触物体掩码，再通过视频修复生成无手帧，以消除动态遮挡对优化的影响 ￼。</p>
<p>1.2 姿态与尺度优化
•	全局尺度校正：优化全帧尺度 sg，使稀疏地标深度与预测深度对齐；
•	位姿细化：联合优化每帧位姿 TWCi 与局部尺度 si，补偿 SfM 在手—物体动态区域的误差，实现一致的 3D 重建 ￼。</p>
<p>1.3 交互表示提取
•	手部中心轨迹：将优化后各帧手部中心点恢复至首帧坐标系，插值形成平滑交互轨迹 τ̂；
•	接触与目标点采样：在首帧均匀下采样手部中心生成接触点 ĉ，在末帧提取目标点 ĝ，用于监督模型的中间预测；
•	表示定义：最终得到的 3D affordance a = {c, τ}，其中 c∈ℝNc×3 为接触点集，τ∈ℝH×3 为轨迹序列 ￼ ￼。</p>
<p>⸻</p>
<ol start="2">
<li>粗—细分级 Affordance 学习</li>
</ol>
<p>2.1 模型结构因式分解</p>
<p>将 affordance 模型 π({Ĩ, D̃},l) 分解为：
1.	粗阶段 πc：从 RGB-D 图像 Ĩ→D̃ 及指令 l 中预测目标点 g 和接触点 c；
2.	细阶段 πf：在粗阶段输出 {g,c} 及场景上下文指导下，通过扩散模型生成精细轨迹 τ ；
整个流程利用测试时可微成本（多目标到达、碰撞避让等）对采样进行引导，增强与新场景/新形体的适应性 ￼ ￼。</p>
<p>2.2 粗阶段：目标与接触点预测
•	输入预处理：使用开集物体检测器裁剪关注物体区域的 RGB-D 图像；
•	网络架构：πc 由两支网络 πgoalc 与 πcontc 组成，分别输出目标和接触热图及（目标点）深度。
•	融合视觉特征、RoI 池化后得到的物体嵌入、语言嵌入（CLIP 特征）和位置编码，通过 Perceiver 与 Transformer 生成每像素概率分布；
•	3D 投影：根据相机内参与热图深度，将像素坐标提升到三维空间，得到 g∈ℝNg×3 与 c∈ℝNc×3  ￼。</p>
<p>2.3 细阶段：基于扩散的轨迹生成
•	条件扩散模型：πf 采用 1D U-Net 架构，输入包含轨迹状态 τk、TSDF 空间特征（由 3D U-Net 从 RGB-D 构建的体素化 TSDF 地图提取）、语言与物体嵌入等；
•	正向/反向过程：遵循扩散概率模型，逐步加入高斯噪声，再由网络学习去噪，直接回归未加噪轨迹 τ̄0；
•	测试时引导：在每个去噪步骤中加入可微成本函数——多目标到达、法线一致、碰撞避让——通过扰动引导采样，提高交互轨迹在新场景/新形体下的合理性与可执行性 ￼ ￼。</p>
<p>⸻</p>
<ol start="3">
<li>输入与输出
•	输入：首帧的“修复”RGB 图像 Ĩ̃0、对应度量深度 D̃0、裁剪的物体图像 Ĩ̃0o 及语言指令 l ￼。
•	输出：3D 接触点集 c 和交互轨迹 τ ，直接可用于机器人控制模块执行操作 ￼。</li>
</ol>
<p>算力要求：
没说</p>
<p>Code availability:
暂时没有 (2025-06-11)</p>
<hr>
<h2 id="phoenix-a-motion-based-self-reflection-framework-for-fine-grained-robotic-action-correction">Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction<a hidden class="anchor" aria-hidden="true" href="#phoenix-a-motion-based-self-reflection-framework-for-fine-grained-robotic-action-correction">#</a></h2>
<p>CVPR 2025</p>
<p>From 人大 &amp; 上海AI lab</p>
<p>要达成的事情：</p>
<p>让机器人能够自我反思到底是哪里做得不好，然后自我调整</p>
<p>动机：</p>
<p>人类可以很自然地反思：为什么失败了，为什么机器人不行呢？</p>
<p>模型实现方式：</p>
<p><img alt="1749711299495" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749711299495.png"></p>
<p>首先，这是一个“语言指导”的RL方法。</p>
<p>对于这个方法，首先由 LLM 生成一个文字指令：</p>
<p>比如：现在我要移动一个杯子，我要怎么做</p>
<p>然后会有一个 Motion Correction Module 来判断这个动作是否是正确的。</p>
<p>如果这个指导是错误的，那么这个文字指令会进入下一个步骤，由 LLM 指导修正，生成一个新的文字指令。</p>
<p>现在这个“正确”的文字指令就会被作为机器人的 Motion task 和其他 observation 一起输入到网络中，进行预测和RL</p>
<p>最后，如果成功了的话，那这个指令就会被加入数据集中，让机器人用 Diffusion Policy 学习这个数据的信息。</p>
<p>对于修正指令这个步骤，模型一共会输出两条语句，分别是 &ldquo;语义上的反思&rdquo; (或者说，该做哪个子任务), 以及动作上的反思 (应该如何做来完成这个任务)</p>
<p>最终，这个数据集的结果会被作为数据集，用 Diffusion Policy 转化为一个 20Hz 的机器人动作。</p>
<p>那对于有些失败的时候的数据，则可以被人工干预纠正，用来继续扩充数据集。</p>
<p>结果：</p>
<p><img alt="1749712321377" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749712321377.png"></p>
<p><img alt="1749712335120" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749712335120.png"></p>
<p>更强的学习能力，更强的泛化能力。</p>
<p>计算要求：</p>
<p>仅需微调 LLAVA 450M + SigLIP 0.89B, 只需在 4070 上即可运行</p>
<p>虚拟环境为 RoboMimic 模拟器</p>
<p>使用了 500 个带有 Ground Truth 的数据，训练了 200 epochs</p>
<hr>
<h2 id="robotic-visual-instruction-a-new-paradigm-for-human-robot-communication">Robotic Visual Instruction: A New Paradigm for Human-Robot Communication<a hidden class="anchor" aria-hidden="true" href="#robotic-visual-instruction-a-new-paradigm-for-human-robot-communication">#</a></h2>
<p>CVPR 2025</p>
<p>From IC + 上海 AI lab + UCSD &hellip;</p>
<p>目标：</p>
<p>更好的人机交互</p>
<p>动机：</p>
<p>语言有很多冗余信息，那在图片中增加信息不就行了？</p>
<p>模型实现方式：</p>
<p>机器人视觉指令 (RoVI)
RoVI 被设计为一种符号视觉语言，它使用简单的几何元素来传达复杂的时空信息：</p>
<p>箭头：指示运动方向和轨迹
圆圈：标记感兴趣的对象或动作目标
颜色：区分多个指令或动作步骤
数字：指示操作顺序
这种视觉语言具有以下几个优点：</p>
<p>空间精确性：视觉标记精确地指示3D空间中的位置和路径
时间清晰性：顺序步骤被清晰地划分
直观设计：这些符号易于人类理解和创建
跨文化实用性：视觉指令超越语言障碍
RoVI指令可以使用数字设备上的简单绘图工具创建，甚至可以在打印图像上手工创建，这使得没有专门技术知识的用户也可以访问该系统。</p>
<p><img alt="1749712762668" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749712762668.png"></p>
<p>靠手动标记了 15K 图片。。。
<del>工作量爆炸💥</del>
而且以后也要人手标。。。</p>
<p>🤔： 为什么不能自动标，难道作者没有想过这一点吗</p>
<p>但总之，现在在这个图像的基础上，VLM 会帮忙生成：</p>
<ol>
<li>任务的文字描述</li>
<li>可以执行这些任务的代码</li>
</ol>
<p>代码中包含：</p>
<p>路径 起点，终点，过程点</p>
<p>然后会根据这些信息计划运动轨迹是什么，又要怎么做才能完成抓取</p>
<p><img alt="1749713264334" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713264334.png"></p>
<p><img alt="1749713277440" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713277440.png"></p>
<p>算力要求：</p>
<p>Nvidia A40</p>
<p>现实实验设置：</p>
<p>UFACTORY X-Arm 6和UR5 两台机械臂
两台经过校准的RealSense D435相机被放置用于俯视和第三人称视角。
两台机械臂都以20赫兹的控制频率在末端执行器增量控制模式下运行。</p>
<p>模拟环境设置：</p>
<p>使用了SAPIEN 作为模拟器。
SIMPLER 作为基础环境。</p>
<hr>
<h2 id="mitigating-the-human-robot-domain-discrepancy-in-visual-pre-training-for-robotic-manipulation">Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation<a hidden class="anchor" aria-hidden="true" href="#mitigating-the-human-robot-domain-discrepancy-in-visual-pre-training-for-robotic-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From 港科广</p>
<p>目标：</p>
<p>缩小人机之间的 Gap</p>
<p>动机：</p>
<p>人机从某种角度来说是“异构”的，特别是人类的演示视频，所以希望能够缩小这个 Gap</p>
<p>问题：</p>
<p>现在有这些 Gap:</p>
<p><strong>视觉环境差异</strong> 人类演示通常发生在自然、多变的环境中，与受控的机器人工作空间相比，具有不同的光照、背景和摄像机视角。</p>
<p><strong>形态差异：</strong> 最显著的差距在于人手和身体与机器人末端执行器和机械臂之间视觉上的巨大差异。这些形态差异意味着，即使执行相同的任务，人类演示和机器人演示中捕捉到的视觉模式也可能大相径庭。</p>
<p><strong>尺度和视角：</strong> 摄像机视角、物体尺度和工作空间配置在人类演示视频和机器人执行环境之间通常差异显著。</p>
<p>解决这一差距的传统方法分为两大类，每类都有显著的局限性：</p>
<p>预训练期间面向操作的代理任务：这些方法试图通过添加手部检测等任务，使人类数据预训练与操作更相关。然而，这些代理任务难以在人类数据上持续定义，并且由于缺乏机器人特定信息，无法明确解决域差异。</p>
<p>在机器人数据上进行任务特定微调：这些方法直接在下游机器人任务上微调预训练模型，但这需要针对每个机器人环境进行定制，并损害了模型在不同场景中的多功能性。</p>
<p>模型实现方式：</p>
<p><img alt="1749713649234" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713649234.png"></p>
<p>basically, 就是说希望用一个 Adaptor 来 fill in the gap.</p>
<p>把从 human demo pretrain embedding 转换成 robot demo embedding.</p>
<p><img alt="1749713795748" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713795748.png"></p>
<p>对于任务感知也是如此。</p>
<p>用的是类似 CLIP 的思路，如果一样则尽量 embedding尽量接近，否则就尽量拉远。</p>
<p>结果：</p>
<p><img alt="1749713835939" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749713835939.png"></p>
<p>Align 一下效果变好了</p>
<p>算力要求：</p>
<p>4 * Nvidia A6000</p>
<p>模拟环境为 RLBench</p>
<p>真实环境是 xArm7 机械臂、Inspire 夹具和 Orbbec Femto Bolt 摄像头</p>
<hr>
<h2 id="momanipvla-transferring-vision-language-action-models-for-general-mobile-manipulation">MoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation<a hidden class="anchor" aria-hidden="true" href="#momanipvla-transferring-vision-language-action-models-for-general-mobile-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From 北邮 + 南阳理工 + 清华</p>
<p>目标：</p>
<p>导航 + 空间操作</p>
<p>动机：</p>
<p>虽然静态的操作已经没问题了，但是若是平台移动就不太好办。</p>
<p>实现细节：</p>
<p>这个团队拆分了这个行动，把整个 task 分为机械臂运动的部分和底座运动的部分</p>
<p>MoManipVLA使用以下方法实现这些优化问题：</p>
<p>双退火搜索算法用于基座位置寻找优化
序贯二次规划（SLSQP）作为基于梯度的局部优化器来优化机械臂的解决方案
该框架与现有的预训练VLA模型集成，提取末端执行器定位所需的路径点。对于状态估计和感知，系统使用：</p>
<p>RGB相机进行视觉感知
关节编码器进行本体感知（感知机器人自身位置）
(optional) 深度感知以增强障碍物避免</p>
<p>结果：</p>
<p><img alt="1749714857163" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749714857163.png"></p>
<p>模拟环境：</p>
<p>模拟环境是 OVMM (Open Vocabulary Mobile Manipulation) 基准，它包含 60 个场景模型，这些模型近似于真实房屋的布局，以及超过 18k 个日常物体的 3D 模型。</p>
<p>真实环境（机械臂）：</p>
<p>在真实世界实验中，研究人员采用了 Hexman Echo Plus 基座和 RM65 机械臂组件作为离线移动平台。</p>
<p>算力：</p>
<p>4 * RTX 3090</p>
<hr>
<h2 id="robospatial-teaching-spatial-understanding-to-2d-and-3d-vision-language-models-for-robotics">ROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics<a hidden class="anchor" aria-hidden="true" href="#robospatial-teaching-spatial-understanding-to-2d-and-3d-vision-language-models-for-robotics">#</a></h2>
<p>CVPR 2025</p>
<p>From OSU and NVIDIA</p>
<p>目标：</p>
<p>引导 VLM 2D &amp; 3D 视觉，理解空间结构。</p>
<p>动机：</p>
<p>VLM 目前无法理解空间结构。
原因并非 VLM 不行，而是数据不够&quot;spatial&quot;</p>
<p>模型实现细节：</p>
<p>首先是数据收集：</p>
<p><img alt="1749717733449" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749717733449.png"></p>
<p>输入一个三维空间，然后提出一些问题，询问物体的方位，比如“杯子在电脑左边吗”</p>
<p>同时，生成一个俯视图，来看看哪里适合放置一个物体。</p>
<p>最后在看看这个物体是否适合被放在这个地方。</p>
<p>对于物体的方位，每一次会从三个角度问问题：</p>
<p>以机器人为中心的视角 (第一视角)
以物体为中心的视角 (第三视角)
以世界为中心的视角 (fix-cam)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-md" data-lang="md"><span class="line"><span class="cl">自我中心：“从您的视角看，书在电脑的左边吗？”
</span></span><span class="line"><span class="cl">以物体为中心：“从电脑的视角看，书在电脑的左边吗？”  
</span></span><span class="line"><span class="cl">以世界为中心：“从海拔高度看，书在电脑的上方吗？”
</span></span></code></pre></div><p>这样的好处是可以让 VLM 有更强的空间理解</p>
<p>结果：</p>
<p><img alt="1749717985437" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749717985437.png"></p>
<p>在训练后，VLM 用于具身智能的 task 可以提升效果，最后超越 GPT-4o</p>
<p>算力要求：</p>
<p>20-40h * 8 * H100</p>
<p>模拟环境：</p>
<p>ROBOSPATIAL 数据集，这包括 <a href="https://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf">ScanNet</a>, <a href="https://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf">Matterport3D</a>, <a href="https://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf">3RScan</a>, 以及两个桌面数据集 HOPE 和 GraspNet-1B。这些数据集包含了 1M 图像和 5k 3D 扫描。</p>
<p>真实环境：</p>
<p>Kinova Jaco 机器人搭配 ZED2 摄像头进行 RGB-D 感知。
机械臂系统实现了使用 cuRobo 进行运动规划的抓取和放置操作。</p>
<hr>
<h2 id="think-small-act-big-primitive-prompt-learning-for-lifelong-robot-manipulation">Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation<a hidden class="anchor" aria-hidden="true" href="#think-small-act-big-primitive-prompt-learning-for-lifelong-robot-manipulation">#</a></h2>
<p>CVPR 2025</p>
<p>From 上交，复旦，上海 AI lab</p>
<p>目标：</p>
<p>如何避免灾难性遗忘</p>
<p>动机：</p>
<p>有些动作有相似之处，比如递筷子和递镊子有相似之处
那能不能通过这些相似之处来学习一些不同动作都有的相同动作呢？</p>
<p>模型实现细节：</p>
<p><img alt="1749718882124" loading="lazy" src="https://tzj2006.github.io/images/2025-06-12/1749718882124.png"></p>
<p>PPL 框架的核心组件包括：</p>
<p>输入编码器：
本体感觉编码器：处理机器人的关节状态和夹爪姿势
视觉编码器：处理场景的 RGB 图像
光流编码器：处理光流信息以捕获运动模式
文本编码器：处理任务的语言指令</p>
<p>基元提示：
跨任务共享的基本运动模式的学习表示
注入到多头自注意力层的键和值中</p>
<p>终身提示 (Lifelong Prompts)：
在终身学习期间为新任务学习的特定于任务的提示
与原始提示连接以自定义模型的行为</p>
<p>运动感知提示查询 (Motion-Aware Prompt Query)：
结合光流和文本指令信息
用于确定不同原始提示的相关性</p>
<p>扩散Transformer (Diffusion Transformer)：
基于条件输入和提示生成机器人动作</p>
<p>算力：</p>
<p>论文中没有直接说明具体的GPU类型和训练时间。</p>
<p>模拟环境：</p>
<p>模拟实验是在基于 <a href="https://arxiv.org/pdf/2310.05905">MimicGen</a> 和 <a href="https://arxiv.org/pdf/2310.05905">LIBERO</a> 构建的大规模技能数据集上进行的。这个数据集包含了来自MimicGen的技能，每个技能都包含1K个人类演示，并具有广泛的初始状态分布，以评估多任务泛化能力。此外，还包含了LIBERO的技能，这是一个终身机器人操作基准。</p>
<p>机械臂：</p>
<p>真实世界的实验是在 Franka Panda 机械臂上进行的。</p>
<p>数据集：</p>
<p>论文使用了在MimicGen和LIBERO基准上构建的大规模技能数据集。</p>
<p>获取方式：</p>
<p>这个数据集是通过人类演示获得的。在模拟环境中，数据集包含了来自MimicGen的1K个人类演示。在真实世界环境中，多任务预训练是在四种不同的技能上进行的，每种技能都包含200个人类演示，并具有广泛的初始状态分布。</p>
<hr>
<h2 id="generating-6dof-object-manipulation-trajectories-from-action-description-in-egocentric-vision">Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision<a hidden class="anchor" aria-hidden="true" href="#generating-6dof-object-manipulation-trajectories-from-action-description-in-egocentric-vision">#</a></h2>
<p>CVPR 2025</p>
<p>From Kyoto University</p>
<p>这篇论文的标题是“从自我中心视角下的动作描述生成6自由度物体操作轨迹”，其核心目标是根据文本描述和初始视觉输入来生成物体在3D空间中的操作轨迹序列。</p>
<p>目标：
从自我中心视频中提取6自由度（6DoF）物体操作轨迹。
基于这些提取的轨迹和相关的动作描述，开发能够根据文本动作描述生成物体操作轨迹的模型。</p>
<p>动机：
开发能够协助人类活动的交互式机器人，其中一个关键能力是让机器人能够按人类指令操作工具和物体。
训练生成此类操作轨迹的模型需要大量多样且详细的物体操作演示，但这在实际中很难大规模收集。
利用日常工作场景中的视频来提取人类演示中的各种物体操作，是一种很有前景的解决方案。
现有的3D物体操作轨迹预测方法受限于昂贵的3D注释成本，难以处理多样化的操作动作。
现有数据集（如HOT3D）中的人类运动轨迹数据有限，不足以开发能够生成物体操作轨迹的模型。</p>
<p>数据来源：
训练数据：
论文使用Ego-Exo4D数据集 [30] 来构建大规模训练数据集，包含6DoF物体操作轨迹以及图像、深度图和动作描述。这个过程是自动化的，不依赖于预设的相机外部参数。</p>
<p>评估数据：
论文使用现有的HOT3D [3] 数据集进行评估。HOT3D是一个用于3D手和物体跟踪的自我中心视角数据集，通过光学标记和多红外OptiTrack相机记录，提供了精确的手和物体6DoF信息。</p>
<p>算力要求：
论文提到模型训练了30个epoch，批处理大小为8。优化器使用了AdamW，基础学习率为2e-5（对于LLMs）和2e-4（对于其他参数）。
为了处理旋转连续性问题，每个角度（roll, pitch, yaw）都用[cos(θ), sin(θ)]表示。
虽然论文没有直接给出具体的算力消耗数字（例如所需的GPU数量或训练时间），但从训练epoch和批处理大小来看，以及使用了BLIP-2、VILA、PointLLM和MiniGPT-3D等大型视觉语言模型作为骨干网络，可以推断出这需要相当大的计算资源。这些模型本身就比较大，并且训练涉及处理大规模视频数据。</p>
<p>公开代码：
论文在摘要中明确提到了数据集和代码的链接：https://biscue5.github.io/egoscaler-project-page/。</p>
<p>模拟环境和现实环境平台：
论文没有提到使用了特定的模拟环境平台。
在现实环境方面，论文主要使用了自我中心视频数据。HOT3D [3] 数据集是通过Project Aria眼镜 [21] 和Quest 3 [65] 记录的，这些是现实世界中的可穿戴设备，用于捕捉自我中心视角下的活动。这表明他们的研究是基于真实世界中的人类活动数据。</p>
<hr>
<h2 id="universal-actions-for-enhanced-embodied-foundation-models">Universal Actions for Enhanced Embodied Foundation Models<a hidden class="anchor" aria-hidden="true" href="#universal-actions-for-enhanced-embodied-foundation-models">#</a></h2>
<p>CVPR 2025 From 清华</p>
<p>这篇文章介绍的 UniAct 框架，目标是解决具身基础模型在处理异构动作数据时面临的挑战，并构建一个能够在通用动作空间中操作的框架。</p>
<p>主要目标：</p>
<p>构建通用动作空间：
学习一种能够捕捉不同机器人通用原子行为的动作空间，从而消除机器人之间因物理形态和控制接口差异造成的动作异构性。</p>
<p>实现跨形态泛化：
使得具身基础模型能够有效利用跨领域数据，并在不同的机器人形态之间实现更好的泛化控制和适应能力。</p>
<p>提高模型效率：
训练一个相对较小（0.5B 参数）但性能优于更大（14倍）现有模型的具身基础模型，证明通用动作的优势。</p>
<p>动机：
数据异构性挑战：
现有的大型基础模型在自然语言处理和计算机视觉领域取得了巨大成功，主要得益于海量的、多样化的互联网数据。然而，将同样的方法应用于具身智能体时，面临一个显著的挑战：不同机器人收集的动作数据存在显著的异构性，因为它们有不同的物理形态和控制接口。这种异构性严重阻碍了跨领域数据共享和通用具身基础模型的发展。</p>
<p>现有解决方案的局限性：
大多数现有方法要么强制性地将不同动作空间视为等效，采用统一的离散化或归一化技术，但这可能导致动作编码的物理意义冲突；要么试图设计一个适用于各种机器人系统的物理可解释动作空间，但这需要大量人工工程，且未能充分利用不同具身动作空间之间的内在联系。</p>
<p>对通用代理的需求： 开发能够处理跨任务、跨环境和跨形态泛化的通用具身基础模型，是构建通用具身智能体的一个有前景的途径。
数据来源：</p>
<p>UniAct-0.5B 模型在训练时整合了来自多个开源机器人数据集的示范数据。这些数据集包括：</p>
<p><a href="https://arxiv.org/abs/2310.08864">Open-X Embodiment (OXE)</a>
<a href="https://arxiv.org/abs/2410.01529">Libero</a>
<a href="https://arxiv.org/abs/2403.12945">Droid</a>
这些数据被标准化，以包含第三人称视角观察和语言指令，同时保留了动作的异构性。总共使用了来自 28 种不同机器人形态的约 100 万个示范数据进行训练。</p>
<p>算力要求：
UniAct-0.5B 的训练是在 64 块 A100 GPU 上进行的，并使用了 <a href="https://dl.acm.org/doi/10.1145/3505830.3506169">DeepSpeed</a> 进行优化，持续了 10 天。</p>
<p>公开代码：
是的，文章中提到了项目的项目页面，通常这意味着代码是公开的： <a href="https://2toinf.github.io/UniAct/">项目页面</a></p>
<hr>
<h2 id="solami-social-visionlanguageaction-modeling-for-immersive-interaction-with-3d-autonomous-characters">SOLAMI: Social Vision‑Language‑Action Modeling for Immersive Interaction with 3D Autonomous Characters<a hidden class="anchor" aria-hidden="true" href="#solami-social-visionlanguageaction-modeling-for-immersive-interaction-with-3d-autonomous-characters">#</a></h2>
<p>CVPR 2025</p>
<p>SOLAMI 这篇文章旨在介绍一个端到端的社交视觉-语言-动作（VLA）建模框架，用于与 3D 自动角色进行沉浸式交互。</p>
<p>文章的目标是构建能够感知、理解并与人类互动的 3D 自动角色，使其具备类似于人类的社交智能，通过多模态响应（语音和动作）驱动角色进行社交互动。</p>
<p>这项研究的动机在于，目前的字符代理在与用户交互时，主要限于文本或语音交互，缺乏更丰富的模态。在社交互动中，沉浸感越深，人类体验越好。因此，研究人员希望构建具有更丰富模态的 3D 自动角色。此外，多模态交互数据非常稀缺，难以获取，这也促使他们开发了数据合成方法。</p>
<p>数据主要来源于以下几个方面：</p>
<p>交互式多模态数据（SynMSI）：
这是一个合成的多模态社交互动数据集，通过自动化流程生成，利用了现有的文本-动作数据集、基于文本的角色扮演模型和语音合成方法。SynMSI 数据集包含 6.3K 多轮多模态对话项。
运动数据：为了进行预训练阶段的运动与文本对齐，以及生成多模态数据用于指令微调，研究人员收集了包含丰富社交动作的现有数据集，例如 HumanML3D (24K 运动-文本对)、Inter-X (20K 运动-文本对和 10K 两人运动对)，以及 DLP-MoCap (2K 运动-文本对)。</p>
<p>语音数据：
用于预训练阶段的语音-文本对齐，使用了 CommonVoice (150K 语音-文本对)、AnyInstruct (200K 语音-文本对和 100K 语音到语音项)，以及通过文本到语音方法（Azure TTS 和 XTTS_v2）生成的合成语音数据 (60K 语音-文本对)。</p>
<p>算力要求：
在预训练阶段，SOLAMI 使用了 32 块 V100 GPU 来训练模型，批处理大小为 256。
在指令微调阶段，SOLAMI 使用了 16 块 V100 GPU，批处理大小为 48。推理时，所有模型都部署在 2 块 H800 GPU 上，并采用 vLLM 框架和异步机制来提高性能并保持公平性。</p>
<p>代码：
<a href="https://solami-ai.github.io/">项目的 GitHub 链接</a></p>
<p>模拟环境在文章中没有明确提及，但实验中提到了使用 VR 界面进行用户研究，其中用户可以与各种 3D 角色进行沉浸式交互。</p>
<p>现实环境指的是 VR 界面，研究人员开发了一个基于 Oculus Quest 3 前端和后端服务的 VR 界面。前端实现用户与 3D 自动角色的沉浸式交互，后端由 2 块 H800 GPU 提供算力支持。在实际使用中，VR 头显捕获用户的语音和身体动作，并将其发送到后端计算节点。</p>
<hr>
<h2 id="a-data-centric-revisit-of-pre-trained-vision-models-for-robot-learning">A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning<a hidden class="anchor" aria-hidden="true" href="#a-data-centric-revisit-of-pre-trained-vision-models-for-robot-learning">#</a></h2>
<p>CVPR 2025 from HKU</p>
<p>这篇文章主要研究了预训练视觉模型（PVMs）在机器人学习任务中的应用，特别是视觉运动控制和感知任务。文章的目标是找出最优的预训练方法和数据来源，以提高PVMs在机器人学习任务中的表现。</p>
<p>动机
文章的动机源于当前PVMs在机器人学习任务中的应用存在一些问题和局限性。尽管PVMs在传统视觉任务中表现出色，但它们在机器人学习任务中的最优配置仍不清楚。文章通过系统性的评估发现，虽然某些PVMs（如DINO和iBOT）在视觉运动控制和感知任务中表现出色，但它们在非对象中心（NOC）数据上的表现会显著下降。这种下降与它们学习对象中心表示的能力减弱密切相关。</p>
<p>数据来源
文章中使用的数据集包括：</p>
<p>对象中心数据集：ImageNet
场景中心数据集：COCO
网络爬取数据：CC12M
以自我为中心的数据：Ego4D
这些数据集被用来评估PVMs在不同类型的数据上的表现。</p>
<p>算力要求
由于PVMs的训练和评估需要大量的计算资源，文章中提到使用了8个A100 GPU进行训练。对于某些任务，如导航任务，需要大约400M到500M步的训练和512到320个并行环境，这对计算资源提出了极高的要求。</p>
<p>代码公开情况
文章中提到，他们的代码和模型是公开可用的，<a href="https://github.com/CVMI-Lab/SlotMIM">链接</a></p>
<p>模拟环境
文章中使用了多个模拟环境平台进行评估，包括：</p>
<p>Franka Kitchen
Meta-World
Habitat（用于导航任务，包含HM3D和Gibson环境）
现实环境
虽然文章主要关注模拟环境中的评估，但提到PVMs在现实环境中的应用潜力。现实环境中的平台并未在文章中具体提及，但提到了多个现实环境中的机器人学习任务和应用。</p>
<p>总结
总的来说，这篇文章通过系统性的评估和实验，提出了SlotMIM方法，以有效地从NOC数据中学习对象中心的表示，并在多个任务中取得了优于现有方法的性能。文章的研究为PVMs在机器人学习任务中的应用提供了新的见解和方法。</p>
<hr>
<h2 id="omnimanip-towards-general-robotic-manipulation-via-object-centric-interaction-primitives-as-spatial-constraints">OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints<a hidden class="anchor" aria-hidden="true" href="#omnimanip-towards-general-robotic-manipulation-via-object-centric-interaction-primitives-as-spatial-constraints">#</a></h2>
<p>这篇文章介绍了一个名为 OmniManip 的新方法，旨在实现通用机器人操作。</p>
<p>这篇文章要做什么，目标是什么？
这篇文章提出了 OmniManip，这是一种开放词汇的操控方法，旨在弥合视觉-语言模型 (VLM) 的高层推理能力与低层精确操控之间的差距。其核心目标是开发一个通用的机器人操控系统，能够通过物体中心交互基元作为空间约束，在非结构化环境中执行各种操控任务，并具有强大的零样本泛化能力。</p>
<p>动机是什么？
开发能够在非结构化环境中进行通用机器人操控的系统是一个重大挑战。虽然 VLM 在高层常识推理方面表现出色，但它们缺乏精确操控任务所需的精细 3D 空间理解能力。现有的解决方案，例如在机器人数据集上微调 VLM，面临数据收集成本高昂和泛化性差的问题。通过将机器人动作抽象为交互基元并利用 VLM 定义空间约束，是解决这些挑战的动机。</p>
<p>数据是从哪里来的？
文章中提到，为了评估 OmniManip 在真实世界场景中的操控能力，他们设计了 12 个任务来评估模型的操控能力，这些任务涵盖了各种对象和复杂环境。虽然没有明确说明具体的数据集来源，但实验部分提到了通过 OmniManip 自动生成演示数据，并收集了每项任务 150 条轨迹用于训练行为克隆策略。</p>
<p>算力要求多少？
文章中没有直接给出具体的算力要求，但提及了多个 VLM 调用会带来计算挑战，即使进行了并行处理也是如此。这暗示了该系统可能需要较高的计算资源。</p>
<p>公开代码吗？
文章中没有明确提到代码是否公开。</p>
<p>模拟环境用的是什么平台？
文章中没有提到具体使用了哪个模拟环境平台。</p>
<p>现实环境用的是什么平台？
现实环境实验平台是基于 Franka Emika Panda 机器人臂搭建的，并配备了 UMI 机械手。感知方面，使用了两台 Intel RealSense D415 深度相机，一台安装在机械手上提供第一人称视角，另一台则放置在机器人对面提供第三人称视角。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://tzj2006.github.io/">TzJ&#39;s Net</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <span>
        · 本站访客数：<span id="busuanzi_value_site_uv"></span>
        · 总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
